<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8">
  <title>SuNT&#39;s Blog | AI in Practical</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This is meta description">
  <meta name="author" content="SuNT">
  <meta name="generator" content="Hugo 0.80.0" />

  <!-- plugins -->
  
  <link rel="stylesheet" href="https://tiensu.github.io/plugins/bootstrap/bootstrap.min.css ">
  
  <link rel="stylesheet" href="https://tiensu.github.io/plugins/slick/slick.css ">
  
  <link rel="stylesheet" href="https://tiensu.github.io/plugins/themify-icons/themify-icons.css ">
  
  <link rel="stylesheet" href="https://tiensu.github.io/plugins/venobox/venobox.css ">
  
  <link rel="stylesheet" href="https://tiensu.github.io/css/override.css">
  <!-- Main Stylesheet -->
  
  <link rel="stylesheet" href="https://tiensu.github.io/scss/style.min.css" media="screen">

  <!--Favicon-->
  <link rel="shortcut icon" href="https://tiensu.github.io/images/favicon.png " type="image/x-icon">
  <link rel="icon" href="https://tiensu.github.io/images/favicon.png " type="image/x-icon">

  <!-- google analitycs -->
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'Your ID', 'auto');
    ga('send', 'pageview');
  </script>

</head><body>
<!-- preloader start -->
<div class="preloader">
  
</div>
<!-- preloader end -->
<!-- navigation -->
<header class="navigation">
  <div class="container">
    
    <nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0">
      <a class="navbar-brand mobile-view" href="https://tiensu.github.io/"><img class="img-fluid"
          src="https://tiensu.github.io/images/logo3.jpg" alt="SuNT&#39;s Blog | AI in Practical"></a>
      <button class="navbar-toggler border-0" type="button" data-toggle="collapse" data-target="#navigation">
        <i class="ti-menu h3"></i>
      </button>

      <div class="collapse navbar-collapse text-center" id="navigation">
        <div class="desktop-view">
          <ul class="navbar-nav mr-auto">
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.facebook.com/tiensunguyen2103"><i class="ti-facebook"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.linkedin.com/in/su-nguyen-tien-aws%C2%AE-5ba74ba6/"><i class="ti-linkedin"></i></a>
            </li>
            
          </ul>
        </div>

        <a class="navbar-brand mx-auto desktop-view" href="https://tiensu.github.io/"><img class="img-fluid"
            src="https://tiensu.github.io/images/logo3.jpg" alt="SuNT&#39;s Blog | AI in Practical"></a>

        <ul class="navbar-nav">
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://tiensu.github.io/about">About</a>
          </li>
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://tiensu.github.io/blog">Post</a>
          </li>
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://tiensu.github.io/contact">Contact</a>
          </li>
          
          
        </ul>

        
        <!-- search -->
        <div class="search pl-lg-4">
          <button id="searchOpen" class="search-btn"><i class="ti-search"></i></button>
          <div class="search-wrapper">
            <form action="https://tiensu.github.io//search" class="h-100">
              <input class="search-box px-4" id="search-query" name="s" type="search" placeholder="Type & Hit Enter...">
            </form>
            <button id="searchClose" class="search-close"><i class="ti-close text-dark"></i></button>
          </div>
        </div>
        

        
      </div>
    </nav>
  </div>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</header>
<!-- /navigation -->

<section class="section-sm">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 mx-auto">
        
        <a href="/categories/rnn"
          class="text-primary">R n n</a>
        
        <a href="/categories/lstm"
          class="text-primary">L s t m</a>
        
        <a href="/categories/attention"
          class="text-primary">Attention</a>
        
        <a href="/categories/transformer"
          class="text-primary">Transformer</a>
        
        <h2>Self-Attention và Multi-head Sefl-Attention trong kiến trúc Transformer</h2>
        <div class="mb-3 post-meta">
          <span>By SuNT</span>
          
          <span class="border-bottom border-primary px-2 mx-1"></span>
          <span>20 April 2021</span>
          
        </div>
        
        <img src="https://tiensu.github.io/images/featured-post/self-attention.png" class="img-fluid w-100 mb-4" alt="Self-Attention và Multi-head Sefl-Attention trong kiến trúc Transformer">
        
        <div class="content mb-5">
          <p>Kiến trúc Transformer với xương sống là Self-Attention đã <em>làm mưa làm gió</em> trong cộng đồng NLP trong 1-2 năm gần đây. Nó đạt được độ chính xác rất cao trong hầu hết các bài toán NLP. Trong bài này, hãy cùng nhau tìm hiểu kỹ hơn về cơ chế Self-Attention, làm tiền đề cho việc tìm hiểu kiến trúc Transformer ở bài sau.</p>
<p><strong>1. Self-Attention là gì?</strong></p>
<p>Chúng ta đều biết rằng Word Embedding là vector đại diện cho ngữ nghĩa của một từ trong câu. Những từ mà có nghĩa tương tự nhau thì vector của chúng cũng sẽ gần giống nhau và ngược lại. Tuy nhiên, trong một câu, ý nghĩa của các từ riêng lẻ không đại diện cho cả câu đó. Ví dụ, xét câu: <em>The bank of a river</em>. Hai từ <em>bank</em> và <em>river</em>, nếu tách riêng thì có ý nghĩa hoàn toàn khác nhau, và nếu mang ý nghĩa đó của chúng vào câu thì sai hoàn toàn.</p>
<p>Cơ chế Self-Attention được đề xuất trong bài báo <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> có thể giải quyết tốt vấn đề này. Ý tưởng làm việc của nó là so sánh các từ với nhau đôi một, bao gồm cả chính nó (<em>self</em>) để tìm ra mức độ quan trọng của mỗi từ mà nó nên chú ý tới (<em>thể hiện qua trọng số</em>).</p>
<p><strong>2. Minh họa cách làm việc của Self-Attention</strong></p>
<p>Cơ chế Attention có thể được hiểu như là sự kết hợp giữa một Query và một cặp Key-Value để cho ra một Output. Công thức tính như sau:


<div style="text-align-last:center">
   <p>$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}}) \times V$</p>
</div>

</p>
<p>Trong đó, $\frac{1}{\sqrt(d_k)}$ là hệ số tỷ lệ. $d_k$ là số chiều của Key.</p>
<p><em><strong>2.1 Bước 1 - Chuẩn bị Input</strong></em></p>
<p>Giả sử, chúng ta có 3 Inputs (<em>3 từ trong Input Sequence</em>), mỗi  Input là một vector 4 chiều như sau:


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_1.png">
</div>

</p>


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_1_1.png">
</div>


<p><em><strong>2.2 Bước 2 - Khởi tạo trọng số (Weight) cho Key, Query và Value</strong></em></p>
<p>Mỗi một Input sẽ được đại diện bởi 3 đại lượng: <strong>key, query</strong> và <strong>value</strong> có số chiều tùy ý. Giả sử, 3 đại lượng này có số chiều là 3. Để tạo ra chúng, ta cần khởi tạo các trọng số cho từng đại lượng. Vì Input có số chiều là 4, (<em>Key, Query, Value</em>) có số chiều là 3 nên các Weights phải có kích thước 4x3. Chúng thường có giá trị nhỏ, được khởi tạo ngẫu nhiên sử dụng một trong số các phân phối Gaussian, Xavier, Kaiming, &hellip;</p>
<ul>
<li>
<p>Weight cho Key:


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_2_1.png">
</div>

</p>
</li>
<li>
<p>Weight cho Query:


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_2_2.png">
</div>

</p>
</li>
<li>
<p>Weight cho Value:


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_2_3.png">
</div>

</p>
</li>
</ul>
<p><em><strong>2.3 Bước 3 - Tính Key, Query, Value</strong></em></p>
<p>Key, Query, Value đạt được bằng cách nhân (<em>dot product</em>) Input với trọng số tương ứng của chúng.</p>
<ul>
<li>Key:


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_3_1.png">
</div>

</li>
</ul>


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_3_1_1.png">
</div>


<ul>
<li>Query:


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_3_2.png">
</div>

</li>
</ul>


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_3_2_1.png">
</div>


<ul>
<li>Value:


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_3_3.png">
</div>

</li>
</ul>


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_3_3_1.png">
</div>


<p>Trong thực tế, một Bias Vector có thể được thêm vào khi tính các giá trị Key, Query và Value.</p>
<p><em><strong>2.4 Bước 4 - Tính Attention Scores</strong></em></p>


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_4_1.png">
</div>


<p>Để tính Attention Scores cho Input1, ta nhân (<em>dot product</em>) Query của nó với tất cả các Keys.


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_4_2.png">
</div>

</p>
<p>Từ bước 4 đến bước 7, mình chỉ tính cho Input1 làm đại diện. Hai Inputs còn lại được tính tương tự.</p>
<p><em><strong>2.5 Bước 5 - Tính Softmax</strong></em>
Để đơn giản, mình bỏ qua hệ số tỉ lệ $\frac{1}{\sqrt(d_k)}$


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_5_1.png">
</div>

</p>
<p>Đưa Attention Scores qua hàm Softmax ta được:


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_5_2.png">
</div>

</p>
<p><em><strong>2.6 Bước 6 - Tính Weighted Values</strong></em></p>


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_6_1.png">
</div>


<p>Softmaxed Attention Score nhân với tất cả các Values ta được Weighted Values.


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_6_6.png">
</div>

</p>
<p><em><strong>2.7 Bước 7 - Tính tổng Weighted Values</strong></em></p>


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_7_1.png">
</div>


<p>Tính tổng Weighted Values theo kiểu <em>element-wise</em> ta được Output1 vector:


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_7_2.png">
</div>

</p>
<p>[2.0, 7.0, 1.5] chính là bộ trọng số thể hiện sự tương quan của Input1 với từng Input (<em>bao gồm chính nó</em>).</p>
<p><em><strong>2.8 Bước 8 - Lặp lại từ bước 4-7 cho Input2 &amp; Input3</strong></em></p>
<p>Chúng ta đã tính xong Output1, lặp lại các bước từ 4-7 đối với Input2 &amp; Input3 ta được Output2 &amp; Output3.


<div style="text-align:center">
   <img style="height:auto" src="/images/post/self_attention_step_8_1.png">
</div>

</p>
<p><strong>Chú ý -</strong> Kích thước của Query và Key phải luôn bằng nhau để có thể thực hiện được <em>dot product</em>, còn kích thước của Value có thể khác. Kich thước của Output sẽ giống với kích thước của Value.</p>
<p><strong>3. Multi-Head Self-Attention</strong></p>
<p>Trong kiến trúc của Transformer, mỗi một Self-Attention module được gọi là một Head. Việc sử dụng nhiều Heads đồng thời gọi là Multi-Head.


<div style="text-align:center">
   <img style="height:auto" src="/images/post/multi-head-self-attention.png">
</div>

</p>
<p>Mỗi Head nhận vào một Input $x$ (<em>Token Embedding và Positional Decoding</em>) và cho ra:


<div style="text-align-last:center">
   <p>$Head_i = Attention_i(x) = softmax(\frac{Q_iK_i^T}{\sqrt(d_k)})V_i$</p>
</div>

</p>
<p>Trong đó, $d_k = d_k$(<em>trong trường hợp một Head</em>) /<em>(số lượng Head)</em>.</p>
<p>Sau khi có được các Output của từng Head, ta sẽ tổng hợp chúng lại thành 1 Output duy nhất.


<div style="text-align-last:center">
   <p>$MultiHead(Q,K,V) = Concat(Head_1, Head_2, ..., Head_h)W^0$</p>
</div>

</p>
<p>$W^0$ là ma trận có chiều rộng bằng với chiều rộng của ma trận Input, mục đích sử dụng của nó là để đưa kích thước của Output về bằng với kích thước của Input.</p>
<p><strong>4. So sánh Attention và Self-Attention</strong></p>
<p>Nếu bạn vẫn còn mơ hồ giữa Attention và Self-Attention thì mình sẽ liệt kê những điểm khác nhau giữa chúng cho bạn.</p>
<ul>
<li>Attention thường sử dụng kết hợp với RNN/LSTM/GRU để cải thiện hiệu năng của mô hình hiện tại. Self-Attention thay thế hoàn toàn RNN/LSTM/GRU.</li>
<li>Attention thường xuất hiện trong kiến trúc có đủ 2 thành phần Encoder và Decoder để truyền thông tin giữa chúng. Ngược lại,Self-Attention thường chỉ áp dụng trong phạm vi một thành phần, hoặc Encoder hoặc Decoder.</li>
<li>Attention chỉ có thể sử dụng 1 lần trong một kiến trúc mô hình, trong khi đó, Self-Attention có thể áp dụng nhiều lần (<em>VD: 18 lần trong Transformer</em>).</li>
<li>Self-Attention, như tên gọi, làm nhiệm vụ mô hình hóa mối quan hệ giữa các từ trong một cùng 1 chuỗi (<em>Query, Key, Value xuất phát từ cùng 1 nguồn</em>), còn Attention thì là 2 chuỗi khác nhau.</li>
<li>Attention có thể kết nối 2 kiểu Input Sequence khác nhau (<em>VD: text &amp; image</em>), Self-Attention thì chỉ làm việc với 1 loại.</li>
<li>Cơ chế Multi-Head thường áp dụng cho Self-Attention. Nhưng về mặt lý thuyết, nó cũng có thể được sử dụng cho Attention.</li>
<li>Tương tự, Query/Key/Value thường sử dụng đối với Self-Attention, nhưng nó cũng có thể được áp dụng cho Attention.</li>
</ul>
<p><strong>5. Kết luận</strong></p>
<p>Trong bài này, chúng ta đã cùng nhau tìm hiểu khá chi tiết về cơ chế Self-Attention cũng như khái niệm Multi-Head Self-Attention.</p>
<p>Ở bài tiếp theo, mình sẽ giới thiệu về mô hình Transformer. Mời các bạn đón đọc.</p>
<p><strong>6. Tham khảo</strong></p>
<ul>
<li><a href="https://blogs.oracle.com/ai-and-datascience/post/multi-head-self-attention-in-nlp">Praphul Singh</a></li>
<li><a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a">Raimi Karim</a></li>
<li><a href="https://datascience.stackexchange.com/questions/49468/whats-the-difference-between-attention-vs-self-attention-what-problems-does-ea">datascience</a></li>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need</a></li>
</ul>

        </div>

        
        
      </div>
    </div>
  </div>
</section>



<footer>
  <div class="container">
    <div class="row justify-content-center">
      <div class="col-12 text-center mb-5">
        <a href="https://tiensu.github.io/"><img src="https://tiensu.github.io/images/logo3.jpg" alt="SuNT&#39;s Blog | AI in Practical" style="height: auto"></a>
      </div>
               
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Contact Me</h6>
        <ul class="list-unstyled">
          
          <li class="mb-3"><a class="text-dark" href="tel:0869644890"><i
                class="ti-mobile mr-3 text-primary"></i>0869644890</a></li>
          
                     
          <li class="mb-3"><i class="ti-location-pin mr-3 text-primary"></i>Hanoi, Vietnam</li>
          
                     
          <li class="mb-3"><a class="text-dark" href="mailto:tiensunguyen2103@gmail.com"><i
                class="ti-email mr-3 text-primary"></i>tiensunguyen2103@gmail.com</a>
          
          </li>
        </ul>
      </div>
      
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Social Contacts</h6>
        <ul class="list-unstyled">
          
          <li class="mb-3"><a class="text-dark" href="https://www.facebook.com/tiensunguyen2103">Facebook</a></li>
          
          <li class="mb-3"><a class="text-dark" href="https://www.linkedin.com/in/su-nguyen-tien-aws%C2%AE-5ba74ba6/">Linkedin</a></li>
          
        </ul>
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Categories</h6>
        <ul class="list-unstyled">
          <li class="mb-3"><a class="text-dark"
              href="/categories/algorithm-optimization">Algorithm Optimization</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/attention">Attention</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/audio-classification">Audio Classification</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/autoencoder">Autoencoder</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/bert">BERT</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/cnn">CNN</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/ctc">CTC</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/data-driff">Data Driff</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/data-imbalance">Data Imbalance</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/data-preparation">Data Preparation</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/data-science">Data Science</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/deep-learning">Deep Learning</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/docker">Docker</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/ebook">Ebook</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/eda">EDA</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/ensemble-learning">Ensemble Learning</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/face-recognition">Face Recognition</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/game">Game</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/human-action-recognition">Human action recognition</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/human-pose">Human pose</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/image-classification">Image Classification</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/kubernetes">Kubernetes</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/lstm">LSTM</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/machine-learning">Machine Learning</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/mlops">MLOps</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/neural-network">Neural Network</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/nlp">NLP</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/object-detection">Object Detection</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/ocr">OCR</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/one-shot-learning">One Shot Learning</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/project-management">Project Management</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/recommender-system">Recommender System</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/rnn">RNN</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/scalability">Scalability</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/siamese-network">Siamese Network</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/speech-recognition">Speech Recognition</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/speech-to-text">Speech To Text</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/text-classification">Text Classification</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/text-detection">Text Detection</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/text-generation">Text Generation</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/text-recognition">Text Recognition</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/time-series">Time Series</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/transformer">Transformer</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/xgboost">XGBoost</a>
          </li>
        </ul>
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Quick Links</h6>
        <ul class="list-unstyled">
          
          <li class="mb-3"><a class="text-dark" href="https://tiensu.github.io/about">About</a></li>
          
          <li class="mb-3"><a class="text-dark" href="https://tiensu.github.io/blog">Post</a></li>
          
          <li class="mb-3"><a class="text-dark" href="https://tiensu.github.io/contact">Contact</a></li>
          
        </ul>
      </div>
      <div class="col-12 border-top py-4 text-center">
        | copyright © 2021 <a href="tiensu.github.io">SuNT</a>. All Rights Reserved |
      </div>
    </div>
  </div>
</footer>

<script>
  var indexURL = "https://tiensu.github.io/index.json"
</script>

<!-- JS Plugins -->

<script src="https://tiensu.github.io/plugins/jQuery/jquery.min.js"></script>

<script src="https://tiensu.github.io/plugins/bootstrap/bootstrap.min.js"></script>

<script src="https://tiensu.github.io/plugins/slick/slick.min.js"></script>

<script src="https://tiensu.github.io/plugins/venobox/venobox.min.js"></script>

<script src="https://tiensu.github.io/plugins/search/fuse.min.js"></script>

<script src="https://tiensu.github.io/plugins/search/mark.js"></script>

<script src="https://tiensu.github.io/plugins/search/search.js"></script>

<!-- Main Script -->

<script src="https://tiensu.github.io/js/script.min.js"></script>




<script src="https://cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js"></script>
<div id="js-cookie-box" class="cookie-box cookie-box-hide">
	This site uses cookies. By continuing to use this website, you agree to their use. <span id="js-cookie-button" class="btn btn-sm btn-primary ml-2">I Accept</span>
</div>
<script>
	(function ($) {
		const cookieBox = document.getElementById('js-cookie-box');
		const cookieButton = document.getElementById('js-cookie-button');
		if (!Cookies.get('cookie-box')) {
			cookieBox.classList.remove('cookie-box-hide');
			cookieButton.onclick = function () {
				Cookies.set('cookie-box', true, {
					expires:  2 
				});
				cookieBox.classList.add('cookie-box-hide');
			};
		}
	})(jQuery);
</script>


<style>
.cookie-box {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  text-align: center;
  z-index: 9999;
  padding: 1rem 2rem;
  background: rgb(71, 71, 71);
  transition: all .75s cubic-bezier(.19, 1, .22, 1);
  color: #fdfdfd;
}

.cookie-box-hide {
  display: none;
}
</style>
</body>
</html>