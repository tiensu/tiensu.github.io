[{"categories":["Data Science","EDA"],"contents":"Nhắc đến cụm từ JAV, mình dám chắc phải đến 99% thanh niên Việt Nam đều biết nó là gì? :D\nTuy vậy, để được gọi là fan của JAV thì có lẽ không có nhiều người. Bởi vì để được coi là fan thì bạn phải am hiểu sâu sắc về JAV, từ thông tin diễn viên, nhà sản xuất, ngày phát hành, \u0026hellip; chứ không phải là chỉ chăm chú xem không bỏ sót một tập phim nào đâu nhé!\nBài hôm nay, mình sẽ giúp các bạn bổ sung thêm một số kiến thức còn thiếu nếu bạn là người đang mong muốn được trở thành fan của JAV, thông qua việc phân tích một số thông tin về các diễn viên JAV.\n1. Sự phổ biến của JAV trên thế giới Đầu tiên, chúng ta sẽ cùng nhau kiểm chứng xem JAV phổ biến trên thế giới như thế nào (tính từ tháng 09/2020 đến tháng 09/2021)? Có bao nhiêu nước/khu vực quan tâm đến JAV và đâu là nơi mà JAV được quan tâm nhiều nhất?\nVào Google Trends và gõ từ khóa JAV, kết quả:\n JAV hiện diện ở 150 nước/khu vực và không ngạc nhiên lắm khi Việt Nam của chúng ta đứng đầu danh sách xét về mức độ quan tâm đến JAV. Tiếp sau đó là người bạn Đông Nam Á (Myanmar), các anh em Đông Dương (Campuchia, Lào). Nhà giàu Đông Á (Hàn Quốc) cũng nằm trong top 5. Điều thú vị là Myanmar và Indonesia, một nước theo Phật giáo và một nước theo Hồi giáo, đều rất cấm kị những sản phẩm tình dục, khiêu dâm, nhưng JAV lại rất phổ biến ở nước này. Thậm chí là với từ khóa Yui Hatano thì top 1 chính là Myanmar. Riêng Nhật Bản, quê hương của JAV, thì lại không thấy xuất hiện trong top 10 danh sách, có thể là do người dân Nhật Bản thường tìm kiếm bằng chữ Hiragana thay vì dùng chữ Latin như đa số các quốc gia và vùng lãnh thổ khác.\nNếu muốn khám phá chi tiết hơn, bạn có thể click chuột vào Việt Nam, sẽ thu được kết quả:\n Xét trong phạm vi lãnh thổ Việt Nam thì các anh em Yên Bái có sở thích mạnh mẽ nhất với JAV. Tiếp theo là Điện Biên, Thanh Hóa, Thái Bình, Hòa Bình trong top 5. Hà Nội và Hồ Chí Minh, hai thành phố lớn nhất nước, tốc độ Internet nhanh nhất nước nhưng lại có vẻ thờ ơ với món này. Hai vị trí tương ứng của 2 thành phố đó trên bảng xếp hạng lần lượt là 53 và 60. Có lẽ các thanh niên ở 2 thành phố này còn có nhiều thú vui khác ngoài JAV. :D\n2. Thu thập dữ liệu Thông tin về các diễn viên JAV được thu thập từ 2 nguồn, tại https://japanpornjav.com và tại https://www.kaggle.com/twopothead/japanese-pornstars-and-adult-videos.\nĐối với nguồn thứ nhất thì mình phải viết code python, sử dụng thư viện Beautifulsoup4 để lấy về, còn nguồn thứ 2 thì chỉ đơn giản là download file csv. Sau đó, qua một số bước tiền xử lý, mình gộp kết quả từ 2 nguồn đó lại thành một dữ liệu chung duy nhất, ghi thành file jp_actress_info.csv, với các thông tin như sau:\n   STT Tên Features Ý Nghĩa     1 Name Tên diễn viên trong tiếng Nhật   2 Image_URL Đường dẫn đến ảnh đại diện của diễn viên   3 Birthday Ngày sinh của diễn viên   4 Blood_Type Nhóm máu của diễn viên   5 Breast_Size Kích thước vòng 1 của diễn viên   6 Waist_Size Kích thước vòng 2 của diễn viên   7 Height Chiều cao của diễn viên   8 Number_HD_Video_Online Số lượng HD video trên trang https://japanpornjav.com của diễn viên   9 Cup_Size Kích thước áo ngực của diễn viên   10 Hips Kích thước vòng 3 của diễn viên   11 Home_Town Quê hương của diễn viên   12 Hobby Sở thích của diễn viên    Có tất cả 1220 diễn viên JAV trong bộ dữ liệu này. Ví dụ thông tin của một vài diễn viên:\n Theo quan sát thống kê thì có khá nhiều thông tin bị thiếu trong mỗi feature:\nInt64Index: 1220 entries, 0 to 1219 Data columns (total 12 columns): # Column Non-Null Count Dtype  --- ------ -------------- ----- 0 Name 1220 non-null object 1 Image_URL 1220 non-null object 2 Birthday 1143 non-null object 3 Blood_Type 608 non-null object 4 Breast_Size 1220 non-null object 5 Waist_Size 1220 non-null object 6 Height 1220 non-null object 7 Number_HD_Video_Online 1220 non-null int64 8 Cup_Size 342 non-null object 9 Hips 406 non-null object 10 Home_Town 293 non-null object 11 Hobby 343 non-null object Để cho việc phân tích được sát với thực tế nhất thì mình sẽ chỉ sử dụng những features không bị thiếu.\n3. Tiến hành EDA 3.1 Phân tích tuổi của các diễn viên Từ thông tin về ngày sinh, sau khi loại bỏ đi những diễn viên mà thiếu hoặc sai định dạng thông tin này, chúng ta sẽ tính toán tuổi của từng diễn viên tính đến thời điểm hiện tại.\ndf_age = df_ja[[\u0026#39;Name\u0026#39;, \u0026#39;Image_URL\u0026#39;, \u0026#39;Birthday\u0026#39;]] df_age[\u0026#39;Birthday\u0026#39;] = pd.to_datetime(df_age[\u0026#39;Birthday\u0026#39;], errors=\u0026#39;coerce\u0026#39;) df_age = df_age.dropna(subset=[\u0026#39;Birthday\u0026#39;]) df_age[\u0026#39;Age\u0026#39;] = df_age[\u0026#39;Birthday\u0026#39;].apply(lambda x: pd.to_datetime(\u0026#39;today\u0026#39;).year - x.year) print(df_age.shape) df_age.head()   Còn 1141 diễn viên có đủ thông tin ngày sinh và độ tuổi.\nXem xét phân bố độ tuổi của các diễn viên:\ndist_data = df_age[df_age[\u0026#39;Age\u0026#39;].notnull()] fig = ff.create_distplot([dist_data[\u0026#39;Age\u0026#39;]], group_labels = [\u0026#39;Age of JA\u0026#39;]) fig.show()   # plot violin chart for Age feature fig = go.Figure(data=go.Violin( y=df_age[\u0026#39;Age\u0026#39;], box_visible=True, line_color=\u0026#39;red\u0026#39;, meanline_visible=True, opacity=0.5, x0=\u0026#39;Age\u0026#39;, points=\u0026#39;all\u0026#39; ) ) fig.update_layout( height=600, margin=dict(t=30, b=30, l=30, r=30) ) fig.show()   Nhận xét:\n Phần lớn các diễn viên JAV có độ tuổi trong khoảng từ 24-31 tuổi. Độ tuổi trung bình là 27.8 Số tuổi nhỏ nhất và lớn nhất của diễn viên JAV lần lượt là 19 và 64 tuổi. 19 tuổi tức là vừa mới tốt nghiệp cấp 3 đã debut luôn. Còn 64 tuổi đời thì ít nhất cũng phải 40 năm tuổi nghề. Thật đáng nể !!!  Có 6 diễn viên ở độ tuổi 19: あまつか亜夢, 朝田ひまり, 北城希, 小野六花, 和知すばる, và 天ノうた.\ndf_age_min = df_age[df_age[\u0026#39;Age\u0026#39;] == 19] ims = [Image.open(requests.get(url, stream=True).raw) for url in df_age_min[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200)    Còn diễn viên lớn tuổi nhất là 真梨邑ケイ:\ndf_age_max = df_age[df_age[\u0026#39;Age\u0026#39;] == 64] ims = [Image.open(requests.get(url, stream=True).raw) for url in df_age_max[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200)   Từ ngày sinh, chúng ta còn có thể suy ra cung hoàng đạo của mỗi diễn viên:\ndf_age[\u0026#39;Zodiac_Sign\u0026#39;] = df_age[\u0026#39;Birthday\u0026#39;].apply(lambda x: zodiac_sign(x.day, x.month)) zodiac_signs = \u0026#39; \u0026#39;.join(df_age[\u0026#39;Zodiac_Sign\u0026#39;]) font_path = \u0026#39;font/NotoSerifCJKjp-Light.otf\u0026#39; mask = np.array(Image.open(\u0026#39;alice_mask.png\u0026#39;)) ... # create a wordcloud  wordcloud = WordCloud(background_color=\u0026#39;white\u0026#39;, mask=mask, font_path=font_path, max_words=2000, contour_width=1, contour_color=\u0026#39;steelblue\u0026#39;, stopwords=stopwords, max_font_size=500 ).generate(zodiac_signs) wordcloud.to_file(\u0026#34;zodiac.png\u0026#34;)   Nhân Mã là chòm sao phổ biến nhất trong các diễn viên được phân tích. Tiếp đến là Cự Giải, Bạch Dương, Song Ngư, \u0026hellip;.\n3.2 Phân tích kích thước 3 vòng của các diễn viên Chúng ta sẽ tách riêng thông tin về kích thước 3 vòng từ bộ dữ liệu ban đầu, và loại bỏ đi những thông tin thiếu, sai rồi tiến hành phân tích:\ndf_bwh = df_ja[[\u0026#39;Name\u0026#39;, \u0026#39;Image_URL\u0026#39;, \u0026#39;Breast_Size\u0026#39;, \u0026#39;Waist_Size\u0026#39;, \u0026#39;Hips\u0026#39;]] df_bwh.drop(df_bwh.index[df_bwh[\u0026#34;Breast_Size\u0026#34;].apply(lambda x: not (x.strip().isnumeric()))], axis=0, inplace=True) df_bwh.drop(df_bwh.index[df_bwh[\u0026#34;Waist_Size\u0026#34;].apply(lambda x: not (x.strip().isnumeric()))], axis=0, inplace=True) df_bwh[\u0026#39;Breast_Size\u0026#39;] = df_bwh[\u0026#39;Breast_Size\u0026#39;].astype(np.float32) df_bwh[\u0026#39;Waist_Size\u0026#39;] = df_bwh[\u0026#39;Waist_Size\u0026#39;].astype(np.float32) # remove outlier df_bwh = df_bwh[df_bwh[\u0026#39;Waist_Size\u0026#39;] \u0026lt; 80] df_bwh = df_bwh[df_bwh[\u0026#39;Breast_Size\u0026#39;] \u0026gt; 40]   Có 1157 diễn viên JAV cho chúng ta phân tích. Riêng thông tin về vòng 3, nếu loại bỏ hết những giá trị NaN thì số lượng diễn viên còn lại khá khiêm tốn (406) nên mình sẽ để nguyên.\nfig = go.Figure() for fe in [\u0026#39;Breast_Size\u0026#39;, \u0026#39;Waist_Size\u0026#39;, \u0026#39;Hips\u0026#39;]: fig.add_trace(go.Violin(x=[fe]*df_bwh.shape[0], y=df_bwh[fe], text=\u0026#39;Dist plot for \u0026#39; + fe.upper(), name=fe.upper(), box_visible=True, meanline_visible=True, points=\u0026#39;all\u0026#39;)) fig.update_layout( height=800, margin=dict(t=30, b=30, l=30, r=30) ) fig.show()   Nhận xét:\n Vòng 1, 3 gần như luôn luôn lớn hơn vòng 2 (điều này là tất nhiên rồi). Kích thước lớn nhất của vòng 1 là 115cm, nhỏ nhất là 72cm, trung bình là 86,74cm, và phân bổ chủ yếu trong khoảng từ 83-89cm. Kích thước lớn nhất của vòng 2 là 70cm, nhỏ nhất là 49cm, trung bình là 58,2cm, và phân bố chủ yếu trong khoảng từ 57-59cm. Kích thước lớn nhất của vòng 3 là 97cm, nhỏ nhất là 75cm, trung bình là 85,6cm, và phân bố chủ yếu trong khoảng từ 83-88cm. Vùng phân bố kích thước vòng 1, 3 lớn hơn vòng 2. Hầu hết số đo 3 vòng của các JAV idols cũng chưa đạt được đến chuẩn 90 - 60 - 90.  Thử xem có diễn viên nào đạt chuẩn số đo 3 vòng không?\ndf_bwh_standard = df_bwh[(df_bwh[\u0026#39;Breast_Size\u0026#39;] == 90) \u0026amp; (df_bwh[\u0026#39;Waist_Size\u0026#39;] == 60) \u0026amp; (df_bwh[\u0026#39;Hips\u0026#39;] == 90)] print(df_bwh_standard.Name) ims = [Image.open(requests.get(url, stream=True).raw) for url in df_bwh_standard[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200) Chỉ có 1 người duy nhất, đó là 白石茉莉奈.\n Còn đối với bộ số trung bình 3 vòng thì sao, liệu có bao nhiêu diễn viên?\ndf_bwh_mean = df_bwh[(df_bwh[\u0026#39;Breast_Size\u0026#39;] == 86) \u0026amp; (df_bwh[\u0026#39;Waist_Size\u0026#39;] == 58) \u0026amp; (df_bwh[\u0026#39;Hips\u0026#39;] == 86)] print(df_bwh_mean.Name) ims = [Image.open(requests.get(url, stream=True).raw) for url in df_bwh_mean[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200) Có 5 người: 吉沢明歩, 藤井シェリー, 水樹りさ, 初美沙希, 向井しほ:\n   Tiếp tục xem cô nàng nào có vòng 1 lớn nhất?\ndf_breast_max = df_bwh[df_bwh[\u0026#39;Breast_Size\u0026#39;] == 115] print(df_breast_max.Name) ims = [Image.open(requests.get(url, stream=True).raw) for url in df_breast_max[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200) Đó là 夢野まりあ, nhìn hình này thì quả là to thật, :D\n Idol nào có vòng 2 nhỏ nhất?\ndf_waist_min = df_bwh[df_bwh[\u0026#39;Waist_Size\u0026#39;] == 49] print(df_waist_min.Name) ims = [Image.open(requests.get(url, stream=True).raw) for url in df_waist_min[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200) Câu trả lời là もりの小鳥:\n Cuối cùng, diễn viên có vòng 3 lớn nhất là \u0026hellip;\ndf_hips_max = df_bwh[df_bwh[\u0026#39;Hips\u0026#39;] == 97] print(df_hips_max.Name) ims = [Image.open(requests.get(url, stream=True).raw) for url in df_hips_max[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200) 小口田桂子:\n 3.3 Phân tích chiều cao của các diễn viên Ở VN, để trở thành người mẫu, hot girl, hay idol của giới trẻ thì chiều cao cũng là một yếu tố. Thử xem đối với các diễn viên JAV của chúng ta thì như nào.\ndf_height = df_ja[[\u0026#39;Name\u0026#39;, \u0026#39;Image_URL\u0026#39;, \u0026#39;Height\u0026#39;]] df_height.drop(df_height.index[df_height[\u0026#34;Height\u0026#34;].apply(lambda x: not (x.strip().isnumeric()))], axis=0, inplace=True) df_height[\u0026#39;Height\u0026#39;] = df_height[\u0026#39;Height\u0026#39;].fillna(-1).astype(np.float32).replace(-1, np.nan) print(df_height.shape) df_height.head()   Có 1163 diễn viên phục vụ cho việc phân tích này. Chiều cao của các diễn viên thể hiện qua đồ thị distplot và violin như sau:\ndist_data = np.array(df_height[\u0026#39;Height\u0026#39;]) dist_data = [x for x in dist_data if np.isnan(x) == False] fig = ff.create_distplot([dist_data], group_labels = [\u0026#39;distplot\u0026#39;]) fig.update_layout(margin=dict(t=30, b=30, l=30, r=30)) fig.show()   # plot violin chart for Height feature fig = go.Figure(data=go.Violin( y=df_height[\u0026#39;Height\u0026#39;], box_visible=True, line_color=\u0026#39;green\u0026#39;, meanline_visible=True, opacity=0.5, x0=\u0026#39;Height\u0026#39;, points=\u0026#39;all\u0026#39; ) ) fig.update_layout( height=600, margin=dict(t=30, b=30, l=30, r=30) ) fig.show()   Nhận xét:\n Chiều cao trung bình của các diễn viên JAV là 158,45cm. Cao nhất là 178cm, thấp nhất là 142cm. Đa số diễn viên có chiều cao trong khoảng 155-162cm.  Với chiều cao như thế này, ta thấy nó khá tương đồng với chiều cao trung bình của người dân Nhật Bản. Từ đó có thể kết luận rằng chiều cao không phải là yếu tố cần thiết, quan trọng để trở thành một diễn viên JAV.\nHai diễn viên có chiều cao lớn nhất (178cm) là 佐藤エル và 司ミコト:\ndf_height_max = df_height[df_height[\u0026#39;Height\u0026#39;] == 178] print(df_height_max.Name) ims = [Image.open(requests.get(url, stream=True).raw) for url in df_height_max[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200)   Còn diễn viên có chiều cao thấp nhất (142cm) là 工藤ララ:\ndf_height_min = df_height[df_height[\u0026#39;Height\u0026#39;] == 142] print(df_height_min.Name) ims = [Image.open(requests.get(url, stream=True).raw) for url in df_height_min[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200)   3.4 Phân tích kích thước áo ngực của các diễn viên Kích thước áo ngực thông thường sẽ tỉ lệ thuận với kích thước vòng 1 của các chị em. Tuy nhiên, thực tế không hoàn toàn vậy, bởi vì một số chị em có sở thích độn ngực đó mà, :D\ndf_cup = df_ja[[\u0026#39;Name\u0026#39;, \u0026#39;Image_URL\u0026#39;, \u0026#39;Cup_Size\u0026#39;]] df_cup.drop(df_cup.index[df_cup[\u0026#34;Cup_Size\u0026#34;].apply(lambda x: (pd.isna(x)))], axis=0, inplace=True) print(df_cup.shape) df_cup.head()   Thông tin về Cup_size không có nhiều, chỉ có 342 diễn viên có thông tin này trong bộ dữ liệu.\nfig = px.bar( df_cup_gr, x=df_cup_gr.index, y=\u0026#39;Count\u0026#39;, color=\u0026#39;Count\u0026#39;, text=\u0026#39;Count\u0026#39; ) fig.show()   Mình thì không rành về khoản kích thước áo ngực của hội chị e lắm, nhưng hình như là tăng dần theo thứ tự A, B, C, D, \u0026hellip; Nhìn vào đồ thị thì ta thấy đa số các JAV idols của chúng ta có Cup_Size ở mức trung bình trở lên, tức là mức D, E, F, \u0026hellip; Từ đó suy ra rằng, mặc dù kích thước vòng 1 có thể không to lắm, nhưng đa số các diễn viên đều thích sử dụng các áo ngực có kích thước lớn hơn, có lẽ là để trông hấp dẫn cánh đàn ông hơn, :D\nきみの奈津 và きみの奈津 là hai diễn viên mặc áo ngực lớn nhất và nhỏ nhất, tương ứng.\ndf_cup_size_max = df_cup[df_cup[\u0026#39;Cup_Size\u0026#39;] == \u0026#39;L\u0026#39;] print(df_cup_size_max.Name) ims = [Image.open(requests.get(url, stream=True).raw) for url in df_cup_size_max[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200)   df_cup_size_min = df_cup[df_cup[\u0026#39;Cup_Size\u0026#39;] == \u0026#39;A\u0026#39;] print(df_cup_size_min.Name) ims = [Image.open(requests.get(url, stream=True).raw) for url in df_cup_size_min[\u0026#39;Image_URL\u0026#39;]] ipyplot.plot_images(ims, max_images=20, img_width=200)   3.5 Phân tích nhóm máu của các diễn viên Liệu rằng nhóm máu có liên quan đến nghề nghiệp không nhỉ?\ndf_blood = df_ja[[\u0026#39;Name\u0026#39;, \u0026#39;Image_URL\u0026#39;, \u0026#39;Blood_Type\u0026#39;]] df_blood.drop(df_blood.index[df_blood[\u0026#34;Blood_Type\u0026#34;].apply(lambda x: x not in [\u0026#39;O\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;AB\u0026#39;])], axis=0, inplace=True) print(df_blood.shape) df_blood.head()   Chúng ta có 605 diễn viên để phân tích.\ndf_blood_gr = df_blood.groupby(\u0026#39;Blood_Type\u0026#39;)[\u0026#39;Name\u0026#39;].count() df_blood_gr = df_blood_gr.to_frame() df_blood_gr.columns = [\u0026#39;Count\u0026#39;] df_blood_gr = df_blood_gr.sort_values(\u0026#39;Count\u0026#39;, ascending=False) df_blood_gr.style.background_gradient(cmap=\u0026#39;plasma_r\u0026#39;)   Hai nhóm máu A và O có số lượng diễn viên nhiều nhất, tiếp đến là nhóm máu B và AB cuối cùng. Khả năng là những phụ nữ có nhóm máu A, O thường sẽ chọn nghề trở thành diễn viên JAV (nếu họ ở Nhật Bản) ???\n3.6 Phân tích sở thích của các diễn viên Các JAV idols của chúng ta không phải chỉ có sở thích đóng phim đâu nhé. Thực ra, đóng phim người lớn là một công việc rất nặng nhọc. Vì thế mà các diễn viên cũng có rất nhiều sở thích, thú vui khác nhau để giải trí, làm dịu bớt mệt mỏi, căng thẳng.\ndf_hobby = df_ja[df_ja[\u0026#39;Hobby\u0026#39;].notnull()] hobby_str = \u0026#39; \u0026#39;.join(df_hobby[\u0026#39;Hobby\u0026#39;]) font_path = \u0026#39;font/NotoSerifCJKjp-Light.otf\u0026#39; mask = np.array(Image.open(\u0026#39;heart_mask.jpg\u0026#39;)) ... # create a wordcloud  wordcloud = WordCloud(background_color=\u0026#39;white\u0026#39;, mask=mask, font_path=font_path, max_words=2000, contour_width=2, contour_color=\u0026#39;steelblue\u0026#39;, stopwords=stopwords, max_font_size=500 ).generate(hobby_str) wordcloud.to_file(\u0026#34;alice.png\u0026#34;) # show worldcloud picture plt.figure(figsize=(8,8)) plt.imshow(wordcloud, interpolation=\u0026#39;bilinear\u0026#39;) plt.axis(\u0026#34;off\u0026#34;) plt.show()   Có thể thấy các thú vui của diễn viên JAV cũng rất đa dạng, vừa đời thường, vửa thời thượng: Nấu ăn, đi mua sắm, hát Karaoke, chơi đàn Piano, khiêu vũ, xem phim, \u0026hellip; và rất nhiều sở thích khác nữa.\n3.7 Phân tích quê hương của các diễn viên Nếu như ở VN, mọi người thường nói, người Nghệ An, Hà Tĩnh thường học rất giỏi, nhiều người làm quan to, sếp lớn. Không biết liệu ở Nhật Bản, yếu tố quê hương có ảnh hưởng đến nghề nghiệp hay không?\nTrong bộ dữ liệu đã có thông tin về nơi sinh ra của các diễn viên JAV rồi, nhưng để có cái nhìn trực quan hơn thì chúng ta sẽ cần thêm thông tin về toạ độ địa lý của nơi đó, rồi hiển thị lên bản đồ.\n# get latitude \u0026amp; longitude geolocator = Nominatim(user_agent=\u0026#39;Home_Town\u0026#39;) df_birthplace[\u0026#39;Home_Town Coord\u0026#39;] = df_birthplace[\u0026#39;Home_Town\u0026#39;].apply(geolocator.geocode).apply(lambda x: (x.latitude, x.longitude)) # separate latitude \u0026amp; longtitude into 2 different columns df_birthplace[[\u0026#39;Latitude\u0026#39;, \u0026#39;Longtitude\u0026#39;]] = df_birthplace[\u0026#39;Home_Town Coord\u0026#39;].apply(pd.Series) df_birthplace.drop([\u0026#39;Home_Town Coord\u0026#39;], axis=1, inplace=True) df_birthplace.head()   hmap = folium.Map(location=[35.9, 139.8], control_scale=True, attr=\u0026#39;USGS style\u0026#39;, zoom_start=5) HeatMap(hm_data, rasdius=10).add_to(hmap)   Tokyo là nơi sản sinh ra nhiều diễn viên JAV nhất. Điều này có vẻ không giống với Việt Nam. Ở Việt Nam thì thường các miền quê nghèo, không có việc làm thì các chị em mới chọn theo nghề này. Còn ở Nhật Bản, Tokyo là thành phố sầm uất bậc nhất lại là quê hương của rất nhiều JAV idols. Điều này có lẽ xuất phát từ văn hóa và pháp luật của Nhật Bản, khi mà JAV là một nghề nghiệp được coi là hợp pháp, và có thu nhập tương đối cao. Và một lý do khác, rất có thể chốn đô thị xa hoa, lộng lẫy đã khiến nhiều cô gái muốn làm giàu nhanh chóng, thay vì phải ngồi làm công việc văn phòng với bàn giấy và máy tính tẻ nhạt.\n3.8 Phân tích mối liên hệ giữa tuổi tác và kích thước 3 vòng Thông thường, tuổi tác thường tỉ lệ nghịch với kích thước 3 vòng. Hãy xem điều đó có đúng với các diễn viên JAV hay không?\nTrước tiên, ta sẽ tạo ra một dataframe chỉ chứa các thông tin về tuổi và kích thước 3 vòng. Sau đó, phân chia tuổi thành cách nhóm A, B, C tương ứng với các khoảng 15-30, 30-40, và \u0026gt; 40 tuổi.\n Mối quan hệ giữa độ tuổi và kích thước 3 vòng được thể hiện trên đồ thị như sau:\n   Nhìn vào đây ta thấy, có vẻ như tuổi tác không ảnh hưởng quá nhiều đến kích thước 3 vòng, khi mà mức trung bình của cả 3 nhóm tuổi đều là 87cm, 85cm, và 87cm tương ứng với vòng 1, vòng 2, và vòng 3. Thậm chí là đối với vòng 1, nhóm diễn viên lớn tuổi có kích thước vòng 1 còn nhỉnh hơn 1 chút xíu so với nhóm nhỏ tuổi. Điều này cũng khá dễ hiểu thôi, bởi vì kích thước 3 vòng là yếu tố quan trọng để thu hút khán giả của cac diễn viên JAV, nên họ sẽ cực kỳ chăm chút. Nếu vì tuổi cao mà kích thước không đạt yêu cầu để hấp dẫn nữa thì chắc là họ sẽ giải nghệ.\n4. Kết luận Trên đây là bức tranh tổng quát về các JAV idols ở xứ sở mặt trời mọc. Hi vọng qua bài viết này, các bạn hiểu thêm được phần nào về nghành công nghiệp phim người lớn nói riêng và việc vận dụng Data Science để tìm ra Insight từ các dữ liệu nói chung. Sau khi đọc xong bài này, các nam thanh niên có thể tự tin chém gió về thần tượng của mình rồi đó !!!\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/108_jav_idols_eda/","tags":["Data Science","EDA"],"title":"Bạn có phải là fan của các JAV idols ???"},{"categories":["Data Science","EDA"],"contents":"Tiếp tục bàn về câu chuyện nhân viên nghỉ việc, và đây là phần 2.\nTrong phần 2 này, chúng ta sẽ cùng nhau xây dựng một ML model để dự đoán xem liệu một nhân viên nào đó có khả năng nghỉ việc hay không?\n1. Phương pháp tiếp cận khi xây dựng một ML model Có một vài phương pháp khác nhau giúp ta có thể xây dựng được một ML model tốt. Hai trong số những phương pháp đó, tương đối phổ biến và rất hay đuợc các Data Scientist trên thế giới sử dụng, đó là:\n Top-Down method:  Trong phương pháp này, một model sẽ được lựa chọn từ đầu, sau đó tinh chỉnh dần dần dựa theo các đặc tính của dữ liệu, các tham số của model, \u0026hellip; cho đến khi nào đạt được model đủ đáp ứng điều kiện bài toán.\n P2P Comparison method:  Ngược lại với phương pháp trên, P2P Comparison không chọn cố định ngay một model nào từ đầu, mà sẽ so sánh, đánh giá để đưa ra kết luận. Kết quả của lần đánh giá đầu tiên được gọi là Base model. Model được lựa chọn sau đó sẽ được tinh chỉnh dần dần để ngày càng tốt hơn. Trong quá trình tinh chỉnh đó, nó luôn được so sánh với Base model để đảm bảo kết quả sau luôn tối ưu hơn kết quả trước.\nNói chung thì phương pháp thứ 2 tỏ ra ưu việt hơn phương pháp thứ nhất trong đa số trường hợp, mặc dù nó sẽ làm ta mất nhiều thời gian hơn. Bởi vì trong phương pháp 1, nếu model được chọn từ đầu mà sai thì sẽ dẫn đến kết quả sau cùng cũng sai. Việc lựa chọn này đa số dựa vào kinh nghiệm của các Data Scientist.\nTrong bài này, mình sẽ áp dụng phương pháp thứ 2 đề thực hiện.\n2. Đánh giá, lựa chọn Base model 2.1 Đọc và kiểm tra dữ liệu Trước tiên ta sẽ đọc vào bộ dữ liệu:\ndf_data = pd.read_csv(\u0026#39;dataset/HR_comma_sep.csv\u0026#39;) df_data.head()   Xem xét một số thông tin về dữ liệu:\ndf_data.info() \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 14999 entries, 0 to 14998 Data columns (total 10 columns): # Column Non-Null Count Dtype  --- ------ -------------- ----- 0 satisfaction_level 14999 non-null float64 1 last_evaluation 14999 non-null float64 2 number_project 14999 non-null int64 3 average_montly_hours 14999 non-null int64 4 time_spend_company 14999 non-null int64 5 Work_accident 14999 non-null int64 6 left 14999 non-null int64 7 promotion_last_5years 14999 non-null int64 8 Department 14999 non-null object 9 salary 14999 non-null object dtypes: float64(2), int64(6), object(2) memory usage: 1.1+ MB Chúng ta đã biết từ bài trước rằng bộ dữ liệu bao gồm 14999 mẫu, và không có missing values. Ở đây, ta quan tâm thêm một thông tin nữa đó là có 2 features đang ở dạng categorical, đó là Department và salary. Để các ML model có thể học được thì chúng phải được chuyển sang dạng numerical. Chi tiết thực hiện ở phần sau.\n2.2 Data Preparation Phần này sẽ bao gồm 2 việc:\na, Phân chia dữ liệu Trước tiên, cần tách dữ liệu thành 2 phần khác nhau: input (features) và output (target).\n# separate input (features) and output (target) y = df_data[[\u0026#39;left\u0026#39;]] X = df_data.drop(\u0026#39;left\u0026#39;, axis=1) Tiếp theo, lại chia tiếp mỗi phần thành 2 tập riêng biệt: Train và Test\n# separate train set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25) print(X_train.shape) print(X_test.shape) Ở đây, mình chia theo tỷ lệ Train:Test = 75%:25%. Kết quả là ta có 11249 mẫu trong tập Train và 3750 mẫu trong tập Test.\nb, Mã hóa các Categorical features Như đã nói ở trên, các features ở dạng categorical cần phải được chuyển sang dạng numerical (mã hóa) thì các ML models mới có thể học được.\nCó một vài phương pháp trong thư viện Scikit-learn giúp ta thực hiện việc này như OrdinalEncoder, OneHotEncoder, \u0026hellip; Mình sẽ chọn OrdinalEncoder.\n# encode categorical features encoder = OrdinalEncoder() encoder.fit(X_train[[\u0026#39;salary\u0026#39;, \u0026#39;Department\u0026#39;]]) X_train[[\u0026#39;salary\u0026#39;, \u0026#39;Department\u0026#39;]] = encoder.transform(X_train[[\u0026#39;salary\u0026#39;, \u0026#39;Department\u0026#39;]]) X_test[[\u0026#39;salary\u0026#39;, \u0026#39;Department\u0026#39;]] = encoder.transform(X_test[[\u0026#39;salary\u0026#39;, \u0026#39;Department\u0026#39;]]) Sau khi khởi tạo instance của OrdinalEncoder thì ta sẽ dùng nó để fit trên toàn bộ tập Train đối với 2 categorical features. Cuối cùng, sử dụng instance đó để thực hiện phép transform để sinh ra dữ liệu mới tương ứng.\n2 công việc trên đây là yêu cầu tối thiểu, bắt buộc để có thể chuyển sang bước huấn luyện, đánh giá model.\n2.3 Đánh giá các models Để chọn được model phù hợp nhất trong số các models có thể, chúng ta sẽ gom tất cả chúng lại và lần lượt đánh giá chúng. Model nào cho kết quả cao nhất thì sẽ được chọn.\n Định nghĩa các models:  # create a dictionary of models model_dict = { \u0026#39;LogisticRegression\u0026#39;: LogisticRegression(max_iter=1000), \u0026#39;GaussianNB\u0026#39;: GaussianNB(), \u0026#39;KNeighborsClassifier\u0026#39;: KNeighborsClassifier(n_neighbors=1), \u0026#39;DecisionTreeClassifier\u0026#39;: DecisionTreeClassifier(min_samples_split=25), \u0026#39;SVM\u0026#39;: svm.SVC(kernel=\u0026#39;rbf\u0026#39;,probability=False), \u0026#39;RandomForestClassifier\u0026#39;: RandomForestClassifier(n_estimators = 10, min_samples_split=2, max_depth=30), } Vì đang xác định Base model nên chúng ta sử dụng các giá trị mặc định của các hyper-parameters.\n Đánh giá các models:  model_scores = [] model_names = [] for name, model in model_dict.items(): model.fit(X_train, np.ravel(y_train)) prediction = model.predict(X_test) acc_score = accuracy_score(y_test, prediction) model_scores.append(acc_score) model_names.append(name) print(\u0026#39;*\u0026#39;*10 + name + \u0026#39;*\u0026#39;*10 + \u0026#39;: {}\u0026#39;.format(acc_score)) Phương pháp đánh giá ở đây là huấn luyện các models trên tập Train, sau đó tính toán độ chính xác thông qua việc dự đoán trên tập Test. Kết quả thu được như sau:\n**********LogisticRegression**********: 0.7586666666666667 **********GaussianNB**********: 0.7994666666666667 **********KNeighborsClassifier**********: 0.9538666666666666 **********DecisionTreeClassifier**********: 0.9744 **********SVM**********: 0.7834666666666666 **********RandomForestClassifier**********: 0.9864   Ta thấy, RandomForestClassifier đạt được độ chính xác cao nhất, 98.64%. Do đó, RandomForestClassifier với 98.64% độ chính xác là Base model của chúng ta.\n3. Tối ưu hóa model Tối ưu hóa hay tinh chỉnh model là việc vận dụng các kỹ thuật xử lý dữ liệu, các kỹ thuật lựa chọn tham số để làm sao thu được model tốt nhất có thể. Tùy từng bài toán mà ta áp dụng một trong 2 loại kỹ thuật trên, hoặc có thể áp dụng đồng thời cả 2 loại đó.\n3.1 Xử lý dữ liệu Dữ liệu input ảnh hưởng khá nhiều đến chất lượng của model. Một số kỹ thuật thường được sử dụng để xử lý dữ liệu như: Remove outlier, Impute Missing values, Scale values, Balance data, Feature Selection, \u0026hellip; Mỗi kỹ thuật sẽ giải quyết một vấn đề của dữ liệu.\na, Remove outlier Outlier còn gọi là dữ liệu ngoại lệ, hay dữ liệu bất thường. Để kiểm tra xem có tồn tại Outlier trong các numberical features hay không, ta sẽ vẽ đồ thị Distplot của chúng.\n Quan sát đồ thị Displot ta thấy, các giá trị của các giá trị đều nằm trong các phạm vi hợp lệ nên không tồn tại Outlier ở đây.\nCòn đối với các categorical features, ta sử dụng phương pháp thống kê để kiểm tra:\n Cũng không có Outlier nào ở đây, tất cả các giá trị đều hợp lệ.\nTóm lại là chúng ta không cần áp dụng kỹ thuật Remove Outlier nào cho bộ dữ liệu này.\nb, Impute Missing values Ở phần đầu ta đã biết rằng không tồn tại Missing values, nên cũng không cần áp dụng kỹ thuật Impute Missing values.\nc, Balance data Kiểm tra xem có tồn tại hiện tượng Imbalance data hay không?\n Rõ rằng là có tồn tại Imbalance data khi mà tỉ lệ giữa 2 lớp khá chệnh lệch, 23.81% - 76.19%. Để giải quyết vấn đề này, ta có nhiều cách: Oversample, Undersample, Classes Weight, \u0026hellip; Mỗi cách lại có các thuật toán khác nhau như SMOTE, SMOTENC, \u0026hellip;\nTrong bài này, ta sẽ sử dụng phương pháp Class Weight. Cách thực hiện rất đơn giản, khi khai báo model, ta chỉ cần thêm tham số class_weight=\u0026lsquo;balanced\u0026rsquo;.\nd, Scale data Nếu giá trị của các features có sự khác biệt lớn về phạm vi, khi huấn luyện, model sẽ có xu hướng thiên vị hơn cho các features có giá trị lớn. Điều này làm giảm độ tin cậy của model. Scale data là phương pháp loại bỏ hiện tượng này, ở đó, các giá trị được đưa về cùng một phạm vi, thường là từ 0 đến 1.\nCó 2 kỹ thuật hay được sử dụng là Normalization và Standarlization. Mình sẽ áp dụng kỹ thuật Standarlization trong bài này.\nChi tiết hơn về các kỹ thuật xử lý và chuẩn bị dữ liệu, các bạn có thể xem thêm các bài viết về chủ đề Data Preparation của mình tại đây.\n3.2 Huấn luyện models Theo như phương pháp P2P Comparison thì ta sẽ chỉ đi tối ưu cho Base model, tức là RandomForestClassifier. Tuy nhiên, ở đây mình sẽ thử đánh giá lại toàn bộ các models sau khi đã áp dụng các kỹ thuật xử lý dữ liệu, xem liệu có gì thay đổi hay không?\n# create a dictionary of models model_dict = { \u0026#39;LogisticRegression\u0026#39;: LogisticRegression(max_iter=500, class_weight=\u0026#39;balanced\u0026#39;), \u0026#39;GaussianNB\u0026#39;: GaussianNB(), \u0026#39;KNeighborsClassifier\u0026#39;: KNeighborsClassifier(), \u0026#39;DecisionTreeClassifier\u0026#39;: DecisionTreeClassifier(class_weight=\u0026#39;balanced\u0026#39;), \u0026#39;SVM\u0026#39;: svm.SVC(class_weight=\u0026#39;balanced\u0026#39;), \u0026#39;RandomForestClassifier\u0026#39;: RandomForestClassifier(class_weight=\u0026#39;balanced\u0026#39;), } Ta vẫn khai báo danh sách các models như phần trước, có thay đổi một chút là thêm tham số class_weight=\u0026lsquo;balanced\u0026rsquo; như đã phân tích ở phần 3.1c. GaussianNB và KNeighborsClassifier là 2 models không hỗ trợ việc này.\nTổng hợp lại từ đầu thì ngoài Balance data, ta cần thực hiện 2 kỹ thuật xử lý dữ liệu sau: Encoder Categorical features và Scale data.\nThư viện Scikit-learn cung cấp cho chúng ta công cụ Pipeline giúp gom các phần xử lý này lại thành một pipeline. Về sau khi dự đoán trên mẫu dữ liệu mới, ta áp dụng đúng pipeline này cho mẫu đó thì sẽ đảm bảo dữ liệu test và dữ liệu train được xử lý giống hệt nhau.\n# define ColumnTransformer to perform transform data trans_list = [(\u0026#39;cat\u0026#39;, OneHotEncoder(), categorical_ix), (\u0026#39;num\u0026#39;, StandardScaler(), numerical_ix)] col_trans = ColumnTransformer(transformers=trans_list) ... for name, model in model_dict.items(): pipeline = Pipeline(steps=[(\u0026#39;preparation\u0026#39;, col_trans), (\u0026#39;model\u0026#39;, model)]) cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42) scores = cross_val_score(pipeline, X, np.ravel(y), scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) ... Và để việc đánh giá các models được khách quan hơn, mình sẽ sử dụng phương pháp Cross-validation.\nKết quả thực hiện:\n**********LogisticRegression********** 0.7598638247720703 0.00971910352647794 **********GaussianNB********** 0.6595039092728485 0.014364290029621824 **********KNeighborsClassifier********** 0.9455297175895042 0.005250667653662472 **********DecisionTreeClassifier********** 0.9819988258839224 0.003200161402127842 **********SVM********** 0.9559569134978875 0.004905774407557175 **********RandomForestClassifier********** 0.9913393907049142 0.0022560674653345505   Ta thấy, hầu hết các models đều có sự cải thiện về độ chính xác so với phiên bản Base của chúng. Riêng đối với model RandomForestClassifier thì độ chính xác tăng từ 98.64% lên 99.14%.\n3.3 Tune hyper-parameters cho RandomForestClassifier model Mỗi model đều có một số siêu tham số có thể cấu hình được để thay đổi cách thức học của model đó. Và chúng ta thường không thể biết được chính xác giá trị nào của các tham số đó làm cho model đạt được kết quả cao nhất.\nCó một vài phương pháp giúp chúng ta tự động hóa việc này: Grid Search, Random Search, \u0026hellip; Các phương pháp đều có chung ý tưởng là từ một tập hợp các giá trị có thể có có từng tham số, các kết hợp giữa chúng được tạo ra và áp dụng vào việc huấn luyện model. Sự kết hợp nào mang lại kết quả tốt nhất sẽ được chọn.\nTừ phần trên chúng ta đã biết rằng RandomForestClassifier là model đang có độ chính xác cao nhất. Ta sẽ thực hiện Tuning các Hyper-parameters của model này xem liệu có thể thu được kết quả tốt hơn hay không?\nrf_model = RandomForestClassifier(random_state=42, class_weight=\u0026#39;balanced\u0026#39;) pipeline = Pipeline(steps=[(\u0026#39;preparation\u0026#39;, col_trans), (\u0026#39;rf\u0026#39;, rf_model)]) cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42) Phần khai báo model và chuẩn bị dữ liệu trong pipeline vẫn giữ nguyên như trên.\nPhần quan trọng ở đây là bước lựa chọn tham số và các giá trị có thể có của chúng để đưa vào search.\nparam_grid = {\u0026#39;rf__n_estimators\u0026#39;: np.arange(50, 150, 10), \u0026#39;rf__max_features\u0026#39;: [\u0026#39;auto\u0026#39;, \u0026#39;sqrt\u0026#39;, \u0026#39;log2\u0026#39;], \u0026#39;rf__max_depth\u0026#39;: np.arange(10, 100, 10), \u0026#39;rf__min_samples_split\u0026#39;: [2, 5, 10], \u0026#39;rf__min_samples_leaf\u0026#39;: [1, 2, 4], \u0026#39;rf__criterion\u0026#39; :[\u0026#39;gini\u0026#39;, \u0026#39;entropy\u0026#39;], \u0026#39;rf__bootstrap\u0026#39;: [True, False]} Bạn có thể lựa chọn tùy ý, càng nhiều giá trị thì thời gian huấn luyện sẽ càng lâu. Tuy nhiên, cũng không nên chọn các giá trị quá gần nhau, vì mức độ ảnh hưởng của chúng đến model không có sự khác biệt rõ rệt. Tốt nhất là mỗi giá trị nên cách nhau 5 đến 10. VD: rf__n_estimators': np.arange(50, 150, 10),.\nBước cuối cùng là khai báo GridSeach và sử dụng nó để huấn luyện model:\ngrid_pipeline = GridSearchCV(pipeline, param_grid, scoring= \u0026#39;accuracy\u0026#39;, n_jobs=-1, cv=cv) results = grid_pipeline.fit(X, np.ravel(y)) print( \u0026#39; Best Mean Accuracy: %.3f\u0026#39; % results.best_score_) print( \u0026#39; Best Config: %s\u0026#39; % results.best_params_) Sau khoảng 25h train liên tục trên máy tính core i7, 64GB Ram, RTX 2080 (8GB) thì mình thu được kết quả:\nBest Mean Accuracy: 0.992 Best Config: {\u0026#39;rf__bootstrap\u0026#39;: False, \u0026#39;rf__criterion\u0026#39;: \u0026#39;entropy\u0026#39;, \u0026#39;rf__max_depth\u0026#39;: 30, \u0026#39;rf__max_features\u0026#39;: \u0026#39;auto\u0026#39;, \u0026#39;rf__min_samples_leaf\u0026#39;: 1, \u0026#39;rf__min_samples_split\u0026#39;: 2, \u0026#39;rf__n_estimators\u0026#39;: 70} Độ chính xác cao nhất đạt được là 99.20%, cao hơn một chút so với con số 99.13% ở trên. Các giá trị của các tham số tương ứng cũng được liệt kê. Về sau, nếu muốn huấn luyện lại model, ta có thể sử dụng luôn bộ giá trị này.\n4. Sử dụng model để dự đoán 4.1 Dự đoán trên tập Test Sau khi có được model tốt nhất, ta thử sử dụng nó để dự đoán các mẫu có trong tập Test, và hiển thị kết quả dưới dạng classification report và confusion matrix:\n# actually, test set is used to train model. So, this is not really valuable. y_pred = grid_pipeline.predict(X_test) # create confusion matrix cm = confusion_matrix(y_test, y_pred) # display classification report print(classification_report(y_test, y_pred))   # display confusion matrix fig, ax = plot_confusion_matrix(conf_mat=cm, show_absolute=True, show_normed=True, colorbar=True) plt.show()   Ta thấy trên tập Test, độ chính xác đạt được là tuyệt đối, 100%. Điều này là bởi vì thực ra trong quá trình huấn luyện model, các mẫu trong tập Test cũng đã được đưa vào học thông qua cơ chế Cross-validation.\n4.2 Dự đoán trên mẫu dữ liệu mới Nếu có một mẫu dữ liệu mới thì sao, ta cần làm gì để đưa vào model dự đoán?\nĐầu tiên, cần đưa mẫu dữ liệu về dạng giống như lúc huấn luyện:\n# create new sample new_sample = [[0.41, 0.43, 3, 153, 3, 1, 1, \u0026#39;sales\u0026#39;, \u0026#39;medium\u0026#39;]] df_new_sample = pd.DataFrame(new_sample) df_new_sample.columns = [\u0026#39;satisfaction_level\u0026#39;, \u0026#39;last_evaluation\u0026#39;, \u0026#39;number_project\u0026#39;, \u0026#39;average_montly_hours\u0026#39;, \u0026#39;time_spend_company\u0026#39;, \u0026#39;Work_accident\u0026#39;, \u0026#39;promotion_last_5years\u0026#39;, \u0026#39;Department\u0026#39;, \u0026#39;salary\u0026#39;] Đến bước dự đoán, ta có thể có 2 cách:\n Dự đoán trực tiếp ra nhãn của mẫu:  # get only predicted class class_pred = grid_pipeline.predict(df_new_sample)[0] print(\u0026#39;Class = {}\u0026#39;.format(class_pred)) Kết quả:\nClass = 0  Dự đoán xác suất của mỗi class:  # get predicted class and it\u0026#39;s probably coresponding pd.DataFrame(grid_pipeline.predict_proba(df_new_sample)*100, columns=grid_pipeline.classes_) Kết quả:\n0\t1 0\t98.571429\t1.428571 Xác suất của class 0 là 98.57%, còn của class 1 là 1.42%.\n5. Save và Load model Model đã huấn luyện nên được lưu lại để có thể sử dụng lại về sau.\n5.1 Save model Để lưu model, ta có thể sử dụng một trong 2 thư viện là pickle hoặc joblib:\n# way 1 pickle.dump(grid_pipeline, open(\u0026#39;model.pkl\u0026#39;, \u0026#39;wb\u0026#39;)) # pickle.dump(grid_pipeline.best_estimator_, open(\u0026#39;model.pkl\u0026#39;, \u0026#39;wb\u0026#39;)) # way 2, same result as way 1 joblib.dump(grid_pipeline, \u0026#39;model.pkl\u0026#39;) # joblib.dump(grid_pipeline.best_estimator_, \u0026#39;model.pkl\u0026#39;) Để load model:\n# way 1 model = pickle.load(open(\u0026#39;model.pkl\u0026#39;, \u0026#39;rb\u0026#39;)) # way 2, same result as way 1 model = joblib.load(\u0026#39;model.pkl\u0026#39;) Kiểm tra lại model vừa load bằng cách sử dụng nó để tạo một dự đoán:\n# use loaded model to make prediction class_id = model.predict(df_new_sample)[0] print(class_id) Kết quả: 0\nNhư vậy là quá trình save và load model được thực hiện chính xác.\n6. Kết luận Trong phần thứ 2 của bài toán phân tích dữ liệu nhân viên, chúng ta đã cùng nhau xây dựng được một ML model tối ưu để có thể dự đoán liệu một nhân viên có khả năng nghỉ việc hay là không? Lần lượt từng bước tiếp cận, từ xây dựng Base model, xử lý dữ liệu, đến việc tối ưu model đều đã được trình bày chi tiết. Hi vọng là các bạn có thể nắm bắt được ý tưởng của mình.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/107_employee_quit_prediction/","tags":["Data Science","EDA"],"title":"Ai sẽ nghỉ việc ???"},{"categories":["Data Science","EDA"],"contents":"Ngành IT là một trong số các ngành có tỉ lệ cạnh tranh về mặt nhân sự tương đối cao hiện nay. Trong bối cảnh nhu cầu tuyển dụng của các công ty rất lớn, nhưng số lượng ứng viên chưa đáp ứng đủ thì các công ty luôn phải tìm mọi cách thu hút và giữ chân nhân tài để phục vụ cho mục tiêu tăng trưởng và phát triển của mình. Là chủ doanh nghiệp hay trưởng bộ phận nhân sự, bạn luôn phải trả lời 2 câu hỏi:\n Tại sao nhân viên lại nghỉ việc? Có thể dự đoán trước được nhân viên nào sẽ nghỉ việc hay không?  Chúng ta sẽ cùng đi tìm trả lời cho 2 câu hỏi đó bằng việc phân tích dữ liệu thực tế một cách khoa học. Mình sẽ đi phân tích các nguyên nhân có thể dẫn đến quyết định đi hay ở của một nhân viên, sau đó sẽ xây dựng một ML model để dự đoán khả năng nghỉ việc của nhân viên đó.\nVì nội dung cần trình bày khá dài nên mình quyết định chia thành 2 phần:\n Phần 1 (Tuần này): Trả lời câu hỏi số 1. Phần 2 (Tuần tiếp theo): Trả lời câu hỏi số 2.  OK, let\u0026rsquo;s go! Chúng ta sẽ đi luôn vào bài viết của tuần này.\n1. Chuẩn bị và khảo sát dữ liệu 1.1 Chuẩn bị dữ liệu Bộ dữ liệu chúng ta sử dụng trong bài hôm nay có tên là *HR_ comma_sep.csv*, được cung cấp bởi nền tảng Kaggle. Download nó tại đây. Nó được tạo thành bởi cuộc khảo sát của khoảng 15.000 nhân viên đến từ các công ty khác nhau trên toàn thế giới. Các thông tin của nó được mô tả trong bảng dưới đây:\n   STT Tên Features Ý Nghĩa     1 satisfaction_level Mức độ thỏa mãn nói chung của nhân viên, có giá trị từ 0-1   2 last_evaluation Kết quả đánh giá gần đây nhất của quản lý dành cho nhân viê đó, có giá trị 0-1   3 number_project Số lượng dự án mà nhân viên đã và đang tham gia.   4 average_montly_hourse Số giờ làm việc trung bình trong 1 tháng của nhân viên   5 time_spend_company Số năm làm viêc tại công ty của nhân viên   6 Work_accident Nhân viên có gặp sự cố, tai nạn gì trong quá trình làm viện tại công ty hay không? 1 - accident, 0 - no accident   7 left 0 - nhân viên ở lại làm việc tại công ty, 1 - nhân viên nghỉ việc tại công ty. Đây là thông tin mà ta sẽ cần dự đoán   8 promotion_last_5years 0 - nhân viên không được thăng tiến trong vòng 5 năm gần đây, 1 - ngược lại   9 Department Phòng/bộ phận làm việc của nhân viên   10 salary Mức lương mà nhân viên được nhận tại thời điểm khảo sát, chia theo 3 mức: thấp, trung bình, cao    1.2 Khảo sát dữ liệu a, Đọc và hiển thị dữ liệu Trước tiên, ta sẽ đọc vào dữ liệu và hiển thị một vài mẫu để bước đầu hình dung về nó:\ndf_data = pd.read_csv(\u0026#39;dataset/HR_comma_sep.csv\u0026#39;) df_data.head()   b, Xem xét các thông tin thống kê của dữ liệu print(\u0026#39;Summary train data\u0026#39;) print(\u0026#39;*\u0026#39;*50) print(f\u0026#39;Shape: {df_data.shape}\u0026#39;) print(\u0026#39;*\u0026#39;*50) print(f\u0026#39;Data description: \\n{df_data.describe()}\u0026#39;) print(\u0026#39;*\u0026#39;*50) Kết quả:\nSummary train data ************************************************** Shape: (14999, 10) ************************************************** Data description: satisfaction_level last_evaluation number_project \\ count 14999.000000 14999.000000 14999.000000 mean 0.612834 0.716102 3.803054 std 0.248631 0.171169 1.232592 min 0.090000 0.360000 2.000000 25% 0.440000 0.560000 3.000000 50% 0.640000 0.720000 4.000000 75% 0.820000 0.870000 5.000000 max 1.000000 1.000000 7.000000 average_montly_hours time_spend_company Work_accident left \\ count 14999.000000 14999.000000 14999.000000 14999.000000 mean 201.050337 3.498233 0.144610 0.238083 std 49.943099 1.460136 0.351719 0.425924 min 96.000000 2.000000 0.000000 0.000000 25% 156.000000 3.000000 0.000000 0.000000 50% 200.000000 3.000000 0.000000 0.000000 75% 245.000000 4.000000 0.000000 0.000000 max 310.000000 10.000000 1.000000 1.000000 promotion_last_5years count 14999.000000 mean 0.021268 std 0.144281 min 0.000000 25% 0.000000 50% 0.000000 75% 0.000000 max 1.000000 ************************************************** Chính xác là có 14.999 mẫu dữ liệu. Ta có thể lướt qua các giá trị thống kê như mean, std, min, max, quantile, \u0026hellip; của các features dạng numerical để sơ bộ nắm bắt được chúng.\nc, Kiểm tra xem có giá trị nào bất thường hay không? print(\u0026#39;--- Check feature by feature ---\u0026#39;) print(\u0026#39;*\u0026#39;*50) for fe in df_data.columns: print(\u0026#39;-\u0026#39;*20) print(df_data[fe].value_counts()) Kết quả:\n--- Check feature by feature --- ************************************************** -------------------- 0.10 358 0.11 335 0.74 257 0.77 252 0.84 247 ... 0.25 34 0.28 31 0.27 30 0.26 30 0.12 30 Name: satisfaction_level, Length: 92, dtype: int64 -------------------- 0.55 358 0.50 353 0.54 350 0.51 345 0.57 333 ... 0.39 52 0.43 50 0.38 50 0.44 44 0.36 22 Name: last_evaluation, Length: 65, dtype: int64 -------------------- 4 4365 3 4055 5 2761 2 2388 6 1174 7 256 Name: number_project, dtype: int64 -------------------- 135 153 156 153 149 148 151 147 160 136 ... 297 7 288 6 299 6 96 6 303 6 Name: average_montly_hours, Length: 215, dtype: int64 -------------------- 3 6443 2 3244 4 2557 5 1473 6 718 10 214 7 188 8 162 Name: time_spend_company, dtype: int64 -------------------- 0 12830 1 2169 Name: Work_accident, dtype: int64 -------------------- 0 11428 1 3571 Name: left, dtype: int64 -------------------- 0 14680 1 319 Name: promotion_last_5years, dtype: int64 -------------------- sales 4140 technical 2720 support 2229 IT 1227 product_mng 902 marketing 858 RandD 787 accounting 767 hr 739 management 630 Name: Department, dtype: int64 -------------------- low 7316 medium 6446 high 1237 Name: salary, dtype: int64 Bất thường ở đây tức là các giá trị tồn tại ở một dạng khác so với quy đinh. VD, đối với thông tin về mức lương, các giá trị quy định là low, medium, high. Nếu có một giá trị không nằm trong số các giá trị quy định kia thì đó là bất thường. Nếu có thì ta sẽ cần phải xử lý chúng.\nĐối với bộ dữ liệu này, ta thấy không có giá trị nào bất thường.\nc, Kiểm tra kiểu dữ liệu và xem có tồn tại missing value hay không? print(f\u0026#39;Data information: {df_data.info()}\u0026#39;) Kết quả:\n\u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 14999 entries, 0 to 14998 Data columns (total 10 columns): # Column Non-Null Count Dtype  --- ------ -------------- ----- 0 satisfaction_level 14999 non-null float64 1 last_evaluation 14999 non-null float64 2 number_project 14999 non-null int64 3 average_montly_hours 14999 non-null int64 4 time_spend_company 14999 non-null int64 5 Work_accident 14999 non-null int64 6 left 14999 non-null int64 7 promotion_last_5years 14999 non-null int64 8 Department 14999 non-null object 9 salary 14999 non-null object dtypes: float64(2), int64(6), object(2) memory usage: 1.1+ MB Data information: None Các features thuộc một trong 3 kiểu dữ liệu: float64, int64, hoặc object và không có missing value nào tồn tại. Có thể nói đây là bộ dataset khá lý tưởng.\n2. Phân tích dữ liệu - Explore Data Analysis Trong phần này, chúng ta sẽ đi phân tích chi tiết dữ liệu, đánh giá mức độ ảnh hưởng của từng thuộc tính để tìm ra cả trả lời cho câu hỏi: Đâu là nguyên nhân dẫn đến quyết định nghỉ việc hay không nghỉ việc của nhân viên?\n2.1 Xem xét mối tương quan giữa các features Để tính toán mức độ tương quan giữa các features với nhau, mà cụ thể trong bài toán này, ta muốn biết các mức độ ảnh hưởng của các yếu tố đến quyết định nghỉ việc hay tiếp tục làm việc tại công ty, ta có thể sử dụng biểu đồ Heatmap như sau:\n# create corrrelation matrix df_corr = df_data.corr() df_corr_round = df_corr.round(3) # draw headmap fig = ff.create_annotated_heatmap( z=df_corr_round.to_numpy(), x=df_corr.columns.tolist(), y=df_corr.columns.tolist(), zmax=1, zmin=-1, showscale=True, hoverongaps=True, colorscale=\u0026#39;Viridis\u0026#39;, annotation_text=df_corr_round.to_numpy() ) fig.update_layout( margin = dict(t=10,r=10,b=10,l=10), showlegend = False, width = 800, height = 600 )   Dựa vào đây, ta thấy rằng mức độ thỏa mãn và quyết định nghỉ việc không thực sự có mối liên hệ nhiều (như ta nghĩ thông thường). Khi mô hình hóa dữ liệu, nếu cần thiết, ta có thể bỏ qua feature này.\nYếu tố ảnh hưởng nhiều nhất là số năm làm viêc tại công ty. Một cách chủ quan, ta có thể nhận định rằng những người làm càng lâu năm thì ít có khả năng nghỉ việc. Ta sẽ xem nhận xét này liệu có đúng hay không ở phần sau.\nCác yếu tố còn lại, sắp xếp theo thứ tự giảm dần mức độ ảnh hưởng là: average_montly_hours, Department, number_project, last_evaluation, salary, promotion_last_5years, Work_accident.\n2.2 Đánh giá sự ảnh hưởng của số năm làm việc tại công ty fig = go.Figure(data=[ go.Bar(name=\u0026#39;Stay\u0026#39;, x=df_tsc.index, y=df_tsc[0], text=df_tsc[0], textposition=\u0026#39;auto\u0026#39;), go.Bar(name=\u0026#39;Left\u0026#39;, x=df_tsc.index, y=df_tsc[1], text=df_tsc[1], textposition=\u0026#39;auto\u0026#39;), ]) # Change the bar mode fig.update_layout(barmode=\u0026#39;group\u0026#39;, title=\u0026#34;Statistic number of working years of employees\u0026#34;, xaxis_title=\u0026#34;Number of working years\u0026#34;, yaxis_title=\u0026#34;Number of employees\u0026#34;, legend_title=\u0026#34;Employee Type\u0026#34;) fig.show()   Nhận xét:\n Những người làm việc từ 2 năm trở xuống và 6 năm trở nên hầu như không nghỉ việc. Những người làm việc được từ 3-5 năm xác suất nghỉ việc cao hơn.  2.3 Đánh giá sự ảnh hưởng của thời gian làm việc trung bình trong tháng # create dataframe with average monthly working hours and left information df_mh = df_data[[\u0026#39;average_montly_hours\u0026#39;, \u0026#39;left\u0026#39;]] # draw violin chart fig = px.violin( df_mh, y=\u0026#34;average_montly_hours\u0026#34;, color=\u0026#39;left\u0026#39;, points=\u0026#39;all\u0026#39;, box=True, title=\u0026#39;Influence of monthly working hours to left/stay company decision: 0 - Stay, 1 - Left\u0026#39;, labels={\u0026#39;average_montly_hours\u0026#39;: \u0026#39;Average Monthly Working Hours\u0026#39;, \u0026#39;left\u0026#39;:\u0026#39;Left/Stay\u0026#39;}) fig.show()   Nhận xét:\n Những người có số giờ làm việc trung bình trong tháng ở mức cao, \u0026gt; 220h(chắc là OT nhiều) có xu hướng nghi việc cao hơn. Những người có số giờ làm việc trung bình trong tháng ở mức thấp, \u0026lt; 160h cũng là những người hay nghỉ việc. Có lẽ vì công việc nhàm chán quá chăng? Những người có số giờ làm việc trung bình trong tháng ở mức vừa phải, 150-260h, ít có khả năng nghỉ viêc hơn.  2.3 So sánh tỉ lệ nghỉ việc giữa các phòng ban # draw bar chart fig = px.bar(df_dpm, y=\u0026#39;Percent\u0026#39;, text=\u0026#39;Percent\u0026#39;, color=\u0026#39;Percent\u0026#39;) fig.show()   Nhận xét:\n HR là phòng ban có tỉ lệ nhân viên nghỉ việc cao nhất. Tỉ lệ thấp nhất là bộ phần Manager. Các phòng ban khác, thứ tự giảm dần như sau: Accounting, Technical, Support, Sales, Marketing, IT, Product Managerment, R\u0026amp;D.  2.4 Đánh giá sự ảnh hưởng của số lượng dự án trung bình 1 năm mà nhân viên đã làm Trong dữ liệu gốc, không có thông tin về số lượng dự án trung bình mà nhân viên làm trong 1 năm. Thông tin này sẽ được tính bằng cách lấy tổng số dự án mà nhân viên đó đã làm, chia cho số năm nhân viên đó làm việc tại công ty. Sau đó lại phân thành 3 nhóm:\n Low: Từ 0-2 dự án một năm Medium: 2-4 dự án một năm High: \u0026gt; 4 dự án một năm  # draw bar chart fig = go.Figure(data=[ go.Bar(name=\u0026#39;Stay\u0026#39;, x=df_project.index, y=df_project[0], text=df_project[0], textposition=\u0026#39;auto\u0026#39;), go.Bar(name=\u0026#39;Left\u0026#39;, x=df_project.index, y=df_project[1], text=df_project[1], textposition=\u0026#39;auto\u0026#39;), ]) fig.update_layout(barmode=\u0026#39;group\u0026#39;, title=\u0026#34;Statistic average number of projects a year of employees\u0026#34;, xaxis_title=\u0026#34;Number of projects\u0026#34;, yaxis_title=\u0026#34;Number of employees\u0026#34;, legend_title=\u0026#34;Employee Type\u0026#34;) fig.show()   Nhận xét:\n Nhân viên mà đã tham gia trung bình 2 dự án 1 năm có xu hướng nghỉ việc nhiều hơn. Đây là nhóm nhân viên mới làm viêc ở công ty 1-2 năm. Nhân viên mà đã tham gia trung bình nhiều hơn 2 dự án 1 năm hầu như không nghỉ việc. Đây là nhóm nhân viên làm viêc lâu năm tại công ty.  2.5 Đánh giá sự ảnh hưởng của kết quả lần đánh giá ( checkpoint) nhân viên gần nhất # create dataframe with last evaluation and left information df_le = df_data[[\u0026#39;last_evaluation\u0026#39;, \u0026#39;left\u0026#39;]] # draw violin chart fig = px.violin( df_le, y=\u0026#34;last_evaluation\u0026#34;, color=\u0026#39;left\u0026#39;, points=\u0026#39;all\u0026#39;, box=True, labels={\u0026#39;last_evaluation\u0026#39;: \u0026#39;Last Evaluation\u0026#39;, \u0026#39;left\u0026#39;: \u0026#39;Left/Stay\u0026#39;}, title=\u0026#39;Influence of last evaluation to left/stay company decision: 0 - Stay, 1 - Left\u0026#39;) fig.show()   Nhận xét:\n Những người được đánh giá ở mức quá cao (\u0026gt; 0.8) và hoặc quá thấp (\u0026lt; 0.6) là những người dễ nghỉ việc. Có lẽ họ thấy rằng công việc hiện tại quá dễ dàng hoăc quá khó đôi với họ nên họ muốn thay đổi, tìm cơ hội mới. Những người được đánh giá ở mức trung bình thường sẽ không rời công ty.  2.6 Đánh giá sư ảnh hưởng của mức lương # draw bar chart salary_level = [\u0026#39;High\u0026#39;, \u0026#39;Low\u0026#39;, \u0026#39;Medium\u0026#39;] fig = go.Figure(data=[ go.Bar(name=\u0026#39;Stay\u0026#39;, x=salary_level, y=df_salary[0], text=df_salary[0], textposition=\u0026#39;auto\u0026#39;), go.Bar(name=\u0026#39;Left\u0026#39;, x=salary_level, y=df_salary[1], text=df_salary[1], textposition=\u0026#39;auto\u0026#39;), ]) fig.update_layout(barmode=\u0026#39;group\u0026#39;, title=\u0026#34;Statistic salary of employees\u0026#34;, xaxis_title=\u0026#34;Salary Level\u0026#34;, yaxis_title=\u0026#34;Number of employees\u0026#34;, legend_title=\u0026#34;Employee Type\u0026#34;) fig.show()   Nhận xét:\n Những người có mức lương cao thì ít nghỉ việc hơn. Những người có mức lương thấp hoặc trung bình rất dễ nghỉ việc.  Điều này chứng tỏ lương vẫn là yếu tố quan trọng để giữ chân nhân viên. :D\n2.7 Đánh giá sự ảnh hưởng của việc thăng tiến trong vòng 5 năm # draw bar chart fig = go.Figure(data=[ go.Bar(name=\u0026#39;Stay\u0026#39;, x=df_pl5.index, y=df_pl5[0], text=df_pl5[0], textposition=\u0026#39;auto\u0026#39;), go.Bar(name=\u0026#39;Left\u0026#39;, x=df_pl5.index, y=df_pl5[1], text=df_pl5[1], textposition=\u0026#39;auto\u0026#39;), ]) fig.update_layout(barmode=\u0026#39;group\u0026#39;, title=\u0026#34;Statistic promotion last 5 years of employees\u0026#34;, xaxis_title=\u0026#34;Promotion last 5 years\u0026#34;, yaxis_title=\u0026#34;Number of employees\u0026#34;, legend_title=\u0026#34;Employee Type\u0026#34;) fig.show()   Nhận xét:\n Sau 5 năm làm việc, nếu không có sự thăng tiến nào thì rất nhiều nhân viên sẽ nghỉ việc. Ngược lại, nếu có sự phát triển về chức vụ sau 5 năm thì số người nghỉ việc sẽ ít hơn.  2.8 Đánh giá sự ảnh hưởng của sự cố trong công việc fig = go.Figure(data=[ go.Bar(name=\u0026#39;Stay\u0026#39;, x=df_wa.index, y=df_wa[0], text=df_wa[0], textposition=\u0026#39;auto\u0026#39;), go.Bar(name=\u0026#39;Left\u0026#39;, x=df_wa.index, y=df_wa[1], text=df_wa[1], textposition=\u0026#39;auto\u0026#39;), ]) # Change the bar mode fig.update_layout(barmode=\u0026#39;group\u0026#39;, title=\u0026#34;Statistic working accident of employees\u0026#34;, xaxis_title=\u0026#34;Woking Accident\u0026#34;, yaxis_title=\u0026#34;Number of employees\u0026#34;, legend_title=\u0026#34;Employee Type\u0026#34;) fig.show()   Nhận xét:\n Những người gặp sự cố nghỉ việc ít hơn những người không gặp sự cố.  Điều này khiến ta có vẻ hơi ngạc nhiên một chút, nhưng nếu phân tích kỹ càng hơn thì có thể sự cố trong công việc không phải lúc nào cũng ảnh hưởng tiêu cực. Nó có thể khiến cho công việc bớt nhàm chán, đơn điệu hơn. Vì thế mà dẫn đến kết quả nêu trên.\n2.9 Đánh giá sự ảnh hưởng của mức độ thỏa mãn # create dataframe with satisfaction level and left information df_sl = df_data[[\u0026#39;satisfaction_level\u0026#39;, \u0026#39;left\u0026#39;]] # draw violin chart fig = px.violin( df_sl, y=\u0026#39;satisfaction_level\u0026#39;, color=\u0026#39;left\u0026#39;, points=\u0026#39;all\u0026#39;, box=True, labels={\u0026#39;satisfaction_level\u0026#39;: \u0026#39;Satisfaction Level\u0026#39;, \u0026#39;left\u0026#39;:\u0026#39;Left/Stay\u0026#39;}, title=\u0026#39;Influence of sastisfaction level to left/stay company decision: 0 - Stay, 1 - Left\u0026#39;) fig.show()   Nhận xét:\n Những người nghỉ việc là những người có mức độ thỏa mãn nằm trong các khoảng: \u0026lt; 0.2, 0.3-0.5, và 0.6-0.8. Với mức độ thoả mãn \u0026lt; 0.2 thì đa số nhân viên nghỉ việc.  Tại sao nhân viên có mức độ thoả mãn cao lại nghỉ việc? Thông thường thì đó sẽ phải là nhóm người gắn bó lâu nhất chứ nhỉ? Ta tiếp tục phân tích sâu hơn.\n2.10 Phân nhóm nhân viên theo mức độ thỏa mãn và kết quả checkpoint a, Đối với các nhân viên nghỉ việc # draw scatter chart with satisfaction level and last evaluation information of people who leave company fig = px.scatter( df_data, x=df_data[\u0026#39;satisfaction_level\u0026#39;][df_data[\u0026#39;left\u0026#39;] == 1], y=df_data[\u0026#39;last_evaluation\u0026#39;][df_data[\u0026#39;left\u0026#39;] == 1], width=1000, height=800, title=\u0026#39;Employees who left\u0026#39;, labels={\u0026#39;x\u0026#39;:\u0026#39;Satisfaction level\u0026#39;, \u0026#39;y\u0026#39;:\u0026#39;Last Evaluation\u0026#39;} ) fig.show()   Nhận xét: Quan sát ta thấy, dựa theo mức độ thỏa mãn và kết quả checkpoint thì các nhân viên có thể phân chia các nhân viên nghỉ việc vào một trong 3 nhóm:\n Nhóm 1: Những người được đánh giá cao nhưng lại có mức độ thỏa mãn thấp. Đây có lẽ là những người giỏi trong công ty, họ làm được việc nhưng không cảm thấy hài lòng với công việc hiện tại nên nghỉ việc. Công ty nên tập trung vào nhóm đối tượng này. Nhóm 2: Những người bị đánh giá thấp, và bản thân họ cũng cảm thấy không vui khi làm việc tại công ty. Đối với những nhân viên thuộc nhóm này thì họ có nghỉ việc chắc cũng không phải là vấn đề quá lớn. Nhóm 3: Những người được đánh giá cao, và lại cũng đang cảm thấy rất thỏa mãn. Nhưng họ cũng nghỉ việc? Tại sao? Nhóm này đang chiếm số lượng nhân viên khá lớn. Phải chăng là do họ tìm được một cơ hội mới tốt hơn hẳn so với công ty hiện tại?  Ta thử áp dụng thuật toán phân cụm Kmeans với số cụm là 3 lên các nhân viên này xem sao:\n# create dataframe with satisfaction level and last evaluation information df_kmeans = df_data[df_data.left == 1].drop([\u0026#39;number_project\u0026#39;, \u0026#39;average_montly_hours\u0026#39;, \u0026#39;time_spend_company\u0026#39;, \u0026#39;Work_accident\u0026#39;, \u0026#39;left\u0026#39;, \u0026#39;promotion_last_5years\u0026#39;, \u0026#39;Department\u0026#39;, \u0026#39;salary\u0026#39;], axis=1) # cluster people who leave company base on satisfaction level and last evaluation information kmeans = KMeans(n_clusters=3, random_state=10).fit(df_kmeans) # print(kmeans.cluster_centers_) # print(kmeans.labels_) # add cluster ID into original dataset df_left = df_data[df_data.left == 1] df_left[\u0026#39;label\u0026#39;] = kmeans.labels_ df_left[\u0026#39;label\u0026#39;] = df_left[\u0026#39;label\u0026#39;].map({0:\u0026#39;Group 2\u0026#39;, 1:\u0026#39;Group 3\u0026#39;, 2:\u0026#39;Group 1\u0026#39;}) Kết quả:\n Lúc này 3 nhóm 1,2,3 đã trở nên rõ ràng hơn.\nTiếp tục xem xét số giờ làm việc trung bình trong tháng của các nhóm này:\n# draw displot chart group1_month_hour = df_left[df_left.label == \u0026#39;Group 1\u0026#39;].average_montly_hours group2_match_month_hour = df_left[df_left.label == \u0026#39;Group 2\u0026#39;].average_montly_hours group3_month_hour = df_left[df_left.label == \u0026#39;Group 3\u0026#39;].average_montly_hours hist_data = [group1_month_hour, group2_match_month_hour, group3_month_hour] group_labels = [\u0026#39;Group 1\u0026#39;, \u0026#39;Group 2\u0026#39;, \u0026#39;Group 3\u0026#39;] fig = ff.create_distplot(hist_data, group_labels, bin_size=2.0, show_curve=True, show_hist=True) fig.update_layout(title_text=\u0026#39;Leavers: Hours per month distribution\u0026#39;) fig.show()   Nhận xét:\n Nhóm 1 là nhóm có số giờ làm việc trung bình trong tháng cao nhất. Tiếp đến là nhóm 3, và thấp nhất là nhóm 2.  Có lẽ nguyên nhân khiến cho nhóm 1 không được thỏa mãn cho lắm là vì họ phải OT quá nhiều. Nhóm 2 làm việc lười nhất, nên hiệu quả không cao là điều dễ hiểu. Còn nhóm 3, họ không phải OT nhiều nên mức độ thoả mãn cao, và củng cố thêm giả thuyết là họ tìm được cơ hội mới tốt hơn ở cty hiện tại nên họ nghỉ việc.\nĐối với công ty, nên tập trung vào nhóm 2 nhiều hơn, quan tâm đến họ hơn, làm cho họ thỏa mãn hơn thì chắc chắn hiệu quả công việc của họ sẽ cao hơn. Còn lý do họ tìm được cơ hội mới tốt hơn, được công ty khác offer lương cao hơn, \u0026hellip; thì cũng rất khó để có thể giữ chân họ. :D\nb, Đối với các nhân viên không nghỉ việc # draw scatter chart with satisfaction level and last evaluation information of people who stay company fig = px.scatter( df_data, x=df_data[\u0026#39;satisfaction_level\u0026#39;][df_data[\u0026#39;left\u0026#39;] == 0], y=df_data[\u0026#39;last_evaluation\u0026#39;][df_data[\u0026#39;left\u0026#39;] == 0], width=1000, height=800, title=\u0026#39;Employees who stay\u0026#39;, labels={\u0026#39;x\u0026#39;:\u0026#39;Satisfaction level\u0026#39;, \u0026#39;y\u0026#39;:\u0026#39;Last Evaluation\u0026#39;} ) fig.show()   Các nhân viên này không có sự phân nhóm rõ rệt như trên nhưng nói chung vẫn tập trung khá nhiều vào vùng mà mức độ thỏa mãn, và điểm đánh giá cao. Chứng tỏ rằng, ko phải tất cả những nhân viên nhóm 3 bên trên đều nghỉ việc.\n3. Kết luận Như vậy là chúng ta đã cùng nhau phân tích khá chi tiết bộ dữ liệu khảo sát nhân viên. Các phân tích đều xoay quanh vấn đề tìm ra câu trả lời cho câu hỏi, tại sao nhân viên lại nghỉ việc?\nTóm tắt lại một số kết luận như sau:\n Những người mới vào công ty hoặc những người làm từ 6 năm trở lên rất ít nghỉ việc. Nếu công ty có thể giữ chân được nhân viên làm việc được 6 năm thì gần như chắc chắn họ sẽ còn gắn bó lâu dài hơn. Nhân viên phải OT quá nhiều, hoặc công việc quá nhàn nhã thì họ cũng dễ nghỉ việc. HR là bộ phận có tỉ lệ nghỉ viêc cao nhất, còn thấp nhất là các cán bộ quản lý. Những nhân viên bị đánh giá thấp sẽ có khả năng nghỉ việc cao hơn. Mức lương càng cao thì càng ít nghỉ việc. Sau 5 năm, nếu nhân viên không có sự thăng tiến thì họ cũng nghỉ việc. Công việc quá nhàm chán cũng làm cho tỉ lệ nghỉ việc tăng cao. Cần tập trung vào nhóm nhân viên được đánh giá cao, nhưng họ lại đang không thỏa mãn với công việc.  Mình hi vọng là đọc đến đây, bạn đã phân nào có được câu trả lời cho mình.\nTrong phần 2, mình sẽ hướng dẫn các bạn xây dựng mô hình ML dự đoán khả năng nghỉ việc của nhân viên, độ chính xác lên đến hơn 99%. Mời các bạn đón đọc vào tuần tiếp theo!\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/106_why_your_employee_quit_job/","tags":["Data Science","EDA"],"title":"Tại sao nhân viên của bạn lại nghỉ việc?"},{"categories":["Data Science","EDA"],"contents":"Gần 3 năm trước, mình có tham gia học/thi để lấy chứng chỉ IBM Data Science Professional Certificate do IBM cấp thông qua nền tảng học trực tuyến Coursera. Người tham gia phải học đủ 9 courses xoay quanh các kiến thức về Data Science (bây giờ số lượng courses đã tăng lên 10). Cuối mỗi courses là các bài tập về kiểm tra, mà người học phải làm và gửi lên hệ thống để được chấm điểm. Đặc biệt là courses cuối cùng, đó là một dạng bài tập lớn, một dự án tổng hợp, áp dụng các kiến thức đã được học trong suốt quá trình (Capstone Project) để giải quyết một dự án Data Science trong thực tế. Nếu vượt qua được dự án này thì người học sẽ được cấp chứng chỉ chuyên gia Data Science của IBM. Nếu bạn quan tâm có thể tìm hiểu và học tại đây.\nTrong bài này mình sẽ trình bày về dự án đó của mình để chia sẻ cùng mọi người!\n1. Định nghĩa vấn đề cần giải quyết Theo báo cáo xếp hạng chỉ số kinh tế toàn cầu, Nhật Bản hiện đang là nước đứng thứ 3, chỉ sau Mỹ và Trung Quốc. Tokyo, nơi mà mình may mắn có dịp được sống và làm việc trong gần 2 năm (2017-2018), cũng là một trong những thành phố hiện đại và năng động nhất trên thế giới. Vì thế, sẽ không có gì ngạc nhiên khi ngày càng có nhiều doanh nhân, nhà đầu tư chọn Tokyo là điểm đến lý tưởng cho các dự án đầy tham vọng của mình. Nếu bạn cũng là một người trong số đó, hay thậm chí chỉ đơn giản là muốn mở một start up business nhỏ nhỏ của mình thì bài viết này có thể hữu ích cho bạn.\nNói đến Tokyo, bên cạnh những thắng cảnh du lịch nổi tiếng thì mình ấn tượng mạnh với văn hóa ẩm thực tại đó. Các nhà hàng Sushi, Rame, Udon, \u0026hellip; hay các quán Bar, Pub, \u0026hellip; các của hàng Caffe, \u0026hellip; luôn chật kín người vào thời gian nghỉ trong ngày. Bạn cũng không cần phải bỏ ra một số tiền quá lớn để có thể thưởng thức các món ăn đặc sắc ở đây. Một bữa trưa văn phòng, theo mình nhớ thì có giá tầm 500-1000 Yên (hoặc có thể do mình ít tiền nên chọn quán rẻ tiền). Sẽ là một ý tưởng không tồi nếu bạn muốn mở một nhà hàng tại Tokyo để phục vụ các món ăn ngon cho khách hàng. Theo thống kê, tỷ suất lợi nhuận cho một nhà hàng kinh doanh tốt dao động trong khoảng 15-20%. Thậm chí có thể lên đến 30% vào các dịp đặc biệt trong năm, như báo cáo tại đây.\nVề mặt địa lý, Tokyo bao gồm 23 quận đặc biệt - special wards - 東京都区部, trong đó có 7 quận được coi là Business Wards gồm \u0026ndash; Chiyoda (千代田区), Chuo (中央区), Shinjuku (新宿区), Shibuya (渋谷区), Shinagawa (品川区), Minato (港区), và Meguro (目黒区) vì sự sầm uất trong kinh doan buôn bán của nó. Chúng ta sẽ tập trung vào phân tích số liệu của 7 quận đó trong bài này.\nTóm lại, vấn đề cần giải quyết trong bài này là: Tìm ra một địa điểm thích hợp để mở một nhà hàng mới tại Tokyo.\nNội dung bài viết bao gồm các phần:\n Chuẩn bị dữ liệu Phân tích dữ liệu Kết luận (trả lời vấn đề nêu ra ở trên)  Không giống như bài trước, khi mà chúng ta đã có sẵn dataset, trong bài này, chúng ta phải tự đi cào dữ liệu từ trên Internet về để phân tích.\n2. Đối tượng độc giả hướng đến Nội dung của bài viết này sẽ phù hợp với những nhóm độc giả sau:\n Những người làm kinh doanh và đang quan tâm đến việc đầu tư hoặc mở một nhà hàng mới tại Tokyo. Những người đang muốn tìm các nhà hàng, quán xá vui chơi, ăn uống, nghỉ ngơi gần nhà, công ty của họ. Những người đang muốn trở thành một Data Scientist, bởi vì sẽ có rất nhiều kỹ thuật chuẩn bị, phân tích, đánh giá dữ liệu được sử dụng trong bài này.  3. Data Preparation 3.1 Tìm thông tin địa lý về các quận của Tokyo Thông tin địa lý của 23 quận thuộc thành phố Tokyo được cung cấp tại địa chỉ Special Wards of Tokyo trên Wikipedia. Các thông tin này ở dạng bảng nên ta có thể dễ dàng lấy về bằng 2 thư viện request và Beautifulsoup4.\n# get html table code response_obj = requests.get(\u0026#39;https://en.wikipedia.org/wiki/Special_wards_of_Tokyo\u0026#39;).text soup = BeautifulSoup(response_obj,\u0026#39;lxml\u0026#39;) tko_wards_table = soup.find(\u0026#39;table\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;wikitable sortable\u0026#39;}) Sau một vài thao tác biến đổi thì ta có được thông tin về diện tích, dân số, các khu phố chính như sau:  Tiếp theo, ta sẽ lấy thông tin kinh độ và vĩ độ của từng quận, sử dụng thư viện Geophy.\n# get latitude \u0026amp; longitude of major districts geolocator = Nominatim(user_agent=\u0026#39;Tokyo_Explorer\u0026#39;) df_tko[\u0026#39;Ward Coord\u0026#39;] = df_tko[\u0026#39;Ward Name\u0026#39;].apply(geolocator.geocode).apply(lambda x: (x.latitude, x.longitude)) # separate latitude \u0026amp; longtitude into 2 different columns df_tko[[\u0026#39;Ward Latitude\u0026#39;, \u0026#39;Ward Longtitude\u0026#39;]] = df_tko[\u0026#39;Ward Coord\u0026#39;].apply(pd.Series) df_tko.drop([\u0026#39;Ward Coord\u0026#39;], axis=1, inplace=True)   Chú ý là một số tên quận có cách viết khác so với bình thường (VD: Chūō, Bunkyō, \u0026hellip;). Ta phải sửa lại cho đúng để có thể lấy được chính xác tọa độ của chúng như kết quả trong bảng trên.\n3.2 Tìm thông tin về giá đất tại mỗi quận Giá đất tại mỗi quận cũng là một yếu tố quan trọng để quyết định nơi sẽ đặt nhà hàng mới. Do đó, ta sẽ lấy thêm thông tin này để phân tích từ trang thông tin tổng hợp land market value area in Tokyo. Nó cũng ở dạng bảng nên ta vẫn dùng hai thư viện requests và Beautifulsoup4 để cào về.\n Bạn có thể lưu lại các thông tin đã có vào file csv để dễ dàng sử dụng về sau.\n3.3 Chọn ra 7 quận để tiến hành phân tích Như đã trình bày trong phần đặt vấn đề bên trên, chúng ta sẽ chỉ tập trung vào 7 business wards để phân tích trong bài này.\nbusiness_wards = [\u0026#39;Chiyoda\u0026#39;, \u0026#39;Chuo\u0026#39;, \u0026#39;Minato\u0026#39;, \u0026#39;Shinjuku\u0026#39;, \u0026#39;Shibuya\u0026#39;, \u0026#39;Shinagawa\u0026#39;, \u0026#39;Meguro\u0026#39;] df_tko_spe = df_tko.loc[df_tko[\u0026#39;Ward Name\u0026#39;].isin(business_wards)].reset_index(drop=True)   3.4 Tìm thông tin về các địa điểm của từng quận Có thể bạn đã biết, Foursquare là mạng xã hội chia sẻ địa điểm hàng đầu trên thế giới. Trên đó có tương đối đầy đủ các địa điểm được chi sẻ bởi người dùng khắp nơi trên thế giới. Các địa điểm được phân chia thành các nhóm liên quan đến nhau để người dùng có thể dễ dàng tìm kiếm.\nĐối với các nhà phát triển, Foursquare cung cấp các API để cho các ứng dụng gọi đến và trả về danh sách các địa điểm dựa vào tọa độ được cung cấp. Để sử dụng được các API này, bạn phải đăng ký tài khoản, và lựa chọn một trong 3 gói dịch vụ của Foursquare.\nSau khi thành công đang kí tài khoản và tạo dự án trên Foursquare thì chúng ta sẽ được cung cấp 2 thông tin là CLIENT_ID và SECRET_ID để sử dụng khi gọi API. API sử dụng trong bài này Venue Recommendation. Khi gọi API để lấy danh sách địa điểm mong muốn, chúng ta có thể đưa thêm các điều kiện như phạm vi tìm kiếm, số lượng tối đa, \u0026hellip; và còn rất nhiều tham số khác để giúp ta lấy được các địa điểm sát với mong muốn hơn.\n# function to get venues from foursquare through foursquare API def getVenues(latitude, longitude, radius=1000): venues_100_list = [] for lat, long in zip(latitude, longitude): url = \u0026#39;https://api.foursquare.com/v2/venues/explore?\u0026amp;client_id={}\u0026amp;client_secret={}\u0026amp;v={}\u0026amp;ll={},{}\u0026amp;radius={}\u0026amp;limit={}\u0026#39;.format( CLIENT_ID, CLIENT_SECRET, VERSION, lat, long, radius, limit) result = requests.get(url).json()[\u0026#39;response\u0026#39;][\u0026#39;groups\u0026#39;][0][\u0026#39;items\u0026#39;] venues_100_list.append(result) return venues_100_list Ở đây, ta chọn 100 địa điểm trong vòng bán kính 1km của mỗi quận. Kết quả trả về dưới dạng Json, sau khi bóc tách ra các thông tin cần thiết thì ta được như sau:\n Như vậy là ta đã có đầy đủ dữ liệu. Mình cũng đã kiểm tra, dữ liệu mà ta cào từ trên Internet về tương đối chuẩn, không bị thiếu. Mặc dù có một vài sai sót nhỏ nhưng mình cũng đã chỉnh sửa luôn trong quá trình chuẩn bị gì liệu bên trên nên ta có thể tiến hành phân tích luôn mà không cần qua bước làm sạch dữ liệu.\n4. Explore Data Analysis 4.1 So sánh diện tích giữa các quận # draw pie chart fig = px.pie(df_tko_spe, values=df_tko_spe[\u0026#39;Ward Area (Km2)\u0026#39;], names=df_tko_spe[\u0026#39;Ward Name\u0026#39;], title=\u0026#39;Percentage in area of 23 wards Tokyo\u0026#39;, color_discrete_sequence=px.colors.sequential.Rainbow) fig.update_layout(margin=dict(t=10, b=10, l=0, r=0)) fig.show()   Xét về diện tích thì quận lớn nhất là Shinagawa và quận nhỏ nhất là Chuo. Nếu giả sử Tokyo chỉ bao gồm 7 quận như đang xét thì 20.2% và 9.03% là phần trăm tương ứng mà Shinagawa và Chuo chiếm của Tokyo. Hai quận Shinjuku và Shibuya xếp ở vị trí thứ 3 và 4 với 16.1% và 13.4%.\n4.2 So sánh dân số và mật độ dân cư # draw group bar chart fig = go.Figure( data=[ go.Bar(name=\u0026#34;Density\u0026#34;, x=df_tko_spe[\u0026#39;Ward Name\u0026#39;], y=df_tko_spe[\u0026#39;Density (/Km2)\u0026#39;], text=df_tko_spe[\u0026#39;Density (/Km2)\u0026#39;], textposition=\u0026#39;auto\u0026#39;), go.Bar(name=\u0026#34;Population\u0026#34;, x=df_tko_spe[\u0026#39;Ward Name\u0026#39;], y=df_tko_spe[\u0026#39;Population\u0026#39;], text=df_tko_spe[\u0026#39;Population\u0026#39;], textposition=\u0026#39;auto\u0026#39;), ], layout=go.Layout(title=\u0026#34;Compare population and density of 7 wards\u0026#34;, yaxis_title=\u0026#34;Value\u0026#34;, xaxis_title=\u0026#34;Wards\u0026#34;) )   Xét về dân số thì Shinagawa vẫn là quận đứng đầu với gần 400 nghìn người. Tiếp đến là Shinjuku và Meguro, \u0026hellip; Quận có dân số ít nhất là Chiyoda với chỉ gần 60 nghìn người.\nXét về mật độ dân số, không ngoài dự tính khi Shinagawa có con số cao nhất, 2284 người/$Km^2$. Chiyoda và Chuo xấp xỉ nhau ở vị trí cuối cùng với các con số tương ứng là 1166 người/$Km^2$ và 1021 người/$Km^2$. Shinjuku và Shibuya là 2 quận khá nổi tiếng về mức độ sầm uất với các hoạt động vui chơi, giải trí hết sức hoành tráng nên không có gì ngạc nhiên khi mật độ dân số của chúng khá cao, 1862 người/$Km^2$ và 15080 người/$Km^2$.\n4.3 So sánh giá đất trung bình của các quận # draw bar chart fig = px.bar(df_tko_spe, x=\u0026#39;Ward Name\u0026#39;, y=\u0026#39;Price Avg (JPY/Sq.M)\u0026#39;, color=\u0026#39;Price Avg (JPY/Sq.M)\u0026#39;, title=\u0026#34;Average of land price in Tokyo\u0026#34;, labels={\u0026#39;Ward Name\u0026#39;: \u0026#39;Wards\u0026#39;, \u0026#39;Price Avg (JPY/Sq.M)\u0026#39;: \u0026#39;Average Land Price ((JPY/Sq.M)\u0026#39;}, text=\u0026#39;Price Avg (JPY/Sq.M)\u0026#39; )   Quận có giá đất đắt đỏ nhất là Chiyoda với gần 2.8 triệu Yên/$Km^2$, và Shinagawa là quận rẻ nhất, chỉ khoảng 78 Man Yên/$Km^2$. Chắc có lẽ vì Shinagawa có nhiều đất quá, :D. Các vị trí tiếp theo sau vị trí đứng đầu là Minato, Chuo, Shibuya, Shinjuku, và Meguro.\n4.4 Hiển thị vị trí của các quận lên bản đồ # create map of Tokyo with 5 major districts are displayed tko_map = folium.Map(location=[tko_lat, tko_lng], zoom_start=12) for lat, long, label in zip(df_tko_spe[\u0026#39;Ward Latitude\u0026#39;], df_tko_spe[\u0026#39;Ward Longtitude\u0026#39;], df_tko_spe[\u0026#39;Ward Name\u0026#39;]): label = folium.Popup(label, parser_html=True) folium.CircleMarker( [lat, long], radius=20, popup=label, color=\u0026#39;magenta\u0026#39;, fill=True, fill_color=\u0026#39;#3186cc\u0026#39;, fill_opacity=0.7 ).add_to(tko_map)   4.5 Thống kê 10 nhóm địa điểm phổ biến nhất tất cả các quận # statistic top 10 categories df_tko_venues_top10 = df_tko_venues[\u0026#39;Venue Category\u0026#39;].value_counts()[0:10].to_frame(name=\u0026#39;Frequency\u0026#39;).reset_index() df_tko_venues_top10.rename(index=str, columns={\u0026#39;index\u0026#39;:\u0026#39;Venue Category\u0026#39;}, inplace=True) # draw bar chart fig = px.bar(df_tko_venues_top10, x=\u0026#39;Venue Category\u0026#39;, y=\u0026#39;Frequency\u0026#39;, color=\u0026#39;Frequency\u0026#39;, title=\u0026#34;Top 10 Most Frequency Occuring Venues in 7 wards of Tokyo\u0026#34;, labels={\u0026#39;Venue Category\u0026#39;: \u0026#39;Venues Category\u0026#39;}, text=\u0026#39;Frequency\u0026#39; )   Xét về tổng thể, các nhà hàng kiểu/món Nhật chiếm đa số, tất nhiên rồi, vì đây là đất nước Nhật Bản mà. Tiếp theo là Ramen, một món ăn quá quen thuộc của người dân Nhật Bản. Hơi ngạc nhiên là nhà hàng Sushi không thấy xuất hiện trong danh sách này.\n 4.6 Thống kê nhóm 5 địa điểm phổ biến tại mỗi quận # display top 5 venues common of each ward in bar charts for df_top5 in df_tko_venues_top5_list: # draw bar chart fig = px.bar(df_top5[1], x=\u0026#39;Venue Category\u0026#39;, y=\u0026#39;Frequency Score\u0026#39;, color=\u0026#39;Frequency Score\u0026#39;, title=\u0026#34;Top 5 Most Frequency Occuring Venues in {} wards of Tokyo\u0026#34;.format(df_top5[0]), labels={\u0026#39;Venue Category\u0026#39;: \u0026#39;Venues Category\u0026#39;, \u0026#39;Frequency Score\u0026#39;:\u0026#39;Frequency Score\u0026#39;}, text=\u0026#39;Frequency Score\u0026#39; )         Caffe (2), Sushi, Convenience Store (2), Sake Bar, Japanese Restaurant là những nhóm địa điểm đứng đầu về mức độ phổ biến trong các quận. Ngoài ra còn có nhà hàng Tempura, các nhà hàng món ăn nước ngoài (Chinese, Italian, French, \u0026hellip;), nhà hàng BBQ, nhà hàng Ramen, nhà hàng Soba, Book Store, \u0026hellip;\nBởi vì ta đang quan tâm đến việc mở nhà hàng mới, nên mình sẽ thu gọn lại chỉ xem xét nhóm địa điểm nhà hàng.\n4.7 Thống kê số lượng nhà hàng ở mỗi quận Có nhiều cách để thể hiện số lượng nhà hàng của mỗi quận. Ở đây mình giới thiệu 3 cách cho các bạn tham khảo.\n Cách thứ nhất: Sử dụng đồ thị dạng Bar  df_tko_venues_rest = df_tko_venues[df_tko_venues[\u0026#39;Venue Category\u0026#39;].str.contains(\u0026#39;Restaurant\u0026#39;)] df_tko_venues_rest = df_tko_venues_rest.reset_index(drop=True) ... fig = px.bar(df_tko_venues_rest_count, # x=\u0026#39;District\u0026#39;,  x=\u0026#39;Ward Name\u0026#39;, y=\u0026#39;Number of Rest\u0026#39;, color=\u0026#39;Number of Rest\u0026#39;, title=\u0026#34;Number of Restaurants in each Ward\u0026#34;, labels={\u0026#39;Ward Name\u0026#39;: \u0026#39;Ward\u0026#39;, \u0026#39;Number of Rest\u0026#39;:\u0026#39;Number of Restaurant\u0026#39;}, text=\u0026#39;Number of Rest\u0026#39; )   Cách thứ 2: Sử dụng bản đồ HeadMap, màu càng đậm thì số lượng nhà hàng càng lớn và ngược lại.\nhm_data = df_tko_venues_rest_count[[\u0026#39;Ward Latitude\u0026#39;, \u0026#39;Ward Longtitude\u0026#39;, \u0026#39;Number of Rest\u0026#39;]] hm_data = hm_data.values.tolist() # hm_data[:5] hmap = folium.Map(location=[tko_lat, tko_lng], control_scale=True, attr=\u0026#39;USGS style\u0026#39;, zoom_start=5) HeatMap(hm_data, rasdius=10).add_to(hmap) # hmap   Trong vòng 1 Km từ trung tâm thì Chuo là quận có số nhà hàng nhiều nhất, 76. Tiếp đó là Minato, Chiyoda lần lượt là 54 và 45. 3 quận Shibuya, Shinagawa, và Shinjuku có số lượng nhà hàng xấp xỉ nhau (31, 30, 29). Đứng ở vị trí thấp nhất là Meguro, chỉ có 24 nhà hàng.\nCách thứ 3: Sử dụng bản đồ kết hợp với CircleMarker. Bán kính của CirleMarker càng lớn thì số lượng nhà hàng càng nhiều và ngược lại.\nfor lat, lon, poi, cluster in zip(df_tko_spe_all[\u0026#39;Ward Latitude\u0026#39;], df_tko_spe_all[\u0026#39;Ward Longtitude\u0026#39;], df_tko_spe_all[\u0026#39;Ward Name\u0026#39;], df_tko_spe_all[\u0026#39;Cluster ID\u0026#39;]): label = folium.Popup(str(poi) + \u0026#39; Cluster \u0026#39; + str(cluster), parse_html=True) folium.CircleMarker( [lat, lon], radius=n_rest[wards.index(poi)]*0.5, popup=label, color=rainbow[cluster-1], fill=True, fill_color=rainbow[cluster-1], fill_opacity=0.7).add_to(map_clusters)   Cách thể hiện số 2\u0026amp;3 còn cho ta biết cụ thể vị trí của mỗi quận trên bản đồ địa lý.\nNgoài ra, ở cách số 3, mình cũng đưa thêm một thông tin nữa, đó là việc phân cụm các quận thành 3 cụm khác nhau dựa trên mức độ tương tự giữa các nhóm địa điểm của mỗi quận. Theo đó thì Shibuya và Minato thuộc cụm số 1 (màu đỏ), Shinjuku, Chiyoda và Shinagawa thuộc cụm số 2 (màu tím). Cuối cùng là Chuo và Meguro thuộc cụm số 3 (màu xanh).\n5. Tổng kết và đánh giá Theo như toàn bộ phân tích trên thì Shinagawa chắc chắc sẽ là nơi tiềm năng để bắt đầu một business mới, bởi vì:\n Giá đất của Shinagawa rẻ nhất so với các quận khác. Dân số của Shinagawa đông nhất so với các quận khác. Hiện nay, số lượng nhà hàng tại Shinagawa còn khá khiêm tốn, Convenience Store đang chiếm đa số tại đây.  Do vậy, nếu bạn mở nhà hàng tại đây thì sẽ ít phải cạnh tranh với các đối thủ khác so với các quận còn lại.\n6. Thảo luận Những phân tích và đánh giá trên đây mới chỉ đề cập đến một vài khía cạnh nhỏ của bài toán thực tế và còn nhiều hạn chê:\n Dữ liệu phụ thuộc vào Foursquare (đây là một nền tảng chia sẻ thông tin địa điểm của người dùng nên thông tin có thể chưa thực sự chính xác) Số lượng quận đươc đem ra phân tích ở đây mới là 7. Số lượng địa điểm và phạm vi tìm kiếm địa điểm vẫn còn hạn chế (100 địa điểm trong vòng 1km) Chưa xét đến các yếu tố khác như: lịch sử, văn hóa, giao thông, \u0026hellip;  Mặc dù vậy, những phân tích này vẫn rất có giá trị trong việc làm thông tin tham khảo và mở ra những hướng phân tích mới khi triển khai vào thực tế.\n7. Kết luận Theo thiết kế của IBM thì nếu người học dành ra khoảng 3h/tuần để học thì sẽ hoàn thành chương trình này trong 11 tháng. Mình thì tập trung học nên mất gần 2 tháng để có được chứng chỉ.\n Mình nghĩ đây là 1 chương trình học rất bổ ích và cần thiết cho những người muốn theo đuổi con đường trở thành một Data Scientist. Đặc biệt là Capstone Project, không dễ dàng để vượt qua nó nhưng khi đã làm được rồi thì cảm giác rất phấn khích. Như kiểu mình đã trở thành một Data Scientist thực thụ rồi, :D\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\n8. Tham khảo Link trong bài.\n","permalink":"https://tiensu.github.io/blog/105_eda_explore_tokyo/","tags":["Data Science","EDA"],"title":"Khám phá thành phố Tokyo xinh đẹp"},{"categories":["Data Science","EDA"],"contents":"Nhật Bản là một trong những quốc gia phải gánh chịu hậu quả nặng nề của đại dịch Covdid-19. Tính từ tháng 03/2020 đến này đã có hơn một triệu ca mắc Covid-19, hàng trăm nghìn người phải nhập viên, và hàng chục nghìn người tử vong. Những con số còn lớn hơn cả thiệt hai của trận sóng thần năm 2011.\nTrong bài hôm nay, chúng ta sẽ cùng nhau phân tích cụ thể, chi tiết hơn về tình hình này thông qua các bảng biểu đồ thị, nhận xét, đánh giá. Dữ liệu để phân tích là dữ liệu thực tế được cập nhật đến ngày 22/08/2021.\nNội dung bài viết bao gồm các phần:\n Chuẩn bị và tìm hiểu dữ liệu. Phân tích dữ liệu: phần này sẽ được chia thành 4 hạng mục nhỏ hơn:  Phân tích dữ liệu trên toàn lãnh thổ Nhật Bản theo 3 khu vực: Nội địa, Sân bay và Khu vực cách ly những người từ nước ngoài trở về. Phân tích dữ liệu chi tiết khu vực nội địa. Phân tích dữ liệu trên toàn lãnh thổ Nhật Bản theo 47 tỉnh thành phố. Phân tích dữ liệu chi tiết tại thành phố Tokyo.    OK, hãy cùng bắt đầu!\n1. Chuân bị dữ liệu Trước tiên, hãy download dữ liệu tại đây và tại đây.\nCó tất cả 5 files như mô tả trong bảng sau:\n   STT Tên file Mô tả Cập nhật mới nhất Ghi chú     1 covid_jpn_total.csv Thống kê số lượng người mắc bênh, tử vong, cần nhập viện, \u0026hellip; theo 3 khu vực: Nội địa, sân bay và khu vực cách ly người trở về. 22-8-2021 Không bao gồm bệnh nhân trên tàu Diamond Princess (Yokahama) và tàu Costa Atlantica (Kanasaki)   2 covid_jpn_prefecture.csv Thống kê số lượng người mắc bênh, tử vong, cần nhập viện, \u0026hellip; theo từng tỉnh, thành phố. 21-8-2021 Không bao gồm bệnh nhân trên tàu Diamond Princess (Yokahama) và tàu Costa Atlantica (Kanasaki)   3 covid_jpn_metadata.csv Một số thông tin bổ sung như mật độ dân cư, số giường bệnh, \u0026hellip; của mỗi tỉnh, thành phố. 22-8-2021    4 tokyo_cases_byarea.csv Thống kê số lượng người mắc bệnh theo từng quận của Tokyo. 09-01-2021    5 tokyo_covid19_patients.csv Thống kê thông tin về giới tính, độ tuổi, tình trạng sức khỏe của các bệnh nhân ở Tokyo. 09-01-2021     Hơi tiếc một chút là hai file cuối cùng chỉ được cập nhật đến ngày 09-01-2021.\nSố liệu thống kê trong mỗi ngày là kết quả tích lũy từ những ngày trước đó.\nChi tiết từng trường thông tin trong mỗi file như sau:\n File covid_jpn_total.csv:     STT Tên Feature Ý nghĩa     1 Date Ngày thu thập dữ liệu   2 Location Nơi kiểm tra: Domestic, Airport hoặc Returnee   3 Positive Số ca duơng tính   4 Tested Số lượng người được thử nghiệm   5 Symptomatic Số ca bệnh có triệu chứng rõ ràng   6 Asymptomatic Số ca bệnh không có triệu chứng rõ ràng   7 Sym-unknown Số ca bệnh mà không biết có triệu chứng hay không   8 Hosp_require Số ca bệnh cần nhập viện điều trị   9 Hosp_mild Số ca bệnh có triệu chứng nhẹ   10 Hosp_severe Số ca bệnh trong tình trạng nguy kịch   11 Hosp_unknown Số ca bệnh mà không biết có triệu chứng hay không   12 Hosp_waiting Số ca bệnh cần nhập viện nhưng không được đáp ứng   13 Discharged Số ca khỏi bệnh được xuất viện   14 Fatal Số người tử vong   15 Vacinated_1st Số người được tiêm vacine mũi thứ nhất   16 Vacinated_2nd Số người được tiêm vacine mũi thứ hai     File covid_jpn_prefecture.csv:     STT Tên Feature Ý nghĩa     1 Date Ngày thu thập dữ liệu   2 Prefecture Tên tỉnh thành (47 tỉnh, thành phố)   3 Positive Số ca duơng tính   4 Tested Số lượng người được thử nghiệm   5 Discharged Số ca khỏi bệnh được xuất viện   6 Fatal Số ca bệnh tử vong   7 Hosp_require Số ca bệnh cần nhập viện điều trị   8 Hosp_severe Số ca bệnh trong tình trạng nguy kịch     File covid_jpn_metadata.csv:     STT Tên Feature Ý nghĩa     1 Prefecture Tên tỉnh thành (47 tỉnh, thành phố)   2 Category Các nhóm thông tin: Dân số, Giường bệnh, \u0026hellip;   3 Item Sub-category: Total, Care, \u0026hellip;   4 Value Giá trị tuơng ứng với các categories, sub-categories   5 Date Ngày thu thập dữ liệu   6 Primary_source Nguồn thu thập dữ liệu thứ nhất   7 Secondary_source Nguồn thu thập dữ liệu thứ hai     File tokyo_cases_byarea.csv:     STT Tên Feature Ý nghĩa     1 #No Số thứ tự   2 Date Ngày thu thập dữ liệu   3 Date (Onset) Ngày bắt đầu có triệu chứng   4 Region Khu vực nơi mà bệnh nhân sinh sống   5 Age Độ tuổi của bệnh nhân   6 Gender Giới tính của bệnh nhân   7 Situation Tình trạng của bệnh nhân     File tokyo_covid19_patients.csv:     STT Tên Feature Ý nghĩa     1 Are Khu vực mà thành phố thộc về: Tama area, Special ward area   2 Municipality Tên thành phố   3 Positive Case Số ca duơng tính   4 Region Khu vực nơi mà bệnh nhân sinh sống   5 Code Mã thành phố    2. Phân tích dữ liệu 2.1 Phân tích dữ liệu trên toàn lãnh thổ Nhật Bản Đầu tiên, import các thư viện cần thiết:  a, Load dữ liệu  Load dữ liệu từ file csv và hiển thị một vài dữ liệu mẫu     Hiển thị thông tin chi tiết hơn dữ liệu   Ta thấy có 16 features như trong phần mô tả bên trên. Điều cần chú ý ở đây là đa số các features đều có missing data. Thậm chí một số features có số lượng missing data rất lớn, vượt quá 50%: Symptomatic, Asymptomatic, Sym-unknown, Hosp_mild, Hosp_unknown, Hosp_waiting, Vaccinated_1st, Vaccinated_2nd. Ta sẽ cần giải quyết vấn đề này trong phần Data Cleaning trước khi tiến hành phân tích.\nb, Data Cleaning Đầu tiên, tạo một bản sao của dữ liệu gốc. Đây là dữ liệu mà ta sẽ làm việc trên đó:\n Tiếp theo, tiến hành một số bước làm sạch dữ liệu như sau:\n Loại bỏ tất cả các hàng mà toàn bộ giá trị của nó đều là Null/NaN.    Loại bỏ tất cả các features mà chứa số lượng missing data vượt quá 50%:    Đặt lại tên cho các features để cho dễ hiểu hơn:    Chuyển feature \u0026lsquo;Date\u0026rsquo; từ dạng Object sang dạng Datetime để dễ dàng thực hiện các theo tác về sau:    Bổ sung thêm các ngày còn thiếu vào dữ liệu  Không phải tất cả các ngày đều có mặt trong dữ liệu. Để cho đầy đủ, ta sẽ thêm chúng vào bằng cách sử dụng kỹ thuật Resample dữ liệu theo ngày.\n  Sắp xếp lại dữ liệu theo chiều giảm dần giá trị của feature \u0026lsquo;Date\u0026rsquo;    Thay thế các missing data bởi các giá trị sinh ra bằng phương pháp nội suy::    Tạo ra một feature mới là tỉ lệ giữa số ca dương tính trên số người được xét nghiệm. Feature này được sử dụng thay thể cho feature \u0026lsquo;Tested\u0026rsquo; bởi vì các giá trị của feature \u0026lsquo;Tested\u0026rsquo; thường lớn hơn rất nhiều lần so với các giá trị của các features khác. Điều này làm cho việc so sánh trên đồ thị không thực sự mang lại nhiều thông tin.    Cuối cùng là xóa feature \u0026lsquo;Tested\u0026rsquo;.   c, Tiến hành EDA  So sánh sự thay đổi về số ca mắc Covid-19 tại 3 khu vực: Domestic, Airport, và Returnee    Quan sát thấy số cả dương tính trong ngày 13/08/2021 tại khu vực Domestic cao một các bất thường. Khả năng cao là có sự nhầm lẫn ở đây. Vì vậy, cách tốt nhất là loại bỏ nó đi để tránh làm Outlie dữ liệu:    Đồ thị thể hiện   Khu vực nội địa của Nhật Bản có sự tăng về só ca nhiễm Covid-19 nhanh nhất so với các khu vực còn lại.\n So sánh sự thay đổi về số lượng bệnh nhân Covid-19 tử vọng tại 3 khu vực: Domestic, Airport, và Returnee    Rất dễ hiểu khi số ca bệnh không qua khỏi tại khu vực Domestic tăng nhanh nhất so với các khu vực còn lại, phù hợp với logic bên trên.\n2.2 Phân tích chi tiết dữ liệu trong khu vực nội địa của Nhật Bản a,b Load Data \u0026amp; Date Cleaning Hai phần này giống hệt mục 2.1 vì phần này sử dụng chung file dataset với phần 2.1.\nc, Tiến hành EDA  Đầu tiên, lựa chọn những mẫu dữ liệu thuộc khu vực Domestic:    Chuyển Date feature thành index của dataset để thuận tiện trong việc vẽ biểu đồ:    So sánh sự thay đổi về số ca dương tính, số ca dương tính trên số ca xét nghiệm, số ca tử vong, số ca cần nhập viện, số ca có triệu chứng nặng, và số ca khỏi bệnh được xuất viện   Đồ thị dạng Line:\n Nhận thấy số ca dương tính và số bệnh nhân được chữa khỏi đều tăng nhanh, nhưng khoảng cách giữa hai nhóm được duy trì khá tốt. Đây có thể coi làm một tín hiệu đáng mừng.\nĐồ thị dạnh Bar trong video:\n   Để có cái nhìn rõ hơn về sự thay đổi của từng nhóm người bên trên, ta có thể thể hiện riêng số liệu của các nhóm đó lên đồ thị. Ví dụ:   Số ca tử vong cũng tăng rất nhanh, liên tục từ 03/2020 đến hiện tại, đặc biệt là từ đầu năm 2021.\n Số lượng người bệnh cần nhập viện có lúc tăng, lúc giảm nhưng xu hướng chung đến hiện tại vẫn là tăng.\n Tỉ lệ giữa số ca dương tính và số người được xét nghiệm có xu hướng giảm từ 03/2020 đến 07/2020, sau đó duy trì ở mức tương đối ổn định đến nay.\n Phân tích tình hình hiện tại (ngày 22/08/2021)   So sánh số ca dương tính, số ca dương tính trên số ca xét nghiệm, số ca tử vong, số ca cần nhập viện, số ca có triệu chứng nặng, và số ca khỏi bệnh được xuất viện\n   Tính đến ngày 21/08/2021, tổng số ca dương tính được ghi nhận là 1273652, số ca chữa khỏi là 1048617, số bênh nhân cần nhập viện là203540, và số bệnh nhân tử vong là 15589.\n2.3 Phân tích dữ liệu của 47 tỉnh, thành phố. Import thư viện:\n a, Load dữ liệu  Load dữ liệu từ file csv và hiển thị ví dụ một số mẫu     Hiển thị thông tin về các features:    Hiển thị danh sách 47 tỉnh:   b, Data Cleaning  Tạo một bản copy của dữ liệu gốc:    Đổi tên các features để cho dễ nhớ hơn:    Xóa các hàng mà toàn bộ giá trị là missing data:    Chuyển feature \u0026lsquo;Date\u0026rsquo; sang dạng DateTime để thuận tiện xử lý về sau:    Bổ sung thêm các ngày còn thiếu vào dữ liệu  Không phải tất cả các ngày đều có mặt trong dữ liệu. Để cho đầy đủ, ta sẽ thêm chúng vào bằng cách sử dụng kỹ thuật Resample dữ liệu theo ngày.\n  Sắp xếp lại dữ liệu theo chiều giảm dần giá trị của feature \u0026lsquo;Date\u0026rsquo;    Thay thế các missing data bởi các giá trị sinh ra bằng phương pháp nội suy::   c, Tiến hành EDA  So sánh sự thay đổi về số ca dương tính của 47 tỉnh, thành theo thời gian.    Tất cả các tỉnh thành đều chứng kiến sự tăng về số ca dương tính với Covid-19. Trong đó, Tokyo là thành phố có tốc độ tăng nhanh nhất so với các tình thành còn lại.\n   So sánh sự thay đổi về số người chết vì Covid-19 của 47 tỉnh, thành theo thời gian.    Tương ứng với số ca dương tính thì số người chết cũng có xu hướng tăng ở cả 47 tỉnh, thành. Từ tháng 03/2020 đến nửa đầu tháng 06/2021, Tokyo luôn có số ca bệnh tử vong cao nhất cả nước, nhưng sau đó thì Osaka đã vượt lên dẫn đầu.\n So sánh sự thay đổi về số bệnh nhân khỏi bệnh được xuất viện của 47 tỉnh, thành theo thời gian.    Tokyo vẫn là địa phương dẫn đầu và bỏ khá xa các địa phương khác về co số này. Sau Tokyo là Osaka.\n$$$ Phân tích số liệu mới nhất (ngày 21/08/2021)\n Chọn ra các số liệu ghi nhận trong ngày 21/08/2021 của 47 tỉnh, thành.    Thêm mới feature \u0026lsquo;Dead Rate\u0026rsquo; thể hiện tỉ lệ giữa số người chết và số người mắc bệnh.    Sắp xếp dữ liệu theo chiều giảm dần của số ca dương tính.    So sánh số lượng bênh nhân Covid-19 giữa các tỉnh, thành:   Hiện tại thì Tokyo đang có số ca bệnh Covid-19 lớn nhất, gần gấp đôi con số của địa phương xếp ngay sau là Osaka.\n So sánh số người chết giữa các tỉnh, thành:   Tuy nhiên, đáng ngạc nhiên là con số người chết tại Osaka lại vượt lên Tokyo.\n So sánh số người phải nhập viện vì Covid-19 giữa các tỉnh, thành:   Số bệnh nhân cần nhập viện điều trị tại Tokyo cũng đang là rất lớn, lên đến hơn 40.000 ca, gần gấp đôi so với Saitama.\n So sánh tỉ lệ giữa số ca tử vong và số ca bệnh:   Về tỉ lệ này thì địa phương đứng đầu là Tokushima, tiếp theo là Hokkaido.\n2.3_* Phân tích dữ liệu của 47 tỉnh thành phố, về mật độ dân cư và số giường bệnh (Mở rộng) Trong dataset của chúng ta còn có một file chứa thông tin về dân cư và cơ sở hạ tầng ý tế của các tỉnh thành ở Nhật Bản. Mặc dù dữ liệu hơi cũ (thống kê từ năm 2020) nhưng cũng đáng để chúng ta xem xét.\nImport thư viện sử dụng:\n a, Load dữ liệu  Load dữ liệu từ file csv và hiển thị một vài dữ liệu    Hiển thị thông tin chi tiết hơn của dữ liệu   b, Data Cleaning  Tạo bản sao của dữ liệu gốc:    Xóa bỏ 2 features có nhiều missing data, và không có nhiều ý nghĩa trong phân tích lần này:    Hợp nhất hai features \u0026lsquo;Category\u0026rsquo; và \u0026lsquo;Item\u0026rsquo; thành feature mới là \u0026lsquo;Title\u0026rsquo; bởi vì chúng mang ý nghĩa tương tự nhau:    Chuyển feature \u0026lsquo;Date\u0026rsquo; sang dạng DateTime và sắp xếp dữ liệu theo chiều giảm của nó:    Tạo bảng pivot_table chỉ chứa thông tin dùng để phân tích trong bài này: Prefecture, Title, và Value    Chuyển một số features tự dạng object về dạng integer/float để thuận tiện xử lý   c, Tiến hành EDA  So sánh mật độ dân cư trên tổng diện tích, mật độ dân cư trên diện tích có khả năng sinh sống của 47 tỉnh, thành.       So sánh số giường bệnh có thể điều trị bênh nhân Covid-19 của 47 tỉnh, thành     2.4 Phân tích dữ liệu tại Tokyo Dữ liệu tại Tokyo nằm chung trong file covid_jpn_prefecture.csv, nên phần load và làm sạch dữ liệu giống như phần 2.3. Sau đó, ta sẽ lọc ra các mẫu thuộc Tokyo để phân tích.\na, Load dữ liệu b, Data Cleaning  Lọc danh sách mẫu dữ liệu thuộc khu vực Tokyo:    Thêm mới feature Positive_Tested_Rate thể hiện tỉ lệ giữa số ca mắc Covid-19 trên số lượng người được xét nghiệm.   c, Tiến hành EDA  Hiển thị danh sách dữ liệu dạng bảng:     So sánh sự thay đổi về số ca dương tính, tỉ lệ mắc Covid-19 trên số người được xét nghiệm, số bệnh nhân khỏi bệnh được xuất viện, số ca tử vong theo thời gian:   Nhận thấy số ca dương tính và số bệnh nhân được công bố khỏi bệnh tăng tương đối đều nhau, khoảng cách giữa chúng được duy trì khá ổn đỉnh. Đây có thể xem là tín hiệu tốt.\n   Sự thay đổi của số ca dương tính theo thời gian   Số người dương tính với Covid-19 đang tăng cao, đặc biệt là từ đầu năm 2021 đến nay.\n Sự thay đổi về số lượng người được xét nghiệm Covid-19   Con số này cũng tăng rất nhanh, gần như là tuyến tính. Có lẽ Tokyo đang tăng cường, tập trung xét nghiệm để bóc tách F0 ra khỏi cộng đồng.\n Sự thay đổi về tỉ lệ giữa số ca mắc Covid-19 và số người được xét nghiệm   Từ tháng 03/2020 đến tháng 06/2020, tỉ lệ này rất cao, khoảng 0.35. Từ tháng 07/2020 trở đi thì tỉ lệ này giảm và duy trì trong khoảng 0.06 đến 0.09. Đây cũng là một tín hiệu tích cực.\n Sự thay đổi về số lượng bệnh nhân nặng, cần nhập viện điều trị   Bình thường nếu F0 không có triệu chứng, hoặc triệu chứng nhẹ thì sẽ được cách ly và theo dõi tại nhà. Nếu trở nặng thì sẽ được nhập viện để điều trị tích cực. Con số này tăng vọt vào tháng 02/2021 và hiện tại (08/2021).\n Sự thay đổi về số người được chữa khỏi.   Số người được công bố hết bệnh tăng tương tự với số người mắc Covid-19.\n Sự thay đổi về số lượng người chết vì Covid-19   Đối với thống kê này, có thể thấy từ cuối tháng 02/2021 đến nay, số lượng người chết tăng rất nhanh. Hiện tại đã lên đến gần 3000 người.\n Phân tích số liệu hiện tại của Tokyo  Lấy dữ liệu trong ngày thống kê mới nhất, 21/08/2021:\n  Thể hiện kết quả lên đồ thị Pie:\n Thể hiện kết quả lên đồ thị Bar:\n Theo thống kê mới nhất vừa cập nhật cách đây khoảng 2h cho ngày 21/08/2021 thì số ca dương tính là 307870, số người được xét nghiệm là 3119292, số bệnh nhân được công bố khỏi bệnh là 260043, và số người cần nhập viện điều trị là 45456.\n2.4_a Phân tích dữ liệu thông tin bệnh nhân tại Tokyo Thông tin chi tiết về các bệnh nhân mắc Covid-19 được lưu ở một file dữ liệu riêng. Thông tin này được cập nhật đến ngày 02/02/2021. Ta sẽ đi phân tích nó.\nImport thư viện sử dụng:\n a, Load dữ liệu  Load dữ liệu từ file csv và hiển thị một vài dữ liệu     Hiển thị thông tin chi tiết hơn về dữ liệu   b, Data Cleaning  Tạo một bản sao của dữ liệu gốc    Xóa feature Date (Onset) bởi vì nó là dữ liệu bị missing quá nhiều    Chuyển feature Date sang dạng DateTime để thuận tiện xử lý    Thay thể các missing data bằng các giá trị mới sinh ra bằng phương pháp nội suy   c, Tiến hành EDA  Phân tích tuổi của bệnh nhân     Có thể số bênh nhân trong độ tuổi 20-30 chiếm tỉ lệ lớn nhất. Tỉ lệ này giảm dần khi số tuổi tăng lên. Người trên 100 tuổi vẫn có nguy cơ mắc Covid-19. Không thấy báo cáo về tỉ lệ nhiễm bệnh của trẻ em và thanh thiếu nên dưới 20 tuổi.\n Phân tích giới tính của bệnh nhân     Nam giới dường như có nguy cơ mắc Covid-19 nhiều hơn so với nữ giới, mặc dù co số chênh lệch không quá lớn.\n2.4_b So sánh số ca mắc Covid-19 tại 23 quận của Tokyo Dữ liệu về Covid-19 tại các quận của Tokyo cũng được thống kê trong một file khác. Tuy nhiên, nó chỉ cập nhật đến tháng 01/2021. Ta sẽ đi phân tích nó.\nImport thư viện sử dụng:\n a, Load dữ liệu  Load dữ liệu từ file csv và hiển thị một vài dữ liệu     Hiển thị thông tin chi tiết hơn dữ liệu    Ta cũng load thêm thông tin địa lý về các quận của Tokyo phục vụ mục đích hiển thị dữ liệu lên bản đồ.   b, Data Cleaning Các dữ liệu trong file csv không có missing data, nên ta chỉ cần bỏ đi feature không cần thiết và sắp xếp lại dữ liệu.\n Tạo bản sao của dữ liệu gốc    Loại bỏ feature Area bởi vì ta sẽ không cần dùng đến nó    Sắp xếp lại dữ liệu theo chiều giảm dần của số ca dương tính với Covid-19    Đối với dữ liệu về địa lý thì cần phải chỉnh lý một chút theo khuyến nghị từ tác giả:   c, Tiến hành EDA  So sánh số ca mắc Covid-19 tại 23 quận trên bảng dữ liệu:     So sánh số ca mắc Covid-19 tại 23 quận trên đồ thị Bar:    So sánh số ca dương tính với Covid-19 trên bản đồ:   Setagaya đang là quận có số ca mắc Covid-19 cao nhất, tiếp sau đó là Shinjuku.\n3. Kết luận Vậy là chúng ta đã hoàn thành việc phân tích dữ liệu thống kê về tình hình Covid-19 tại Nhật Bản. Đây là dữ liệu thực tế, cập nhật nóng hổi đến hiện tại giúp chúng ta có thể áp dụng những kiến thức về Data Science vào trong bài toán thực tế. Mình nghĩ là có khá nhiều kiến thức thú vị mà bạn đã học được thông qua bài này. Cảm ơn các bạn đã theo dõi!\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\n4. Tham khảo [1] Taha Anwar, \u0026ldquo;Creating a Virtual Pen And Eraser with OpenCV\u0026rdquo;, Available online: https://learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/ (Accessed on 15 Aug 2021).\n","permalink":"https://tiensu.github.io/blog/104_eda_covid_in_japan/","tags":["Data Science","EDA"],"title":"Phân tích tình hình Covid-19 tại Nhật Bản"},{"categories":["Deep Learning","Game"],"contents":"Hồi bé, khi xem mấy bộ phim viễn tưởng, thấy các diễn viên huơ huơ tay trong không khi và viết ra những chữ loằng ngoằng trên màn hình, mình thấy thật cool ngầu. Chắc không ít lần mình ước có thể thực hiện được như họ.\nVới công nghệ ngày nay thì việc này hoàn toàn không khó. Bạn chỉ cần bỏ ra ít tiền, sắm cho mình một số thiết bị cầm tay xịn xịn như ipad, tablet, \u0026hellip; kèm bút cảm ứng xịn xịn là có thể thực hiện được ước mơ ngày bé rồi.\nNhưng nếu bạn không có nhiều tiền thì sao? Câu trả lời sẽ có trong bài hôm nay. Mình sẽ hướng dẫn các bạn tạo ra một virtual pen, có thể viết, vẽ trong không khí bất cứ gì bạn muốn, và những thứ đó sẽ thực sự được vẽ ra trên màn hình máy tính của bạn. Điều đặc biệt là bạn không cần phải sắm thêm bất cứ phần cứng đắt tiền nào cả. Chỉ cần một máy tính (không có màn hình cảm ứng), một camera và một chiếc bút bình thường. Về mặt công nghệ, chúng ta cũng chỉ phải sử dụng đến vài kỹ thuật xử lý ảnh đơn giản trong thư viện OpenCV, không cần AI hay Deep Learning gì cao siêu gì sất.\nBố cục bài viết sẽ bao gồm 2 phần:\n Phần 1: Giới thiệu chung về thuật toán sử dụng Phần 2: Là 6 bước thực hiện cụ thể. Mỗi bước là một module hoàn chỉnh, nhìn thấy được kết quả, bước sau kế thừa từ bước trước. Trong đó, 3 bước đầu tiên có thể tái sử dụng cho nhiều bài toán tương tự khác.  1. Giới thiệu thuật toán Ý tưởng của thuật toán như sau.\n Đầu tiên, chúng ta sử dụng Color Masking để nhận diện và tách riêng bút viết ra khỏi nền trong video. Ở đây, cần chú ý nên chọn một chiếc bút có màu sắc nổi bật, khác biệt so với background để dễ dàng nhận diện và tách. Sử dụng kỹ thuật Contour Detection để phát hiện và bám theo vị trí của bút trên màn hình. Vẽ các đường theo tọa độ (x,y) trong quá trình di chuyển của bút.  Chỉ vậy thôi là ta đã có được một chiếc Virtual Pen để xài rồi.\n2. Các bước thực hiện 2.1 Tìm Color Range của bút Bước đầu tiên và quan trọng nhất là tìm ra khoảng giá trị phù hợp của bút để có thể dựa vào đó nhận diện chính xác bút vẽ.\nĐể làm điều này được dễ dàng hơn, chúng ta nên chuyển hệ màu của ảnh từ BGR (Blue Green Red) sang hệ màu HSV (* Hue Saturation, Value*). Xem thêm về các *Color Space* tại đây.\nMình cũng sẽ tạo ra một giao diện đơn giản với các thanh trượt (trackbars) để các bạn có thể tùy chỉnh theo bút của các bạn. Sau khi tìm được khoảng giá trị phù hợp rồi, chúng ta sẽ lưu lại để sử dụng trong các bước tiếp theo (bấm phím s).\nCode thực hiện bước này như sau:\n      Các bạn có thể xem video mình thực hiện bước này như dưới đây:\n Tại bước này, bạn cũng không cần phải chọn được khoảng giá trị quá chuẩn chỉ, có thể vẫn còn một chút nhiễu kiểu như các chấm trắng li ti trong ảnh cũng chấp nhận được. Ở bước thứ 2, chúng ta sẽ có cách loại bỏ chúng.\n2.2 Loại bỏ nhiễu Sử dụng một số phép biến đổi nâng cao như Erosion và Dilation, ta có thể loại bỏ đuợc 1 số nhiễu nhỏ còn tồn tại ở bước 1. Chi tiết về 2 phép toán này, các bạn có thể tham khảo tại đây.\nTrong bài này, mình sử dụng chung một kernel matrix 5x5 cho cả 2 Erosion và Dilation. Số lượng vòng lặp (iterations) và kích thước của kernel matrix có thể phụ thuộc vào đối tượng cần phát hiện và loại nhiễu cần loại bỏ. Nếu hiểu sâu sắc về 2 phép biến đổi này thì bạn có thể thử thay đổi 2 tham số kể trên xem sao.\nCode thực hiện của bước này như sau:\n     Kết quả:\n Đến đây, nếu vẫn còn nhiễu \u0026hellip; cũng không sao. Bước tiếp theo, chúng ta sẽ có một mẹo nhỏ để loại bỏ chúng. :D\n2.3 Theo dõi chuyển động của bút Bước này cũng khá quan trọng và thú vị. Nó sử dụng một kỹ thuật xử lý ảnh rất phổ biến và hữu dụng, đó là Contour Detection. Hiểu một cách đơn giản thì Contour là một vùng diện tích mà một đối tượng chiếm cứ trên bức ảnh. Tìm hiểu thêm về nó tại đây.\nTại bước này, Contour được sử dụng để tìm ra chính xác vị trí của bút và theo dõi chuyển động của nó khi ta viết. Vị trí được thể hiện bằng 4 giá trị (x,y,w,h) có được thông qua gọi hàm cv2.getBoundingRect(). Ta sẽ đánh dấu nó bằng một hình chữ nhật bao quanh đối tượng để dễ bề theo dõi. Để loại bỏ bớt nhiễu (nếu có), ta chỉ lấy đối tượng nào mà Contour của nó có diện tích lớn nhất, còn lại thì bỏ đi. Diện tích này là kết quả trả về của hàm cv2.contourArea().\nCode thực hiện như sau:\n       Kết quả thực hiện:\n 2.4 Bắt đầu viết/vẽ Mọi thứ đã được chuẩn bị cơ bản. Giờ là lúc ta có thể bắt đầu thiết lập để viết/vẽ được rồi.\nĐể viết/vẽ, ta chỉ cần nối toạ độ của bút ở frame trước đó ($x_1,y_1$) với tọa độ của bút trong frame hiện tại ($x_2,y_2$). Với FPS của camera vào khoảng 20 đến 24 fps thì mắt người nhìn vào sẽ giống như là đang viết/vẽ realtime vậy.\nChú ý là ở đây, mình sử dụng thêm một canvas màu đen, có kích thước đúng bằng kích thức của frame để vẽ lên nó. Sau đó mới tiến hành tích hợp canvas vào frame hiện tại. Việc tách riêng phần vẽ sang một nơi khác có tác dụng ở bước số 5 và số 6.\nCode thực hiện cho bước này:\n       Kết quả thực hiện:\n Mình có thêm chức năng xóa toàn bộ những gì đã vẽ khi bấm phím c để tiện cho việc test. Trong bước tiếp theo, ta sẽ làm cho chức năng xóa này chuyên nghiệp hơn.\n2.5 Thêm chức năng xóa toàn bộ Trong trường hợp viết/vẽ sai, bạn muốn làm lại từ đầu thì sao? Chức năng xóa sẽ giúp bạn thực hiện điều đó. Bước này ta sẽ thêm vào chức năng xóa toàn bộ những gì đã viết/vẽ. Bước tiếp theo, ta sẽ thêm nốt chức năng xóa từng phần theo ý muốn.\nMột điều rất hiển nhiên là diện tích của Contour sẽ tăng lên khi đối tượng tiến vào gần camera. Sử dụng thuộc tính này, ta sẽ trigger môt sự kiện khi diện tích Contour của bút lớn hơn 1 giá trị ngưỡng để xóa toàn bộ nội dung mà ta đã viết/vẽ trước đó. Còn để thực hiện xóa thì chỉ cần set giá trị cho canvas bằng None là được.\nCode thực hiện như sau:\n        Kết quả thực hiện:\n Nếu bạn để ý thì mình đã thêm vào đoạn xử lý từ dòng 44-47 trong hàm draw_line() để việc vẽ trông tự nhiên hơn. Vẫn chỉ là các thao tác xử lý ảnh cơ bản thôi, nhưng mang lại hiệu quả khá tốt trong trường hợp của mình. Bạn có thể sử dụng hoặc không tùy theo tình huống cụ thể phía bạn.\n2.6 Thêm chức năng xóa từng phần Đây là bước cuối cùng để hoàn thiện sản phẩm của chúng ta. Tưởng tượng bạn đang vẽ một bức tranh rất đẹp, nhưng chẳng may sai một nét mà lại xóa đi vẽ lại từ đầu thì thật là khó chịu vào mất thời gian. Vì thế, mình sẽ thêm vào chức năng xóa từng phần, tức chỉ xóa những gì mà ta muốn.\nCó 2 công việc cần thực hiện để hoàn thành chức năng này:\n Chuyển đổi qua lại giữa 2 chế độ viết/vẽ và xóa. Xóa  Đối với công việc thứ nhất, ta có thể thực hiện rất dễ dàng bằng cách bấm các phím khác nhau. Nhưng mình không muốn làm như thế, mình không muốn chạm vào bất cứ thứ gì, chỉ thao tác trong không khí mà thôi. Cách làm sẽ là yêu cầu người dùng đưa bàn tay lên góc trên bên trái màn hình. Sử dụng kỹ thuật Background Subtraction, chúng ta có thể phát hiện ra điều đó và thực hiện chuyển qua lại giữa 2 chế độ viết/vẽ và xóa.\nĐối với công việc thứ 2, ta cũng có thể thực hiện dễ dàng bằng cách vẽ lên canvas màu đen đúng tại vị trí muốn xóa.\nCode thực hiện đầy đủ như sau:\n          Và đây là thành quả cuối cùng:\n Khá ấn tượng đấy chứ, :D!\nMẹo nhỏ: Để ngắt quãng các nét viết/vẽ khi di chuyển bút sang vị trí khác, mình đã che một bên bút bằng màu sắc khác ngoài dải phát hiện đối tượng mà ta chọn ở bước 1. Khi muốn ngắt, ta chỉ cần quay phần có màu khác đó hướng về phía camera. Còn khi muốn vẽ ta làm ngược lại.\n3. Hạn chế và hướng cải tiến Nếu theo dõi kỹ từ đầu và test thực tế thì bạn có thể nhận ra 2 vấn đề của ứng dụng này:\n Bút viết phải chọn lựa và tinh chỉnh (chọn khoảng màu) trước khi bắt đầu ứng dụng. Khi tốc độ di chuyển của bút quá nhanh thì nét vẽ sẽ bị đứt đoạn.  Với vấn đề thứ nhất, thoạt đầu mình nghĩ nó là hạn chế, nhưng chợt thấy các thiết bị có tính năng bút cảm ứng cũng chỉ cho phép sử dụng 1 loại bút duy nhất mà nó cung cấp, nên có thể nói đây cũng ko hẳn là hạn chế. Nếu muốn sử dụng được nhiều loại bút hơn thì ta có thể nghĩ đến giải pháp thay thế kỹ thuật nhận diện đối tượng bằng OpenCV bằng kỹ thuật Object Detection trong Deep Learning. Tất nhiên, cái giá phải trả là mất nhiều thời gian, công sức để thực hiện hơn so với cách làm hiện tại.`\nVấn đề thứ 2 thì đúng là hạn chế. Có thể giải quyết vấn đề này bằng cách thêm vào kỹ thuật Tracking đối tượng. Khi đó thì nét vẽ sẽ liên tục thậm chí khi người dùng di chuyển bút với tốc độ nhanh hơn.\nNếu 2 vấn đề nêu trên được giải quyết một cách triệt để thì ứng dụng này hoàn toàn có thể triển khai đến cho end-user sử dụng được. :D\n4. Kết luận Vậy là chúng ta đã hoàn thành việc xây dựng ứng dụng bút ảo bằng OpenCV. Mình nghĩ là có khá nhiều kiến thức thú vị mà bạn đã học được thông qua bài này. Cảm ơn các bạn đã theo dõi!\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\n5. Tham khảo [1] Taha Anwar, \u0026ldquo;Creating a Virtual Pen And Eraser with OpenCV\u0026rdquo;, Available online: https://learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/ (Accessed on 15 Aug 2021).\n","permalink":"https://tiensu.github.io/blog/103_create_virtual_pen/","tags":["Deep Learning","Game"],"title":"Tạo Bút Ảo với OpenCV"},{"categories":["Deep Learning","Game"],"contents":"Hà Nội, ngày giãn cách xã hội thứ 22, năm 2021!\nHà Nội và cả nước VN những ngày này đang oằn mình chống chọi với đại dịch COVID-19. Mọi công dân được chính quyền yêu cầu ở nhà, chỉ được phép ra đường trong những trường hợp đặc biệt \u0026hellip;\nĐây có lẽ là lúc mà chúng ta có nhiều thời gian rảnh nhất. Bạn thường làm gì khi tự nhiên có nhiều thời gian rảnh như vậy? Xem phim, đọc sách, ngủ, ăn, chơi game, \u0026hellip;\nNói về chơi game, có một trò mà từ người lớn đến con nít đều biết. Không chỉ có ở VN, mà ở Nhật, ở Philippines, ỏ Trung Quốc, ở Ấn Độ \u0026hellip; đều có. Đó là trò chơi Oẳn Tù Tì. Người Nhật gọi là じゃんけんぽん, người Trung Quốc gọi là 剪刀 、 石頭 、 布, còn người Philippines gọi là jack en poy, \u0026hellip; Ban đầu mình cứ nghĩ trò này xuất phát từ các nước Á châu, và chỉ những nước này mới chơi nó. Nhưng theo Wikipedia thì \u0026ldquo;Oẳn tù tì\u0026rdquo; ở Việt Nam là phát âm trại từ tiếng Anh: one, two, three (một, hai, ba) ở miền Nam trước 1975, có lẽ vì trò chơi này du nhập khi người Mỹ xuất hiện vào đầu thập niên 1960. Trò chơi tồn tại đến tận ngày nay với tên gọi \u0026ldquo;Oẳn tù tì\u0026rdquo;. Thật ngạc nhiên phải không?\nOẳn tù tì là trò chơi đơn giản, dễ chơi, nhanh chóng có kết quả nên hay được sử dụng để quyết định thắng thua mang tính may rủi, giống như trò tung đồng xu vậy.\nHôm nay, mình sẽ hướng dẫn mọi người tạo ra game Ỏăn tù tì để chơi với máy tính cho đỡ buồn chán trong những ngày giãn cách xã hội. Đặc biệt game này có sử dụng công nghệ AI nha mọi người! :D\nCách thực hiện bao gồm các bước nhau sau:\n Thu thập dữ liệu các cử chỉ tay tương ứng với: Kéo, Búa, Giấy. Huấn luyện AI model để nhận diện các cử chỉ tay bên trên. Cho máy tính sinh ra ngẫu nhiên một cử chỉ khi bạn đưa cử chỉ của bạn ra. Quyết định người thắng dựa theo luật của game.  Ok, bắt tay vào công việc thôi! Mình sẽ đưa luôn toàn bộ code vào trong bài để mọi người dễ theo dõi.\nBài viết được chia thành 2 phần:\n Phần 1: Huấn luyện AI model để nhận diện các ký hiệu tay: Búa - rock, giấy - paper, kéo - scrissor, và không gì cả - nothing. Phân 2: Tạo game.  1. Huấn luyện AI model Đầu tiên, hãy import các thư viện sử dụng:\n Một số biến, hàm dùng chung để hiển thị giao diện:\n   1.1 Thu thập dữ liệu Dữ liệu ở đây bao gồm hình ảnh của 3 loại cử chỉ của tay (búa, giấy, kéo) và 1 loại không có gì (nothing). Mỗi loại có 100 hình ảnh. Mình test thử thì thấy 100 hình ảnh cho ta model nhận diện khá tốt rồi. Bạn có thể thử với số lượng khác.\nChúng ta sẽ thiết kế một giao diên đơn giản bằng OpenCV để thu thập dữ liệu này. Người chơi được yêu cầu giơ tay theo từng loại cử chỉ vào khu vực ROI có kích thước 224x224 (bằng kích thước input vào model) và nhấn các phím theo chỉ dẫn trên màn hình. Ảnh trong vùng ROI sẽ được lưu lại làm dữ liệu huấn luyện model.\nCode thực hiện:\n   Khi thực thi code trên sẽ như sau:\n Một số lưu ý:\n  Trong quá trình máy tính chụp ảnh các ký hiệu tay của bạn, cố gắng di chuyển tay lại gần, ra xa, sang trái, sang phải, xoay, \u0026hellip; nhưng không ra khỏi vùng ROI. Mục đích là để có thể thu được dữ liệu đa dạng hơn, giúp cho mô hình AI học tốt hơn.\n  Mặc dù chỉ có 3 ký hiệu tay nhưng chúng ta cần tạo thêm một lớp dữ liệu nữa, đó là nothing. Điều này là bởi vì khi chúng ta không chơi (đưa tay ra khỏi vùng ROI) thì mô hình AI vẫn sẽ cố gắng dự đoán ra một trong các lớp mà ta đã huấn luyện. Như thế thì máy tính sẽ tưởng là chúng ta đang chơi và sẽ xử lý sai logic mà ta mong muốn. Dữ liệu của lớp nothing này, lý tưởng nhất là tập các hình ảnh được chụp một cách ngẫu nhiên. Trong bài này, để cho đơn giản, mình chỉ lấy 100 ảnh tĩnh tại vị trí mình đang ngồi. Cách làm này có thể dẫn đến việc mô hình AI sẽ dự đoán sai khi chúng ta chơi ở một nơi khác. Mình sẽ đưa ra một số gợi ý để cải thiện độ chính xác của mô hình AI ở phần cuối bài này.\n  Hình ảnh chụp được từ camera sẽ không được lưu xuống ổ cứng máy tính mà vẫn giữ nguyên trong bộ nhớ RAM. Mình làm thế là để giảm thời gian đọc/ghi (I/O latency). Nếu bạn muốn chụp nhiều ảnh hơn, dung lượng RAM hạn chế, hoặc muốn sử dụng lại về sau thì nên xem xét lưu chúng vào HDD.\n  Vì không lưu vào HDD nên để chắc chắn là đã lấy được dữ liệu chính xác thì ta sẽ thử hiển thị ngẫu nhiên 8 ảnh của mỗi lớp:\n 1.2 Tiền xử lý dữ liệu Ở bước này, chúng ta sẽ kết hợp tất cả hình ảnh vào một mảng, tất cả nhãn vào một mảng tương ứng. Sau đó tiến hành một số cac xử lý để phù hợp với yêu cầu của mô hình AI như: normalization, label encoder, \u0026hellip; Cuối cùng là chia dữ liệu thành các tập train và test.\n 1.3 Train AI model Mình sẽ chọn một pre-trained model trong Keras  để thực hiện fine-tune theo phương pháp Transfer Learning. Để cân bằng giữa tốc độ và độ chính xác, MobileNetV2 sẽ được sử dụng.\nDownload MobileNetV2 model và loại bỏ phần head của nó:\n Thêm mới các lớp FC vào để tạo thành model hoàn chỉnh theo yêu cầu của chúng ta. Dropout và Global Average Pooling được thêm vào để hạn chế Overfitting. Số lượng node ở lớp đầu ra của model phải là 4 (= số lớp cần dự đoán).\n Sử dụng thêm kỹ thụât Augmentation để hạn chế Overfitting:\n Model sẽ được train với optimizer là Adam, loss function là categorical_crossentropy vì đây là bài toán phân loại nhiều lớp. Accuracy là metric mà chúng ta cần quan tâm.\n Tiến hành train model với 15 epochs và batch_size 20.\n Kết quả train:\n Model rất nhanh đã hội tụ và đạt độ chính xác cao. Có lẽ vì dữ liệu của chúng ta khá đơn giản.\nLưu lại model để sử dụng về sau:\n 1.4 Kiểm tra hoạt động của AI model Chúng ta thử test độ chính xác của model vừa huấn luyện trên camera video.\nLoad model lên:\n Hàm dự đoán. Chú ý rằng mỗi frame của video cần phải được xử lý chính xác như lúc huấn luyện mô hình.\n  Đọc camera video và đưa vào model để dự đoán:\n Mình tết quả như trong video sau:\n OK, như vậy là chúng ta đã có được model AI để dự đoán ký hiệu tay của người chơi. Chúng ta sẽ chuyển qua xây dựng logic chơi game.\n2. Tạo Game Phần này thì chủ yếu là vận dụng kiến thức về xử lý ảnh và cách sử dụng thư viện OpenCV. Kết quả dự đoán của AI model đã được huấn luyện ở phần 1 sẽ là input cho game.\nLuật chơi của game sẽ đơn giản như sau:\n Áp dụng luật chơi chuẩn của game Oẳn tù tì: Kéo thắng Giấy, Búa thắng Kéo, \u0026hellip; Mgười và máy tính sẽ chơi với nhau 10 lần, mỗi lần thắng được cộng 1 điểm, hòa thì không có điểm. Sau 10 lần chơi, số điểm của bên nào lớn hơn thì người đó thắng cả game.  Mỗi lần người chơi đưa ra một ký hiệu tay thì máy tính cũng sẽ lựa chọn ngẫu nhiên một ký hiệu tay để chơi cùng.\nVề phần dự đoán của AI model, để tăng độ chính xác thì kết quả dự đoán cuối cùng sẽ được căn cứ theo kết quả của 5 lần dự đoán liên tiếp. Trong 5 lần đó, nhãn nào chiếm đa số sẽ là nhãn được chọn.\nImport các thư viện cần sử dụng:\n Khai báo các biến toàn cục sử dụng trong toàn bộ chương trình:\n  Các hàm tạo UI thì vẫn như phần 1:\n  Tiếp theo là load AI model đã huấn luyện lên để chuẩn bị sử dụng:\n Hàm tạo đoán từ AI model. Hàm này có khác một chút so với hàm cùng tên ở phần 1, vì chúng ta cần nhiều giá trị trả về hơn để phục vụ việc tính toán logic game.\n  Hàm tìm người chiến thắng trong mỗi lần chơi. Đây chính xác là hàm mô tả luật chơi kinh điển của game Oẳn tù tì:\n Hàm quyết định người chơi chiến thắng cuối cùng và trả về hình ảnh tương ứng để hiển thị lên giao diện:\n Hàm hiển thị hình ảnh hand sign của máy tính mỗi lần chơi.\n Hàm hiển thị ảnh của người chiến thắng lên giao diện:\n Hàm tính điểm mỗi lần chơi:\n Hàm cập nhật thông tin điểm, hướng dẫn chơi game lên giao diện:\n Hàm chơi game:\n Cuối cùng là vòng lặp main, đọc frame từ camera video và gọi các hàm khác để tạo thành game đầy đủ chức năng:\n Đây là video mình chơi thử:\n Khá thú vị phải không các bạn? :D\n3. Hướng cải tiến và phát triển tiếp theo 3.1 Cải tiến AI model Đây là hạn chế lớn nhất cần giải quyết nếu muốn phát triển sâu hơn game này. Vì dữ liệu được thu thập trong một không gian cố định nên khả năng bị overfitting là rất lớn (bạn có thể mang thử ra một địa điể khá để kiểm tra). Cách cải tiến thì rất đơn giản nhưng cũng mất rất nhiều thời gian, đó là tập hơn thêm thật nhiều ảnh của cả 4 nhãn ở các địa điểm, môi trường, không gian, thời gian, \u0026hellip; khác nhau,c àng đa dạng càng tốt. Sau đó huấn luyện lại model AI trên dữ liệu đó.\n3.2 Cải tiến giao diện người chơi Vì bài này mình chủ yếu minh họa cách sử dụng mô hình AI và kỹ thuật xử lý ảnh nên không chú trọng vào giao diện. Nếu muốn phát triển sâu hơn, thu hút nhiều người chơi hơn thì chắc chắn phần UI/UX phải được cải tiến theo hướng đơn giản, đẹp mắt, dễ sử dụng. Chi tiết về các kỹ thuật tạo UX/UI mình xin phép không bàn sâu thêm trong bài này.\n3.3 Đưa game lên các thiết bị mobile Nếu bạn muốn kiếm tiền bằng game này (có khả năng lắm chứ), có thể nghĩ đến việc triển khai nó lên điện thoại, máy tính bảng để thu hút nhiều người chơi. Cách làm chắc cũng đơn giản thôi vì các framework Tensorflow và Keras đều hỗ trợ việc porting AI model của nó lên các thiết bị đầu cuối cho cả 2 hệ điều hành là Android và iOS.\n4. Kết luận Vậy là chúng ta đã hoàn thành việc xây dựng game Oẳn tù tì với AI và Computer Vision. Mình nghĩ là có khá nhiều kiến thức thú vị mà bạn đã học được thông qua bài này. Cảm ơn các bạn đã theo dõi!\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\n5. Tham khảo [1] Taha Anwar, \u0026ldquo;Playing Rock, Paper, Scissors with AI\u0026rdquo;, Available online: https://learnopencv.com/playing-rock-paper-scissors-with-ai/ (Accessed on 10 Aug 2021).\n","permalink":"https://tiensu.github.io/blog/102_playing-rock-paper-scissors-with-ai/","tags":["Deep Learning","Game"],"title":"AI for Fun: Chơi Oản Tù Tì với AI"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 23 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 2 về về Dimensionality Reduction. Trong bài này, chúng ta sẽ tìm hiểu về 3 thuật toán: LDA, PCA và SVD, cách sử dung chúng để thực hiện Dimensionality Reduction.\n1. Linear Discriminant Analysis - LDA LDA thực chất là một thuật toán Linear ML cho bài toán Multiclass Classification. LDA hoạt động bằng cách tìm kiếm một sự kết hợp tuyến tính giữa các features để có thể tối đa hóa khả năng phân biệt giữa các classes và tối thiểu hóa khả năng phân biệt giữa các samples trong mỗi class.\nCó một thuật toán khác, cũng viết tắt là LDA. Đó là Latent Dirichlet Allocation, chuyên dùng cho Dimensionality Reduction trong bài toán Text Classification. Nhưng nó nằm ngoài phạm vi bài này, mình chỉ đưa ra đây để về sau nếu có gặp thì các bạn đỡ bối rối.\nTrong scikit-learn, LDA được implemented bởi class LinearDiscriminantAnalysis(). Cách sử dụng tương tự như các kỹ thuật transforms khác. Tham số cần quan tâm là n_components chỉ ra số chiều còn lại của dữ liệu sau khi giảm. Giá trị tối đa của nó = số lượng (classes - 1).\n... # prepare dataset data = ... # define transform lda = LinearDiscriminantAnalysis() # prepare transform on dataset lda.fit(data) # apply transform to dataset transformed = lda.transform(data) Nếu sử dụng kết hợp với model trong một pipeline thì sẽ như sau:\n... # define the pipeline steps = [(\u0026#39;lda\u0026#39;, LinearDiscriminantAnalysis()), (\u0026#39;m\u0026#39;, GaussianNB())] model = Pipeline(steps=steps) ... # define the pipeline steps = [(\u0026#39;s\u0026#39;, StandardScaler()), (\u0026#39;lda\u0026#39;, LinearDiscriminantAnalysis()), (\u0026#39;m\u0026#39;, GaussianNB())] model = Pipeline(steps=steps) 2. Principal Component Analysis - PCA PCA có lẽ là thuật toán Dimensionality Reduction phổ biến nhất. Khi nhắc đến Dimensionality Reduction, người ta thường nghĩ ngay đến PCA đầu tiên.\nPCA hoạt động theo nguyên tắc feature projection, tức là từ m-dimension ban đầu của dữ liệu trong không gian S, thông qua một pháp ánh xạ sẽ được chuyển thành n-dimension trong không gian R mà m \u0026gt; n.\nTrong scikit-learn, PCA được implemented bởi class PCA. Các sử dụng tương tự như các kỹ thuật transforms khác. Tham số cần chú ý của class PCA cũng là n_components như của LDA.\nCách sử dụng PCA:\n... data = ... # define transform pca = PCA() # prepare transform on dataset pca.fit(data) # apply transform to dataset transformed = pca.transform(data) Nếu kết hợp với pipeline:\n... # define the pipeline steps = [(\u0026#39;pca\u0026#39;, PCA()), (\u0026#39;m\u0026#39;, LogisticRegression())] model = Pipeline(steps=steps) ... # define the pipeline steps = [(\u0026#39;norm\u0026#39;, MinMaxScaler()), (\u0026#39;pca\u0026#39;, PCA()), (\u0026#39;m\u0026#39;, LogisticRegression())] model = Pipeline(steps=steps) 3. Singular Value Decomposition - SVD SVD cũng là một trong những thuật toán Dimensionality Reduction phổ biến. Cách thức hoạt động của nó tương tự như PCA, chỉ có điều nó thường được áp dụng cho các tập dữ liệu bị sparse, tức là dữ liệu mà ở đó có rất nhiều các giá trị của các features bằng 0.\nMột số VD của sparse data phù hợp để áp dụng SVD là:\n Recommender System Customer-Production purchases User-Song Listener Counts User-Movie Ratings Text Classification One Hot Encoding Bag-of-Words Counts TF/IDF  Trong scikit-learn, SVD được implemented bởi class TruncatedSVD(). Cách sử dụng tương tự như các kỹ thuật transforms khác. Tham số cần quan tâm là n_components.\nCách sử dụng SVD:\n... data = ... # define transform svd = TruncatedSVD() # prepare transform on dataset svd.fit(data) # apply transform to dataset transformed = svd.transform(data) Nếu kết hợp vào pipeline:\n... # define the pipeline steps = [(\u0026#39;svd\u0026#39;, TruncatedSVD()), (\u0026#39;m\u0026#39;, LogisticRegression())] model = Pipeline(steps=steps) ... # define the pipeline steps = [( \u0026#39;norm\u0026#39;, MinMaxScaler()), (\u0026#39;svd\u0026#39;, TruncatedSVD()), (\u0026#39;m\u0026#39;, LogisticRegression())] model = Pipeline(steps=steps) 2. Thực hành LDA, PCA và SVD 2.1 Tạo Dataset Chúng ta sẽ tạo ra một tập gồm có 1000 mẫu dữ liệu, 10 classes, số chiều là 20, sử dụng hàm make_classification().\n# test classification dataset from sklearn.datasets import make_classification # define dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7, n_classes=10) # summarize the dataset print(X.shape, y.shape) Kết quả thực hiện:\n(1000, 100) (1000,) 2.2 Thực hành LDA a, Áp dụng LDA và Naive Bayes để mô hình hóa dữ liệu # evaluate lda with naive bayes algorithm for classification from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB # define dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7, n_classes=10) # define the pipeline steps = [(\u0026#39;lda\u0026#39;, LinearDiscriminantAnalysis(n_components=5)), (\u0026#39;m\u0026#39;, GaussianNB())] model = Pipeline(steps=steps) # evaluate model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.366 (0.052) b, Tune n_components của LDA # compare lda number of components with naive bayes algorithm for classification from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB from matplotlib import pyplot # get the dataset def get_dataset(): X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7, n_classes=10) return X, y # get a list of models to evaluate def get_models(): models = dict() for i in range(1,10): steps = [(\u0026#39;lda\u0026#39;, LinearDiscriminantAnalysis(n_components=i)), (\u0026#39;m\u0026#39;, GaussianNB())] models[str(i)] = Pipeline(steps=steps) return models # evaluate a given model using cross-validation def evaluate_model(model, X, y): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores # define dataset X, y = get_dataset() # get the models to evaluate models = get_models() # evaluate the models and store results results, names = list(), list() for name, model in models.items(): scores = evaluate_model(model, X, y) results.append(scores) names.append(name) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (name, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=names, showmeans=True) pyplot.show() Kết quả thực hiện:\n\u0026gt;1 0.148 (0.030) \u0026gt;2 0.209 (0.040) \u0026gt;3 0.265 (0.044) \u0026gt;4 0.280 (0.047) \u0026gt;5 0.296 (0.051) \u0026gt;6 0.324 (0.054) \u0026gt;7 0.341 (0.045) \u0026gt;8 0.363 (0.048) \u0026gt;9 0.366 (0.052)   n_components=9 cho ta kết quả tốt nhất trong tất cả.\nc, Dự đoán trên mẫu dữ liệu mới # make predictions using lda with naive bayes from sklearn.datasets import make_classification from sklearn.pipeline import Pipeline from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB # define dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7, n_classes=10) # define the model steps = [(\u0026#39;lda\u0026#39;, LinearDiscriminantAnalysis(n_components=9)), (\u0026#39;m\u0026#39;, GaussianNB())] model = Pipeline(steps=steps) # fit the model on the whole dataset model.fit(X, y) # make a single prediction row = [[2.3548775, -1.69674567, 1.6193882, -1.19668862, -2.85422348, -2.00998376, 16.56128782, 2.57257575, 9.93779782, 0.43415008, 6.08274911, 2.12689336, 1.70100279, 3.32160983, 13.02048541, -3.05034488, 2.06346747, -3.33390362, 2.45147541, -1.23455205]] yhat = model.predict(row) print(\u0026#39;Predicted Class: %d\u0026#39; % yhat[0]) Kết quả thực hiện:\nPredicted Class: 6 2.3 Thực hành PCA a, Áp dụng PCA và Logistic Regression để mô hình hóa dữ liệu # evaluate pca with logistic regression algorithm for classification from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7) # define the pipeline steps = [(\u0026#39;pca\u0026#39;, PCA(n_components=10)), (\u0026#39;m\u0026#39;, LogisticRegression())] model = Pipeline(steps=steps) # evaluate model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.816 (0.034) b, Tune n_components của PCA # compare pca number of components with logistic regression algorithm for classification from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from matplotlib import pyplot # get the dataset def get_dataset(): X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7) return X, y # get a list of models to evaluate def get_models(): models = dict() for i in range(1,21): steps = [(\u0026#39;pca\u0026#39;, PCA(n_components=i)), (\u0026#39;m\u0026#39;, LogisticRegression())] models[str(i)] = Pipeline(steps=steps) return models # evaluate a given model using cross-validation def evaluate_model(model, X, y): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores # define dataset X, y = get_dataset() # get the models to evaluate models = get_models() # evaluate the models and store results results, names = list(), list() for name, model in models.items(): scores = evaluate_model(model, X, y) results.append(scores) names.append(name) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (name, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=names, showmeans=True) pyplot.xticks(rotation=45) pyplot.show() Kêt quả thực hiện:\n\u0026gt;1 0.542 (0.048) \u0026gt;2 0.713 (0.048) \u0026gt;3 0.720 (0.053) \u0026gt;4 0.723 (0.051) \u0026gt;5 0.725 (0.052) \u0026gt;6 0.730 (0.046) \u0026gt;7 0.805 (0.036) \u0026gt;8 0.800 (0.037) \u0026gt;9 0.814 (0.036) \u0026gt;10 0.816 (0.034) \u0026gt;11 0.819 (0.035) \u0026gt;12 0.819 (0.038) \u0026gt;13 0.819 (0.035) \u0026gt;14 0.853 (0.029) \u0026gt;15 0.865 (0.027) \u0026gt;16 0.865 (0.027) \u0026gt;17 0.865 (0.027) \u0026gt;18 0.865 (0.027) \u0026gt;19 0.865 (0.027) \u0026gt;20 0.865 (0.027)   n_components từ 15 đến 20 cho kết quả tốt nhất.\nc, Tạo dự đoán trên mẫu dữ liệu mới # make predictions using pca with logistic regression from sklearn.datasets import make_classification from sklearn.pipeline import Pipeline from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7) # define the model steps = [(\u0026#39;pca\u0026#39;, PCA(n_components=15)), (\u0026#39;m\u0026#39;, LogisticRegression())] model = Pipeline(steps=steps) # fit the model on the whole dataset model.fit(X, y) # make a single prediction row = [[0.2929949, -4.21223056, -1.288332, -2.17849815, -0.64527665, 2.58097719, 0.28422388, -7.1827928, -1.91211104, 2.73729512, 0.81395695, 3.96973717, -2.66939799, 3.34692332, 4.19791821, 0.99990998, -0.30201875, -4.43170633, -2.82646737, 0.44916808]] yhat = model.predict(row) print(\u0026#39;Predicted Class: %d\u0026#39; % yhat[0]) Kết quả thực hiện:\nPredicted Class: 1 2.4 Thực hành SVD a. Áp dụng SVD và Logistic Regression để mô hình hóa dữ liệu # evaluate svd with logistic regression algorithm for classification from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.decomposition import TruncatedSVD from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7) # define the pipeline steps = [(\u0026#39;svd\u0026#39;, TruncatedSVD(n_components=10)), (\u0026#39;m\u0026#39;, LogisticRegression())] model = Pipeline(steps=steps) # evaluate model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.814 (0.034) b, Tune n_components của SVD # compare svd number of components with logistic regression algorithm for classification from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.decomposition import TruncatedSVD from sklearn.linear_model import LogisticRegression from matplotlib import pyplot # get the dataset def get_dataset(): X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7) return X, y # get a list of models to evaluate def get_models(): models = dict() for i in range(1,20): steps = [(\u0026#39;svd\u0026#39;, TruncatedSVD(n_components=i)), (\u0026#39;m\u0026#39;, LogisticRegression())] models[str(i)] = Pipeline(steps=steps) return models # evaluate a given model using cross-validation def evaluate_model(model, X, y): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores # define dataset X, y = get_dataset() # get the models to evaluate models = get_models() # evaluate the models and store results results, names = list(), list() for name, model in models.items(): scores = evaluate_model(model, X, y) results.append(scores) names.append(name) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (name, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=names, showmeans=True) pyplot.xticks(rotation=45) pyplot.show() Kết quả thực hiện:\n\u0026gt;1 0.542 (0.046) \u0026gt;2 0.626 (0.050) \u0026gt;3 0.719 (0.053) \u0026gt;4 0.722 (0.052) \u0026gt;5 0.721 (0.054) \u0026gt;6 0.729 (0.045) \u0026gt;7 0.802 (0.034) \u0026gt;8 0.800 (0.040) \u0026gt;9 0.814 (0.037) \u0026gt;10 0.814 (0.034) \u0026gt;11 0.817 (0.037) \u0026gt;12 0.820 (0.038) \u0026gt;13 0.820 (0.036) \u0026gt;14 0.825 (0.036) \u0026gt;15 0.865 (0.027) \u0026gt;16 0.865 (0.027) \u0026gt;17 0.865 (0.027) \u0026gt;18 0.865 (0.027) \u0026gt;19 0.865 (0.027)   c, Tạo dự đoán trên mẫu dữ liệu mới # make predictions using svd with logistic regression from sklearn.datasets import make_classification from sklearn.pipeline import Pipeline from sklearn.decomposition import TruncatedSVD from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7) # define the model steps = [(\u0026#39;svd\u0026#39;, TruncatedSVD(n_components=15)), (\u0026#39;m\u0026#39;, LogisticRegression())] model = Pipeline(steps=steps) # fit the model on the whole dataset model.fit(X, y) # make a single prediction row = [[0.2929949, -4.21223056, -1.288332, -2.17849815, -0.64527665, 2.58097719, 0.28422388, -7.1827928, -1.91211104, 2.73729512, 0.81395695, 3.96973717, -2.66939799, 3.34692332, 4.19791821, 0.99990998, -0.30201875, -4.43170633, -2.82646737, 0.44916808]] yhat = model.predict(row) print(\u0026#39;Predicted Class: %d\u0026#39; % yhat[0]) Kết quả thực hiện:\nPredicted Class: 1 3. Kết luận Bài thứ 2 về Dimensionality Reduction, chúng ta đã cùng tìm hiểu 3 thuật toán là LDA, PCA, và SVD. Chúng ta cũng đã thực hành chúng trên tập dữ liệu tự sinh. Đây cũng là bài cuối cùng trong chuỗi các bài viết về DP4ML. Hi vọng đã phần nào giúp các bạn hiểu và biết cách áp dụng các kỹ thuật đó vào quá trình chuẩn bị dữ liệu của mình để có thể tạo ra các mô hình tốt hơn.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\n4. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/101_data_prepeation_for_ml_dimensionality_reduction_2/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Dimensionality Reduction - Phần 2 - How to Perform LDA and PCA Dimensionality Reduction"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 22 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài đầu tiên về về Dimensionality Reduction. Trong bài này, chúng ta sẽ tìm hiểu một số kiến thức cơ bản về nó. Từ bài sau chúng ta sẽ đi vào tìm hiểu và thực hành với từng thuật toán cụ thể.\n1. Dimensionality Reduction là gì? Trong một tập dữ liệu, số lượng các cột hay số lượng features được hiểu là số chiều của dữ liệu. VD, nếu một dataset có 100 features thì số chiều của nó là 100. Dimensionality Reduction là tập hợp các kỹ thuật, thuật toán làm giảm số chiều của dữ liệu. Trước đây, Dimensionality Reduction thường được dùng trong các bài toán Visulization. Tuy nhiên, gần đây thì nó cũng được áp dụng vào việc chuẩn bị dữ liệu để huấn luyện các mô hình học máy.\nTại sao cần phải giảm số chiều của dữ liệu?\nMột tập dữ liệu được coi là High-dimensionality khi số chiều của là hàng trăm hay hàng nghìn. Nếu để nguyên như thế để đưa vào cho model học tập thì sẽ rất dễ dẫn đến hiện tượng Overfitting. Chưa kể là thời gian huấn luyện cũng sẽ rất lâu và cũng không chắc chắn được là nó có hội tụ hay không? Vì thế, thực hiện Dimensionality Reduction là một việc cần thiết, nên được tiến hành trong quá trình chuẩn bị dữ liệu.\n2. Các kỹ thuật, phương pháp thực hiện Dimensionality Reduction 2.1 Feature Selection Không có gì ngạc nhiên khi Feature Selection, về bản chất cũng chính là một phương pháp của Dimensionality Reduction. Mình đã từng đề cập và so sánh Feature Selection và Dimensionality Reduction trong bài viết về Feature Selection rồi. Bạn có thể xem lại tại đây.\n2.2 Matrix Factorization Đây là kỹ thuật xuất phát từ đại số tuyến tính. Ma trận dữ liệu được phân rã thành các thành phần riêng rẽ của nó. Sau đó, chúng được xếp hạng theo một tiêu chí nào đó để rồi chỉ một số thành phần thỏa mãn điều kiện mới được chọn làm đại diện cho tập dữ liệu ban đầu. Principal Components Analysis - PCA là thuật toán phổ biến nhất hoạt động theo phương pháp này.\n2.3 Manifold Learning Manifold Learning là kỹ thuật có nguồn gốc từ lĩnh vực thống kê. Nó hoạt động bằng cách ánh xạ dữ liệu lên một không gian mới có số chiều nhỏ hơn số chiều ban đầu của dữ liệu, nhưng vẫn đảm bảo tính đại diện, tính liên kết của dữ liệu đó. Một số thuật toán phổ biến là:\n Kohonen Self-Organizing Map (SOM) Sammons Mapping Multidimensional Scaling (MDS) t-distributed Stochastic Neighbor Embedding (t-SNE)  2.3 Autoencoder Phương pháp này có thể bạn đã nghe qua. Nó là một kiến trúc của Deep Learning. Bạn có thể tìm hiểu thêm về nó tại đây.\nĐể áp dụng Autoencoder vào Dimensionality Reduction, ta chỉ cần sử dụng thành phần Encoder của nó, phần Decoder có thể bỏ qua.\n3. Một số gợi ý khi thực hiện Dimensionality Reduction  Không có phương pháp nào là tốt nhất trong mọi trường hợp. Hãy thử-sai. Matrix Factorization và Manifold Learning yêu cầu dữ liệu phải có cùng scale hoặc phân phối. Vì thế hãy áp dụng các kỹ thuật như normalization hoặc standardization trước khi thực hiện Dimensionality Reduction.  4. Kết luận Bài đầu tiên về Dimensionality Reduction, chúng ta đã cùng tìm hiểu một số kiến thức rất cơ bản về nó. Bài tiếp theo chúng ta sẽ tìm hiểu chi tiết về kỹ thuật LDA của Dimensionality Reduction. Mời các bạn đón đọc.\n5. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/100_data_prepeation_for_ml_dimensionality_reduction_1/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Dimensionality Reduction - Phần 1 - Giới thiệu Dimensionality Reduction"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 21 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 8 về về Data Transforms. Trong bài này, chúng ta sẽ tìm hiểu cách thức save và load Data Transforms. Chúng ta cũng sẽ thực hành trên bộ dữ liệu giả ngẫu nhiên được sinh ra.\nMột điều rất quan trọng khi giải quyết bài toán ML là các kỹ thuật DP4ML nào áp dụng cho Training Data thì cũng phải được áp dụng cho New Data trong tương lai. Nếu mỗi lần cần dự đoán, lại phải thực hiện lại việc fitting các kỹ thuật DP4ML trên tập train thì sẽ rất mất thời gian. Một cách giải quyết hiệu quả là lưu lại các kỹ thuật DP4ML đó thành 1 file, khi nào cần thì chỉ việc load lên và sử dụng.\n1. Tạo Dataset Chúng ta sẽ sử dụng hàm make_blobs() để tạo ra tập dữ liệu gồm 1000 mẫu, mỗi mẫu có 10 features, và thuộc một trong 2 lớp.\n# example of creating a test dataset and splitting it into train and test sets from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split # prepare dataset X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # summarize the scale of each input variable for i in range(X_test.shape[1]): print('\u0026gt;%d, train: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f' % (i, X_train[:, i].min(), X_train[:, i].max(), X_test[:, i].min(), X_test[:, i].max())) Kết quả thực hiện:\n\u0026gt;0, train: min=-4.532, max=1.496, test: min=-4.208, max=1.502 \u0026gt;1, train: min=1.161, max=8.365, test: min=0.514, max=6.835 \u0026gt;2, train: min=-12.799, max=-2.350, test: min=-13.251, max=-2.298 \u0026gt;3, train: min=-6.736, max=10.176, test: min=-6.540, max=10.960 \u0026gt;4, train: min=-12.151, max=-3.827, test: min=-12.747, max=-4.879 \u0026gt;5, train: min=-10.774, max=5.887, test: min=-11.169, max=6.068 \u0026gt;6, train: min=-8.694, max=1.201, test: min=-8.426, max=1.204 \u0026gt;7, train: min=-5.880, max=3.847, test: min=-5.524, max=3.553 \u0026gt;8, train: min=-10.177, max=0.464, test: min=-10.118, max=1.256 \u0026gt;9, train: min=-8.925, max=4.209, test: min=-8.630, max=3.078 Ta thấy rằng các features có khoảng giá trị khác nhau, ngụ ý rằng chúng ta phải thực hiện một số kỹ thuật DP4ML đối với nó.\n2. Scale the Datase Ta sẽ sử dụng MinMaxScaler để scale các features:\n# example of scaling the dataset from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler # prepare dataset X, y = make_blobs(n_samples=1000, centers=2, n_features=10, random_state=1) # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # define scaler scaler = MinMaxScaler() # fit scaler on the training dataset scaler.fit(X_train) # transform both datasets X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # summarize the scale of each input variable for i in range(X_test.shape[1]): print(\u0026#39;\u0026gt;%d, train: min=%.3f, max=%.3f, test: min=%.3f, max=%.3f\u0026#39; % (i, X_train_scaled[:, i].min(), X_train_scaled[:, i].max(), X_test_scaled[:, i].min(), X_test_scaled[:, i].max())) Kết quả thực hiện:\n\u0026gt;0, train: min=0.000, max=1.000, test: min=0.054, max=1.001 \u0026gt;1, train: min=0.000, max=1.000, test: min=-0.090, max=0.788 \u0026gt;2, train: min=0.000, max=1.000, test: min=-0.043, max=1.005 \u0026gt;3, train: min=0.000, max=1.000, test: min=0.012, max=1.046 \u0026gt;4, train: min=0.000, max=1.000, test: min=-0.072, max=0.874 \u0026gt;5, train: min=0.000, max=1.000, test: min=-0.024, max=1.011 \u0026gt;6, train: min=0.000, max=1.000, test: min=0.027, max=1.000 \u0026gt;7, train: min=0.000, max=1.000, test: min=0.037, max=0.970 \u0026gt;8, train: min=0.000, max=1.000, test: min=0.006, max=1.075 \u0026gt;9, train: min=0.000, max=1.000, test: min=0.022, max=0.914 Tất cả các features bây giờ đều đã nằm trong khoảng [0,1].\n3. Save Model and Data Scaler Sử dụng LogisticRegression, ta sẽ mô hình hóa tập dữ liệu này. Sau đó, cả model và MaxMinScaler sẽ được lưu lại thành file, sử dụng pickle framework.\n# example of fitting a model on the scaled dataset from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LogisticRegression from pickle import dump # prepare dataset X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) # split data into train and test sets X_train, _, y_train, _ = train_test_split(X, y, test_size=0.33, random_state=1) # define scaler scaler = MinMaxScaler() # fit scaler on the training dataset scaler.fit(X_train) # transform the training dataset X_train_scaled = scaler.transform(X_train) # define model model = LogisticRegression(solver=\u0026#39;lbfgs\u0026#39;) model.fit(X_train_scaled, y_train) # save the model dump(model, open(\u0026#39;model.pkl\u0026#39;, \u0026#39;wb\u0026#39;)) # save the scaler dump(scaler, open(\u0026#39;scaler.pkl\u0026#39;, \u0026#39;wb\u0026#39;)) Sau khi chạy đoạn chương trình trên, 2 files model.pkl và scaler.pkl được sinh ra.\n4. Load Model and Data Scaler Phần này, ta sẽ tiến hành load model và scaler object để thực hiện dự đoán trên một dữ liệu mới.\n# load model and scaler and make predictions on new data from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from pickle import load # prepare dataset X, y = make_blobs(n_samples=1000, centers=2, n_features=10, random_state=1) # split data into train and test sets _, X_test, _, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # load the model model = load(open(\u0026#39;model.pkl\u0026#39;, \u0026#39;rb\u0026#39;)) # load the scaler scaler = load(open(\u0026#39;scaler.pkl\u0026#39;, \u0026#39;rb\u0026#39;)) # check scale of the test set before scaling print(\u0026#39;Raw test set range\u0026#39;) for i in range(X_test.shape[1]): print(\u0026#39;\u0026gt;%d, min=%.3f, max=%.3f\u0026#39; % (i, X_test[:, i].min(), X_test[:, i].max())) # transform the test dataset X_test_scaled = scaler.transform(X_test) print(\u0026#39;Scaled test set range\u0026#39;) for i in range(X_test_scaled.shape[1]): print(\u0026#39;\u0026gt;%d, min=%.3f, max=%.3f\u0026#39; % (i, X_test_scaled[:, i].min(), X_test_scaled[:, i].max())) # make predictions on the test set yhat = model.predict(X_test_scaled) # evaluate accuracy acc = accuracy_score(y_test, yhat) print(\u0026#39;Test Accuracy:\u0026#39;, acc) Kết quả thực hiện:\nRaw test set range \u0026gt;0, min=-4.208, max=1.502 \u0026gt;1, min=0.514, max=6.835 \u0026gt;2, min=-13.251, max=-2.298 \u0026gt;3, min=-6.540, max=10.960 \u0026gt;4, min=-12.747, max=-4.879 \u0026gt;5, min=-11.169, max=6.068 \u0026gt;6, min=-8.426, max=1.204 \u0026gt;7, min=-5.524, max=3.553 \u0026gt;8, min=-10.118, max=1.256 \u0026gt;9, min=-8.630, max=3.078 Scaled test set range \u0026gt;0, min=-0.025, max=1.024 \u0026gt;1, min=-0.245, max=0.996 \u0026gt;2, min=-0.147, max=1.306 \u0026gt;3, min=0.012, max=1.106 \u0026gt;4, min=-0.082, max=1.000 \u0026gt;5, min=-0.060, max=1.078 \u0026gt;6, min=-0.054, max=1.154 \u0026gt;7, min=0.000, max=1.168 \u0026gt;8, min=0.013, max=1.140 \u0026gt;9, min=-0.106, max=1.011 Test Accuracy: 1.0 Đoạn chương trình này có thể coi như template cho các bài toán tương tự.\n5. Kết luận Bài thứ 8 cũng là bài cuối cùng về chủ đề Data Transforms. Trong bài này mình đã giới thiệu cách thức save và load model cũng như Transform Object để thuận tiện trong việc sử dụng. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo chúng ta sẽ chuyển sang tìm hiểu về nhiệm vụ Dimensionality Reduction. Mời các bạn đón đọc.\n5. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/99_data_prepeation_for_ml_data_transforms_8/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Data Transform - Phần 8 - How to Save and Load Data Transforms"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 20 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 7 về về Data Transforms. Trong bài này, chúng ta sẽ tìm hiểu cách thức để thực hiện Data Transforms đôi với target dạng numerical. Chúng ta cũng sẽ thực hành trên bộ dữ liệu thực tế.\n1. Hai cách thực hiện Data Transforms cho Target Variable Cũng giống như features, target cũng cần phải được áp dụng các kỹ thuật Data Transforms trước khi đưa cho mô hình để học. Có 2 cách thực hiện Data Transforms cho target là: Manual và Automative\n1.1 Manual Transform of the Target Variable Các bước thực hiện như sau:\n Tạo transform object. VD: MinMaxScaler. Fit transform object vào tập train. Áp dụng transform object vào 2 tập train/test. Chuyển đổi ngược kết quả dự đoán về dạng ban đầu (khi chưa transform).  VD:\n... # create target scaler object target_scaler = MinMaxScaler() target_scaler.fit(train_y) # transform target variables train_y = target_scaler.transform(train_y) test_y = target_scaler.transform(test_y) ... # invert transform on predictions yhat = model.predict(test_X) yhat = target_scaler.inverse_transform(yhat) Cách làm này khá bất tiện vì không sử dụng được một số hàm của scikit-learn, ví dụ như cross_val_score() để nhanh chóng đánh giá model.\n1.2 Automatic Transform of the Target Variable Scikit-learn cung cấp cho chúng ta một cách thực hiện khác dễ dàng hơn, tự dộng hóa hơn bằng cách sử dụng class TransformedTargetRegressor(). TransformedTargetRegressor làm việc theo cách đóng gói model và Tranform Object vào làm một. Transform Object của target sử dung cùng Training Data để fit model. Khi có dữ liệu mới đến để dự đoán, nó sẽ áp dụng Inverse Transform để trả về kết quả đúng như khi chưa thực hiện Data Transforms.\nĐể sử dụng TransformedTargetRegressor, cần cung cấp 2 tham số là model và Transform Object.\n... # define the target transform wrapper wrapped_model = TransformedTargetRegressor(regressor=model, transformer=MinMaxScaler()) ... # use the target transform wrapper wrapped_model.fit(train_X, train_y) yhat = wrapped_model.predict(test_X) 2. Thực hành TransformedTargetRegressor 2.1 Boston Housing Dataset Chúng ta sẽ sử dụng bộ dữ liệu Boston Housing để thực hành việc sử dụng TransformedTargetRegressor. Bộ dữ liệu này đã được sử dụng trong bài số 3 để minh họa cách xử lý Outlier. Nó bao gồm 506 mẫu, mỗi mẫu có 13 features và một target. Base model đạt được kết quả MAE = 6.659.\n2.2 Sử dụng TransformedTargetRegressor  Khai báo pipeline thực hiện Data Transforms cho target và model huấn luyện:  ... # prepare the model with input scaling pipeline = Pipeline(steps=[(\u0026#39;normalize\u0026#39;, MinMaxScaler()), (\u0026#39;model\u0026#39;, HuberRegressor())]) Các features sẽ được scale sử dụng MinMaxScaler, còn model sử dụng là HuberRegressor.\n Định nghĩa TransformedTargetRegressor  ... # prepare the model with target scaling model = TransformedTargetRegressor(regressor=pipeline, transformer=MinMaxScaler())  Đánh giá model sử dụng 10-fold Cross-Validation  ... # evaluate model cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;neg_mean_absolute_error\u0026#39;, cv=cv, n_jobs=-1) Code đầy đủ như sau:\n# example of normalizing input and output variables for regression. from numpy import mean from numpy import absolute from numpy import loadtxt from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedKFold from sklearn.pipeline import Pipeline from sklearn.linear_model import HuberRegressor from sklearn.preprocessing import MinMaxScaler from sklearn.compose import TransformedTargetRegressor # load data dataset = loadtxt(\u0026#39;housing.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split into inputs and outputs X, y = dataset[:, :-1], dataset[:, -1] # prepare the model with input scaling pipeline = Pipeline(steps=[(\u0026#39;normalize\u0026#39;, MinMaxScaler()), (\u0026#39;model\u0026#39;, HuberRegressor())]) # prepare the model with target scaling model = TransformedTargetRegressor(regressor=pipeline, transformer=MinMaxScaler()) # evaluate model cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;neg_mean_absolute_error\u0026#39;, cv=cv, n_jobs=-1) # convert scores to positive scores = absolute(scores) # summarize the result s_mean = mean(scores) print(\u0026#39;Mean MAE: %.3f\u0026#39; % (s_mean)) Kết quả thực hiện:\nMean MAE: 3.203 Kết quả này tốt hơn Base model rất nhiều rồi.\nThử thêm một phép transform nữa vào features và target xem sao:\n# example of power transform input and output variables for regression. from numpy import mean from numpy import absolute from numpy import loadtxt from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedKFold from sklearn.pipeline import Pipeline from sklearn.linear_model import HuberRegressor from sklearn.preprocessing import PowerTransformer from sklearn.preprocessing import MinMaxScaler from sklearn.compose import TransformedTargetRegressor # load data dataset = loadtxt(\u0026#39;housing.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split into inputs and outputs X, y = dataset[:, :-1], dataset[:, -1] # prepare the model with input scaling and power transform steps = list() steps.append((\u0026#39;scale\u0026#39;, MinMaxScaler())) steps.append((\u0026#39;power\u0026#39;, PowerTransformer())) steps.append((\u0026#39;model\u0026#39;, HuberRegressor())) pipeline = Pipeline(steps=steps) # prepare the model with target scaling model = TransformedTargetRegressor(regressor=pipeline, transformer=PowerTransformer()) # evaluate model cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;neg_mean_absolute_error\u0026#39;, cv=cv, n_jobs=-1) # convert scores to positive scores = absolute(scores) # summarize the result s_mean = mean(scores) print(\u0026#39;Mean MAE: %.3f\u0026#39; % (s_mean)) Kết quả thực hiện:\nMean MAE: 2.972 Ta thấy MAE có sự cải thiện thêm được 1 chút, giảm từ 3.203 xuống còn 2.972. Bạn có thể thử thêm các phép biến đổi khác xem kết quả có được cải thiện hơn ko?\n3. Kết luận Bài thứ 7 về chủ đề Data Transforms, mình đã giới thiệu về kỹ thuật class TransformedTargetRegressor() của thư viện scikit-learn giúp chúng ta thực hiện được việc Data Transforms cho target. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo chúng ta sẽ tìm hiểu cách thức save và load Data Transforms. Mời các bạn đón đọc.\n4. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/98_data_prepeation_for_ml_data_transforms_7/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Data Transform - Phần 7 - How to Transform the Target in Regression"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 19 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 6 về về Data Transforms. Trong bài này, chúng ta sẽ tìm hiểu cách thức để thực hiện Data Transforms đôi với những bộ dữ liệu có cả 2 loại features là numerical và categorical. Chúng ta cũng sẽ thực hành trên bộ dữ liệu thực tế.\n1. \u0026ldquo;Best Practice\u0026rdquo; khi cần thực hiện nhiều kỹ thuật DP4ML Khi chuẩn bị dữ liệu cho các mô hình ML, chúng ta thường phải thực hiện qua nhiều công đoạn: Thay thế Missing Data, Scaling Numerical Values, \u0026hellip; Cách làm tốt nhất là gom chúng lại với nhau trong 1 Pipeline để tiện cho việc xử lý.\n... # define pipeline pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39;, SimpleImputer(strategy=\u0026#39;median\u0026#39; )), (\u0026#39;s\u0026#39;, MinMaxScaler())]) # transform training data train_X = scaler.fit_transform(train_X) 2. Khó khăn khi thực hiện Data Transforms cho các features khác loại Các bài viết từ trước đến giờ chỉ đề cập đến bộ dữ liệu chỉ có 1 loại features là numerical hoặc categorical. Nhưng trong thực tế, dữ liệu thường sẽ có cả 2 loại này. Cách làm truyền thống là tách riêng các features theo từng loại để xử lý theo các kỹ thuật của chúng. Sau đó mới gộp lại thành 1 dữ liệu chung. Cách làm này nghe thì đơn giản nhưng lại khá mất thời gian và hiệu quả không cao.\nRất may là scikit-learn cung cấp cho chúng ta class ColumnTransformer() để dễ dàng thực hiện nhiệm vụ này.\n3. Sử dụng ColumnTransformer ColumnTransformer() cho phép chúng ta áp dụng một hoặc nhiều kỹ thuật DP4ML tới mội hoặc nhiều features riêng biệt. Như vậy, ta có thể thực hiện các phép biến đổi A, B, C lên các features dạng numerical, thực hiện các phép biến đổi D, E, F lên các features dạng categorical.\nĐể sử dụng ColumnTransformer(), bạn phải chỉ ra 3 thông tin:\n Tên của phép biến đổi Kỹ thuật biến đổi Số thứ tự features cần áp dụng phép biến đổi  VD:\ntransformer = ColumnTransformer(transformers=[( \u0026#39; cat \u0026#39; , OneHotEncoder(), [0, 1])]) Một VD khác, áp dụng SimpleImputer với median tới các numerical features tại cột số 0, 1. Áp dụng SimpleImputer với most_frequent tới categorical features tại cột số 2, 3.\n... t = [(\u0026#39;num\u0026#39;, SimpleImputer(strategy=\u0026#39;median\u0026#39;), [0, 1]), (\u0026#39;cat\u0026#39;, SimpleImputer(strategy=\u0026#39;most_frequent\u0026#39;), [2, 3])] transformer = ColumnTransformer(transformers=t) Mặc định, các features mà không được đề cập đến trong ColumnTransformer thì sẽ bị xóa bỏ khỏi dataset. Để thay đổi điều này, set giá trị cho tham sô remainder=\u0026lsquo;passthrough\u0026rsquo;.\nMột khi ColumnTransformer() đã được khai báo, ta có thể áp dụng nó vào bộ dữ liệu của mình:\ndata = transformer.fit_transform(data) ColumnTransformer() cũng có thể được sử dụng trong một Pipeline. Đây là một Best Practice để đảm bảo rằng các kỹ thuật DP4ML được thực hiện đầy đủ khi huấn luyện model cũng như khi sử dụng model để dự đoán.\n... # define model model = LogisticRegression() # define transform transformer = ColumnTransformer(transformers=[(\u0026#39;cat\u0026#39;, OneHotEncoder(), [0, 1])]) # define pipeline pipeline = Pipeline(steps=[(\u0026#39;t\u0026#39;, transformer), (\u0026#39;m\u0026#39;, model)]) # fit the model on the transformed data model.fit(train_X, train_y) # make predictions 4. Thực hành ColumnTransformer() 4.1 Abalone Regression dataset Abalone Regression dataset là tập dữ liệu chứa thông tin về con bào ngư. Nó bao gồm 4177 mẫu, mỗi mẫu có 8 features và 1 target. Nhiệm vụ của các mô hình ML đối với tập dữ liệu này là dự đoán số tuổi của bào ngư. Download và tìm hiểu thông tin về Abalone Regression dataset tại đây và tại đây\nBase model đang đạt được kết quả MAE=2.363(std=0.092) khi đánh giá trên 10-fold cross-validation.\nVD một vài mẫu của bộ dữ liệu này:\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15 M,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7 F,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9 M,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10 I,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7 ... 4.2 Thực hiện Data Transforms và mô hình hóa dữ liệu Ta thấy rằng, feature đầu tiên có dạng categorical, trong khi các features còn lại ở dạng numerical. Do đó, mình dự dịnh sẽ thực hiện One-Hot Encoding đối với feature đầu tiên đó, các features còn lại sẽ được thực hiện normalization. Cuối cùng, model SVR sẽ được sử dụng để mô hình hóa dữ liệu.\n# example of using the ColumnTransformer for the Abalone dataset from numpy import mean from numpy import std from numpy import absolute from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import KFold from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import MinMaxScaler from sklearn.svm import SVR # load dataset dataframe = read_csv(\u0026#39;abalone.csv\u0026#39;, header=None) # split into inputs and outputs last_ix = len(dataframe.columns) - 1 X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix] print(X.shape, y.shape) # determine categorical and numerical features numerical_ix = X.select_dtypes(include=[\u0026#39;int64\u0026#39;, \u0026#39;float64\u0026#39;]).columns categorical_ix = X.select_dtypes(include=[\u0026#39;object\u0026#39;, \u0026#39;bool\u0026#39;]).columns # define the data preparation for the columns t = [(\u0026#39;cat\u0026#39;, OneHotEncoder(), categorical_ix), (\u0026#39;num\u0026#39;, MinMaxScaler(), numerical_ix)] col_transform = ColumnTransformer(transformers=t) # define the model model = SVR(kernel=\u0026#39;rbf\u0026#39;,gamma=\u0026#39;scale\u0026#39;,C=100) # define the data preparation and modeling pipeline pipeline = Pipeline(steps=[(\u0026#39;prep\u0026#39;,col_transform), (\u0026#39;m\u0026#39;, model)]) # define the model cross-validation configuration cv = KFold(n_splits=10, shuffle=True, random_state=1) # evaluate the pipeline using cross validation and calculate MAE scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;neg_mean_absolute_error\u0026#39;, cv=cv, n_jobs=-1) # convert MAE scores to positive values scores = absolute(scores) # summarize the model performance print(\u0026#39;MAE: %.3f(%.3f)\u0026#39; % (mean(scores), std(scores))) Kết quả thực hiện:\n(4177, 8) (4177,) MAE: 1.465 (0.047) Code này có thể coi như template cho các dự án tương tự của bạn.\n5. Kết luận Bài thứ 6 về chủ đề Data Transforms, mình đã giới thiệu về kỹ thuật class ColumnTransformer() của thư viện scikit-learn giúp chúng ta thực hiện được các kỹ thuật Data Transforms cho từng feature riêng biệt. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo chúng ta sẽ tìm hiểu cách thức thực hiện Data Transforms target thay vì features như các bài trước đó. Mời các bạn đón đọc.\n6. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/97_data_prepeation_for_ml_data_transforms_6/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Data Transform - Phần 6 - How to Transform Numerical and Categorical Data"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 18 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 5 về về Data Transforms. Trong bài này, chúng ta sẽ tìm hiểu cách thức để sinh ra thêm features mới cho bộ dữ liệu huấn luyện ML model. Chúng ta cũng sẽ thực hành trên bộ dữ liệu thực tế.\nThông thường, các features của một mô hình dự đoán tương tác theo những cách mà ta không biết trước và thường là phi tuyến tính. Những tương tác này có thể được xác định và mô hình hóa bằng một thuật toán ML. Một cách tiếp cận khác là thiết kế các features mới thể hiện những tương tác này và xem liệu chúng có cải thiện hiệu suất mô hình hay không? Features mới có thể được tạo ra bằng cách thực hiện lũy thừa các giá trị đang có của features. Trong một số trường hợp, việc làm này có thể giúp thể hiện tốt hơn các mối quan hệ quan trọng giữa các features và target.\n1. Polynomial Features and Interaction Features Polynomal features là các features mới được tạo ra bằng cách lấy lũy thừa từ các features đã tồn tại. VD, Polynomial feature sinh ra từ feature $X$ là $X^2$, \u0026hellip;\nBậc (degree) của Polynomial feature quy định số lượng features mới được tạo ra. VD, bậc 3 thì sẽ có 3 features mới được tạo ra từ một feature ban đầu.\nInteraction features là các features mới được sinh ra bằng cách kết hợp 2 features đã tồn tại qua một phép toán. VD, nhân feature $X$ với feature $Y$ ta được Interaction feature $Z$.\n2. Polynomial/Interaction Feature Transform Polynomial/Interaction Feature Transform được implemented trong thư viện scikit-learn thông qua class PolynomialFeatures(). Các features có thể được tạo ra từ class này là:\n Bias (=1) Lũy thừa theo bậc (VD: $x^1, x^2, \u0026hellip;$) Kết hợp giữa các features đôi một (VD: $x_1 * x_2, x_2 * x_3, \u0026hellip;$)  VD, với 2 features ban đầu là 2 và 3, bậc là 2 thì các features có thể được tạo ra là:\n 1 $2^1 = 2$ $3^1 = 3$ $2^2 = 4$ $3^2 = 9$ 2 *3 = 6  Các tham số quan trọng của class PolynomialFeatures() là:\n degree: bậc của Polynomial. interaction_only: chỉ tính tương tác giữa các features ban đầu, không tính features mới sinh ra. include_bias: có bao gồm bias hay không? Mặc định là True.  VD sử dụng class PolynomialFeatures() trong scikit-learn:\n# demonstrate the types of features created from numpy import asarray from sklearn.preprocessing import PolynomialFeatures # define the dataset data = asarray([[2,3],[2,3],[2,3]]) print(data) # perform a polynomial features transform of the dataset trans = PolynomialFeatures(degree=2) data = trans.fit_transform(data) print(data) Kết quả thực hiện:\n[[2 3] [2 3] [2 3]] [[1. 2. 3. 4. 6. 9.] [1. 2. 3. 4. 6. 9.] [1. 2. 3. 4. 6. 9.]] 3. Thực hành Polynomial/Interaction Feature Transform trên tập dữ liệu Sonar 3.1, Áp dụng Polynomial/Interaction Feature Transform trên tập Sonar # visualize a polynomial features transform of the sonar dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import PolynomialFeatures # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a polynomial features transform of the dataset trans = PolynomialFeatures(degree=3) data = trans.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # summarize print(dataset.shape) Kết quả thực hiện:\n(208, 39711) Ta thấy số lượng features tăng từ 60 lên 39711.\n3.2 Mô hình dữ liệu với kNN # evaluate knn on the sonar dataset with polynomial features transform from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import PolynomialFeatures from sklearn.pipeline import Pipeline # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline trans = PolynomialFeatures(degree=3) model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.800 (0.077) Độ chính xác tăng lên 1 chút so với khi không thực hiện bất kỳ phép biến đổi nào, 79.7%.\n3.3 Tune bậc của Polynomial Ta chỉ chọn giá trị của degree từ 0 đến 4, vì càng nhiều features sinh ra, khả năng xảy ra Overfitting càng cao.\n# explore the effect of degree on accuracy for the polynomial features transform from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import PolynomialFeatures from sklearn.preprocessing import LabelEncoder from sklearn.pipeline import Pipeline from matplotlib import pyplot # get the dataset def get_dataset(filename): # load dataset dataset = read_csv(filename, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) return X, y # get a list of models to evaluate def get_models(): models = dict() for d in range(1,5): # define the pipeline trans = PolynomialFeatures(degree=d) model = KNeighborsClassifier() models[str(d)] = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) return models # evaluate a given model using cross-validation def evaluate_model(model, X, y): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores # define dataset X, y = get_dataset(\u0026#39;sonar.csv\u0026#39;) # get the models to evaluate models = get_models() # evaluate the models and store results results, names = list(), list() for name, model in models.items(): scores = evaluate_model(model, X, y) results.append(scores) names.append(name) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (name, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=names, showmeans=True) pyplot.show() Kết quả thực hiện:\n\u0026gt;1 0.797 (0.073) \u0026gt;2 0.793 (0.085) \u0026gt;3 0.800 (0.077) \u0026gt;4 0.795 (0.079)   Ta thấy chỉ có degree=3 thì độ chính xác mới cao hơn so với khi không thực hiện bất kỳ phép biến đổi nào, 79.7%.\n4. Kết luận Bài thứ 5 về chủ đề Data Transforms, mình đã giới thiệu về kỹ thuật sinh thêm features mới là Polynominal/Interaction. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo chúng ta sẽ tìm hiểu cách thức thực hiện Data Transforms đối với dữ liệu cả 2 loại features là categorical và numerical. Mời các bạn đón đọc.\n5. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/96_data_prepeation_for_ml_data_transforms_5/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Data Transform - Phần 5 - How to Derive New Input Variables"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 17 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 4 về về Data Transforms. Trong bài này, chúng ta sẽ tìm hiểu cách thức thực hiện Transforms để chuyển đổi dữ liệu từ dạng numerical sang dạng categorical. Chúng ta cũng sẽ thực hành trên bộ dữ liệu thực tế.\n1. Discretization Transforms Các features dạng numerical thường có phân phối ngẫu nhiên bất kỳ. Ngoài cách đưa chúng về gần với phân phối Gaussian như bài trước thì chúng ta có thể áp dụng phép biến đổi Discretization để chuyển phân phối của chúng sang một dạng khác mà có thể giúp các mô hình học dễ hơn.\nDiscretization Transforms hay Categorization Transforms là một cách gom các giá trị của feature thành một số hữu hạn các nhóm, hay bin.\nVí dụ: Feature Age có giá trị nằm trong khoảng từ 1-100, ta chia thành 3 bins: 1-30, 31-60, 61-100.\nCó 3 kỹ thuật thực hiện Discretization Transforms:\n Uniform: Chiều rộng của mỗi bin trên trục X là bằng nhau. Quantile: Số lượng phần tử trong mỗi bin là bằng nhau. Clustered: Mỗi bin có một điểm trung tâm.  Trong scikit-learn, Discretization Transforms được implemented bởi lớp KBinsDiscretizer(). 3 tham số:\n strategy: chỉ ra tên kỹ thuật thực hiện (uniform, quantile, clustered), n_bins: chỉ ra số lượng bin được tạo ra, encode: quy định cách thức chuyển đổi (ordinal, one-hot).  Ví dụ về việc sử dụng KBinsDiscretizer():\n# demonstration of the discretization transform from numpy.random import randn from sklearn.preprocessing import KBinsDiscretizer from matplotlib import pyplot # generate gaussian data sample data = randn(1000) # histogram of the raw data pyplot.hist(data, bins=25) pyplot.show() # reshape data to have rows and columns data = data.reshape((len(data),1)) # discretization transform the raw data kbins = KBinsDiscretizer(n_bins=10, encode=\u0026#39;ordinal\u0026#39;, strategy=\u0026#39;uniform\u0026#39;) data_trans = kbins.fit_transform(data) # summarize first few rows print(data_trans[:10, :]) # histogram of the transformed data pyplot.hist(data_trans, bins=10) pyplot.show() Kết quả thực hiện:\n Dữ liệu ban đầu:    Dữ liệu sau khi thực hiện Discretization:  [[5.] [6.] [4.] [6.] [5.] [5.] [4.] [4.] [5.] [5.]]   2. Thực hành Discretization trên tập dữ liệu Sonar 2.1 Uniform Discretization Transform a, Áp dụng Uniform Discretization Transform trên tập dữ liệu Sonar # visualize a uniform ordinal discretization transform of the sonar dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import KBinsDiscretizer from matplotlib import pyplot # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a uniform discretization transform of the dataset trans = KBinsDiscretizer(n_bins=10, encode=\u0026#39;ordinal\u0026#39;, strategy=\u0026#39;uniform\u0026#39;) data = trans.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show()   Ta thấy hình dạng histogram của các features sau khi biến đổi gần giống như của dữ liệu ban đầu.\nb, Mô hình hóa thuật toán kNN trên dữ liệu đã biến đổi # evaluate knn on the sonar dataset with uniform ordinal discretization transform from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import KBinsDiscretizer from sklearn.pipeline import Pipeline # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline trans = KBinsDiscretizer(n_bins=10, encode=\u0026#39;ordinal\u0026#39;, strategy=\u0026#39;uniform\u0026#39;) model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.829 (0.079) Hãy nhớ lại bài trước, độ chính xác khi mô hình hóa tập Sonar khi chưa áp dụng bất kỳ phép biến đổi nào là 79.7%. Với việc áp dụng Uniform Discretization Transform, độ chính xác đã tăng lên 82.9%.\n2.2 Clustered Discretization Transform a, Áp dụng Clustered Discretization Transform trên tập dữ liệu Sonar # visualize a k-means ordinal discretization transform of the sonar dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import KBinsDiscretizer from matplotlib import pyplot # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a k-means discretization transform of the dataset trans = KBinsDiscretizer(n_bins=3, encode=\u0026#39;ordinal\u0026#39;, strategy=\u0026#39;kmeans\u0026#39;) data = trans.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show()   Chúng ta có thể thấy rằng các các features được tổ chức thành ba nhóm, một số nhóm trong số đó có vẻ khá đồng đều về mặt quan sát, và một số khác thì ít hơn.\nb, Mô hình hóa thuật toán kNN trên dữ liệu đã biến đổi # evaluate knn on the sonar dataset with k-means ordinal discretization transform from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import KBinsDiscretizer from sklearn.pipeline import Pipeline # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline trans = KBinsDiscretizer(n_bins=3, encode=\u0026#39;ordinal\u0026#39;, strategy=\u0026#39;kmeans\u0026#39;) model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.814 (0.084) Ta thấy độ chính xác giảm đi một chút so với khi áp dụng Uniform Discretization Transform, chỉ còn 81.4%.\n2.3 Quantile Discretization Transform a, Áp dụng Quantile Discretization Transform trên tập dữ liệu Sonar # visualize a quantile ordinal discretization transform of the sonar dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import KBinsDiscretizer from matplotlib import pyplot # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a quantile discretization transform of the dataset trans = KBinsDiscretizer(n_bins=10, encode=\u0026#39;ordinal\u0026#39;, strategy=\u0026#39;quantile\u0026#39;) data = trans.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show()   Ta thấy tất các features sau khi biến đổi đều có cùng kiểu phân phối và chúng được sắp xếp vào 10 bins khác nhau, số lượng giá trị của mỗi bin là như nhau.\nb, Mô hình hóa thuật toán kNN trên dữ liệu đã biến đổi # evaluate knn on the sonar dataset with quantile ordinal discretization transform from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import KBinsDiscretizer from sklearn.pipeline import Pipeline # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline trans = KBinsDiscretizer(n_bins=10, encode=\u0026#39;ordinal\u0026#39;, strategy=\u0026#39;quantile\u0026#39;) model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.840 (0.072) Độ chính xác trong trường hợp này là cao nhất so với 2 trường hợp kia.\nc, Tune số lượng bins n_bins là tham số có ảnh hưởng lớn đến khả năng học hỏi của model. Ta thử đi tuning chúng xem sao.\n# explore number of discrete bins on classification accuracy from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import KBinsDiscretizer from sklearn.preprocessing import LabelEncoder from sklearn.pipeline import Pipeline from matplotlib import pyplot # get the dataset def get_dataset(): # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) return X, y # get a list of models to evaluate def get_models(): models = dict() for i in range(2,11): # define the pipeline trans = KBinsDiscretizer(n_bins=i, encode=\u0026#39;ordinal\u0026#39;, strategy=\u0026#39;quantile\u0026#39;) model = KNeighborsClassifier() models[str(i)] = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) return models # evaluate a given model using cross-validation def evaluate_model(model, X, y): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores # get the dataset X, y = get_dataset() # get the models to evaluate models = get_models() # evaluate the models and store results results, names = list(), list() for name, model in models.items(): scores = evaluate_model(model, X, y) results.append(scores) names.append(name) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (name, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=names, showmeans=True) pyplot.show() Kết quả thực hiện:\n\u0026gt;2 0.822 (0.066) \u0026gt;3 0.869 (0.073) \u0026gt;4 0.838 (0.078) \u0026gt;5 0.838 (0.070) \u0026gt;6 0.836 (0.071) \u0026gt;7 0.852 (0.072) \u0026gt;8 0.837 (0.077) \u0026gt;9 0.841 (0.069) \u0026gt;10 0.840 (0.072)   Khá ngạc nhiên là khi n_bins=3 ta thu được độ chính xác cao nhất 86.9%. Tuy nhiên khi nhìn vào biểu đồ Box thì phân phối của nó đang hơi lệch về bên phải.\n3. Kết luận Bài thứ 4 về chủ đề Data Transforms, mình đã giới thiệu 3 kỹ thuật thực hiện Data Transforms để chuyển features dạng numerical sang dạng categorical, đó là Uniform Discretization, Clustered Discretization, và Quantile Discretization. Việc chuyển đổi này nhằm mục đích thay đổi phân phối của các features mà trong 1 số trường hợp nó sẽ mang lại hiệu quả cho model. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo chúng ta sẽ tìm hiểu cách thức sinh thêm features mới sử dụng kỹ thuật Polynominal/Interaction. Mời các bạn đón đọc.\n4. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/95_data_prepeation_for_ml_data_transforms_4/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Data Transform - Phần 4 - How to Transform Numerical to Categorical Data"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 16 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 3 về về Data Transforms. Trong bài này, chúng ta sẽ tìm hiểu cách thức thực hiện Transforms làm cho phân phối của các features trở nên gần với Gaussian hơn. Chúng ta cũng sẽ thực hành trên bộ dữ liệu thực tế.\n1. Make Data More Gaussian Các thuật toán ML như Linear Regression, Gaussian Naive Bayes giả định các features có phân phối xác suất Gaussian. Trong khi đó, dữ liệu của bạn có thể không có phân phối Gaussian hoặc chỉ gần giống Gaussian do có giá trị outlier. Nếu chúng ta có thể làm cho phân phối của dữ liệu gần với Gaussian hơn thì độ chính xác của mô hình có thể được tăng lên đáng kể. Power Transform như Box-Cox transform và Yeo-Johnson transform, Quantile Transforms như Normal Quantile transform và Uniform Quantile transform là các giải pháp tự động giúp bạn thực hiện các biến đổi này.\n1.1 Ví dụ sử dụng Power Transforms trong scikit-learn Power Transforms được cung cấp trong thư viện scikit-learn thông qua lớp PowerTransformer(). Tham số method sẽ chỉ ra phương pháp thực hiện Power Transforms là Box-Cox hay Yeo-Johnson.\n# demonstration of the power transform on data with a skew from numpy import exp from numpy.random import randn from sklearn.preprocessing import PowerTransformer from matplotlib import pyplot # generate gaussian data sample data = randn(1000) # add a skew to the data distribution data = exp(data) # histogram of the raw data with a skew pyplot.hist(data, bins=25) pyplot.show() # reshape data to have rows and columns data = data.reshape((len(data),1)) # power transform the raw data power = PowerTransformer(method=\u0026#39;yeo-johnson\u0026#39;, standardize=True) data_trans = power.fit_transform(data) # histogram of the transformed data pyplot.hist(data_trans, bins=25) pyplot.show()  Phân phối dữ liệu ban đầu:    Phân phối dữ liệu sau khi thực hiện Power Transforms   1.2 Ví dụ sử dụng Quantile Transforms trong scikit-learn Quantile Transforms được cung cấp trong thư viện scikit-learn thông qua lớp QuantileTransformer(). Tham số output_distribution sẽ chỉ ra phương pháp thực hiện Power Transforms là normal hay uniform. Tham số n_quantiles chỉ ra số lượng quantile sử dụng, nó không được lớn hơn số lượng mẫu trong dataset.\n# demonstration of the quantile transform from numpy import exp from numpy.random import randn from sklearn.preprocessing import QuantileTransformer from matplotlib import pyplot # generate gaussian data sample data = randn(1000) # add a skew to the data distribution data = exp(data) # histogram of the raw data with a skew pyplot.hist(data, bins=25) pyplot.show() # reshape data to have rows and columns data = data.reshape((len(data),1)) # quantile transform the raw data quantile = QuantileTransformer(output_distribution=\u0026#39;normal\u0026#39;) data_trans = quantile.fit_transform(data) # histogram of the transformed data pyplot.hist(data_trans, bins=25) pyplot.show()  Phân phối dữ liệu ban đầu    Phân phối dữ liệu sau khi thực hiện Quantile Transforms   Ta thấy, dữ liệu trước và sau khi thực hiện Transforms đã có sự biến đổi rõ rệt. Dữ liệu ban đầu có phân phối lệch hẳn về bên trái, còn dữ liệu sau có phân phối tương đối Gaussian với mean ~ 0 và std ~ 1.\n2. Thực hành Power Transforms 2.1 Sonar Dataset Sonar Dataset là bộ dữ liệu cho bài toán Binary Classification. Nó gồm 208 mẫu dữ liệu, mỗi mẫu có 60 features và 1 target. Baseline model đạt được độ chính xác là 53.4%. 88% là độ chính xác cao nhất có thể đạt được trên tập dữ liệu này đến hiện tại. Bạn có thể download và tìm hiểu thêm về nó tại đây và tại đây.\nLoad và kiểm tra thử tập dữ liệu này:\n# load and summarize the sonar dataset from pandas import read_csv from matplotlib import pyplot # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) # summarize the shape of the dataset print(dataset.shape) # summarize each variable print(dataset.describe()) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show() Kết quả thực hiện:\n(208, 61) 0 1 2 3 4 5 6 7 ... 52 53 54 55 56 57 58 59 count 208.000000 208.000000 208.000000 208.000000 208.000000 208.000000 208.000000 208.000000 ... 208.000000 208.000000 208.000000 208.000000 208.000000 208.000000 208.000000 208.000000 mean 0.029164 0.038437 0.043832 0.053892 0.075202 0.104570 0.121747 0.134799 ... 0.010709 0.010941 0.009290 0.008222 0.007820 0.007949 0.007941 0.006507 std 0.022991 0.032960 0.038428 0.046528 0.055552 0.059105 0.061788 0.085152 ... 0.007060 0.007301 0.007088 0.005736 0.005785 0.006470 0.006181 0.005031 min 0.001500 0.000600 0.001500 0.005800 0.006700 0.010200 0.003300 0.005500 ... 0.000500 0.001000 0.000600 0.000400 0.000300 0.000300 0.000100 0.000600 25% 0.013350 0.016450 0.018950 0.024375 0.038050 0.067025 0.080900 0.080425 ... 0.005075 0.005375 0.004150 0.004400 0.003700 0.003600 0.003675 0.003100 50% 0.022800 0.030800 0.034300 0.044050 0.062500 0.092150 0.106950 0.112100 ... 0.009550 0.009300 0.007500 0.006850 0.005950 0.005800 0.006400 0.005300 75% 0.035550 0.047950 0.057950 0.064500 0.100275 0.134125 0.154000 0.169600 ... 0.014900 0.014500 0.012100 0.010575 0.010425 0.010350 0.010325 0.008525 max 0.137100 0.233900 0.305900 0.426400 0.401000 0.382300 0.372900 0.459000 ... 0.039000 0.035200 0.044700 0.039400 0.035500 0.044000 0.036400 0.043900 [8 rows x 60 columns]   Có thể thấy, hầu hết các phân phối của các features đều bị lệch, không giống với Gaussian.\nThử mô hình hóa tập dữ liệu này bằng thuật toán kNN:\n# evaluate knn on the raw sonar dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define and configure the model model = KNeighborsClassifier() # evaluate the model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report model performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.797 (0.073) 2.2 Box-Cox Transform Box-Cox transform được đặt theo tên hai tác giả của phương pháp này. Nó chỉ áp dụng cho các giá trị dương.\nBởi vì tập dữ liệu Sonar có chứa giá trị 0 nên không thể sử dụng trực tiếp cho Box-Cox được. Nếu bạn cố tình thực hiện sẽ sinh ra lỗi ValueError: The Box-Cox transformation can only be applied to strictly positive data.\nĐể giải quyết vấn đề này, ta sử dụng thêm một phép biến đổi normalization với feature_range=(1,2).\n# visualize a box-cox transform of the scaled sonar dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import PowerTransformer from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline from matplotlib import pyplot # Load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a box-cox transform of the dataset scaler = MinMaxScaler(feature_range=(1, 2)) power = PowerTransformer(method=\u0026#39;box-cox\u0026#39;) pipeline = Pipeline(steps=[(\u0026#39;s\u0026#39;, scaler),(\u0026#39;p\u0026#39;, power)]) data = pipeline.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show()   Ta thấy rằng phân phối của các features đã giống Gaussian hơn rất nhiều.\nThử mô hình hóa tập dữ liệu sau khi thực hiện Power Transforms này với thuật toán kNN:\n# evaluate knn on the box-cox sonar dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import PowerTransformer from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline scaler = MinMaxScaler(feature_range=(1, 2)) power = PowerTransformer(method=\u0026#39;box-cox\u0026#39;) model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;s\u0026#39;, scaler),(\u0026#39;p\u0026#39;, power), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.811 (0.085) Độ chính xác đã tăng lên 1 chút, từ 79.7% lên thành 81.1%.\n2.3 Yeo-Johnson transform Yeo-Johnson transform cũng được đặt tên theo 2 tác giả của nó. Khác với Box-Cox là Yeo-Johnson không yêu cầu dữ liệu phải dương. Nó hỗ trợ cả giá trị âm và 0.\n# visualize a yeo-johnson transform of the sonar dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import PowerTransformer from matplotlib import pyplot # Load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a yeo-johnson transform of the dataset pt = PowerTransformer(method=\u0026#39;yeo-johnson\u0026#39;) data = pt.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show()   Giống như Box-Cox, phân phối của các features giống Gaussian hơn rất nhiều so với dữ liệu gốc ban đầu.\nMô hình hóa tập dữ liệu này sau khi thực hiện Power Transforms với thuật toán kNN:\n# evaluate knn on the yeo-johnson sonar dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import PowerTransformer from sklearn.pipeline import Pipeline # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline power = PowerTransformer(method=\u0026#39;yeo-johnson\u0026#39;) model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;p\u0026#39;, power), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.808 (0.082) Độ chính cũng đc cải thiện hơn một chút so với mốc 79.7% ban đầu, lên thành 80.8%.\nHãy nhớ lại bài trước, Standardization cũng giúp ta đưa dữ liệu về phân phối gần Gaussian. Thử kết hợp cả 2 kỹ thuật này xem sao.\n# evaluate knn on the yeo-johnson standardized sonar dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import PowerTransformer from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline scaler = StandardScaler() power = PowerTransformer(method=\u0026#39;yeo-johnson\u0026#39;) model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;s\u0026#39;, scaler), (\u0026#39;p\u0026#39;, power), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.816 (0.077) Thật vui là độ chính xác đã tăng lên một chút, thành 81.6%.\n3. Thực hành Quantile Transforms 3.1 Normal Quantile Transform Áp dụng Normal Quantile Transform trên tập Sonar.\n# visualize a normal quantile transform of the sonar dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import QuantileTransformer from matplotlib import pyplot # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a normal quantile transform of the dataset trans = QuantileTransformer(n_quantiles=100, output_distribution=\u0026#39;normal\u0026#39;) data = trans.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show()   Phân phối của các features sau khi thực hiện Normal Quantile Transform đã giống Gaussian hơn rất nhiều.\nMô hình hóa tập dữ liệu với thuật toán kNN:\n# evaluate knn on the sonar dataset with normal quantile transform from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import QuantileTransformer from sklearn.pipeline import Pipeline # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline trans = QuantileTransformer(n_quantiles=100, output_distribution=\u0026#39;normal\u0026#39;) model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.817 (0.087) Độ chính xác trong trường hợp này cao hơn tất cả các trường hợp của Power Transforms một chút.\n3.2 Uniform Quantile Transform Áp dụng Normal Quantile Transform trên tập Sonar.\n# visualize a uniform quantile transform of the sonar dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import QuantileTransformer from matplotlib import pyplot # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a uniform quantile transform of the dataset trans = QuantileTransformer(n_quantiles=100, output_distribution=\u0026#39;uniform\u0026#39;) data = trans.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show()   Ta thấy phân phối của các features gần như đồng nhất (uniform) sau khi thực hiện Uniform Quantile Transform.\nMô hình hóa tập dữ liệu với thuật toán kNN:\n# evaluate knn on the sonar dataset with uniform quantile transform from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import QuantileTransformer from sklearn.pipeline import Pipeline # load dataset dataset = read_csv(\u0026#39;sonar.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline trans = QuantileTransformer(n_quantiles=100, output_distribution=\u0026#39;uniform\u0026#39;) model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.845 (0.074) Độ chính xác đã tăng lên khá nhiều so với các kỹ thuật transforms khác, lên đến 84.5%.\nĐối với Uniform Quantile Transform thì n_quantiles là tham số có thể ảnh hưởng đến hiệu năng của model. Ta thử đi tune nó xem sao:\n# explore number of quantiles on classification accuracy from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import QuantileTransformer from sklearn.preprocessing import LabelEncoder from sklearn.pipeline import Pipeline from matplotlib import pyplot # get the dataset def get_dataset(filename): # load dataset dataset = read_csv(filename, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) return X, y # get a list of models to evaluate def get_models(): models = dict() for i in range(1,100): # define the pipeline trans = QuantileTransformer(n_quantiles=i, output_distribution=\u0026#39;uniform\u0026#39;) model = KNeighborsClassifier() models[str(i)] = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) return models # evaluate a given model using cross-validation def evaluate_model(model, X, y): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores # define dataset X, y = get_dataset(\u0026#39;sonar.csv\u0026#39;) # get the models to evaluate models = get_models() # evaluate the models and store results results = list() for name, model in models.items(): scores = evaluate_model(model, X, y) results.append(mean(scores)) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (name, mean(scores), std(scores))) # plot model performance for comparison pyplot.plot(results) pyplot.show() Kết quả thực hiện:\n\u0026gt;1 0.534 (0.016) \u0026gt;2 0.813 (0.085) \u0026gt;3 0.840 (0.080) \u0026gt;4 0.854 (0.075) \u0026gt;5 0.848 (0.072) ... \u0026gt;95 0.843 (0.074) \u0026gt;96 0.845 (0.074) \u0026gt;97 0.846 (0.073) \u0026gt;98 0.843 (0.073) \u0026gt;99 0.846 (0.075)   84,6% là kết quả tốt nhất đạt được khi n_quantiles=100.\n4. Kết luận Bài thứ 3 về chủ đề Data Transforms, mình đã giới thiệu 4 kỹ thuật thực hiện Data Transforms để làm cho dữ liệu có phân phối gần với Gaussian hơn là Box-Cox, Yeo-Johnson, Normal Quantile, Uniform Quantile, từ đó nâng cao hiệu năng của model. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo chúng ta sẽ tìm hiểu cách thức thực hiện Data Transforms để chuyển đổi features từ numerical sang categorical. Mời các bạn đón đọc.\n5. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/94_data_prepeation_for_ml_data_transforms_3/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Data Transform - Phần 3 - How to Make Distributions More Gaussian"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 15 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 2 về về Data Transforms. Trong bài này, chúng ta sẽ tìm hiểu cách thức thực hiện Transforms cho features dạng categorical. Chúng ta cũng sẽ thực hành trên bộ dữ liệu thực tế.\n1. Nominal and Ordinal Variables Kiểu dữ liệu categorical có thể được chia thành 2 loại:\n  Nominal: Các features bao gồm một số hữu hạn các giá trị không có sự liên quan thứ tự với nhau. VD: Feature Animals bao gồm các giá trị: Dog, Cat, Rabit, Chicken, \u0026hellip;\n  Ordinal: Các features bao gồm một số hữu hạn các giá trị có sự liên hệ thứ tự với nhau. VD: Feature Rank bao gồm các giá trị: First, Second, Third, \u0026hellip;\n  Đa số các thuật toán ML đều không thể làm việc trực tiếp với dữ liệu dạng categorical (trừ Decision Tree). Chúng ta phải chuyển các giá trị của features từ categorical sang numerical trước khi đưa vào huấn luyện các mô hình. Quá trình chuyển đổi này được gọi là Encoding.\n2. Encoding Categorical Data Có 3 kỹ thuật phổ biến để thực hiện encoding categorical data, đó là:\n Ordinal Encoding One Hot Encoding Dummy Variable Encoding  2.1 Ordinal Encoding Ordinal Encoding quy định mỗi giá trị duy nhất của Feature là 1 số tự nhiên, bắt đầu từ số 0. VD: Feature Colors có các giá trị: Red, Green, Yellow thì Red:0, Green:1, Yellow:2. Bởi vì các số tự nhiên, bản chất đã có sự liên quan về thứ tự nên cách này chỉ phù hợp cho dữ liệu categorical dạng ordinal.\nTrong scikit-learn, Ordinal Encoding được thực hiện bởi lớp OrdinalEncoder(). Cách sử dụng như ví dụ sau:\n# example of a ordinal encoding from numpy import asarray from sklearn.preprocessing import OrdinalEncoder # define data data = asarray([[\u0026#39;red\u0026#39;], [\u0026#39;green\u0026#39;], [\u0026#39;blue\u0026#39;]]) print(data) # define ordinal encoding encoder = OrdinalEncoder() # transform data result = encoder.fit_transform(data) print(result) Kết quả thực hiện:\n[[\u0026#39;red\u0026#39;] [\u0026#39;green\u0026#39;] [\u0026#39;blue\u0026#39;]] [[2.] [1.] [0.]] Ordinal Encoding chỉ áp dụng cho các features, đối với target, ta sử dụng LabelEncoder(). Cách làm việc của 2 lớp này hoàn toàn tương tự nhau.\n2.2 One Hot Encoding One Hot Encoding áp dụng cho các features dạng categorical là nominal. Mỗi giá trị của feature được chuyển thành 1 giá trị nhị phân (bao gồm các con số 0 và 1) mà ở đó, số lượng các số 0 và 1 bằng số lượng giá trị duy nhất của feature, số 1 chỉ xuất hiện tại một vị trí trong mảng đại diện cho giá trị duy nhất của feature đó, các vị trí khác trong số nhị phân mang giá trị 0.\nXem ví dụ dưới đây:\n# example of a one hot encoding from numpy import asarray from sklearn.preprocessing import OneHotEncoder # define data data = asarray([[\u0026#39;red\u0026#39;], [\u0026#39;green\u0026#39;], [\u0026#39;blue\u0026#39;]]) print(data) # define one hot encoding encoder = OneHotEncoder(sparse=False) # transform data onehot = encoder.fit_transform(data) print(onehot) Kết quả thực hiện:\n[[\u0026#39;red\u0026#39;] [\u0026#39;green\u0026#39;] [\u0026#39;blue\u0026#39;]] [[0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] Ta thấy [0. 0. 1] đại diện cho red, [0. 1. 0.] đại diên cho green, còn [1. 0. 0.] đại diện cho blue. Nếu biết trước danh sách các giá trị duy nhất của feature, ta có thể thông báo cho OneHotEncoder() thông qua tham số categories. Nếu không được chỉ rõ thì mặc đinh, categories sẽ lấy theo tập dữ liệu mà nó được fit. Và về sau, nếu gặp giá trị khác trong tập fit đó thì OneHotEncoder() sẽ trả về lỗi xử lý.\n2.3 Dummy Variable Encoding One Hot Encoding tạo ra một số nhị phân đại diện cho mỗi giá trị duy nhất của feature. Tuy nhiên, dễ nhận thấy rằng cách biểu diễn này có một chút dư thừa.\nVD: Giả sử feature của ta có 3 giá trị duy nhất là A,B, C. Nếu ta biết [1,0,0] là A, [0,1,0] là B thì giá trị còn lại [0,0,1] chắc chắn là C. Vậy thì ta có thể không cần một số nhị phân để biểu diễn C nữa. Tổng quát ra thì nếu ta có N giá trị duy nhất của feature thì ta chỉ cần N-1 giá trị nhị phân để biểu diễn cho feature đó.\nGiữ nguyên ý tưởng đó nhưng thay đổi cách làm một chút. Ta vẫn giữ nguyên N giá trị nhị phân để biểu diễn N giá trị duy nhất của feature, nhưng chiều dài của mỗi giá trị nhị phân đó thì giảm đi 1. Cụ thể: A -\u0026gt; [1,0], B -\u0026gt; [0,1], C -\u0026gt; [0,0].\nĐể áp dụng Dummy Variable Encoding trong scikit-learn, chúng ta vẫn sử dụng lớp OneHotEncoder() và set giá trị cho tham số drop bằng first.\nVí dụ:\n# example of a dummy variable encoding from numpy import asarray from sklearn.preprocessing import OneHotEncoder # define data data = asarray([[\u0026#39;red\u0026#39;], [\u0026#39;green\u0026#39;], [\u0026#39;blue\u0026#39;]]) print(data) # define one hot encoding encoder = OneHotEncoder(drop=\u0026#39;first\u0026#39;, sparse=False) # transform data onehot = encoder.fit_transform(data) print(onehot) Kết quả thực hiện:\n[[\u0026#39;red\u0026#39;] [\u0026#39;green\u0026#39;] [\u0026#39;blue\u0026#39;]] [[0. 1.] [1. 0.] [0. 0.]] 3. Thực hành các kỹ thuật Data Transforms 3.1 Breast Cancer Dataset Phần này chúng ta sẽ thực hành trên tập dữ liệu Breast Cancer đã được đề cập đến ở bài số 9. Đây là bộ dữ liệu cho bài toán Binary Classification, có 289 mẫu, mỗi mẫu có 9 features.\nĐọc và kiểm tra dữ liệu:\n# load and summarize the dataset from pandas import read_csv # load the dataset dataset = read_csv(\u0026#39;breast-cancer.csv\u0026#39;, header=None) # retrieve the array of data data = dataset.values # separate into input and output columns X = data[:, :-1].astype(str) y = data[:, -1].astype(str) # summarize print(\u0026#39;Input\u0026#39;, X.shape) print(\u0026#39;Output\u0026#39;, y.shape) Kết quả thực hiện:\nInput (286, 9) Output (286,) 3.2 Ordinal Encoding Áp dụng kỹ thuật Ordinal Encoding trên tập Breast Cancer:\n# ordinal encode the breast cancer dataset from pandas import read_csv from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder # load the dataset dataset = read_csv(\u0026#39;breast-cancer.csv\u0026#39;, header=None) # retrieve the array of data data = dataset.values # separate into input and output columns X = data[:, :-1].astype(str) y = data[:, -1].astype(str) # ordinal encode input variables ordinal_encoder = OrdinalEncoder() X = ordinal_encoder.fit_transform(X) # ordinal encode target variable label_encoder = LabelEncoder() y = label_encoder.fit_transform(y) # summarize the transformed data print(\u0026#39;Input\u0026#39;, X.shape) print(X[:5, :]) print(\u0026#39;Output\u0026#39;, y.shape) print(y[:5]) Kết quả thực hiện:\nInput (286, 9) [[2. 2. 2. 0. 1. 2. 1. 2. 0.] [3. 0. 2. 0. 0. 0. 1. 0. 0.] [3. 0. 6. 0. 0. 1. 0. 1. 0.] [2. 2. 6. 0. 1. 2. 1. 1. 1.] [2. 2. 5. 4. 1. 1. 0. 4. 0.]] Output (286,) [1 0 1 0 1] Ta thấy tất cả các giá trị categorical đều đã được chuyển sang numerical. Số lượng mẫu và features không đổi.\nThử mô hình hóa thuật toán LogisticRegression trên tập dữ liệu Breast Cancer đã được encode:\n# evaluate logistic regression on the breast cancer dataset with an ordinal encoding from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.metrics import accuracy_score # load the dataset dataset = read_csv(\u0026#39;breast-cancer.csv\u0026#39;, header=None) # retrieve the array of data data = dataset.values # separate into input and output columns X = data[:, :-1].astype(str) y = data[:, -1].astype(str) # split the dataset into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # ordinal encode input variables ordinal_encoder = OrdinalEncoder() ordinal_encoder.fit(X_train) X_train = ordinal_encoder.transform(X_train) X_test = ordinal_encoder.transform(X_test) # ordinal encode target variable label_encoder = LabelEncoder() label_encoder.fit(y_train) y_train = label_encoder.transform(y_train) y_test = label_encoder.transform(y_test) # define the model model = LogisticRegression() # fit on the training set model.fit(X_train, y_train) # predict on test set yhat = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả thực hiện:\nAccuracy: 75.79 3.3 One Hot Encoding Áp dụng kỹ thuật One Hot Encoding trên tập dữ liệu Breast Cancer.\n# one-hot encode the breast cancer dataset from pandas import read_csv from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder # load the dataset dataset = read_csv(\u0026#39;breast-cancer.csv\u0026#39;, header=None) # retrieve the array of data data = dataset.values # separate into input and output columns X = data[:, :-1].astype(str) y = data[:, -1].astype(str) # one hot encode input variables onehot_encoder = OneHotEncoder(sparse=False) X = onehot_encoder.fit_transform(X) # ordinal encode target variable label_encoder = LabelEncoder() y = label_encoder.fit_transform(y) # summarize the transformed data print(\u0026#39;Input\u0026#39;, X.shape) print(X[:5, :]) Kết quả thực hiện:\nInput (286, 43) [[0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.] [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]] Ta thấy rằng số lượng mẫu không đổi, nhưng số lượng features tăng lên đáng kể, từ 9 lên thành 43. Và tất cả các giá trị bây giờ đều là kiểu nhị phân.\nThử đánh giá thuật toán LogisticRegression trên tập dữ liệu Breast Cancer đã được encode này:\n# evaluate logistic regression on the breast cancer dataset with a one-hot encoding from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.metrics import accuracy_score # load the dataset dataset = read_csv(\u0026#39;breast-cancer.csv\u0026#39;, header=None) # retrieve the array of data data = dataset.values # separate into input and output columns X = data[:, :-1].astype(str) y = data[:, -1].astype(str) # split the dataset into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # one-hot encode input variables onehot_encoder = OneHotEncoder() onehot_encoder.fit(X_train) X_train = onehot_encoder.transform(X_train) X_test = onehot_encoder.transform(X_test) # ordinal encode target variable label_encoder = LabelEncoder() label_encoder.fit(y_train) y_train = label_encoder.transform(y_train) y_test = label_encoder.transform(y_test) # define the model model = LogisticRegression() # fit on the training set model.fit(X_train, y_train) # predict on test set yhat = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả thực hiện:\nAccuracy: 70.53 4. Common Questions Q1: Làm thế nào nếu dữ liệu bao gồm cả 2 loại features: categorical và numerical? Hai phương án:\n Tách riêng các features theo từng loại và thực hiện Data Transforms riêng theo kỹ thuật của chúng. Sau đó, tổng hợp kết quả lại để thu được bộ dữ liệu hoàn chỉnh đã được transforms. Sử dụng ColumnTransformer, sẽ được miêu tả trong các bài tiếp theo.  Q2: Làm thế nào nếu dữ liệu có số lượng features rất lớn (hàng trăm, nghìn, \u0026hellip; features)? Chúng ta vẫn thực hiện các kỹ thuật Data Transforms một cách bình thường. Mặc dù dữ liệu sau khi encode có thể rất lớn nhưng nói chung các thuật toán ML đều có thể xử lý được.\nQ3: Kỹ thuật encoding nào là tốt nhất? Như mọi lần, không có gì là tốt nhất. Hãy thử-sai trên tập dữ liệu mà chúng ta có.\n5. Kết luận Bài thứ 2 về chủ đề Data Transforms, mình đã giới thiệu 3 kỹ thuật thực hiện Data Transforms cho dữ liệu dạng categorical, đó là Ordinal Encoding, One Hot Encoding và Dummy Variable Encoding. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo chúng ta sẽ tìm hiểu cách thức thực hiện Data Transforms để làm cho phân phối của các features gần với phân phối Gaussian hơn. Mời các bạn đón đọc.\n6. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/93_data_prepeation_for_ml_data_transforms_2/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Data Transform - Phần 2 - How to Encode Categorical Data"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 14 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài đầu tiên về về Data Transforms. Trong bài này, chúng ta sẽ tìm hiểu về các kiến thức cơ bản của Data Transforms, các kỹ thuật thực hiện Transforms cho features dạng numerical. Chúng ta cũng sẽ thực hành trên bộ dữ liệu thực tế.\n1. Data Transforms Trong một bộ dữ liệu, pham vi giá trị, sự phân phối, đơn vị đo, \u0026hellip; của các features có thể khác nhau, tùy theo từng bài toán cụ thể. Những sự khác biệt này làm tăng độ khó của vấn đề đang được mô hình hóa, hoặc tệ hơn là làm cho mô hình trở nên thiên vị hơn đối với những features có khoảng giá trị lớn hơn, có phân phối lớn hơn, \u0026hellip; Đó là điều mà chúng ta hoàn toàn không mong muốn. Để hạn chế vấn đề này, dữ liệu cần phải được transform trước khi đưa vào để huấn luyện mô hình.\nHai kỹ thuật phổ biến nhất để thực hiện transform đối với dữ liệu dạng numerical là Normalization và Standardization. Normalization chuyển đổi phạm vi giá trị của tất cả các features về từ 0 đến 1. Còn Standardization chuyển đổi phân phối của tất cả các features về phân phối chuẩn (là phân phối mà có giá trị trung bình mean = 0, và độ lệch chuẩn std = 1). Chúng ta sẽ tìm hiểu kỹ hơn về 2 kỹ thuật này trong các phần tiếp theo.\nViệc thực hiện Data Transform áp dụng cho cả features và target.\n2. Numerical Data Scaling Methods 2.1 Data Normalization Công thức tính giá trị của feature khi thực hiện normalized như sau:\n$y = \\frac{x- min}{max - min}$\n Trong đó:\n x là giá trị ban đầu của feature. y là giá trị sau khi normalized của feature. min/max là giá trị lớn nhất/nhỏ nhất của feature trong toàn bộ tập dữ liệu.  Thư viện scikit-learn cung cấp lớp MinMaxScaler() giúp chúng ta thực hiện công việc này một cách đơn giản. Các bước thực hiện như sau:\n Khai báo một instance của lớp MinMaxScaler(). Fit instance vừa tạo trên tập train (thực chất là tìm giá trị max/min của mỗi features trong tập train) sưr dụng hàm fit(). Áp dụng instance đã fit lên tập train/test, và các mẫu dữ liệu mới về sau, sử dụng hàm transforms().  Hai hàm fit() và transform() có thể gộp chung thành hàm fit_transform() nếu chúng ta chỉ có 1 tập dữ liệu cần xử lý (VD: chỉ có tập train, \u0026hellip;)\nMặc định MinMaxScaler() sẽ đưa giá trị của features về khoảng [0,1], tuy nhiên, chúng ta có thể chỉ định một khoảng giá trị mong muốn khác thông qua tham số feature_range.\nVí dụ sử dụng MinMaxScaler() để thực hiện normalization:\n# example of a normalization from numpy import asarray from sklearn.preprocessing import MinMaxScaler # define data data = asarray([[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]]) print(data) # define min max scaler scaler = MinMaxScaler() # transform data scaled = scaler.fit_transform(data) print(scaled) Kết quả thực hiện:\n[[1.0e+02 1.0e-03] [8.0e+00 5.0e-02] [5.0e+01 5.0e-03] [8.8e+01 7.0e-02] [4.0e+00 1.0e-01]] [[1. 0. ] [0.04166667 0.49494949] [0.47916667 0.04040404] [0.875 0.6969697 ] [0. 1. ]] Rõ ràng là các giá trị bay giờ đều nằm trong khoảng [0,1]. Giá trị min trở thành 0, còn giá trị max trở thành 1.\nỞ chiều ngược lại, chúng ta có thể chuyển đổi ngược các giá trị đã được normalized về lại giá trị ban đâu bằng cách sử dụng hàm inverse_transform() của lớp MinMaxScaler(). Việc này hữu ích khi chúng ta muốn tìm lại chính xác kết quả dự đoán của model, bởi vì kết quả trả về từ model là giá trị đã được normalized, mà chúng ta muốn biết giá trị khi chưa thực hiện normalize của nó.\n2.2 Data Standardization Công thức tính giá trị của feature khi thực hiện standardized như sau: $y = \\frac{x- mean}{std}$\n Trong đó:\n  x là giá trị ban đầu của feature.\n  y là giá trị sau khi standardized feature.\n  mean là giá trị trung bình của feature. $y = \\frac{1}{N} \\sum _{i=1}^n x_i$\n   std là giá trị độ lệch chuẩn feature.\n  $y = \\sqrt{\\frac{\\sum_{i=1}^n(x_i-mean)^2}{N - 1}}$\n Scikit-learn cung cấp lớp StandardScaler() để thực hiện hiện standardization. Cách sử dụng hoàn toàn tương tự như lớp MinMaxScaler().\nVí dụ:\n# example of a standardization from numpy import asarray from sklearn.preprocessing import StandardScaler # define data data = asarray([[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]]) print(data) # define standard scaler scaler = StandardScaler() # transform data scaled = scaler.fit_transform(data) print(scaled) Kết quả thực hiện:\n[[1.0e+02 1.0e-03] [8.0e+00 5.0e-02] [5.0e+01 5.0e-03] [8.8e+01 7.0e-02] [4.0e+00 1.0e-01]] [[ 1.26398112 -1.16389967] [-1.06174414 0.12639634] [ 0. -1.05856939] [ 0.96062565 0.65304778] [-1.16286263 1.44302493]] 3. Thực hành các kỹ thuật Data Transforms 3.1 Diebetes Dataset Chúng ta tiếp tục sử dụng bộ dữ liệu Diabetes đã được giới thiệu ở bài thứ 4.\nĐầu tiên, hãy load bộ dữ liệu này lên:\n# load and summarize the diabetes dataset from pandas import read_csv from matplotlib import pyplot # load the dataset dataset = read_csv(\u0026#39;pima-indians-diabetes.csv\u0026#39;, header=None) # summarize the shape of the dataset print(dataset.shape) # summarize each variable print(dataset.describe()) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show() Kết quả thực hiện:\n(768, 9) 0 1 2 3 4 5 6 7 8 count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958 std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000 75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000   Ta thấy, các features đang có khoảng giá trị và phân phối khác xa nhau.\nTiếp theo, tạo Base model với dữ liệu được giữ nguyên như ban đầu. Ở đây, mình lựa chọn thuật toán KNN cho đơn giản.\n# evaluate knn on the raw diabetes dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder # load the dataset dataset = read_csv(\u0026#39;pima-indians-diabetes.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define and configure the model model = KNeighborsClassifier() # evaluate the model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report model performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.717 (0.040) Base model cho ta độ chính xác là 71.7%. Hãy thử xem giá trị này có tăng lên khi áp dụng 2 kỹ thuật Data Transforms hay không?\n3.2 Normalization Data Áp dụng kỹ thuật normalization trên tập Diebetes.\n# visualize a minmax scaler transform of the diabetes dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import MinMaxScaler from matplotlib import pyplot # load the dataset dataset = read_csv(\u0026#39;pima-indians-diabetes.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a robust scaler transform of the dataset trans = MinMaxScaler() data = trans.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # summarize print(dataset.describe()) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show() Kết quả thực hiện:\n0 1 2 3 4 5 6 7 count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 0.226180 0.607510 0.566438 0.207439 0.094326 0.476790 0.168179 0.204015 std 0.198210 0.160666 0.158654 0.161134 0.136222 0.117499 0.141473 0.196004 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 0.058824 0.497487 0.508197 0.000000 0.000000 0.406855 0.070773 0.050000 50% 0.176471 0.587940 0.590164 0.232323 0.036052 0.476900 0.125747 0.133333 75% 0.352941 0.704774 0.655738 0.323232 0.150414 0.545455 0.234095 0.333333 max 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000   Rõ ràng rằng tất các các features đều có giá trị nằm trong khoảng [0,1].\nThử đánh giá mô hình kNN trên tập Diebetes đã được normalized:\n# visualize a standard scaler transform of the diabetes dataset # evaluate knn on the diabetes dataset with minmax scaler transform from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline # load the dataset dataset = read_csv(\u0026#39;pima-indians-diabetes.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline trans = MinMaxScaler() model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.739 (0.053) Độ chính xác tăng từ 71.7% lên gần 74%. Một sự cải thiện đáng kể.\n3.3 Standardization Data Áp dụng kỹ thuật standardization trên tập Diebetes:\n# visualize a standard scaler transform of the diabetes dataset from pandas import read_csv from pandas import DataFrame from sklearn.preprocessing import StandardScaler from matplotlib import pyplot # load the dataset dataset = read_csv(\u0026#39;pima-indians-diabetes.csv\u0026#39;, header=None) # retrieve just the numeric input values data = dataset.values[:, :-1] # perform a robust scaler transform of the dataset trans = StandardScaler() data = trans.fit_transform(data) # convert the array back to a dataframe dataset = DataFrame(data) # summarize print(dataset.describe()) # histograms of the variables fig = dataset.hist(xlabelsize=4, ylabelsize=4) [x.title.set_size(4) for x in fig.ravel()] # show the plot pyplot.show() Kết quả thực hiện:\n0 1 2 3 4 5 6 7 count 7.680000e+02 7.680000e+02 7.680000e+02 7.680000e+02 7.680000e+02 7.680000e+02 7.680000e+02 7.680000e+02 mean -6.476301e-17 -9.251859e-18 1.503427e-17 1.006140e-16 -3.006854e-17 2.590520e-16 2.451743e-16 1.931325e-16 std 1.000652e+00 1.000652e+00 1.000652e+00 1.000652e+00 1.000652e+00 1.000652e+00 1.000652e+00 1.000652e+00 min -1.141852e+00 -3.783654e+00 -3.572597e+00 -1.288212e+00 -6.928906e-01 -4.060474e+00 -1.189553e+00 -1.041549e+00 25% -8.448851e-01 -6.852363e-01 -3.673367e-01 -1.288212e+00 -6.928906e-01 -5.955785e-01 -6.889685e-01 -7.862862e-01 50% -2.509521e-01 -1.218877e-01 1.496408e-01 1.545332e-01 -4.280622e-01 9.419788e-04 -3.001282e-01 -3.608474e-01 75% 6.399473e-01 6.057709e-01 5.632228e-01 7.190857e-01 4.120079e-01 5.847705e-01 4.662269e-01 6.602056e-01 max 3.906578e+00 2.444478e+00 2.734528e+00 4.921866e+00 6.652839e+00 4.455807e+00 5.883565e+00 4.063716e+00   Biểu đồ histogram của các features được tạo, mặc dù các phân phối trông không khác nhiều so với các phân phối của dữ liệu ban đầu, ngoại trừ tỷ lệ của chúng trên trục x, nhưng chúng ta có thể thấy rằng trung tâm của mỗi phân phối đều tập trung vào điểm giá trị 0.\nThử đánh giá mô hình kNN trên tập Diebetes đã được standardized:\n# evaluate knn on the diabetes dataset with standard scaler transform from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline # load the dataset dataset = read_csv(\u0026#39;pima-indians-diabetes.csv\u0026#39;, header=None) data = dataset.values # separate into input and output columns X, y = data[:, :-1], data[:, -1] # ensure inputs are floats and output is an integer label X = X.astype(\u0026#39;float32\u0026#39;) y = LabelEncoder().fit_transform(y.astype(\u0026#39;str\u0026#39;)) # define the pipeline trans = StandardScaler() model = KNeighborsClassifier() pipeline = Pipeline(steps=[(\u0026#39;t\u0026#39;, trans), (\u0026#39;m\u0026#39;, model)]) # evaluate the pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report pipeline performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.741 (0.050) Độ chính xác tăng lên một chút so với khi thực hiện normalization, từ 73.9% lên 74.1%.\n4. Common Questions Q1: Khi nào thì nên sử dụng Normalization/Standardization? Ưu tiên lựa chọn theo thứ tự sau:\n Nếu dữ liệu của bạn có phân phối chuẩn hoặc gần chuẩn, thì nên áp dụng Standardization. Nếu dữ liệu của bạn có số lượng lớn các features, phạm vi giá trị của các features đó khác nhau nhiều (10s, 100s, 1000s, 0.01, 0.001, \u0026hellip;) thì nên sử dụng Normalization. Thử cả 2 kỹ thuật nếu bạn có đủ thời gian. Nếu dữ liệu của bạn không giống 3 trường hợp mô tả ở trên thì không cần thiết phải thực hiện Data Transforms.  Q2: Có nên thực hiện Standardization trước, sau đó lại thực hiện Normalization không? Việc kết hợp cả 2 kỹ thuật Data Transforms có thể là một ý tưởng tốt. Hãy thử nó nếu bạn có đủ thời gian.\n5. Kết luận Bài đầu tiên về chủ đề Data Transforms, mình đã giới thiệu 2 kỹ thuật thực hiện Data Transforms cho dữ liệu dạng numerical, đó là normalization và standardizaton. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo chúng ta sẽ tìm hiểu cách thức thực hiện Data Transforms đối với dữ liệu có dạng categorical. Mời các bạn đón đọc.\n6. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/92_data_prepeation_for_ml_data_transforms_1/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Data Transform - Phần 1 - How to Scale Numerical Data"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 13 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 6 về Feature Selection. Trong bài này, chúng ta sẽ tìm hiểu về Feature Importance và cách sử dụng nó để lựa chọn features thông qua việc thực hành trên một bộ dữ liệu giả được sinh ra ngẫu nhiên.\n1. Feature Importance Score (FIS) Feature Importance đề cập đến các kỹ thuật xác định điểm số - score cho các features dựa trên cách mối liên hệ của chúng với target. Có nhiều loại và cách tính score cho các features như Statistic Correlation Score, Coeficients Score, Permutatation Score, \u0026hellip; FIS đóng một vai trò quan trọng trong việc mô hình hóa dữ liệu, nó giúp chúng ta có cái nhìn sâu sắc về dữ liệu, về mô hình và là cơ sở để thực hiện Feature Selection.\nTrong bài này, chúng ta sẽ thực hiện tính toán FIS theo ba cách:\n FIS from model coefficients FIS from Decision Tree FIS from Permutation Testing  2. Create Test Data Trước tiên, chúng ta sẽ tạo ra 2 bộ dữ liệu để thực hành. Một bộ cho bài toán Classification và 1 bộ cho bài toán Regression. Cả 2 bộ đều có 1000 mẫu, mỗi mầu có 10 features, 5 features trong số đó là informative, còn lại là redundant.\n2.1 Classification Dataset # test classification dataset from sklearn.datasets import make_classification # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # summarize the dataset print(X.shape, y.shape) 2.2 Regression Dataset # test regression dataset from sklearn.datasets import make_regression # define dataset X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) # summarize the dataset print(X.shape, y.shape) 3. Coefficients as Feature Importance Tất cả các thuật toán Regression/Classification như Linear Regression, Logistic Regression, Ridge Regression, LASSO, Elastic Net, \u0026hellip; đều sử dụng phương pháp đánh trọng số cho các features để thực hiện phép dự đoán. Các trọng số (coeficients) này có thể được coi là một loại FIS.\n3.1 Linear Regression Feature Importance Chúng ta sẽ thực hiện fit LinearRegression model trên tập Regression dataset và xem các coefficients của nó:\n# linear regression feature importance from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression from matplotlib import pyplot # define dataset X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) # define the model model = LinearRegression() # fit the model model.fit(X, y) # get importance importance = model.coef_ # summarize feature importance for i,v in enumerate(importance): print(\u0026#39;Feature: %0d, Score: %.5f\u0026#39; % (i,v)) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() Kết quả thực hiện:\nFeature: 0, Score: -0.00000 Feature: 1, Score: 12.44483 Feature: 2, Score: -0.00000 Feature: 3, Score: -0.00000 Feature: 4, Score: 93.32225 Feature: 5, Score: 86.50811 Feature: 6, Score: 26.74607 Feature: 7, Score: 3.28535 Feature: 8, Score: -0.00000 Feature: 9, Score: 0.00000   Kết quả này nói cho ta biết rằng model tìm thấy 5 Important Features, những features khác có thể được loại bỏ khi mô hình hóa dữ liệu.\n3.2 Logistic Regression Feature Importance # logistic regression for feature importance from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from matplotlib import pyplot # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # define the model model = LogisticRegression() # fit the model model.fit(X, y) # get importance importance = model.coef_[0] # summarize feature importance for i,v in enumerate(importance): print(\u0026#39;Feature: %0d, Score: %.5f\u0026#39; % (i,v)) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() Kết quả thực hiện:\nFeature: 0, Score: 0.16320 Feature: 1, Score: -0.64301 Feature: 2, Score: 0.48497 Feature: 3, Score: -0.46190 Feature: 4, Score: 0.18432 Feature: 5, Score: -0.11978 Feature: 6, Score: -0.40602 Feature: 7, Score: 0.03772 Feature: 8, Score: -0.51785 Feature: 9, Score: 0.26540   Đối với bài toán Classification và thuật toán Logistic Regression thì các coefficients có cả giá trị âm và dương. Giá trị âm chỉ ra rằng feature đó dự đoán cho class 0, còn giá trị dương chỉ ra rằng feature đó dự đoán cho class 1. Các giá trị coefficients ở đây không thể coi là FIS và không được sử dụng để thực hiện Feature Selection.\n4. Decision Tree Feature Importance Chúng ta sử dụng thuật toán CART được implemented trong scikit-learn (DecisionTreeRegressor và DecisionTreeClassifier). Sau khi fit, model cung cấp thuộc tính *feature_importances_* có thể được sử dụng như là FIS.\n4.1 CART Feature Importance a, CART Regression Feature Importance # decision tree for feature importance on a regression problem from sklearn.datasets import make_regression from sklearn.tree import DecisionTreeRegressor from matplotlib import pyplot # define dataset X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) # define the model model = DecisionTreeRegressor() # fit the model model.fit(X, y) # get importance importance = model.feature_importances_ # summarize feature importance for i,v in enumerate(importance): print(\u0026#39;Feature: %0d, Score: %.5f\u0026#39; % (i,v)) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() Kết quả thực hiện:\nFeature: 0, Score: 0.00280 Feature: 1, Score: 0.00485 Feature: 2, Score: 0.00243 Feature: 3, Score: 0.00200 Feature: 4, Score: 0.51685 Feature: 5, Score: 0.43703 Feature: 6, Score: 0.02744 Feature: 7, Score: 0.00259 Feature: 8, Score: 0.00297 Feature: 9, Score: 0.00103   Kết quả chỉ ra có 3/10 Important Features.\nb, CART Classification Feature Importance # decision tree for feature importance on a classification problem from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier from matplotlib import pyplot # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # define the model model = DecisionTreeClassifier() # fit the model model.fit(X, y) # get importance importance = model.feature_importances_ # summarize feature importance for i,v in enumerate(importance): print(\u0026#39;Feature: %0d, Score: %.5f\u0026#39; % (i,v)) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() Kết quả thực hiện:\nFeature: 0, Score: 0.02155 Feature: 1, Score: 0.01029 Feature: 2, Score: 0.17994 Feature: 3, Score: 0.29655 Feature: 4, Score: 0.08793 Feature: 5, Score: 0.01210 Feature: 6, Score: 0.18119 Feature: 7, Score: 0.05454 Feature: 8, Score: 0.12754 Feature: 9, Score: 0.02837   Kết quả chỉ ra có 4-5/10 Important Features.\n4.2 Random Forest Feature Importance a, Random Forest Regression Feature Importance # random forest for feature importance on a regression problem from sklearn.datasets import make_regression from sklearn.ensemble import RandomForestRegressor from matplotlib import pyplot # define dataset X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) # define the model model = RandomForestRegressor() # fit the model model.fit(X, y) # get importance importance = model.feature_importances_ # summarize feature importance for i,v in enumerate(importance): print(\u0026#39;Feature: %0d, Score: %.5f\u0026#39; % (i,v)) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() Kết quả thực hiện:\nFeature: 0, Score: 0.00307 Feature: 1, Score: 0.00553 Feature: 2, Score: 0.00292 Feature: 3, Score: 0.00271 Feature: 4, Score: 0.52968 Feature: 5, Score: 0.42187 Feature: 6, Score: 0.02528 Feature: 7, Score: 0.00298 Feature: 8, Score: 0.00320 Feature: 9, Score: 0.00275 Kết quả chỉ ra chỉ có 2-3/10 Important Features.\n b, Random Forest Classification Feature Importance # random forest for feature importance on a classification problem from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from matplotlib import pyplot # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # define the model model = RandomForestClassifier() # fit the model model.fit(X, y) # get importance importance = model.feature_importances_ # summarize feature importance for i,v in enumerate(importance): print(\u0026#39;Feature: %0d, Score: %.5f\u0026#39; % (i,v)) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() Kết quả thực hiện:\nFeature: 0, Score: 0.05545 Feature: 1, Score: 0.11714 Feature: 2, Score: 0.14782 Feature: 3, Score: 0.17944 Feature: 4, Score: 0.08900 Feature: 5, Score: 0.11283 Feature: 6, Score: 0.11459 Feature: 7, Score: 0.04678 Feature: 8, Score: 0.09017 Feature: 9, Score: 0.04678   Kết quả chỉ ra có 4-5/10 Important Features.\n5. Permutation Feature Importance 5.1 Permutation Feature Importance for Regression # permutation feature importance with knn for regression from sklearn.datasets import make_regression from sklearn.neighbors import KNeighborsRegressor from sklearn.inspection import permutation_importance from matplotlib import pyplot # define dataset X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) # define the model model = KNeighborsRegressor() # fit the model model.fit(X, y) # perform permutation importance results = permutation_importance(model, X, y, scoring=\u0026#39;neg_mean_squared_error\u0026#39;) # get importance importance = results.importances_mean # summarize feature importance for i,v in enumerate(importance): print(\u0026#39;Feature: %0d, Score: %.5f\u0026#39; % (i,v)) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() Kết quả thực hiện:\nFeature: 0, Score: 203.08837 Feature: 1, Score: 299.90705 Feature: 2, Score: 155.87319 Feature: 3, Score: 31.30704 Feature: 4, Score: 9642.31128 Feature: 5, Score: 8482.24184 Feature: 6, Score: 869.35524 Feature: 7, Score: 148.71948 Feature: 8, Score: 86.35811 Feature: 9, Score: 85.22271   Kết quả chỉ ra có 2-3/10 Important Features.\n5.2 Permutation Feature Importance for Classification # permutation feature importance with knn for classification from sklearn.datasets import make_classification from sklearn.neighbors import KNeighborsClassifier from sklearn.inspection import permutation_importance from matplotlib import pyplot # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # define the model model = KNeighborsClassifier() # fit the model model.fit(X, y) # perform permutation importance results = permutation_importance(model, X, y, scoring=\u0026#39;accuracy\u0026#39;) # get importance importance = results.importances_mean # summarize feature importance for i,v in enumerate(importance): print(\u0026#39;Feature: %0d, Score: %.5f\u0026#39; % (i,v)) # plot feature importance pyplot.bar([x for x in range(len(importance))], importance) pyplot.show() Kết quả thực hiện:\nFeature: 0, Score: 0.04860 Feature: 1, Score: 0.06240 Feature: 2, Score: 0.05600 Feature: 3, Score: 0.09520 Feature: 4, Score: 0.05200 Feature: 5, Score: 0.05840 Feature: 6, Score: 0.07580 Feature: 7, Score: 0.05880 Feature: 8, Score: 0.06000 Feature: 9, Score: 0.03020   Kết quả chỉ ra có 2-3/10 Important Features.\n6. Feature Selection with Importance Phần này, chúng ta sẽ sử dụng FIS để thực hiện Feature Selection, sau đó sẽ tiến hành mô hình hóa dữ liệu theo các features đã được chọn.\n6.1 Base model Base model sử dụng tất cả features:\n# evaluation of a model using all features from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # define the dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # fit the model model = LogisticRegression(solver=\u0026#39;liblinear\u0026#39;) model.fit(X_train, y_train) # evaluate the model yhat = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả thực hiện:\nAccuracy: 84.55 6.2 Model with Selected Features Ta có thể sử dụng bấy kì phương pháp tính FIS bên trên để lựa chọn features. Ở đây, mình sẽ dùng FIS cung cấp bởi Random Forest. Lớp SelectFromModel định nghĩa cả model mà ta muốn sử dụng để tính FIS (RandomForestClassifier) và số lượng tối đa features chúng ta muốn chọn.\n# evaluation of a model using 5 features chosen with random forest importance from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # feature selection def select_features(X_train, y_train, X_test): # configure to select a subset of features fs = SelectFromModel(RandomForestClassifier(n_estimators=1000), max_features=5) # learn relationship from training data fs.fit(X_train, y_train) # transform train input data X_train_fs = fs.transform(X_train) # transform test input data X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # define the dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) # fit the model model = LogisticRegression(solver=\u0026#39;liblinear\u0026#39;) model.fit(X_train_fs, y_train) # evaluate the model yhat = model.predict(X_test_fs) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả thực hiện:\nAccuracy: 84.55 Trong trường hợp này thì độc chính xác đạt được khi sử dụng các features từ FIS đúng bằng độ chính xác của Base model.\n7. Kết luận Bài thứ 6 về chủ đề Feature Selection, mình đã giới thiệu cách sử dụng Feature Importance Score trong việc Feature Selection. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo chúng ta sẽ chuyển sang nhiệm vụ mới của Data Preparation, đó là Data Transforms. Mời các bạn đón đọc.\n8. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/91_data_prepeation_for_ml_feature_selection_6/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Feature Selection - Phần 6 - How to Use Feature Importance"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 12 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 5 về Feature Selection. Trong bài này, chúng ta sẽ tìm hiểu phương pháp Recursive Feature Elimination (RFE) để lựa chọn features thông qua việc thực hành trên một bộ dữ liệu giả được sinh ra ngẫu nhiên.\n1. Recursive Feature Elimination (RFE) RFE là một thuật toán Feature Selection phổ biến. Ưu điểm của nó là dễ cấu hình và sử dụng, và hiệu quả cao trong việc lựa chọn các features sử dụng mối liên hệ giữa các features đó với target.\nThực chất, RFE là một thuật toán lựa Feature Selection kiểu wrapper. Điều này có nghĩa là một thuật toán học ML được đưa ra và sử dụng bên trong để thực hiện Feature Selection.\nCó hai tùy chọn cấu hình quan trọng khi sử dụng RFE:\n Lựa chọn số lượng features cần chọn Lựa chọn thuật toán được sử dụng để giúp chọn features.  2. RFE with scikit-learn Thư viện scikit-learn cung cấp một implementation của RFE thông qua RFE class. Để sử dụng nó, cần cung cấp 2 tham số là thuật toán sử dụng (estimator) và số lượng features cần chọn (n_features_to_select).\n... # define the method rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=3) # fit the model rfe.fit(X, y) # transform the data X, y = rfe.transform(X, y) Thông thương, k-Folde Cross-Validation được sử dụng để đánh giá thuật toán ML trong RFE.\n2.1 RFE for Classification Trong phần này, chúng ta sẽ sử dụng RFE cho bài toán Classification. Đầu tiên, chúng ta sẽ sinh ra 1000 mẫu dữ liệu, mỗi mẫu có 10 features, trong đó có 5 features là informative, 5 features là redundant, sử dụng hàm make_classification().\n# test classification dataset from sklearn.datasets import make_classification # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # summarize the dataset print(X.shape, y.shape) Tiếp theo, chúng ta khai báo RFE, sử dụng thuật toán DecisionTree và số lượng features cần chọn là 5 để mô hình hóa bộ dữ liệu vừa tạo.\n# evaluate RFE for classification from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.feature_selection import RFE from sklearn.tree import DecisionTreeClassifier from sklearn.pipeline import Pipeline # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # create pipeline rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) model = DecisionTreeClassifier() pipeline = Pipeline(steps=[('s',rfe),('m',model)]) # evaluate model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) # report performance print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.887 (0.036) RFE cũng có thể được sử dụng như một model và có thể đưa ra dự đoán trên một mẫu dữ liệu mới.\n# make a prediction with an RFE pipeline from sklearn.datasets import make_classification from sklearn.feature_selection import RFE from sklearn.tree import DecisionTreeClassifier from sklearn.pipeline import Pipeline # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # create pipeline rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) model = DecisionTreeClassifier() pipeline = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) # fit the model on all available data pipeline.fit(X, y) # make a prediction for one example data = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]] yhat = pipeline.predict(data) print(\u0026#39;Predicted Class: %d\u0026#39; % (yhat)) Kết quả thực hiện:\nPredicted Class: 1 2.2 RFE for Regression Tương tự như trên, đầu tiên ta sẽ tạo ra bộ dữ liệu Regression:\n# test regression dataset from sklearn.datasets import make_regression # define dataset X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) # summarize the dataset print(X.shape, y.shape) Tiếp theo sẽ mô hình hóa bộ dữ liệu vừa tạo với RFE và DecisionTree.\n# evaluate RFE for regression from numpy import mean from numpy import std from sklearn.datasets import make_regression from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedKFold from sklearn.feature_selection import RFE from sklearn.tree import DecisionTreeRegressor from sklearn.pipeline import Pipeline # define dataset X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) # create pipeline rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5) model = DecisionTreeRegressor() pipeline = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) # evaluate model cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;neg_mean_absolute_error\u0026#39;, cv=cv, n_jobs=-1) # report performance print(\u0026#39;MAE: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nMAE: -27.239 (2.710) Sử dụng RFE để tạo dự đoán trên mẫu dữ liệu mới:\n# make a regression prediction with an RFE pipeline from sklearn.datasets import make_regression from sklearn.feature_selection import RFE from sklearn.tree import DecisionTreeRegressor from sklearn.pipeline import Pipeline # define dataset X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) # create pipeline rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5) model = DecisionTreeRegressor() pipeline = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) # fit the model on all available data pipeline.fit(X, y) # make a prediction for one example data = [[-2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]] yhat = pipeline.predict(data) print(\u0026#39;Predicted: %.3f\u0026#39; % (yhat)) Kết quả thực hiện:\nPredicted: -84.288 3. RFE Hyperparameters Như đã nói ở trên, có 2 tham số quan trọng cần lưu ý khi sử dụng RFE là thuật toán sử dụng và số lượng features cần chọn. Phần này, chúng ta sẽ thử thực hiện tuning để tìm ra giá trị tối ưu của 2 tham số đó.\n3.1 Explore Number of Features # explore the number of selected features for RFE from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.feature_selection import RFE from sklearn.tree import DecisionTreeClassifier from sklearn.pipeline import Pipeline from matplotlib import pyplot # get the dataset def get_dataset(): X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) return X, y # get a list of models to evaluate def get_models(): models = dict() for i in range(2, 10): rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i) model = DecisionTreeClassifier() models[str(i)] = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) return models # evaluate a given model using cross-validation def evaluate_model(model, X, y): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores # define dataset X, y = get_dataset() # get the models to evaluate models = get_models() # evaluate the models and store results results, names = list(), list() for name, model in models.items(): scores = evaluate_model(model, X, y) results.append(scores) names.append(name) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (name, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=names, showmeans=True) pyplot.show() Kết quả thực hiện:\n\u0026gt;2 0.715 (0.044) \u0026gt;3 0.817 (0.034) \u0026gt;4 0.874 (0.030) \u0026gt;5 0.888 (0.032) \u0026gt;6 0.894 (0.027) \u0026gt;7 0.887 (0.030) \u0026gt;8 0.884 (0.027) \u0026gt;9 0.885 (0.025)   Nhìn vào kết quả này thì ta thấy n_features_to_select=6 là giá trị tối ưu nhất.\n3.2 Automatically Select the Number of Features Tham số n_features_to_select tối ưu cũng có thể được lựa chọn để sử dụng một cách tự động. Điều này đạt được bằng cách thực hiện k-Fold Cross-Validation đối với từng giá trị của n_features_to_select, sau đó lựa chọn giá trị n_features_to_select làm cho giá trị Score lớn nhất để sử dụng:\n# automatically select the number of features for RFE from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.feature_selection import RFECV from sklearn.tree import DecisionTreeClassifier from sklearn.pipeline import Pipeline # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # create pipeline rfe = RFECV(estimator=DecisionTreeClassifier()) model = DecisionTreeClassifier() pipeline = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) # evaluate model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) n_scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # report performance print(\u0026#39;Accuracy: %.3f(%.3f)\u0026#39; % (mean(n_scores), std(n_scores))) Kết quả thực hiện:\nAccuracy: 0.886 (0.029) Trong trường hợp này, RFECV với DecisionTree đã tự động luwuas chọn số lượng features tối ưu, sau đó fit trên toàn bộ tập dữ liệu và thu được kết quả: 88.6%.\n3.3 Which Features Were Selected Nếu muốn biết cụ thể features nào được lựa chọn, features nào bị loại bỏ, ta làm như sau:\n# report which features were selected by RFE from sklearn.datasets import make_classification from sklearn.feature_selection import RFE from sklearn.tree import DecisionTreeClassifier # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) # define RFE rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) # fit RFE rfe.fit(X, y) # summarize all features for i in range(X.shape[1]): print(\u0026#39;Column: %d, Selected=%s, Rank: %d\u0026#39; % (i, rfe.support_[i], rfe.ranking_[i])) Kết quả thực hiện:\nColumn: 0, Selected=False, Rank: 5 Column: 1, Selected=False, Rank: 4 Column: 2, Selected=True, Rank: 1 Column: 3, Selected=True, Rank: 1 Column: 4, Selected=True, Rank: 1 Column: 5, Selected=False, Rank: 6 Column: 6, Selected=True, Rank: 1 Column: 7, Selected=False, Rank: 2 Column: 8, Selected=True, Rank: 1 Column: 9, Selected=False, Rank: 3 3.4 Explore Base Algorithm # explore the algorithm wrapped by RFE from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression from sklearn.linear_model import Perceptron from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.pipeline import Pipeline from matplotlib import pyplot # get the dataset def get_dataset(): X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1) return X, y # get a list of models to evaluate def get_models(): models = dict() # lr rfe = RFE(estimator=LogisticRegression(), n_features_to_select=5) model = DecisionTreeClassifier() models[\u0026#39;lr\u0026#39;] = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) # perceptron rfe = RFE(estimator=Perceptron(), n_features_to_select=5) model = DecisionTreeClassifier() models[\u0026#39;per\u0026#39;] = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) # cart rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) model = DecisionTreeClassifier() models[\u0026#39;cart\u0026#39;] = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) # rf rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=5) model = DecisionTreeClassifier() models[\u0026#39;rf\u0026#39;] = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) # gbm rfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=5) model = DecisionTreeClassifier() models[\u0026#39;gbm\u0026#39;] = Pipeline(steps=[(\u0026#39;s\u0026#39;,rfe),(\u0026#39;m\u0026#39;,model)]) return models # evaluate a given model using cross-validation def evaluate_model(model, X, y): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores # define dataset X, y = get_dataset() # get the models to evaluate models = get_models() # evaluate the models and store results results, names = list(), list() for name, model in models.items(): scores = evaluate_model(model, X, y) results.append(scores) names.append(name) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (name, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=names, showmeans=True) pyplot.show() Kết quả thực hiện:\n\u0026gt;lr 0.887 (0.030) \u0026gt;per 0.844 (0.034) \u0026gt;cart 0.889 (0.031) \u0026gt;rf 0.856 (0.039) \u0026gt;gbm 0.892 (0.026)   Số liệu chỉ ra rằng thuật toán Gradient Boosting là lựa chọn tốt nhất trong trường hợp này.\n4. Kết luận Bài thứ 5 về chủ đề Feature Selection, mình đã giới thiệu cách sử dụng thuật toán Recursive Feature Elimination (RFE) trong việc Feature Selection. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo mình sẽ giới thiệu về cách sử dụng Feature Importance để thực hiện Feature Selection. Mời các bạn đón đọc.\n5. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/90_data_prepeation_for_ml_feature_selection_5/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Feature Selection - Phần 5 - Use RFE for Feature Selection"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 11 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 4 về Feature Selection. Trong bài này, chúng ta sẽ tìm hiểu phương pháp lựa chọn features đối với dữ liệu có Ouput Variance thuộc kiểu Numberical thông qua việc thực hành trên một bộ dữ liệu giả được sinh ra ngẫu nhiên.\n1. Regression Dataset Chúng ta sẽ sử dụng tập dữ liệu Regresion (tức dữ liệu có Ouput Variance ở dạng Numerical) làm cơ sở của bài thực hành này. Nhớ lại rằng một bài toán hồi quy là một bài toán mà chúng ta muốn dự đoán một giá trị số cụ thể. Trong trường hợp này, chúng ta cũng sẽ tạo ra một tập dữ liệu mà các Input Variables cũng có dạng Numerical. Hàm make regression() từ thư viện scikit-learn có thể được sử dụng để định nghĩa một tập dữ liệu như vậy. Nó cung cấp khả năng kiểm soát số lượng mẫu, số lượng các Input Variables và quan trọng là số lượng các Input Variables đó có liên quan và không liên quan đến Output Variable. Điều này rất quan trọng vì chúng ta đặc biệt mong muốn một tập dữ liệu mà chúng ta biết trước có một số Input Variables không liên quan đến Output Variable. Cụ thể ở bài này, mình sẽ tạo ra một tập dữ liệu với 1.000 mẫu, mỗi mẫu có 100 Input Variables, trong đó 10 mẫu là thông tin liên quan và 90 mẫu còn lại là không liên quan đến Output Variable.\n... # generate regression dataset X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1) Chúng ta hy vọng là các kỹ thuật Feature Selection có thể xác định đúng hoặc gần đúng các Input Variables có liên quan đến Output Variable như chúng ta đã định nghĩa từ đầu. Sau khi được xác định điều đó, chúng ta có thể tiến hành chia dữ liệu thành các train/test để tạo ra mô hình dự đoán.\n# load and summarize the dataset from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split # generate regression dataset X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # summarize print(\u0026#39;Train\u0026#39;, X_train.shape, y_train.shape) print(\u0026#39;Test\u0026#39;, X_test.shape, y_test.shape) Kết quả thực hiện:\nTrain (670, 100) (670,) Test (330, 100) (330,) 2. Numerical Feature Selection Có 2 kỹ thuật Feature Selection phổ biến có thể sử dụng cho Numerical Input/Output Variables, đó là Correlation Statistic và Mutual Information.\n2.1 Correlation Statistic Correlation (sự tương quan) là thước đo mức độ thay đổi của hai biến số cùng nhau. Có lẽ thước đo Correlation phổ biến nhất là Pearson\u0026rsquo;s Correlation. Nó giả định phân phối Gauss cho mỗi biến và báo cáo về mối quan hệ tuyến tính của chúng.\nĐiểm tương quan tuyến tính thường là một giá trị từ -1 đến 1 với 0 thể hiện không có mối quan hệ nào. Đối với việc lựa chọn Input Variables, chúng ta thường quan tâm đến giá trị dương càng lớn càng tốt, bởi vì đó mối quan hệ càng lớn và nhiều khả năng Input Variable đó nên được chọn để huấn luyện mô hình. Như vậy, mối tương quan tuyến tính có thể được chuyển đổi thành một thống kê tương quan chỉ với các giá trị dương.\nCorrelation Statistic được implement trong scikit-learn bằng hàm f_regression(). Cách sử dụng nó tương tự như các bài trước.\n# example of correlation feature selection for numerical data from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_regression from matplotlib import pyplot # feature selection def select_features(X_train, y_train, X_test): # configure to select all features fs = SelectKBest(score_func=f_regression, k=\u0026#39;all\u0026#39;) # learn relationship from training data fs.fit(X_train, y_train) # transform train input data X_train_fs = fs.transform(X_train) # transform test input data X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # load the dataset X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) # what are scores for the features for i in range(len(fs.scores_)): print(\u0026#39;Feature %d: %f\u0026#39; % (i, fs.scores_[i])) # plot the scores pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) pyplot.show() Kết quả thực hiện:\nFeature 0: 0.009419 Feature 1: 1.018881 Feature 2: 1.205187 Feature 3: 0.000138 Feature 4: 0.167511 Feature 5: 5.985083 Feature 6: 0.062405 Feature 7: 1.455257 Feature 8: 0.420384 Feature 9: 101.392225 Feature 10: 0.387091 Feature 11: 1.581124 Feature 12: 3.014463 Feature 13: 0.232705 Feature 14: 0.076281 Feature 15: 4.299652 Feature 16: 1.497530 Feature 17: 0.261242 Feature 18: 5.960005 Feature 19: 0.523219 Feature 20: 0.003365 Feature 21: 0.024178 Feature 22: 0.220958 Feature 23: 0.576770 Feature 24: 0.627198 Feature 25: 0.350687 Feature 26: 0.281877 Feature 27: 0.584210 Feature 28: 52.196337 Feature 29: 0.046855 Feature 30: 0.147323 Feature 31: 0.368485 Feature 32: 0.077631 Feature 33: 0.698140 Feature 34: 45.744046 Feature 35: 2.047376 Feature 36: 0.786270 Feature 37: 0.996190 Feature 38: 2.733533 Feature 39: 63.957656 Feature 40: 231.885540 Feature 41: 1.372448 Feature 42: 0.581860 Feature 43: 1.072930 Feature 44: 1.066976 Feature 45: 0.344656 Feature 46: 13.951551 Feature 47: 3.575080 Feature 48: 0.007299 Feature 49: 0.004651 Feature 50: 1.094585 Feature 51: 0.241065 Feature 52: 0.355137 Feature 53: 0.020294 Feature 54: 0.154567 Feature 55: 2.592512 Feature 56: 0.300175 Feature 57: 0.357798 Feature 58: 3.060090 Feature 59: 0.890357 Feature 60: 122.132164 Feature 61: 2.029982 Feature 62: 0.091551 Feature 63: 1.081123 Feature 64: 0.056041 Feature 65: 2.930717 Feature 66: 0.054886 Feature 67: 1.332787 Feature 68: 0.145579 Feature 69: 0.986331 Feature 70: 0.092661 Feature 71: 0.083219 Feature 72: 0.198847 Feature 73: 2.065792 Feature 74: 0.236594 Feature 75: 0.512608 Feature 76: 1.095650 Feature 77: 0.015359 Feature 78: 2.193730 Feature 79: 1.574530 Feature 80: 5.360863 Feature 81: 0.041874 Feature 82: 5.717705 Feature 83: 0.436560 Feature 84: 5.594438 Feature 85: 0.000065 Feature 86: 0.026748 Feature 87: 0.408422 Feature 88: 2.092557 Feature 89: 9.568498 Feature 90: 0.642445 Feature 91: 0.065794 Feature 92: 198.705931 Feature 93: 0.073807 Feature 94: 1.048605 Feature 95: 0.004106 Feature 96: 0.042110 Feature 97: 0.034228 Feature 98: 0.792433 Feature 99: 0.015365   Correlation Score của mỗi features và target được tính toán và hiển thị lên đồ thị. Quan sát ta thấy, có khoảng 8 đến 10 features là có mức đó liên quan nhiều nhất đến target. Set k=10 khi sử dụng Correlation Statistic với SelectKBest().\n2.2 Mutual Information Feature Selection Vẫn lại là Mutual Information, nó vẫn được sử dụng ở đây. Tuy nhiên, khác với các lần trước là bài toán Binary Classification (sử dụng hàm mutual_info_classif) thì lần này là bài toán Regression nên hàm implement của Mutual Information là mutual_info_regression.\n# example of mutual information feature selection for numerical input data from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import mutual_info_regression from matplotlib import pyplot # feature selection def select_features(X_train, y_train, X_test): # configure to select all features fs = SelectKBest(score_func=mutual_info_regression, k=\u0026#39;all\u0026#39;) # learn relationship from training data fs.fit(X_train, y_train) # transform train input data X_train_fs = fs.transform(X_train) # transform test input data X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # load the dataset X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) # what are scores for the features for i in range(len(fs.scores_)): print(\u0026#39;Feature %d: %f\u0026#39; % (i, fs.scores_[i])) # plot the scores pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) pyplot.show() Kết quả thực hiện:\nFeature 0: 0.045484 Feature 1: 0.000000 Feature 2: 0.000000 Feature 3: 0.000000 Feature 4: 0.024816 Feature 5: 0.000000 Feature 6: 0.022659 Feature 7: 0.000000 Feature 8: 0.000000 Feature 9: 0.074320 Feature 10: 0.000000 Feature 11: 0.000000 Feature 12: 0.000000 Feature 13: 0.000000 Feature 14: 0.020390 Feature 15: 0.004307 Feature 16: 0.000000 Feature 17: 0.000000 Feature 18: 0.016566 Feature 19: 0.003688 Feature 20: 0.007579 Feature 21: 0.018640 Feature 22: 0.025206 Feature 23: 0.017967 Feature 24: 0.069173 Feature 25: 0.000000 Feature 26: 0.022232 Feature 27: 0.000000 Feature 28: 0.007849 Feature 29: 0.012849 Feature 30: 0.017402 Feature 31: 0.008083 Feature 32: 0.047321 Feature 33: 0.002829 Feature 34: 0.028968 Feature 35: 0.000000 Feature 36: 0.071652 Feature 37: 0.027969 Feature 38: 0.000000 Feature 39: 0.064796 Feature 40: 0.137695 Feature 41: 0.008732 Feature 42: 0.003983 Feature 43: 0.000000 Feature 44: 0.009387 Feature 45: 0.000000 Feature 46: 0.038385 Feature 47: 0.000000 Feature 48: 0.000000 Feature 49: 0.000000 Feature 50: 0.000000 Feature 51: 0.000000 Feature 52: 0.000000 Feature 53: 0.008130 Feature 54: 0.041779 Feature 55: 0.000000 Feature 56: 0.000000 Feature 57: 0.000000 Feature 58: 0.031228 Feature 59: 0.002689 Feature 60: 0.146192 Feature 61: 0.000000 Feature 62: 0.000000 Feature 63: 0.000000 Feature 64: 0.018194 Feature 65: 0.021368 Feature 66: 0.046071 Feature 67: 0.034707 Feature 68: 0.033530 Feature 69: 0.002262 Feature 70: 0.018332 Feature 71: 0.000000 Feature 72: 0.000000 Feature 73: 0.074876 Feature 74: 0.000000 Feature 75: 0.004429 Feature 76: 0.002617 Feature 77: 0.031354 Feature 78: 0.000000 Feature 79: 0.000000 Feature 80: 0.000000 Feature 81: 0.033931 Feature 82: 0.010400 Feature 83: 0.019373 Feature 84: 0.000000 Feature 85: 0.033191 Feature 86: 0.000000 Feature 87: 0.028745 Feature 88: 0.000000 Feature 89: 0.000000 Feature 90: 0.000000 Feature 91: 0.017698 Feature 92: 0.129797 Feature 93: 0.000000 Feature 94: 0.002171 Feature 95: 0.029995 Feature 96: 0.000000 Feature 97: 0.014428 Feature 98: 0.000000 Feature 99: 0.000000   Trong trường hợp này thì số lượng features có sự liên quan nhiều đến target lớn hơn con số 10. Điều này có lẽ do ảnh hưởng của nhiễu mà ta thêm vào khi tạo dữ liệu.\n3. Modeling With Selected Features 3.1 Base model Base model được xây dựng với tất cả dữ liệu mặc định.\n# evaluation of a model using all input features from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error # load the dataset X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # fit the model model = LinearRegression() model.fit(X_train, y_train) # evaluate the model yhat = model.predict(X_test) # evaluate predictions mae = mean_absolute_error(y_test, yhat) print(\u0026#39;MAE: %.3f\u0026#39; % mae) Kết quả thực hiện:\nMAE: 0.086 3.2 Model Built Using Correlation Features # evaluation of a model using 10 features chosen with correlation from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_regression from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error # feature selection def select_features(X_train, y_train, X_test): # configure to select a subset of features fs = SelectKBest(score_func=f_regression, k=10) # learn relationship from training data fs.fit(X_train, y_train) # transform train input data X_train_fs = fs.transform(X_train) # transform test input data X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # load the dataset X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) # fit the model model = LinearRegression() model.fit(X_train_fs, y_train) # evaluate the model yhat = model.predict(X_test_fs) # evaluate predictions mae = mean_absolute_error(y_test, yhat) print(\u0026#39;MAE: %.3f\u0026#39; % mae) Kết quả thực hiện:\nMAE: 2.74 Trong trường hợp này, chúng ta thấy rằng mô hình đạt được MAE khoảng 2.7, lớn hơn nhiều so với Base model (0.086). Mặc dù đã nhận ra được các features nào là quan trọng cần chọn, nhưng việc xây dựng một mô hình từ những features này lại không tạo ra một mô hình có kết quả tốt hơn. Điều này có thể là do các features quan trọng đối với mục tiêu đã bị bỏ qua, có nghĩa là phương pháp Correlation Statistic đang bị đánh lừa về những gì là quan trọng khi lựa chọn features.\nChúng ta sẽ thử lại bằng cách tăng số lượng features được lựa chọn lên thành 90. Kết quả thu được MAE = 0.085, tốt hơn một chút so với Base model.\n3.3 Model Built Using Mutual Information Features # evaluation of a model using 88 features chosen with mutual information from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import mutual_info_regression from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error # feature selection def select_features(X_train, y_train, X_test): # configure to select a subset of features fs = SelectKBest(score_func=mutual_info_regression, k=88) # learn relationship from training data fs.fit(X_train, y_train) # transform train input data X_train_fs = fs.transform(X_train) # transform test input data X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # load the dataset X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) # fit the model model = LinearRegression() model.fit(X_train_fs, y_train) # evaluate the model yhat = model.predict(X_test_fs) # evaluate predictions mae = mean_absolute_error(y_test, yhat) print(\u0026#39;MAE: %.3f\u0026#39; % mae) Kết quả thực hiện:\nMAE: 0.084 Model lần này có kết quả tốt hơn hai lần trước một chút, vẫn lựa chọn 88 features.\n3.4 Tune the Number of Selected Features Biết được rằng Mutual Information cho kết quả tốt nhất rồi. Ta tiếp tục thử xem liệu có giá nào của k để thu được model tốt hơn nữa không bằng cách thực thiện tune giá trị của k. Lần này chúng ta sẽ sử dụng metric neg_mean_absolute_error để đánh giá.\n# compare different numbers of features selected using mutual information from sklearn.datasets import make_regression from sklearn.model_selection import RepeatedKFold from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import mutual_info_regression from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV # define dataset X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1) # define the evaluation method cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) # define the pipeline to evaluate model = LinearRegression() fs = SelectKBest(score_func=mutual_info_regression) pipeline = Pipeline(steps=[(\u0026#39;sel\u0026#39;,fs), (\u0026#39;lr\u0026#39;, model)]) # define the grid grid = dict() grid[\u0026#39;sel__k\u0026#39;] = [i for i in range(X.shape[1]-20, X.shape[1]+1)] # define the grid search search = GridSearchCV(pipeline, grid, scoring=\u0026#39;neg_mean_absolute_error\u0026#39;, n_jobs=-1, cv=cv) # perform the search results = search.fit(X, y) # summarize best print(\u0026#39;Best MAE: %.3f\u0026#39; % results.best_score_) print(\u0026#39;Best Config: %s\u0026#39; % results.best_params_) # summarize all means = results.cv_results_[\u0026#39;mean_test_score\u0026#39;] params = results.cv_results_[\u0026#39;params\u0026#39;] for mean, param in zip(means, params): print(\u0026#39;\u0026gt;%.3fwith: %r\u0026#39; % (mean, param)) Kết quả thực hiện:\nBest MAE: -0.082 Best Config: {\u0026#39;sel__k\u0026#39;: 81} \u0026gt;-1.100 with: {\u0026#39;sel__k\u0026#39;: 80} \u0026gt;-0.082 with: {\u0026#39;sel__k\u0026#39;: 81} \u0026gt;-0.082 with: {\u0026#39;sel__k\u0026#39;: 82} \u0026gt;-0.082 with: {\u0026#39;sel__k\u0026#39;: 83} \u0026gt;-0.082 with: {\u0026#39;sel__k\u0026#39;: 84} \u0026gt;-0.082 with: {\u0026#39;sel__k\u0026#39;: 85} \u0026gt;-0.082 with: {\u0026#39;sel__k\u0026#39;: 86} \u0026gt;-0.082 with: {\u0026#39;sel__k\u0026#39;: 87} \u0026gt;-0.082 with: {\u0026#39;sel__k\u0026#39;: 88} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 89} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 90} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 91} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 92} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 93} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 94} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 95} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 96} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 97} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 98} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 99} \u0026gt;-0.083 with: {\u0026#39;sel__k\u0026#39;: 100} Rất may là ta đã đi đúng hướng, không lãng phí thời gian vì ta đã tìm được model tốt hơn, MAE=0.082 với k=81.\nCuối cùng, nếu muốn xem cụ thể giá trị MAE là bao nhiêu đối với từng giá trị của k, ta code như sau:\n# compare different numbers of features selected using mutual information from numpy import mean from numpy import std from sklearn.datasets import make_regression from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedKFold from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import mutual_info_regression from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from matplotlib import pyplot # define dataset X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1) # define number of features to evaluate num_features = [i for i in range(X.shape[1]-19, X.shape[1]+1)] # enumerate each number of features results = list() for k in num_features: # create pipeline model = LinearRegression() fs = SelectKBest(score_func=mutual_info_regression, k=k) pipeline = Pipeline(steps=[(\u0026#39;sel\u0026#39;,fs), (\u0026#39;lr\u0026#39;, model)]) # evaluate the model cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;neg_mean_absolute_error\u0026#39;, cv=cv, n_jobs=-1) results.append(scores) # summarize the results print(\u0026#39;\u0026gt;%d%.3f(%.3f)\u0026#39; % (k, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=num_features, showmeans=True) pyplot.show() Kết quả thực hiện:\n\u0026gt;81 -0.082 (0.006) \u0026gt;82 -0.082 (0.006) \u0026gt;83 -0.082 (0.006) \u0026gt;84 -0.082 (0.006) \u0026gt;85 -0.082 (0.006) \u0026gt;86 -0.082 (0.006) \u0026gt;87 -0.082 (0.006) \u0026gt;88 -0.082 (0.006) \u0026gt;89 -0.083 (0.006) \u0026gt;90 -0.083 (0.006) \u0026gt;91 -0.083 (0.006) \u0026gt;92 -0.083 (0.006) \u0026gt;93 -0.083 (0.006) \u0026gt;94 -0.083 (0.006) \u0026gt;95 -0.083 (0.006) \u0026gt;96 -0.083 (0.006) \u0026gt;97 -0.083 (0.006) \u0026gt;98 -0.083 (0.006) \u0026gt;99 -0.083 (0.006) \u0026gt;100 -0.083 (0.006)   4. Kết luận Bài thứ 4 về chủ đề Feature Selection, mình đã giới thiệu 2 kỹ thuật Correlation Statistic t và Mutual Information áp dụng cho Input/Output Variables dạng Numerical. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo sẽ là cách sử dụng thuật toán Recursive Feature Elimination (RFE) trong việc Feature Selection. Mời các bạn đón đọc.\n5. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/89_data_prepeation_for_ml_feature_selection_4/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Feature Selection - Phần 4 - Select Features for Numerical Output"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 10 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 3 về Feature Selection. Trong bài này, chúng ta sẽ tìm hiểu phương pháp lựa chọn features đối với dữ liệu có Input Variance thuộc kiểu Numberical thông qua việc thực hành trên một bộ dữ liệu cụ thể.\n1. Pima Indians Dataset Pima Indians Dataset là bộ dữ liệu đã được chúng ta sử dụng trước đó trong bài thứ 4 về DP4ML. Trong bài này, chúng ta tiếp tục sử dụng nó.\nCode đọc và chuẩn bị dữ liệu như sau:\n# load and summarize the dataset from pandas import read_csv from sklearn.model_selection import train_test_split # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] return X, y # load the dataset X, y = load_dataset(\u0026#39;pima-indians-diabetes.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # summarize print(\u0026#39;Train\u0026#39;, X_train.shape, y_train.shape) print(\u0026#39;Test\u0026#39;, X_test.shape, y_test.shape) Vì các Input/Output Variables đều đã ở dạng Numerical nên chúng ta không cần thực hiện Transform Data nữa.\n2. Numerical Feature Selection Hai kỹ thuật có thể áp dụng cho Input Variables dạng Numerical để thực hiện Feature Selection là: ANOVE F-Statistic và Mutual Information Statistic.\n2.1 ANOVA F-Test (Statistic) ANOVA - Analysis Of Variance, là một bài kiểm tra giả thuyết thống kê tham số để xác định xem các giá trị trung bình của hai hoặc nhiều mẫu dữ liệu có cùng phân phối với nhau hay không. F-Statistic, hay F-test, là một loại kiểm tra thống kê tính toán tỷ lệ giữa các giá trị phương sai, chẳng hạn như phương sai từ hai mẫu khác nhau. Phương pháp ANOVA là một loại F-Statistic, được gọi ở đây là ANOVA F-Test.\nĐiều quan trọng, ANOVA được sử dụng khi một biến là số và một biến là Categorical, chẳng hạn như biến đầu vào số và biến mục tiêu phân loại trong nhiệm vụ phân loại. Kết quả của thử nghiệm này có thể được sử dụng để lựa chọn features bằng cách loại bỏ những features độc lập với Output Variables khỏi tập dữ liệu.\nTương tự như Chi-Squared và Mutual Information ở bài trước, ANOVA F-Test cũng được implement trong Scikit-learn bởi hàm f-classif().\n# example of anova f-test feature selection for numerical data from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif from matplotlib import pyplot # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] return X, y # feature selection def select_features(X_train, y_train, X_test): # configure to select all features fs = SelectKBest(score_func=f_classif, k=\u0026#39;all\u0026#39;) # learn relationship from training data fs.fit(X_train, y_train) # transform train input data X_train_fs = fs.transform(X_train) # transform test input data X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # load the dataset X, y = load_dataset(\u0026#39;pima-indians-diabetes.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) # what are scores for the features for i in range(len(fs.scores_)): print(\u0026#39;Feature %d: %f\u0026#39; % (i, fs.scores_[i])) # plot the scores pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) pyplot.show() Kết quả thực hiện:\nFeature 0: 16.527385 Feature 1: 131.325562 Feature 2: 0.042371 Feature 3: 1.415216 Feature 4: 12.778966 Feature 5: 49.209523 Feature 6: 13.377142 Feature 7: 25.126440   Nhìn vào đây, ta có thể chọn k=6 khi sử dụng kỹ thuật này.\n2.2 Mutual Information Feature Selection Kỹ thuật này mình đã giới thiệu ở bài số 9. Cách sử dụng nó cho Input Variables dạng Numerical cũng không có gì thay đổi.\n# example of mutual information feature selection for numerical input data from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import mutual_info_classif from matplotlib import pyplot # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] return X, y # feature selection def select_features(X_train, y_train, X_test): # configure to select all features fs = SelectKBest(score_func=mutual_info_classif, k=\u0026#39;all\u0026#39;) # learn relationship from training data fs.fit(X_train, y_train) # transform train input data X_train_fs = fs.transform(X_train) # transform test input data X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # load the dataset X, y = load_dataset(\u0026#39;pima-indians-diabetes.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) # what are scores for the features for i in range(len(fs.scores_)): print(\u0026#39;Feature %d: %f\u0026#39; % (i, fs.scores_[i])) # plot the scores pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) pyplot.show() Kết quả:\nFeature 0: 0.000000 Feature 1: 0.120654 Feature 2: 0.022504 Feature 3: 0.027918 Feature 4: 0.031081 Feature 5: 0.073868 Feature 6: 0.019561 Feature 7: 0.030316   Căn cứ vào đây thì có lẽ k=2 hoặc k=3 sẽ là lựa chọn phù hợp.\n3. Modeling With Selected Features Vẫn lựa chọn thuật toán Logistic Regression để thực hiện mô hình hóa tập dữ liệu Pima Indians với các kỹ thuật Feature Selection bên trên.\n3.1 Base model Base model được xây dựng với tất cả dữ liệu mặc định.\n# evaluation of a model using all input features from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] return X, y # load the dataset X, y = load_dataset(\u0026#39;pima-indians-diabetes.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # fit the model model = LogisticRegression(solver=\u0026#39;liblinear\u0026#39;) model.fit(X_train, y_train) # evaluate the model yhat = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả:\nAccuracy: 77.56 3.2 Model Built Using ANOVA F-test Features # evaluation of a model using 4 features chosen with anova f-test from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] return X, y # feature selection def select_features(X_train, y_train, X_test): # configure to select a subset of features fs = SelectKBest(score_func=f_classif, k=4) # learn relationship from training data fs.fit(X_train, y_train) # transform train input data X_train_fs = fs.transform(X_train) # transform test input data X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # load the dataset X, y = load_dataset(\u0026#39;pima-indians-diabetes.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) # fit the model model = LogisticRegression(solver=\u0026#39;liblinear\u0026#39;) model.fit(X_train_fs, y_train) # evaluate the model yhat = model.predict(X_test_fs) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả:\nAccuracy: 78.74 3.3 Model Built Using Mutual Information Features # evaluation of a model using 4 features chosen with mutual information from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import mutual_info_classif from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] return X, y # feature selection def select_features(X_train, y_train, X_test): # configure to select a subset of features fs = SelectKBest(score_func=mutual_info_classif, k=4) # learn relationship from training data fs.fit(X_train, y_train) # transform train input data X_train_fs = fs.transform(X_train) # transform test input data X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # load the dataset X, y = load_dataset(\u0026#39;pima-indians-diabetes.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test) # fit the model model = LogisticRegression(solver=\u0026#39;liblinear\u0026#39;) model.fit(X_train_fs, y_train) # evaluate the model yhat = model.predict(X_test_fs) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả:\nAccuracy: 77.16 Kết quả từ 3 lần thí nghiệm cho thấy ANOVA F-Test cho kết quả tốt nhất.\n3.4 Tune the Number of Selected Features Mặc dù đã biết là ANOVA F-Test cho kết quả tốt nhất rồi, nhưng mình vẫn muốn xem xem liệu có thể có kết quả nào tốt hơn không? Ở đây, việc lựa chọn giá trị cho k vẫn đang theo cảm tính, mà k ảnh hưởng lớn đến kết quả cuối cùng. Vì vậy, mình sẽ thử tune một số giá trị của k xem thế nào. Mình sẽ sử dụng Grid-Search và k-Fold Cross-Validation để thí nghiệm:\n# compare different numbers of features selected using anova f-test from pandas import read_csv from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] return X, y # define dataset X, y = load_dataset(\u0026#39;pima-indians-diabetes.csv\u0026#39;) # define the evaluation method cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # define the pipeline to evaluate model = LogisticRegression(solver=\u0026#39;liblinear\u0026#39;) fs = SelectKBest(score_func=f_classif) pipeline = Pipeline(steps=[(\u0026#39;anova\u0026#39;,fs), (\u0026#39;lr\u0026#39;, model)]) # define the grid grid = dict() grid[\u0026#39;anova__k\u0026#39;] = [i+1 for i in range(X.shape[1])] # define the grid search search = GridSearchCV(pipeline, grid, scoring=\u0026#39;accuracy\u0026#39;, n_jobs=-1, cv=cv) # perform the search results = search.fit(X, y) # summarize best print(\u0026#39;Best Mean Accuracy: %.3f\u0026#39; % results.best_score_) print(\u0026#39;Best Config: %s\u0026#39; % results.best_params_) Hơi buồn là kết quả lại thấp hơn lúc đầu:\nBest Mean Accuracy: 0.770 Best Config: {\u0026#39;anova__k\u0026#39;: 5} Nếu muốn biết chi tiết hơn kết quả độ chính xác tương ứng với giá trị của k, chúng ta có thể thực hiện như sau:\n# compare different numbers of features selected using anova f-test from numpy import mean from numpy import std from pandas import read_csv from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from matplotlib import pyplot # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] return X, y # evaluate a given model using cross-validation def evaluate_model(model): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) return scores # define dataset X, y = load_dataset(\u0026#39;pima-indians-diabetes.csv\u0026#39;) # define number of features to evaluate num_features = [i+1 for i in range(X.shape[1])] # enumerate each number of features results = list() for k in num_features: # create pipeline model = LogisticRegression(solver=\u0026#39;liblinear\u0026#39;) fs = SelectKBest(score_func=f_classif, k=k) pipeline = Pipeline(steps=[(\u0026#39;anova\u0026#39;,fs), (\u0026#39;lr\u0026#39;, model)]) # evaluate the model scores = evaluate_model(pipeline) results.append(scores) # summarize the results print(\u0026#39;\u0026gt;%d%.3f(%.3f)\u0026#39; % (k, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=num_features, showmeans=True) pyplot.show() Kết quả:\n\u0026gt;1 0.748 (0.048) \u0026gt;2 0.756 (0.042) \u0026gt;3 0.761 (0.044) \u0026gt;4 0.759 (0.042) \u0026gt;5 0.770 (0.041) \u0026gt;6 0.766 (0.042) \u0026gt;7 0.770 (0.042) \u0026gt;8 0.768 (0.040)   Nếu nhìn vào kết quả này thì k=5 sẽ tốt hơn k=7 vì mức độ phân tán (độ lệch chuẩn) của các gía trị độ chính xác trong các lần test khi k=5 nhỏ hơn khi k=7.\n4. Kết luận Bài thứ 3 về chủ đề Feature Selection, mình đã giới thiệu 2 kỹ thuật ANOVA F-Test và Mutual Information áp dụng cho Input Variables dạng Numerical. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo sẽ là các kỹ thuật Feature Selection áp dụng cho Output Variables dạng Numerical. Mời các bạn đón đọc.\n5. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/88_data_prepeation_for_ml_feature_selection_3/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Feature Selection - Phần 3 - Select Numerical Input Features"},{"categories":["Deep Learning","Human Action Recognition"],"contents":"Human Action Recognition (HAR) là quá trình sử dụng những cảnh quay trong video để nhận diện, phân loại các hành động khác nhau được thực hiện bởi người trong video đó. Nó được ứng dụng rất rộng rãi trong các lĩnh vực như giám sát, thể dục, thể thao, \u0026hellip;\nGiả sử, bạn muốn tạo một ứng dụng dạy học Yoga trực tuyến. Trước tiên, bạn cần quay các video hướng dẫn để người học theo dõi và làm theo. Sau đó, mỗi người học tự tập và quay lại video của mình. Họ gửi các video đó lên ứng dụng của bạn. Dựa vào video nhận được, ứng dụng có thể đánh giá được mức độ chính xác trong mỗi động tác của người học. Từ đó đưa ra gợi ý cải thiện, \u0026hellip; Thật tuyệt vời phải không?\n Trong bài này, chúng ta sẽ cùng nhau tạo ra một model để nhận diện một số hành dộng của người, sử dụng pose estimation và mạng LSTM. Pytorch_Lightning được sử dụng trong bài này.\n1. Tổng quan sơ đồ kiến trúc  Để phân loại một hành động, trước tiên chúng ta cần xác định vị trí các bộ phận cơ thể khác nhau trong mọi khung hình, sau đó phân tích chuyển động của các bộ phận đó theo thời gian.\nBước đầu tiên đạt được bằng cách sử dụng Detectron2, nó xuất ra tư thế của cơ thể (17 Keypoints) sau khi quan sát một khung hình trong video.\nBước thứ hai là phân tích chuyển động của cơ thể theo thời gian và đưa ra dự đoán được thực hiện bằng mạng LSTM. Đầu vào là các Keypoints từ một chuỗi khung được, đầu ra là loại hành đồng được dự đoán.\n2. Chuẩn bị dữ liệu Đối với ứng dụng này, chúng ta chỉ cần huấn luyện mạng LSTM để phân loại các hành động, còn phần Pose Estimation thì chúng ta sẽ sử dụng pre-trained có sẵn cung cấp bởi Detectron2.\nBộ dữ liệu được dùng để huấn luyện mạng LSTM được tạo thành bằng cách sử dụng OpenPose trên các video của tập dữ liệu Berkeley Multimodal Human Action Database (MHAD). Sử dụng cách thức tương tự, chúng ta cũng có thể tạo ra bộ dữ liệu của riêng mình.\nDownload bộ dữ liệu tại đây. Nó bao gồm 6 hành động: JUMPING, JUMPING_JACKS, BOXING, WAVING_2HANDS, WAVING_1HAND, CLAPPING_HANDS.\n Training Data bao gồm các chuỗi 17 Keypoints kết hợp với một nhãn tương ứng. Mỗi Keypoint là một cặp tọa độ (x,y).  Mỗi lần phân loại, chúng ta sẽ sử dụng 32 frames liên tiếp nhau. Như vậy thì kích thước dữ liệu của một Input Data sẽ là 32x34:\n !head -2 RNN-HAR-2D-Pose-database/X_train.txt --- 295.914,161.579,307.693,203.413,281.546,203.368,274.997,251.562,267.194,293.253,337.619,204.669,347.958,255.443,341.541,295.866,286.81,289.393,297.196,355.832,297.22,405.371,321.967,291.959,327.143,358.408,328.528,411.922,294.546,156.42,305.002,156.418,0,0,318.083,161.632 295.855,161.6,307.684,203.408,281.529,203.385,274.989,251.574,267.191,291.961,337.615,204.646,347.974,254.209,344.093,295.816,286.803,289.377,297.165,355.827,297.205,404.095,323.248,290.652,324.564,358.409,328.493,410.63,293.252,157.686,303.706,157.706,0,0,318.024,161.654 Bởi vì OpenPose trả về kết quả là 18 Keypoints, trong khi kết quả của Detectron2 chỉ là 17 Keypoints nên chúng ta sẽ phải thực hiện một bước chuyển đổi trước khi sử dụng bộ dữ liệu này.\nWINDOW_SIZE = 32 # 32 continuous frames class PoseDataset(Dataset): def __init__(self, X, Y): self.X = X self.y = Y def __len__(self): return len(self.y) def __getitem__(self, idx): return self.X[idx], self.y[idx] openpose_to_detectron_mapping = [0, 1, 28, 29, 26, 27, 32, 33, 30, 31, 8, 9, 2, 3, 10, 11, 4, 5, 12, 13, 6, 7, 20, 21, 14, 15, 22, 23, 16, 17, 24, 25, 18, 19] class PoseDataModule(pl.LightningDataModule): def __init__(self, data_root, batch_size): super().__init__() self.data_root = data_root self.batch_size = batch_size self.X_train_path = self.data_root + \u0026#34;X_train.txt\u0026#34; self.X_test_path = self.data_root + \u0026#34;X_test.txt\u0026#34; self.y_train_path = self.data_root + \u0026#34;Y_train.txt\u0026#34; self.y_test_path = self.data_root + \u0026#34;Y_test.txt\u0026#34; # Detectron2 produces only 17 key points while OpenPose produces 18 (or more) key points. def convert_to_detectron_format(self, row): row = row.split(\u0026#39;,\u0026#39;) # filtering out coordinate of neck joint from the training/validation set originally generated using OpenPose. temp = row[:2] + row[4:] # change to Detectron2 order of key points temp = [temp[i] for i in openpose_to_detectron_mapping] return temp def load_X(self, X_path): file = open(X_path, \u0026#39;r\u0026#39;) X = np.array( [elem for elem in [ self.convert_to_detectron_format(row) for row in file ]], dtype=np.float32 ) file.close() blocks = int(len(X) / WINDOW_SIZE) X_ = np.array(np.split(X, blocks)) return X_ # Load the networks outputs def load_y(self, y_path): file = open(y_path, \u0026#39;r\u0026#39;) y = np.array( [elem for elem in [ row.replace(\u0026#39; \u0026#39;, \u0026#39; \u0026#39;).strip().split(\u0026#39; \u0026#39;) for row in file ]], dtype=np.int32 ) file.close() # for 0-based indexing return y - 1 def prepare_data(self): pass def setup(self, stage=None): X_train = self.load_X(self.X_train_path) X_test = self.load_X(self.X_test_path) y_train = self.load_y(self.y_train_path) y_test = self.load_y(self.y_test_path) self.train_dataset = PoseDataset(X_train, y_train) self.val_dataset = PoseDataset(X_test, y_test) def train_dataloader(self): # train loader train_loader = torch.utils.data.DataLoader( self.train_dataset, batch_size=self.batch_size, shuffle=True ) return train_loader def val_dataloader(self): # validation loader val_loader = torch.utils.data.DataLoader( self.val_dataset, batch_size=self.batch_size, shuffle=False ) return val_loader 3. Xây dựng mô hình 3.1 Human Pose Estimation model Phần này, chúng ta sử dụng pre-trained R50-FPN model từ Detectron2 Model Zoo\n# obtain detectron2\u0026#39;s default config cfg = get_cfg() # load the pre trained model from Detectron2 model zoo cfg.merge_from_file(model_zoo.get_config_file(\u0026#34;COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\u0026#34;)) # set confidence threshold for this model cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # load model weights cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\u0026#34;COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\u0026#34;) # create the predictor for pose estimation using the config pose_detector = DefaultPredictor(cfg) 3.2 Định nghĩa LSTM model LSTM model sẽ được khởi tạo với hidden_dim = 50, optimizer là Adam và sử dụng ReduceLROnPlateau scheduler để giảm learning_rate. Ở đây, mình chỉ sử dụng 1 LSTM layer, bạn có thể thí nghiệm với nhiều LSTM layers hơn.\n# We have 6 output action classes. TOT_ACTION_CLASSES = 6 #lstm classifier definition class ActionClassificationLSTM(pl.LightningModule): # initialise method def __init__(self, input_features, hidden_dim, learning_rate=0.001): super().__init__() # save hyperparameters self.save_hyperparameters() # The LSTM takes word embeddings as inputs, and outputs hidden states # with dimensionality hidden_dim. self.lstm = nn.LSTM(input_features, hidden_dim, num_layers=2, batch_first=True) # The linear layer that maps from hidden state space to classes self.linear = nn.Linear(hidden_dim, TOT_ACTION_CLASSES) def forward(self, x): # invoke lstm layer lstm_out, (ht, ct) = self.lstm(x) # invoke linear layer return self.linear(ht[-1]) def training_step(self, batch, batch_idx): # get data and labels from batch x, y = batch # reduce dimension y = torch.squeeze(y) # convert to long y = y.long() # get prediction y_pred = self(x) # calculate loss loss = F.cross_entropy(y_pred, y) # get probability score using softmax prob = F.softmax(y_pred, dim=1) # get the index of the max probability pred = prob.data.max(dim=1)[1] # calculate accuracy acc = torchmetrics.functional.accuracy(pred, y) dic = { \u0026#39;batch_train_loss\u0026#39;: loss, \u0026#39;batch_train_acc\u0026#39;: acc } # log the metrics for pytorch lightning progress bar or any other operations self.log(\u0026#39;batch_train_loss\u0026#39;, loss, prog_bar=True) self.log(\u0026#39;batch_train_acc\u0026#39;, acc, prog_bar=True) #return loss and dict return {\u0026#39;loss\u0026#39;: loss, \u0026#39;result\u0026#39;: dic} def training_epoch_end(self, training_step_outputs): # calculate average training loss end of the epoch avg_train_loss = torch.tensor([x[\u0026#39;result\u0026#39;][\u0026#39;batch_train_loss\u0026#39;] for x in training_step_outputs]).mean() # calculate average training accuracy end of the epoch avg_train_acc = torch.tensor([x[\u0026#39;result\u0026#39;][\u0026#39;batch_train_acc\u0026#39;] for x in training_step_outputs]).mean() # log the metrics for pytorch lightning progress bar and any further processing self.log(\u0026#39;train_loss\u0026#39;, avg_train_loss, prog_bar=True) self.log(\u0026#39;train_acc\u0026#39;, avg_train_acc, prog_bar=True) def validation_step(self, batch, batch_idx): # get data and labels from batch x, y = batch # reduce dimension y = torch.squeeze(y) # convert to long y = y.long() # get prediction y_pred = self(x) # calculate loss loss = F.cross_entropy(y_pred, y) # get probability score using softmax prob = F.softmax(y_pred, dim=1) # get the index of the max probability pred = prob.data.max(dim=1)[1] # calculate accuracy acc = torchmetrics.functional.accuracy(pred, y) dic = { \u0026#39;batch_val_loss\u0026#39;: loss, \u0026#39;batch_val_acc\u0026#39;: acc } # log the metrics for pytorch lightning progress bar and any further processing self.log(\u0026#39;batch_val_loss\u0026#39;, loss, prog_bar=True) self.log(\u0026#39;batch_val_acc\u0026#39;, acc, prog_bar=True) #return dict return dic def validation_epoch_end(self, validation_step_outputs): # calculate average validation loss end of the epoch avg_val_loss = torch.tensor([x[\u0026#39;batch_val_loss\u0026#39;] for x in validation_step_outputs]).mean() # calculate average validation accuracy end of the epoch avg_val_acc = torch.tensor([x[\u0026#39;batch_val_acc\u0026#39;] for x in validation_step_outputs]).mean() # log the metrics for pytorch lightning progress bar and any further processing self.log(\u0026#39;val_loss\u0026#39;, avg_val_loss, prog_bar=True) self.log(\u0026#39;val_acc\u0026#39;, avg_val_acc, prog_bar=True) def configure_optimizers(self): # adam optimiser optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate) # learning rate reducer scheduler scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\u0026#39;min\u0026#39;, factor=0.5, patience=10, min_lr=1e-15, verbose=True) # scheduler reduces learning rate based on the value of val_loss metric return {\u0026#34;optimizer\u0026#34;: optimizer, \u0026#34;lr_scheduler\u0026#34;: {\u0026#34;scheduler\u0026#34;: scheduler, \u0026#34;interval\u0026#34;: \u0026#34;epoch\u0026#34;, \u0026#34;frequency\u0026#34;: 1, \u0026#34;monitor\u0026#34;: \u0026#34;val_loss\u0026#34;}} 4. Huấn luyện mô hình Sử dụng ModelCheckpoint callback và LearningRateMonitor, chúng ta sẽ huấn luyện mạng LSTM như sau:\ndef do_training_validation(): pl.seed_everything(21) parser = ArgumentParser() parser = pl.Trainer.add_argparse_args(parser) parser = configuration_parser(parser) # args = parser.parse_args() args, unknown = parser.parse_known_args() # init model  hidden_dim = 50 WINDOW_SIZE = 32 model = ActionClassificationLSTM(WINDOW_SIZE, hidden_dim, learning_rate=args.learning_rate) data_module = PoseDataModule(data_root=args.data_root, batch_size=args.batch_size) #save only the top 1 model based on val_loss checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\u0026#39;val_loss\u0026#39;) lr_monitor = LearningRateMonitor(logging_interval=\u0026#39;step\u0026#39;) #trainer trainer = pl.Trainer.from_argparse_args(args, # fast_dev_run=True, max_epochs=args.epochs, deterministic=True, gpus=1, progress_bar_refresh_rate=1, callbacks=[EarlyStopping(monitor=\u0026#39;train_loss\u0026#39;, patience=15), checkpoint_callback, lr_monitor]) trainer.fit(model, data_module) return model Kết quả huấn luyện model:\n  Train Accuracy:    Train Loss:\n    Validation Accuracy:    Validation Loss:   5. Thực hiên Inference # how many frames to skip while inferencing # configuring a higher value will result in better FPS (frames per rate), but accuracy might get impacted SKIP_FRAME_COUNT = 1 # analyse the video def analyse_video(pose_detector, lstm_classifier, video_path): # open the video cap = cv2.VideoCapture(video_path) # width of image frame width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # height of image frame height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # frames per second of the input video fps = int(cap.get(cv2.CAP_PROP_FPS)) # total number of frames in the video tot_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # video output codec fourcc = cv2.VideoWriter_fourcc(*\u0026#39;mp4v\u0026#39;) # extract the file name from video path file_name = ntpath.basename(video_path) # video writer vid_writer = cv2.VideoWriter(\u0026#39;res_{}\u0026#39;.format( file_name), fourcc, 30, (width, height)) # counter counter = 0 # buffer to keep the output of detectron2 pose estimation buffer_window = [] # start time start = time.time() label = None # iterate through the video while True: # read the frame ret, frame = cap.read() # return if end of the video if ret == False: break # make a copy of the frame img = frame.copy() if(counter % (SKIP_FRAME_COUNT+1) == 0): # predict pose estimation on the frame outputs = pose_detector(frame) # filter the outputs with a good confidence score persons, pIndicies = filter_persons(outputs) if len(persons) \u0026gt;= 1: # pick only pose estimation results of the first person. # actually, we expect only one person to be present in the video. p = persons[0] # draw the body joints on the person body draw_keypoints(p, img) # input feature array for lstm features = [] # add pose estimate results to the feature array for i, row in enumerate(p): features.append(row[0]) features.append(row[1]) # append the feature array into the buffer # not that max buffer size is 32 and buffer_window operates in a sliding window fashion if len(buffer_window) \u0026lt; WINDOW_SIZE: buffer_window.append(features) else: # convert input to tensor model_input = torch.Tensor(np.array(buffer_window, dtype=np.float32)) # add extra dimension model_input = torch.unsqueeze(model_input, dim=0) # predict the action class using lstm y_pred = lstm_classifier(model_input) prob = F.softmax(y_pred, dim=1) # get the index of the max probability pred_index = prob.data.max(dim=1)[1] # pop the first value from buffer_window and add the new entry in FIFO fashion, to have a sliding window of size 32. buffer_window.pop(0) buffer_window.append(features) label = LABELS[pred_index.numpy()[0]] #print(\u0026#34;Label detected \u0026#34;, label) # add predicted label into the frame if label is not None: cv2.putText(img, \u0026#39;Action: {}\u0026#39;.format(label), (int(width-400), height-50), cv2.FONT_HERSHEY_COMPLEX, 0.9, (102, 255, 255), 2) # increment counter counter += 1 # write the frame into the result video vid_writer.write(img) # compute the completion percentage percentage = int(counter*100/tot_frames) # return the completion percentage # yield \u0026#34;data:\u0026#34; + str(percentage) + \u0026#34;\\n\\n\u0026#34; # show video results cv2.imshow(\u0026#34;image\u0026#34;, img) # Press Q on keyboard to exit if cv2.waitKey(25) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break Kết quả thực hiện trên video:\n 6. Kết luận Trong bài này, chúng ta đã cùng nhau thực hành xây dựng một mô hình để nhận diện hành động của người trong video bằng cách sử dụng kết hợp Detectron2 cho Pose Estimation và LSTM cho phân loại. Có rất nhiều thứ có thể cải tiến để có được kết quả tốt hơn mà bạn có thể thử nếu áp dụng vào bài toán thực tế:\n Tăng FPS để có thể chạy được realtime: Tối ưu hóa model (pruning, quantization), loại bỏ bớt frame khi nhận diện, sử dụng multi-threading, \u0026hellip; Sử dụng các Pose Estimation model khác như AlphaPose, OpenPose, \u0026hellip;  Toàn bộ source code của bài này, các bạn xem tại đây\nMời các bạn đón đọc.!\n7. Tham khảo [1] Bibin Sebastian, “Human Action Recognition using Detectron2 and LSTM”, Available online: https://learnopencv.com/human-action-recognition-using-detectron2-and-lstm/ (Accessed on 30 Jul 2021).\n","permalink":"https://tiensu.github.io/blog/87_human_action_recognition_using_deep_learning/","tags":["Deep Learning","Human Action Recognition"],"title":"Nhận diện hành động của người sử dụng Deep Learning"},{"categories":["Deep Learning","Human Pose"],"contents":"1. Giới thiệu Human Pose Estimation (HPE) 1.1 Định nghĩa Human Pose Human Pose là sự thể hiện định hướng của một người ở định dạng đồ họa (khung xương). Về cơ bản, nó là một tập hợp các tọa độ có thể được kết nối để mô tả tư thế của một người. Mỗi phối hợp trong khung xương được gọi là một bộ phận (hoặc một khớp - joint, một điểm chính - keypoint). Một kết nối hợp lệ giữa hai phần được gọi là một cặp (hoặc một chi - limb). Lưu ý rằng, không phải tất cả các kết hợp bộ phận đều tạo ra các cặp hợp lệ. Dưới đây là một ví dụ:\n 1.2 Ứng dụng của Human Pose Estimation Biết được định hướng của một người sẽ mở ra con đường cho một số ứng dụng trong thực tế:\n Nhận diện hành động của người.  Theo dõi các tư thế khác nhau của một người trong một khoảng thời gian để nhận dạng hoạt động.\n  Motion Capture and Augmented Reality  Đây là ứng dụng liên quan đến lĩnh vực đồ họa. Bằng việc biết được Pose của của một người, hệ thống có thể sinh ra các đạo cụ, trang phục vừa vặn với người trong video.\n  Huấn luyện Robots  Bằng việc xác định Human Pose, Robots có thể hiểu được những chỉ dẫn của con người và thực hiện các hành động theo chỉ dẫn đó.\n Motion Tracking for Consoles  Đây cũng là ứng dụng trong lĩnh vực đồ họa. Tư thế, hành động của người có thể được nhận diện, sau đó được mô phỏng lại bằng một nhân vật ảo trong game, phim, \u0026hellip;\n 1.3 Multi-Person Pose Estimation Multi-Person Pose Estimation sẽ khó hơn nhiều so với Single-Person Pose Estimation, bởi vì chúng ta không biết vị trí của mỗi người dẫn đến việc nhẫm lẫn giữa các bộ phận của mỗi người với nhau. Gọi là râu ông nọ cắm cằm bà kia. Để giải quyết vấn đề này, có thể tiếp cận 1 trong 2 cách sau:\n Top-Down: Đầu tiên, sử dụng kỹ thuật Object Detection để xác định vị trí của từng người trước, sau đó mới thực hiện Pose Estimation cho mỗi người trong từng vị trí cụ thể đó. Ưu điểm của cách này là đơn giản, dễ thực hiện. Còn độ chính xác thì còn tùy vào từng ngữ cảnh. Down-Top: Ngược lại với cách trên, cách này sẽ phát hiện toàn bộ các bộ phận của mọi người trong ảnh trước, sau đó mới liên kết lại với nhau để xác định Pose của mỗi người.   1.4 Một số phương pháp thực hiện Human Pose Estimation a, OpenPose OpenPose có lẽ là phương thức phổ biến nhất dành cho HPE bởi vì tài liệu hướng dẫn của nó được tổ chức khá chi tiết, rõ ràng trên github. Sử dụng cách tiếp cận bottom-up, đầu tiên, OpenPose sẽ phát hiện tất cả các keypoints của mọi người trong ảnh, sau đó mới phân chia mỗi keypoint về từng người cụ thể.\n VGG19 trong OpenPose chịu trách nhiệm trích xuất các đặc trưng từ hình ảnh. Các đặc trưng này sau đó được đưa vào hai nhánh song song của các lớp Conv. Nhánh đầu tiên dự đoán 18 phần của bộ xương tư thế người. Nhánh thứ hai dự đoán một tập hợp 38 Part Affinity Fields (PAFs) thể hiện mức độ liên kết giữa các bộ phận đó.\n Các giai đoạn kế tiếp được sử dụng để tinh chỉnh các dự đoán được thực hiện bởi mỗi nhánh. Để được giải thích kỹ hơn về thuật toán, bạn có thể tham khảo bài báo của họ và bài đăng trên blog này.\nb, DeepCut DeepCut cũng sử dụng chiến lược bottom-up đối với trường hợp Multi-Person Pose Estimation.\n c, RMPE (AlphaPose) RMPE là một đại diện phổ biến của HPE theo chiến lược top-bottom. Phương pháp thường phụ thuộc vào độ chính xác của thuật toán phát hiện người trước đó. Nếu việc phát hiện đối tượng người không chính xác thì hiệu quả của HPE cũng theo đó mà giảm xuống.\n d, Mask RCNN Mask RCNN là một kiến trúc phổ biến để thực Semantic và Instance Segmantation. Mô hình này dự đoán song song cả vị trí của các đối tượng khác nhau trong hình ảnh và Mask của đối tượng về mặt ngữ nghĩa. Kiến trúc cơ bản có thể được mở rộng khá dễ dàng để ước tính tư thế của con người.\n Mask RCNN hiện tại đã được tích hợp vào Detectron2 platform của Facebook, giúp các nhà phát triển dễ dàng sử dụng nó.\n2. Detectron2 platform 2.1 Giới thiệu Detectron2 là một platform của Facebook AI Research (FAIR) dành cho các tác vụ Object Detection, Human Pose, Segmentation,\u0026hellip; Platform này được phát triển bằng Pytorch và cung cấp dưới dạng mã nguồn mở cho các nhà phát triển. Phiên bản trước đó, Detectron được xây dựng bằng Caffe2 framework.\nFAIR tuyên bố như sau:\n“We builtDetectron2 to meet the research needs of Facebook AI and to provide the foundation for object detection in production use cases at Facebook. We are now using Detectron2 to rapidly design and train the next-generation pose detection models that power Smart Camera, the AI camera system in Facebook’s Portal video-calling devices. By relying on Detectron2 as the unified library for object detection across research and production use cases, we are able to rapidly move research ideas into production models that are deployed at scale.”\n Bạn có thể tìm thấy các pre-trained model của mà Detectron2 cung cấp tại Detectron2 Model Zoo.\nBên cạnh đó, FAIR cũng mới phát hành Detectron2go với sự bổ sung thêm một số tính năng giúp triển khai các model dễ dàng hơn trong sản phẩm thực tế.\n2.2 Thực hành với Detectron2 a, Giới thiệu Wrapper Detectron2 project Detectron2 có thể được sử dụng trực tiếp từ code trên github của nó. Tuy nhiên, ở đây mình hướng dẫn các bạn sử dụng thông qua một wrapper-detectron2-project tên là detectron2-pipeline. Project này có những ưu điểm sau:\n Dễ dàng tích hợp với các model khác nhau từ Detectron2 Model Zoo. Tăng tốc độ xử lý video, hình ảnh bằng việc chia xử lý ra nhiều threads khác nhau. Tối ưu hóa tốc độ train/inference bằng việc tận dụng tối đa sức mạnh của GPU/CPU.  $ git clone git://github.com/jagin/detectron2-pipeline.git $ cd detectron2-pipeline Cấu trúc của project này như sau:\n├── assets ├── configs ├── environment.yml ├── LICENSE ├── output ├── pipeline │ ├── annotate_image.py │ ├── annotate_video.py │ ├── async_predict.py │ ├── capture_image.py │ ├── capture_images.py │ ├── capture_video.py │ ├── display_video.py │ ├── __init__.py │ ├── libs │ │ ├── async_predictor.py │ │ ├── file_video_capture.py │ │ ├── __init__.py │ │ └── webcam_video_capture.py │ ├── pipeline.py │ ├── predict.py │ ├── save_image.py │ ├── save_video.py │ ├── separate_background.py │ └── utils │ ├── colors.py │ ├── detectron.py │ ├── fs.py │ ├── __init__.py │ ├── text.py │ └── timeme.py ├── process_images.py ├── process_video.py ├── pytest.ini ├── README.md └── tests Các folders/files dùng chung:\n- utils/: common utility scripts, - pipeline/lib/file_video_capture.py: video file capturing helper class utilizing threading and the queue to obtain FPS speedup, - pipeline/lib/webcam_video_capture.py: helper class for capturing webcam in a separate thread, - pipeline/capture_image.py: pipeline task to capture single image file, - pipeline/capture_images.py: pipeline task to capture images from a directory, - pipeline/capture_video.py: pipeline task to capture video stream from file or webcam using a faster, threaded method for reading video frames. - pipeline/display_video.py: pipeline task to display images as a video, - pipeline/pipeline.py: common pipeline class fo all pipeline tasks, - pipeline/save_image.py: pipeline task to save images, - pipeline/save_video.py: pipeline task to save a video. Các files dưới đây là nơi chúng ta sẽ sử dụng, chỉnh sửa để áp dụng vào bài toán cụ thể:\n- pipeline/annotate_image.py: pipeline task for image annotation, - pipeline/annotate_video.py: pipeline task for video annotation, - pipeline/lib/async_predictor.py: asynchronous predictor utilizing multiprocessing to run the inferences in parallel in separate processes, - pipeline/predict.py: pipeline task to perform a prediction, - pipeline/async_predict.py: pipeline task to perform prediction asynchronously using multiprocessing, - pipeline/separate_background.py: custom pipeline task to separate the background from foreground instances as an example use of the semantic segmentation model from Detectron2. Các files cấu hình models:\n├── configs │ ├── COCO-Detection │ │ ├── faster_rcnn_R_50_FPN_3x.yaml │ │ └── retinanet_R_50_FPN_3x.yaml │ ├── COCO-InstanceSegmentation │ │ └── mask_rcnn_R_50_FPN_3x.yaml │ ├── COCO-Keypoints │ │ └── keypoint_rcnn_R_50_FPN_3x.yaml │ └── COCO-PanopticSegmentation │ └── panoptic_fpn_R_50_3x.yaml b, Tạo môi trường ảo bằng Conda $ conda env create -f environment.yml $ conda activate detectron2-pipeline c, Cài đặt Detectron2 $ cd .. $ git clone https://github.com/facebookresearch/detectron2.git $ cd detectron2 $ git checkout 3def12bdeaacd35c6f7b3b6c0097b7bc31f31ba4 $ python setup.py build develop Commit 3def12bdeaacd35c6f7b3b6c0097b7bc31f31ba4 là phiên bản ổn định để sử dụng.\nd, Image Processing  Các options:    Ví dụ với InstanceSegmentation model (mặc định):  $ python process_images.py -i assets/images/others -p    Ví dụ với Human Pose Estimation model:  $ python process_images.py -i assets/images/others/couple.jpg -p --config-file configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml   e, Video Processing  Các options:    Ví dụ với InstanceSegmentation model (mặc định) sử dụng webcam/camera:  $ python process_video.py -i 0 -d -p  Ví dụ với InstanceSegmentation model (mặc định) sử dụng video file:  $ python process_video.py -i assets/videos/walk.small.mp4 -p -d -ov walk.avi    Ví dụ với PanopticSegmentation model sử dụng video file:  $ python process_video.py -i assets/videos/traffic.small.mp4 -p -d -ov traffic.avi --config-file configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml    Ví dụ với Human Pose Estimation model sử dụng video file:  $ python process_video.py -i assets/videos/walk.small.mp4 -p -d --config-file configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml   f, Human Pose Tracking PoseFlow là một phương pháp tracking cho Human Pose rất hiệu quả. Nó được mô tả trong bài báo PoseFlow: Efficient Online Pose Tracking của Yuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, Cewu Lu.\n Mình cảm thấy bài báo hơi khó đọc 1 chút. Tuy nhiên mình không đi quá chi tiết vào thuật toán mà đơn giản chỉ muốn sử dụng nó.\nPoseFlow cũng đã được tích hợp vào AlphaPose, bạn có thể thử. Còn ở đây mình sẽ áp dụng nó với Mask RCNN trong Detectron2.\n$ process_video.py -i assets/videos/walk.small.mp4 -p -d --config-file configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml -tp   Kết quả cuối cùng sẽ phụ thuộc vào một số yếu tố:\n ĐỘ chính xác của model phát hiện người. Độ chính xác của model Pose Estimation. Chất lượng của hình ảnh. Các options khác nhau của tracking (xem trong file processing_video.py, dòng 51-60)  f, Background Separation Một ứng dụng thú vị khác của Detectron2 là Background Separation, giống như viêc chụp ảnh làm mờ hậu cảnh, chỉ tập trung vào tượng chính. Thuật toán được diễn giải như sơ đồ dưới đây:\n  Ví dụ với hình ảnh:  $ python process_images.py -i assets/images/others/couple.jpg -sb  Ví dụ với video:  python process_video.py -i assets/videos/walk.small.mp4 -sb -d   3. Kết luận Nếu xét riêng về Human Pose Estimation, các model trong Detectron2 có thể không phải là tốt nhất, xem bảng so sánh sau:\n Từ bảng trên, có thể thấy AlphaPose cho kết quả tốt hơn so với 2 phương pháp OpenPose (CMU-Pose) và Detectron2 (Mask RCNN). Tuy nhiên, bù lại thì Detectron2 (Mask RCNN) lại dễ dàng sử dụng hơn. Tùy vào yêu cầu bài toán cụ thể mà bạn có thể chọn lựa phương pháp phù hợp cho mình.\nNhư vậy, bài này mình đã giới thiệu qua cho các bạn về Human Pose Estimation và cách sử dụng Detectron2 platform. Bài tiếp theo, chúng ta sẽ thực hành phân loại một số hành động của con người trong video sử dụng những kiến thức đã đề cập trong bài hôm nay.\nMời các bạn đón đọc.!\n4. Tham khảo [1] Bharath Raj, “An Overview of Human Pose Estimation with Deep Learning”, Available online: https://medium.com/beyondminds/an-overview-of-human-pose-estimation-with-deep-learning-d49eb656739b (Accessed on 31 Jul 2021).\n[2] Jarosław Gilewski, “How to embed Detectron2 in your computer vision project”, Available online:https://medium.com/deepvisionguru/how-to-embed-detectron2-in-your-computer-vision-project-817f29149461 (Accessed on 31 Jul 2021).\n[3] Jarosław Gilewski, “PoseFlow — real-time pose tracking”, Available online: https://medium.com/deepvisionguru/poseflow-real-time-pose-tracking-7f8062a7c996 (Accessed on 31 Jul 2021).\n","permalink":"https://tiensu.github.io/blog/86_human_pose_estimation/","tags":["Deep Learning","Human Pose"],"title":"Giới thiệu Human Pose Estimation và Detectron2 platform"},{"categories":["Deep Learning","Human Action Recognition"],"contents":"Trong bài viết này, chúng ta cùng tìm hiểu về Video Classification. Chúng ta sẽ điểm danh sơ lược một số phương pháp tiếp cận để giải quyết bài toán này.\n1. Video Classification là gì? Nếu nói đến Image Classification thì chắc các bạn đều biết, còn Video Classification thì có thể nhiều người chưa nghe qua. Về bản chất, video là tập hợp các frame ảnh được hiển thị liên tiếp nhau với tốc độ khoảng 24 ảnh/giây. Điều này làm cho mắt người có cảm giác sự việc diễn ra trong video là liên tục. Video Classification tức là phân loại, dự đoán xem đối tượng, sự vật, sự việc trong một đoạn video là gì.\nNếu bạn sử dụng những kỹ thuật của Image Classification vào bài toán Video Classification thì \u0026hellip; cũng có thể đúng trong 1 số trường hợp, khi mà cái cần phân lại trong video tương đối rõ ràng, ko có sự nhập nhằng, liên kết giữa các frames với nhau. Ví dụ, phân biệt hành vi đứng và ngồi của người trong video.\nTuy nhiên, với những bài toán mà thông tin cần xác định trải dài qua nhiều frames, nếu chúng ta chỉ sử dụng một frame để nhận biết thì sẽ dẫn đến kết quả sai lầm. Ví dụ, phân biệt hành vi đang đứng, đang ngồi của người trong video. Hay phân biệt hành vi lấy hàng, trả hàng lên kệ trong siêu thị, \u0026hellip;\n Đối với những tình huống kiểu như vậy, chúng ta phải kết hợp nhiều frames lại với nhau, đồng thời có tính đến thứ tự theo thời gian xuất hiện của chúng mới có thể giải quyết được.\n2. Các phương pháp Video Classification 2.1 Phương pháp Single-Frame CNN  Đây chính xác là cách tiếp cận sử dụng kỹ thuật của bài toán Image Classification. Chúng ta cho lần lượt từng frame của video chạy qua mô hình Image Classification. Kết quả cuối cùng là trung bình của tất các kết quả dự đoán trên mỗi frame. Tất nhiên, chúng ta có thể chỉ dùng 1 số lượng frames nhất định chứ không cần toàn bộ frames trong video. Giá trị này gọi là window_size.\n2.2 Phương pháp Late Fusion  Về cơ bản, phương pháp Late Fusion rất giống với phương pháp Single-Frame CNN nhưng phức tạp hơn một chút. Sự khác biệt ở đây là trong phương pháp Single-Frame CNN, việc tính trung bình trên tất cả các xác suất dự đoán được thực hiện sau khi mô hình đã hoàn thành công việc của nó, nhưng trong phương pháp Late Fusion, quá trình lấy trung bình (hoặc một số kỹ thuật Fusion khác) được tích hợp luôn vào kiến trúc mạng của chính nó. Do đó, cấu trúc thời gian của chuỗi frame được duy trì.\nCụ thể, một Fusion layer được sử dụng để hợp nhất các kết quả đầu ra của của các mạng riêng biệt, những cái hoạt động trên các frames khác nhau. Layer này thường được xây dựng bằng một số kỹ thuật như Max Pooling, Average Pooling, hay Flattening.\nPhương pháp Late Fusion được cho là có thể học được các thông tin về không gian và thời gian của những đối tượng, hành động xuất hiện trong video. Do đó, độ chính xác cũng cao hơn phương pháp Single-Frame CNN.\n2.3 Phương pháp Early Fusion  Cách tiếp cận của phương pháp này ngược với phương pháp Late Fusion. Thông tin về thời gian và không gian của các frames được hợp nhất trước khi chuyển đến cho mô hình. Mô hình sẽ học để xác định sự chuyển động của các pixels giữa các frames liền kề nhau.\nMột video có kích thước T x 3 x H x W, trong đó T à khoảng thời gian, 3 kênh RGB, chiều dài H và chiều rộng W, sau khi được Early Fusion sẽ trở thành một Tensor có kích thước 3T x H x W.\n2.4 Phương pháp CRNN  Ý tưởng của phương pháp này là sử dụng một mạng CNN để trích xuất các đặc trưng của mỗi frame. Đầu ra của các mạng này sau đó được cung cấp cho một mạng LSTM đa lớp Many-to-One để kết hợp và đưa ra kết quả dự đoán. LSTM là kiến trúc mạng lý tưởng cho việc xử lý dữ liệu dạng chuỗi, có liên quan đến yếu tố thời gian.\nBạn có thể đọc chi tiết về phương pháp này trong bài báo Action Recognition in Video Sequences using Deep Bi-Directional LSTM With CNN Features của Amin Ullah (IEEE 2017).\n2.5 Phương pháp Pose Detection + LSTM  Một ý tưởng thú vị khác là sử dụng một mô hình Pose Detection để nhận về các Key Points từ cơ thể con người trong mỗi frame. Sau đó, đưa các Key Points này đi qua mạng LSTM đề quyết định hành vi của người đó trong video. Một số mô hình Pose Detection nổi tiếng có thể sử dụng là Open Pose, Detectron2, \u0026hellip;Trong bài tiếp theo, chúng ta sẽ sử dụng phương pháp này đề nhận diện các hành vi của con người.\n2.6 Phương pháp Optical Flow + CNN  Optical Flow là một mô hình mô tả sự chuyển động có thể nhìn thấy được của các đối tượng trong các frames của video. Sự chuyển động này được thể hiện thông qua một vector chuyển động. Optical Flow được sử dụng rất hiệu quả trong các ứng dụng theo dõi chuyển động. Sẽ là một ý tưởng không tồi nếu kết hợp Optical Flow với mạng CNN để lấy được các thông tin về sự chuyển động (thông tin thời gian) và thông tin về không gian của các đối tượng trong video. Bài báo A Comprehensive Review on Handcrafted and Learning-Based Action Representation Approaches for Human Activity Recognition của Allah Bux Sargano (2017) đề cập chi tiết đến phương pháp này.\nMô tả qua về cách thức hoạt động. Sẽ có 2 CNN Networks hoạt động đồng thời.\n Network thứ nhất là Spatial ConvNet. Nó nhận một frame đơn lẻ để xử lý và sinh ra kết quả dự đoán dựa trên thông tin không gian. Network thứ hai là Temporal ConvNet. Nó nhận vào một chuỗi các frame liên tiếp để tính toán vector chuyển động của đối tượng. Sử dụng vector này để sinh ra kết quả dự đoán.  Trung bình dự đoán của 2 Networks được sử dụng làm kết quả cuối cùng.\n3.7 Phương pháp SlowFast  Tương tự như phương pháp Optical Flow + CNN, phương pháp này cũng sử dụng song song 2 Networks. Một Network hoạt động trên luồng video có độ phân giải thấp gọi là Slow branch, một Network hoạt động trên video có độ phân giải cao hơn gọi là Fast branch. Đầu ra của mỗi branch được kết hợp lại để sinh ra kết quả dự đoán cuối cùng.\nChi tiết về phương pháp này có thể tham khảo ở bài báo SlowFast Networks for Video Recognition của Christoph Feichtenhofer ( ICCV 2019).\n2.8 Phương pháp 3D CNN ( Slow Fusion)  Phương pháp này sử dụng 3D CNN để xử lý đồng thời cả thông tin không gian và thời gian trong cùng 1 mạng duy nhất. Tên khác của nó là Slow Fusion. Không giống như Late/Early Fusion, nó kết hợp luôn thông tin thời gian và không gian tại mỗi lớp Conv xuyên suốt toàn mạng 3D CNN. Dữ liệu đầu vào, do đó cũng phải có 4 chiều T x C x W x H.\nNhược điểm của phương pháp này là nó yêu cầu tài nguyên tính toán lớn bởi vì nó làm việc với dữ liệu 4 chiều. Chi tiết hơn, các bạn có thể xem tại bài báo 3D Convolutional Neural Networks for Human Action Recognition.\nMột paper khác so sánh toàn bộ các phương pháp mình vừa nêu tên ở đây là Large-scale Video Classification with Convolutional Neural Networks. Bài báo này của tác giả Andrej Karpathy và đựơc trình bày tại hội nghị CVPR năm 2014.\n3. Kết luận Bài này mình đã giới thiệu qua cho các bạn một số phương pháp để giải quyết bài toán Video Classification. Bài tiếp theo, chúng ta sẽ thực hành phân loại một số hành động của con người trong video sử dụng một trong các phương pháp đó!\nMời các bạn đón đọc.!\n4. Tham khảo [1] Taha Anwar, “Introduction to Video Classification and Human Activity Recognition”, Available online: https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/ (Accessed on 27 Jul 2021).\n","permalink":"https://tiensu.github.io/blog/85_video_classification/","tags":["Deep Learning","Human Action Recognition"],"title":"Các phương pháp Video Classification"},{"categories":["Audio Classification","Speech Recognition","Speech-to-Text"],"contents":"Đây là bài cuối cùng trong chuỗi 6 bài về Audio Deep Learning. Trong bài này, chúng ta sẽ tổng hợp lại các kiến trúc mô hình DL để giải quyết bài toán Speech Synthesis.\nSpeech Synthesis là bài toán sinh ra Speech từ một văn bản cho trước. Một số ứng dụng của nó như: tổng đài trả lời tự động, đọc báo tự động, \u0026hellip;\nTrước khi các kỹ thuật DL phát triển, đã có một vài phương pháp truyền thống được đưa ra để giải quyết bài toán này. Nổi bật trong số đó là 2 phương pháp Concatenation Synthesis và Parametric Synthesis. Cũng giống như nhiều bài toán khác, các phương pháp này thường mang lại hiệu quả không cao, khó áp dụng được vào thực tế. Với sự bùng nổ của kỷ nguyên AI trong thời đại ngày nay, có khá nhiều kiến trúc mô hình DL được đề xuất cho bài toán này. Chúng ta sẽ lần lượt tìm hiểu về chúng ngay sau đây.\n1. Metric đánh giá DL model\nTrước khi đi vào tìm hiểu chi tiết từng kiến trúc mô hình DL, chúng ta nên biết qua về Metric dùng để đánh giá các mô hình này. Metric đó có tên là Mean Opinion Score (MOS). Nó xuất phát từ lĩnh vực viễn thông (Telecommunication), có dải giá trị từ 0 đến 5, tương ứng với chất lượng âm thanh tăng dần. Về bản chất, MOS là trung bình ý kiến đánh giá của nhiều người đối với âm thanh đó. Hãy nhớ lại, khi chúng ta thực hiện cuộc gọi audio/video qua ứng dụng Skype hoặc Facebook, sau khi kết thúc cuộc gọi luôn xuất hiện màn hình yêu cầu chúng ta đánh giá chất lượng cuộc gọi đó. Khi thu thập được đủ số lượng ý kiến đánh giá, nhà phát triển sẽ tính toán ra được MOS.\nDưới đây là bảng so sánh MOS của một số kiến trúc mô hình từ trang paperwithcode  7. Kết luận\nNhư vậy là chúng ta đã kết thúc bài thứ 5 tại đây. Qua bài này, chúng ta đã hiểu được phần nào rõ hơn về bài toán ASR, từ kiến trúc cho đến cách làm việc\nTrong bài tiếp theo, bài cuối cùng trong chuỗi bài về Audio Deep Learning, chúng ta sẽ tìm hiểu một số thuật toán, kiến trúc mô hình của bài toán tổng hợp tiếng nói - Speech Synthesis hay Text-to-Speech. Mời các bạn đón đọc.\n8. Tham khảo\n[1] Ketan Doshi, \u0026ldquo;Audio Deep Learning Made Simple: Automatic Speech Recognition (ASR), How it Works\u0026rdquo;, Available online: https://towardsdatascience.com/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706 (Accessed on 05 Jun 2021).\n[2] Scott Duda, \u0026ldquo;Urban Environmental Audio Classification Using Mel Spectrograms\u0026rdquo;, Available online: https://scottmduda.medium.com/urban-environmental-audio-classification-using-mel-spectrograms-706ee6f8dcc1 (Accessed on 05 Jun 2021).\n","permalink":"https://tiensu.github.io/blog/ddd_audio_deep_learning_part_6./","tags":["Audio Classification","Speech Recognition","Speech-to-Text"],"title":"Tìm hiểu bài toán Automatic Speech Recognition (ASR)"},{"categories":["Deep Learning","NLP","Text Generation"],"contents":"Viết hơi nhiều về chủ đề DP4ML rồi, mình tạm dừng 1 chút để chuyển qua làm 1 cái gì đấy cho đỡ chán.\n Chiều chiều nắng xế bên sông Có cô em gái ngóng trông đợi chờ Đợi chờ biết đến bao giờ? ...  Có thể bạn đã biết về AI và những ứng dụng của nó trong nhiều lĩnh vực của cuộc sống: Y tế, giáo dục, giao thông, nông nghiệp, công nghiệp, \u0026hellip; Nhưng liệu bạn có biết là AI còn tham gia vào cả các lĩnh vực mà đòi hỏi sự sáng tạo của con người như sáng tác thơ, nhạc, truyện, vẽ tranh, \u0026hellip; những việc tưởng chừng như không thể thay thế được của con người?\nTrong bài này, mình sẽ cùng các bạn xây dựng một model AI đơn giản để sáng tác thơ thử xem sao ha! :D\n1. Giới thiệu mô hình Seq2Seq trong các bài toán NLP Text Generation và Machine Translation là hai trong số các bài toán điển hình của NLP. Nếu như Text Generation có nhiệm vụ sinh ra các văn bản mới thì Machine Translation lại chịu trách nhiệm dịch văn bản từ một ngôn ngữ gốc sang các ngôn ngữ khác nhau. Ví dụ, từ tiếng Việt sang tiếng Nhật, từ tiếng Việt sang tiếng Anh, \u0026hellip; Cả 2 bài toán này đều có một đặc điểm chung là nhận dữ liệu đầu vào là một chuỗi các từ (input sequence) và sinh ra một chuỗi các từ khác (target sequence). Chính vì thế mà kiến trúc mô hình để giải quyết các dạng bài toán như thế này được gọi là Seq2Seq.\nMô hình Seq2Seq được giới thiệu lần đầu vào năm 2014 bởi Google. Mục đích của nó là ánh xạ một Input Sequence Data thành một Output Sequence Data. Chiều dài của 2 Sequence Data không nhất thiết phải giống nhau. Ví dụ khi dịch câu có 5 từ What are you doing now? từ tiếng Anh sang câu có 7 ký tự 今天你在做什麼？ trong tiếng Trung Quốc.\nNgoài hai bài toán kể trên thì mô hình Seq2Seq còn có thể giải quyết các bài toán sau:\n Text Summarization - Đây là bài toán tóm tắt nội dung của một văn bản dài thành một đoạn văn bản ngắn hơn. Kể từ khi được Google giới thiệu năm 2014, nó đã trở nên khá phổ biến. Machine Translation - Dịch văn bản giữa các ngôn ngữ khác nhau. Google Translate chính là một sản phẩm của bài toán này. Image/Video Captioning - Đưa cho máy tính một bức ảnh hoặc một video, nó sẽ trả lại cho bạn một (hoặc một vài) câu miêu tả nội dung của bức ảnh / Video đó. Mình đang nghĩ rằng phần thi đầu tiên của kỳ thi TOEIC (phần thi miêu tả tranh) có thể chính là một ứng dụng thực tế của bài toán này. Speech Recognition - Bài toán trong lĩnh vực Audio, còn được gọi là Speech To Text, tức chuyển đổi âm thanh thành văn bản. Music Generation - Đây là một bài toán rất thú vị, máy tính có thể sáng tác nhạc cho bạn. Nghe chắc sẽ rất ngầu! :D Recommendation Engine - Hệ thống khuyến nghị có lẽ đã không còn xa lạ với mọi người. Có rất nhiều các để tạo ra nó, và mô hình Seq2Seq với kiến trúc Encoder-Decoder cũng là một trong số đó, cho kết quả rất khả quan. Chatbot - Hay còn gọi là hệ thống Question-Answer. Siri hay Alexa là ví dụ thực tế.  Về kiến trúc, mô hình Seq2Seq bao gồm 2 thành phần: Encoder và Decoder. Mỗi một thành phần bao gồm nhiều NN Layers xếp chồng lên nhau (stack). NN Layer có thể là CNN, RNN, LSTM. GRU, …\n  Encoder làm nhiệm vụ mã hóa các Input Sequences thành các vectors trong miền không gian đa chiều (Latent Space). Mỗi vectors này được gọi là Internal State của Input Sequence tuơng ứng. Decoder làm nhiệm vụ chuyển đổi các Internal States sang các Target Sequences.  Chi tiết hơn về mô hình Seq2Seq, cách làm việc cũng như hạn chế và cách khác phục hạn chế của nó, mời các bạn tham khảo trong bài viết của mình tại đây. Còn ở bài này, mình chỉ tập trung vào phần thực hành thôi.\n2. Chuẩn bị dữ liệu Bất kỳ bài toán AI nào cũng vậy, dữ liệu là yếu tố quyết định lớn đến sự thành công. Để huấn luyện mô hình làm thơ, mình sẽ sử dụng các câu thơ trong tác phẩm Truyện Kiều của đại thi hào dân tộc Nguyễn Du.\n Copy toàn bộ phần nội dung của Truyện Kiều tại đây hoặc một nơi nào đó tùy bạn, vào file tên là truyenkieu.txt. Có 2 phiên bản của Truyện Kiều, bản Kinh có 3258 câu, bản Phường có 3254 câu. Mình không nhớ là copy ở đâu về nhưng của mình là bản Kinh. Bạn sử dụng bản nào cũng được vì số lượng chỉ hơn kém nhau 2 câu, không đáng kể.\n2.1 Đọc dữ liệu Trước tiên, chúng ta sẽ đọc vào dữ liệu vừa copy về để bắt đầu quá trình xử lý.\nwith open(\u0026#39;truyenkieu.txt\u0026#39;,\u0026#39;r\u0026#39;) as f: data = f.read() # separate data sentence by sentence and remove blank sentences data_list = [line for line in data.split(\u0026#39;\\n\u0026#39;) if line != \u0026#39;\u0026#39;] # display 10 first sentences data_list[:10] --- [\u0026#39;01.Trăm năm trong cõi người ta,\u0026#39;, \u0026#39;Chữ tài chữ mệnh khéo là ghét nhau.\u0026#39;, \u0026#39;Trải qua một cuộc bể dâu,\u0026#39;, \u0026#39;Những điều trông thấy mà đau đớn lòng.\u0026#39;, \u0026#39;Lạ gì bỉ sắc tư phong,\u0026#39;, \u0026#39;Trời xanh quen thói má hồng đánh ghen.\u0026#39;, \u0026#39;Cảo thơm lần giở trước đèn,\u0026#39;, \u0026#39;Phong tình có lục còn truyền sử xanh.\u0026#39;, \u0026#39;Rằng năm Gia Tĩnh triều Minh,\u0026#39;, \u0026#39;Bốn phương phẳng lặng, hai kinh vững vàng.\u0026#39;] 2.2 Làm sạch dữ liệu Có thể quan sát thấy dữ liệu còn đang rất bẩn, lộn xộn, chúng ta cần phải làm sạch trước khi đưa cho model để học. Một số bước tiền xử lý làm sạch dữ liệu như sau:\na, Chuyển chữ hoa thành chữ thường data_list = [x.lower() for x in data_list] data_list[:10] --- [\u0026#39;01.trăm năm trong cõi người ta,\u0026#39;, \u0026#39;chữ tài chữ mệnh khéo là ghét nhau.\u0026#39;, \u0026#39;trải qua một cuộc bể dâu,\u0026#39;, \u0026#39;những điều trông thấy mà đau đớn lòng.\u0026#39;, \u0026#39;lạ gì bỉ sắc tư phong,\u0026#39;, \u0026#39;trời xanh quen thói má hồng đánh ghen.\u0026#39;, \u0026#39;cảo thơm lần giở trước đèn,\u0026#39;, \u0026#39;phong tình có lục còn truyền sử xanh.\u0026#39;, \u0026#39;rằng năm gia tĩnh triều minh,\u0026#39;, \u0026#39;bốn phương phẳng lặng, hai kinh vững vàng.\u0026#39;] b, Loại bỏ dấu câu remove_digits = str.maketrans(\u0026#39;\u0026#39;, \u0026#39;\u0026#39;, string.digits) removed_digits_text = [] for sent in tk_text_: sentance = [w.translate(remove_digits) for w in sent.split(\u0026#39; \u0026#39;)] removed_digits_text.append(\u0026#39; \u0026#39;.join(sentance)) tk_text_ = removed_digits_text tk_text_[:10] --- [\u0026#39;01trăm năm trong cõi người ta\u0026#39;, \u0026#39;chữ tài chữ mệnh khéo là ghét nhau\u0026#39;, \u0026#39;trải qua một cuộc bể dâu\u0026#39;, \u0026#39;những điều trông thấy mà đau đớn lòng\u0026#39;, \u0026#39;lạ gì bỉ sắc tư phong\u0026#39;, \u0026#39;trời xanh quen thói má hồng đánh ghen\u0026#39;, \u0026#39;cảo thơm lần giở trước đèn\u0026#39;, \u0026#39;phong tình có lục còn truyền sử xanh\u0026#39;, \u0026#39;rằng năm gia tĩnh triều minh\u0026#39;, \u0026#39;bốn phương phẳng lặng hai kinh vững vàng\u0026#39;] c, Loại bỏ số remove_digits = str.maketrans(\u0026#39;\u0026#39;, \u0026#39;\u0026#39;, string.digits) removed_digits_text = [] for sent in tk_text_: sentance = [w.translate(remove_digits) for w in sent.split(\u0026#39; \u0026#39;)] removed_digits_text.append(\u0026#39; \u0026#39;.join(sentance)) tk_text_ = removed_digits_text tk_text_[:10] --- [\u0026#39;trăm năm trong cõi người ta\u0026#39;, \u0026#39;chữ tài chữ mệnh khéo là ghét nhau\u0026#39;, \u0026#39;trải qua một cuộc bể dâu\u0026#39;, \u0026#39;những điều trông thấy mà đau đớn lòng\u0026#39;, \u0026#39;lạ gì bỉ sắc tư phong\u0026#39;, \u0026#39;trời xanh quen thói má hồng đánh ghen\u0026#39;, \u0026#39;cảo thơm lần giở trước đèn\u0026#39;, \u0026#39;phong tình có lục còn truyền sử xanh\u0026#39;, \u0026#39;rằng năm gia tĩnh triều minh\u0026#39;, \u0026#39;bốn phương phẳng lặng hai kinh vững vàng\u0026#39;] d, Loại bỏ khoảng trắng đầu và cuối mỗi câu # removing the starting and ending whitespaces tk_text_ = [x.strip() for x in tk_text_] tk_text_[:10] --- [\u0026#39;trăm năm trong cõi người ta\u0026#39;, \u0026#39;chữ tài chữ mệnh khéo là ghét nhau\u0026#39;, \u0026#39;trải qua một cuộc bể dâu\u0026#39;, \u0026#39;những điều trông thấy mà đau đớn lòng\u0026#39;, \u0026#39;lạ gì bỉ sắc tư phong\u0026#39;, \u0026#39;trời xanh quen thói má hồng đánh ghen\u0026#39;, \u0026#39;cảo thơm lần giở trước đèn\u0026#39;, \u0026#39;phong tình có lục còn truyền sử xanh\u0026#39;, \u0026#39;rằng năm gia tĩnh triều minh\u0026#39;, \u0026#39;bốn phương phẳng lặng hai kinh vững vàng\u0026#39;] e, Kiểm tra hiện tượng 2 câu nằm cùng 1 dòng Lỗi này thường xuất hiện do lỗi copy Truyện Kiều từ trên website về file txt.\n# check to see if there is 2 sentences are same line for ins in tk_text_: if len(ins.split()) \u0026gt; 8: print(ins) Nếu thấy có dòng nào được in ra thì chúng ta tìm đến dòng đó để sửa trong file truyenkieu.txt.\n2.3 Chuẩn bị dữ liệu a, Chia dữ liệu thành Input/Target Sequences Chúng ta sẽ chia 3258 câu thơ thành 2 phần:\n Input Sentenses: là những câu có 6 từ. Target Sentences: là những câu có 8 từ.  # create pair of input/output sequence input_sentences, target_sentences = [], [] for index, seq_txt in enumerate(tk_text_): if index%2 == 0: input_sentences.append(seq_txt) else: target_sentences.append(seq_txt) print(len(input_sentences), len(target_sentences)) --- (1629, 1629) Ngoài ra, theo cách hoạt động của mô hình Seq2Seq, mỗi Target Sentence cần được thêm token báo hiệu bắt đầu và kết thúc.\n# add start and end token to target sequences target_sentences = [\u0026#39;start \u0026#39;+ ts + \u0026#39; end\u0026#39; for ts in target_sentences] b, Thực hiện Tokenize đối với Input/Target Sentences Các mô hình AI chỉ làm việc với dữ liệu ở dạng số, vì thế chúng ta phải chuyển các Input/Target Sentences từ dạng chuỗi văn bản sang dạng số tuơng ứng. Mình sẽ sử dụng lớp Tokenizer() của thư viện Keras để làm việc này.\n Input Sentences  Xây dựng bộ từ điển (vocabulary):\n# prepare the tokenizer on the input sentence input_tokenizer = Tokenizer() input_tokenizer.fit_on_texts(input_sentences) --- Input Vocabulary Size: 1866 Thực hiện Tokenize:\n# perform tokenize on input setences input_seq = input_tokenizer.texts_to_sequences(input_sentences) max_len_input_seq = max([len(seq) for seq in input_seq]) input_seq = pad_sequences(input_seq, maxlen=max_len_input_seq, padding=\u0026#39;pre\u0026#39;) input_seq[:10] --- array([[ 183, 102, 13, 665, 6, 64], [ 865, 125, 2, 538, 152, 453], [ 103, 274, 866, 167, 275, 104], [1183, 666, 126, 276, 41, 277], [ 1, 102, 73, 867, 454, 184], [ 7, 10, 868, 455, 246, 278], [ 2, 667, 57, 869, 1184, 11], [ 115, 11, 58, 539, 870, 456], [ 87, 540, 247, 541, 318, 248], [ 384, 153, 668, 385, 669, 319]], dtype=int32) Ngoài ra, hãy nhớ rằng bắt buộc các Input Sequences phải có độ dài bằng nhau. Vì vậy, chúng ta sẽ thêm các ký tự ‘0’ để tạo chuỗi có cùng độ dài. Điều này sẽ được thực hiện bởi pad_sequence.\n Target Sentences  Xây dựng từ điển:\n# prepare the tokenizer on the target sentence target_tokenizer = Tokenizer() target_tokenizer.fit_on_texts(target_sentences) --- Target Vocabulary Size: 1987 Thực hiện Tokenize:\n# perform tokenize on target setences target_seq = target_tokenizer.texts_to_sequences(target_sentences) max_len_target_seq = max([len(seq) for seq in target_seq]) target_seq = pad_sequences(target_seq, maxlen=max_len_target_seq, padding=\u0026#39;pre\u0026#39;) target_seq[:10] --- array([[ 1, 111, 137, 111, 258, 317, 9, 1304, 99, 2], [ 1, 61, 126, 85, 46, 16, 178, 773, 8, 2], [ 1, 48, 90, 402, 774, 354, 64, 355, 356, 2], [ 1, 155, 26, 12, 645, 17, 646, 1305, 90, 2], [ 1, 127, 481, 1306, 775, 49, 179, 647, 35, 2], [ 1, 280, 776, 100, 7, 259, 259, 1307, 777, 2], [ 1, 197, 128, 9, 111, 357, 558, 990, 280, 2], [ 1, 559, 281, 9, 282, 231, 9, 559, 232, 2], [ 1, 3, 5, 3, 318, 129, 358, 648, 129, 2], [ 1, 482, 86, 198, 1308, 403, 991, 649, 992, 2]], dtype=int32) c, Tạo Input/Output cho model Mô hình Seq2Seq yêu cầu 2 Inputs là Encoder Input, Decoder Input và một Output là Decoder Output.\n# create encoder/decoder input/output for i, (input_text, target_text) in enumerate(zip(input_sentences, target_sentences)): for t, word in enumerate(input_text.split()): encoder_input_data[i, t] = input_word2index[word] for t, word in enumerate(target_text.split()): # decoder_target_data is ahead of decoder_input_data by one timestep decoder_input_data[i, t] = target_word2index[word] if t \u0026gt; 0: # decoder_target_data will be ahead by one timestep # and will not include the start character. decoder_target_data[i, t - 1, target_word2index[word]] = 1. OK, như vậy là phần chuẩn bị dữ liệu đã xong. Chúng ta chuyển qua phần xây dựng mô hình AI.\n3. Xây dựng mô hình Seq2Seq 3.1 Định nghĩa kiến trúc mô hình Mô hình Seq2Seq thông thường có những hạn chế nhất định của nó, nên ở đây mình sẽ sử dụng thêm cơ chế Attention để tăng hiệu quả của mô hình.\nCác thư viện Keras, Tensorflow hay Pytorch đều chưa chính thức tích hợp Attention (mình tin điều này sẽ xảy ra sớm thôi). Vì vậy, chúng ta sẽ sử dụng lớp Attention được xây dựng tại đây. Download nó về và đặt trong cùng thư mực dự án.\n Định nghĩa Encoder  Encoder sẽ bao gồm 1 lớp Embedding và 3 lớp LSTM xếp chồng liên tiếp nhau. Lớp Embedding làm nhiệm vụ chuyển các Input Sequences thành các Embedded Vectors sử dụng thuật toán Word2Vec hoặc GloVe.\n# Encoder  encoder_inputs = Input(shape=(max_len_input_sentence,)) enc_emb = Embedding(input_vocab_size, latent_dim, trainable=True)(encoder_inputs) #LSTM 1  encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True) encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) #LSTM 2  encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True) encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) #LSTM 3  encoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True) encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) Chú ý rằng, tại các lớp LSTM cần đặt tham số return_sequences=True, return_state=True để nó trả về hidden_state và cell_state tại mỗi time_step.\n Định nghĩa Decoder  Decoder cũng gồm 1 lớp Embedding và 1 lớp LSTM.\n# Set up the decoder.  decoder_inputs = Input(shape=(None,)) dec_emb_layer = Embedding(target_vocab_size, latent_dim,trainable=True) dec_emb = dec_emb_layer(decoder_inputs) #LSTM using encoder_states as initial state decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) Tuơng tự Encoder, lớp LSTM của Decoder cũng phải thiết lập 2 tham số return_sequences=True, return_state=True. Ngoài ra, trạng thái khởi tạo của Decoder được gán bằng giá trị cell_state và hidden_state của ở đầu ra của Encoder.\n Định nghĩa Attention layer  Lớp Attention được khởi tạo với 2 tham số là encoder_outputs của Encoder và decoder_outputs của Decoder.\n#Attention Layer attn_layer = AttentionLayer(name=\u0026#39;attention_layer\u0026#39;) attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])  Kết hợp Output của Decoder và Attention  Output của Decoder và Attention được kết hợp lại thành một Output duy nhất:\n# Concat attention output and decoder LSTM output  decoder_concat_input = Concatenate(axis=-1, name=\u0026#39;concat_layer\u0026#39;)([decoder_outputs, attn_out])  Định nghĩa TimeDistributed  Đây là lớp chịu trách nhiệm sinh ra kết quả cuối cùng của mô hình. Định nghĩa nó bằng cách sử dụng lớp TimeDistributed của Keras và truyền tham số là decoder_concat_input. Hàm softmax trả về xác suất của mỗi từ trong vocabulary. Từ nào có xác suất lớn nhất sẽ được chọn.\n# Dense layer decoder_dense = TimeDistributed(Dense(target_vocab_size, activation=\u0026#39;softmax\u0026#39;)) decoder_outputs = decoder_dense(decoder_concat_input)  Mô hình đầy đủ  Kết hợp đầy đủ Input/Output của Encoder/Decoder với lớp Model của Keras, ta có được mô hình đầy đủ như sau:\n# Define the model model = Model([encoder_inputs, decoder_inputs], decoder_outputs) model.summary()   Để có cái nhìn trực quan hơn về kiến trúc thành phần và sự liên kết giữa các lớp, chúng ta sẽ thể hiện nó dưới dạng hình ảnh như dưới đây.\n 3.2 Huấn luyện mô hình Chúng ta sẽ huấn luyện mô hình theo các tham số sau:\n x = [encoder_input_data, decoder_input_data] y = decoder_target_data, epoch: 500 batch_size: 64  history = model.fit( x=[encoder_input_data, decoder_input_data], y=decoder_target_data, batch_size=64, epochs=500) Kết quả huấn luyện:\n Độ chính xác đạt được là ~88.6%, loss = ~0.1482%.\n3.3 Lưu mô hình Hãy tiến hành lưu lại mô hình để\nsử dụng trong tuơng lai.\nmodel_json = model.to_json() with open(\u0026#34;output/PoemGen.json\u0026#34;, \u0026#34;w\u0026#34;) as json_file: json_file.write(model_json) # serialize weights to HDF5 model.save_weights(\u0026#34;output/PoemGen_model_weight.h5\u0026#34;) print(\u0026#34;Saved model to disk\u0026#34;) 3.4 Kiểm tra mô hình Chúng ta sẽ thử sử dụng mô hình vừa huấn luyện để sinh ra thử một vài câu thơ xem sao. Code thực hiện như sau:\na, Load mô hình Trước tiên, load mô hình đã được lưu trước đó:\n# loading the model architecture and asigning the weights json_file = open(\u0026#39;output/PoemGen_2.json\u0026#39;, \u0026#39;r\u0026#39;) loaded_model_json = json_file.read() json_file.close() model_loaded = model_from_json(loaded_model_json, custom_objects={\u0026#39;AttentionLayer\u0026#39;: AttentionLayer}) # load weights into new model model_loaded.load_weights(\u0026#34;output/PoemGen_model_weight_2.h5\u0026#34;) b, Thực hiện Inference # encoder inference encoder_inputs = model_loaded.input[0] #loading encoder_inputs encoder_outputs, state_h, state_c = model_loaded.layers[6].output #loading encoder_outputs # print(encoder_inputs.shape) encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c]) # decoder inference # Below tensors will hold the states of the previous time step decoder_state_input_h = Input(shape=(latent_dim,)) decoder_state_input_c = Input(shape=(latent_dim,)) decoder_hidden_state_input = Input(shape=(max_len_input_seq,latent_dim)) # Get the embeddings of the decoder sequence decoder_inputs = model_loaded.layers[3].output #print(decoder_inputs.shape) dec_emb_layer = model_loaded.layers[5] dec_emb2= dec_emb_layer(decoder_inputs) # To predict the next word in the sequence, set the initial states to the states from the previous time step decoder_lstm = model_loaded.layers[7] decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c]) #attention inference attn_layer = model_loaded.layers[8] attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2]) concate = model_loaded.layers[9] decoder_inf_concat = concate([decoder_outputs2, attn_out_inf]) # A dense softmax layer to generate prob dist. over the target vocabulary decoder_dense = model_loaded.layers[10] decoder_outputs2 = decoder_dense(decoder_inf_concat) # Final decoder model decoder_model = Model( [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c], [decoder_outputs2] + [state_h2, state_c2]) def decode_sequence(input_seq): # Encode the input as state vectors. e_out, e_h, e_c = encoder_model.predict(input_seq) # Generate empty target sequence of length 1. target_seq = np.zeros((1,1)) # Chose the \u0026#39;start\u0026#39; word as the first word of the target sequence target_seq[0, 0] = target_word2index[\u0026#39;start\u0026#39;] stop_condition = False decoded_sentence = \u0026#39;\u0026#39; while not stop_condition: output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c]) # Sample a token sampled_token_index = np.argmax(output_tokens[0, -1, :]) if sampled_token_index == 0: break else: sampled_token = target_index2word[sampled_token_index] if(sampled_token !=\u0026#39;end\u0026#39;): decoded_sentence += \u0026#39; \u0026#39;+ sampled_token # Exit condition: either hit max length or find stop word. if (sampled_token == \u0026#39;end\u0026#39; or len(decoded_sentence.split()) \u0026gt;= (max_len_target_seq)): stop_condition = True # Update the ta`rget sequence (of length 1). target_seq = np.zeros((1,1)) target_seq[0, 0] = sampled_token_index # Update internal states e_h, e_c = h, c return decoded_sentence Thực hiện Inference với 1 vài câu thơ trong tập dữ liệu ban đầu:\nfor seq_index in [141, 2001, 3002]: input_seq = encoder_input_data[seq_index: seq_index + 1] decoded_sentence = decode_sequence(input_seq) print(\u0026#39;-\u0026#39;) print(\u0026#39;Input sentence:\u0026#39;, input_sentences[seq_index: seq_index + 1]) print(\u0026#39;Decoded sentence:\u0026#39;, decoded_sentence) --- - Input sentence: [\u0026#39;song hồ nửa khép cánh mây\u0026#39;] Decoded sentence: tường đông ghé mắt ngày ngày hằng trông - Input sentence: [\u0026#39;khen “tài nhả ngọc phun châu\u0026#39;] Decoded sentence: nàng ban ả tạ cũng đâu thế này - Input sentence: [\u0026#39;duyên hội ngộ đức cù lao\u0026#39;] Decoded sentence: bâng khuâng nào đã biết ai mà nhìn Ta thấy câu thơ sinh ra đúng như trong bộ dữ liệu ban đầu.\nHàm sinh ra câu thơ mới dựa trên một Input Sentence người dùng nhập vào.\ndef make_a_poem_sentence(input_txt): input_seq = [] for t, word in enumerate(input_txt.split()): input_seq.append(input_word2index[word]) input_seq = np.array(input_seq) input_seq = pad_sequences([input_seq], maxlen=max_len_input_seq, padding=\u0026#39;pre\u0026#39;) decoded_txt = decode_sequence(input_seq) print(\u0026#39;Generated poem sentence:\u0026#39;, decoded_txt) Kiểm tra thử kết quả:\ninput_txt = \u0026#39;hạ về xanh biếc trên sông\u0026#39; make_a_poem_sentence(input_txt) --- Generated poem sentence: một thanh còn để mấy mùa chia trăng Nghe vẻ cũng \u0026ldquo;xuôi xuôi\u0026rdquo; nhỉ! :D\n4. Hướng phát triển Mặc dù mô hình đạt đuợc độ chính xác khá cao nhưng vẫn còn nhiều hạn chế. Một số phuơng hướng để nâng cao chất lượng của mô hình như sau:\n Thu thập thêm nhiều dữ liệu hơn nữa. Càng nhiều càng tốt. Sử dụng kiến trúc tiên tiến hơn, Transformer chẳng hạn. Sử dụng Dropout và một số phuơng pháp Regularization để giảm Overfitting. Thực hiện Hyper-parameter Tuning: learning rate, batch size, \u0026hellip; Sử dụng Bidirectional LSTM thay vì LSTM, sử dụng thêm nhiều lớp LSTM, \u0026hellip; Sử dụng Beam Search thay vì Greedy Search.  5. Kết luận Vậy là chúng ta vừa cùng nhau hoàn thành xây dựng một AI model để giúp chúng ta có thể tạo ra được nhưng câu thơ hay, thú vị. Hi vọng bạn có thể học được một chút gì đó từ bài viết này của mình.\nMô hình vẫn còn nhiều hạn chế cần cải thiện để có thể sử dụng được trong thực tế. Nếu bạn có hứng thú, có thể liên hệ với mình để cùng nhau tiếp tục phát triển thêm.\nVà cuối cùng, mình bật mí rằng bài thơ ở phần đầu bài viết này là sản phẩm của sự kết hợp giữa AI và con người, chứ không phải hoàn toàn bằng AI đâu nhé! Để AI có thể sáng tác được một bài thơ \u0026ldquo;mượt mà\u0026rdquo; như thế chắc sẽ cần phải làm thêm nhiều việc nữa. :D\nToàn bộ source code các bạn tham khảo tại đây\nCảm ơn các bạn đã đọc bài!\n6. Tham khảo [1] SuNT, “Tìm hiểu cơ chế Attention trong mô hình Seq2Seq”, Available online: https://tiensu.github.io/blog/58_attention/ (Accessed on 23 Jul 2021).\n[2] Thushan Ganegedara, “Attention in Deep Networks with Keras”, Available online: https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39 (Accessed on 23 Jul 2021).\n[3] Harshil Patel, “Neural Machine Translation (NMT) with Attention Mechanism”, Available online: https://towardsdatascience.com/neural-machine-translation-nmt-with-attention-mechanism-5e59b57bd2ac (Accessed on 23 Jul 2021).\n","permalink":"https://tiensu.github.io/blog/84_make_poem_with_ai/","tags":["Deep Learning","NLP","Text Generation"],"title":"AI for Fun: Làm thơ cùng AI"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 9 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 2 về Feature Selection. Trong bài này, chúng ta sẽ tìm hiểu phương pháp lựa chọn features đối với dữ liệu có Input Variance thuộc kiểu Categorical thông qua việc thực hành trên một bộ dữ liệu cụ thể.\n1. Breast Cancer Categorical Dataset Breast Cancer Categorical Dataset là bộ dữ liệu thường được dùng cho việc nghiên cứu các thuật toán ML từ thập niên 80s. Nó bao gồm 286 mẫu dữ liệu đại diện cho các bệnh nhân, và 9 Input Variables. Thông tin về các Input Variables được miêu tả ở đây. Đây là bài toán Binary Classification, mỗi bệnh nhân thuộc 1 trong 2 nhãn là có bệnh hoặc không có bệnh.\nCode dưới đây đọc vào dữ liệu, in ra một vài samples, chia dữ liệu thành Input/Output Variables, sau đó lại chia thành 2 tập train/test.\n# load the dataset def load_dataset(filename): # load the dataset data = read_csv(filename, header=None) # display some samples print(data.head()) # retrieve array dataset = data.values # split into input and output variables X = dataset[:, :-1] y = dataset[:,-1] # format all fields as string X = X.astype(str) return X, y # load the dataset X, y = load_dataset( \u0026#39; breast-cancer.csv \u0026#39; ) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # summarize print( \u0026#39; Train \u0026#39; , X_train.shape, y_train.shape print( \u0026#39; Test \u0026#39; , X_test.shape, y_test.shape) Kết quả thực hiện:\n0 1 2 3 4 5 6 7 8 9 0 \u0026#39;40-49\u0026#39; \u0026#39;premeno\u0026#39; \u0026#39;15-19\u0026#39; \u0026#39;0-2\u0026#39; \u0026#39;yes\u0026#39; \u0026#39;3\u0026#39; \u0026#39;right\u0026#39; \u0026#39;left_up\u0026#39; \u0026#39;no\u0026#39; \u0026#39;recurrence-events\u0026#39; 1 \u0026#39;50-59\u0026#39; \u0026#39;ge40\u0026#39; \u0026#39;15-19\u0026#39; \u0026#39;0-2\u0026#39; \u0026#39;no\u0026#39; \u0026#39;1\u0026#39; \u0026#39;right\u0026#39; \u0026#39;central\u0026#39; \u0026#39;no\u0026#39; \u0026#39;no-recurrence-events\u0026#39; 2 \u0026#39;50-59\u0026#39; \u0026#39;ge40\u0026#39; \u0026#39;35-39\u0026#39; \u0026#39;0-2\u0026#39; \u0026#39;no\u0026#39; \u0026#39;2\u0026#39; \u0026#39;left\u0026#39; \u0026#39;left_low\u0026#39; \u0026#39;no\u0026#39; \u0026#39;recurrence-events\u0026#39; 3 \u0026#39;40-49\u0026#39; \u0026#39;premeno\u0026#39; \u0026#39;35-39\u0026#39; \u0026#39;0-2\u0026#39; \u0026#39;yes\u0026#39; \u0026#39;3\u0026#39; \u0026#39;right\u0026#39; \u0026#39;left_low\u0026#39; \u0026#39;yes\u0026#39; \u0026#39;no-recurrence-events\u0026#39; 4 \u0026#39;40-49\u0026#39; \u0026#39;premeno\u0026#39; \u0026#39;30-34\u0026#39; \u0026#39;3-5\u0026#39; \u0026#39;yes\u0026#39; \u0026#39;2\u0026#39; \u0026#39;left\u0026#39; \u0026#39;right_up\u0026#39; \u0026#39;no\u0026#39; \u0026#39;recurrence-events\u0026#39; Train (191, 9) (191,) Test (95, 9) (95,) 2. Transform Data Có thể nhận ra rằng, tất cả Input Variables đều có dạng Categorical. Vì thế chúng ta cần chuyển chúng sang dạng Integer để model có thể học được. Lớp OridinalEncoder trong Scikit-learn giúp chúng ta thực hiện việc này.\n# prepare input data def prepare_inputs(X_train, X_test): oe = OrdinalEncoder() oe.fit(X_train) X_train_enc = oe.transform(X_train) X_test_enc = oe.transform(X_test) return X_train_enc, X_test_enc Output Variable cũng phải chuyển sang Integer. Vì đây là bài toán Binary Classification nên ánh xạ hai nhãn thành 0 và 1. Sử dụng lớp LabelEncoder của Scikit-learn để thực hiện.\n# prepare target def prepare_targets(y_train, y_test): le = LabelEncoder() le.fit(y_train) y_train_enc = le.transform(y_train) y_test_enc = le.transform(y_test) return y_train_enc, y_test_enc Code đầy đủ như sau:\n# example of loading and preparing the breast cancer dataset from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder # load the dataset def load_dataset(filename): # load the dataset data = read_csv(filename, header=None) # retrieve array dataset = data.values # split into input and output variables X = dataset[:, :-1] y = dataset[:,-1] # format all fields as string X = X.astype(str) return X, y # prepare input data def prepare_inputs(X_train, X_test): oe = OrdinalEncoder() oe.fit(X_train) X_train_enc = oe.transform(X_train) X_test_enc = oe.transform(X_test) return X_train_enc, X_test_enc # prepare target def prepare_targets(y_train, y_test): le = LabelEncoder() le.fit(y_train) y_train_enc = le.transform(y_train) y_test_enc = le.transform(y_test) return y_train_enc, y_test_enc # load the dataset X, y = load_dataset(\u0026#39;breast-cancer.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # prepare input data X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) # prepare output data y_train_enc, y_test_enc = prepare_targets(y_train, y_test) # summarize print(\u0026#39;Train\u0026#39;, X_train_enc.shape, y_train_enc.shape) print(\u0026#39;Test\u0026#39;, X_test_enc.shape, y_test_enc.shape) 3. Categorical Feature Selection Có 2 kỹ thuật có thể áp dụng cho Input Variables dạng Categorical trong việc Feature Selection là Chi-Squared Statistic và Mutual Information Statistic.\n3.1 Chi-Squared Statistic Chi-Squared Statistic - Kiểm định giả thuyết thống kê chi bình phương ($\\chi^2$) là một ví dụ về kiểm định tính độc lập giữa các biến phân loại. Kết quả của thử nghiệm này có thể được sử dụng để lựa chọn các Input Variables. Nếu Input Variables độc lập với Output Variables thì chúng có thể bị xóa khỏi tập dữ liệu.\nScikit-learn thực hiện Chi-Squared Statistic thông qua hàm chi2(). Kết hợp với hàm SelectKBest(), ta có thể lựa chọn được các features mong muốn như code dưới đây:\n... fs = SelectKBest(score_func=chi2, k=\u0026#39;all\u0026#39;) fs.fit(X_train, y_train) X_train_fs = fs.transform(X_train) X_test_fs = fs.transform(X_test) Muốn biêt và so sánh Score của mỗi feature, chúng ta có thể thể hiện chúng lên đồ thị. Score càng lớn càng tốt.\n... # what are scores for the features for i in range(len(fs.scores_)): print(\u0026#39;Feature %d: %f\u0026#39; % (i, fs.scores_[i])) # plot the scores pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) pyplot.show() Code đầy đủ:\n# example of chi squared feature selection for categorical data from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 from matplotlib import pyplot # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] # format all fields as string X = X.astype(str) return X, y # prepare input data def prepare_inputs(X_train, X_test): oe = OrdinalEncoder() oe.fit(X_train) X_train_enc = oe.transform(X_train) X_test_enc = oe.transform(X_test) return X_train_enc, X_test_enc # prepare target def prepare_targets(y_train, y_test): le = LabelEncoder() le.fit(y_train) y_train_enc = le.transform(y_train) y_test_enc = le.transform(y_test) return y_train_enc, y_test_enc # feature selection def select_features(X_train, y_train, X_test): fs = SelectKBest(score_func=chi2, k=\u0026#39;all\u0026#39;) fs.fit(X_train, y_train) X_train_fs = fs.transform(X_train) X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs # load the dataset X, y = load_dataset(\u0026#39;breast-cancer.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # prepare input data X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) # prepare output data y_train_enc, y_test_enc = prepare_targets(y_train, y_test) # feature selection X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc) # what are scores for the features for i in range(len(fs.scores_)): print(\u0026#39;Feature %d: %f\u0026#39; % (i, fs.scores_[i])) # plot the scores pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_) pyplot.show() Kết quả thực hiện:\nFeature 0: 0.472553 Feature 1: 0.029193 Feature 2: 2.137658 Feature 3: 29.381059 Feature 4: 8.222601 Feature 5: 8.100183 Feature 6: 1.273822 Feature 7: 0.950682 Feature 8: 3.699989   Theo kết quả này thì có lẽ 4/9 feature liên quan nhất đến Output Variables là các features 3, 4, 5, 8. Do đó, ta sẽ chọn k=4 để lấy ra 4 features này.\n3.2 Mutual Information Feature Selection Mutual Information xuất phát từ lĩnh vực Lý thuyết Thông tin. Nó đo đặc mức độ suy giảm của một biến khi đã biết giá trị của một biến khác. Trong Scikit-learn, Mutual Information được implement trong hàm mutual info classif(). Cách sử dụng cũng tương tự như Chi-Squared Statistic.\n# feature selection def select_features(X_train, y_train, X_test): fs = SelectKBest(score_func=mutual_info_classif, k=\u0026#39;all\u0026#39;) fs.fit(X_train, y_train) X_train_fs = fs.transform(X_train) X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs, fs Kết quả thực hiện:\nFeature 0: 0.000000 Feature 1: 0.045906 Feature 2: 0.063114 Feature 3: 0.018165 Feature 4: 0.007357 Feature 5: 0.041046 Feature 6: 0.049755 Feature 7: 0.000000 Feature 8: 0.024087   Theo Mutual Information thì các features 2, 3, 5, 6 có mức độ liên quan đến Output Variables nhất. Chọn k=4.\n4. Modeling With Selected Features Sử dụng 2 kỹ thuật bên trên, chúng ta sẽ đi đánh giá độ chính xác của Logistic Regression model trên tập dữ liệu Breast Cancer Categorical Dataset.\n4.1 Model Built using All Features Đầu tiên, chúng ta sẽ xây dựng Base model làm cơ sở để so sánh, đánh giá. Các model về sau được mong đợi phải có độ chính xác cao hơn độ chính xác của Base model này.\n# evaluation of a model using all input features from pandas import read_csv from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] # format all fields as string X = X.astype(str) return X, y # prepare input data def prepare_inputs(X_train, X_test): oe = OrdinalEncoder() oe.fit(X_train) X_train_enc = oe.transform(X_train) X_test_enc = oe.transform(X_test) return X_train_enc, X_test_enc # prepare target def prepare_targets(y_train, y_test): le = LabelEncoder() le.fit(y_train) y_train_enc = le.transform(y_train) y_test_enc = le.transform(y_test) return y_train_enc, y_test_enc # load the dataset X, y = load_dataset(\u0026#39;breast-cancer.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # prepare input data X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) # prepare output data y_train_enc, y_test_enc = prepare_targets(y_train, y_test) # fit the model model = LogisticRegression(solver=\u0026#39;lbfgs\u0026#39;) model.fit(X_train_enc, y_train_enc) # evaluate the model yhat = model.predict(X_test_enc) # evaluate predictions accuracy = accuracy_score(y_test_enc, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả:\nAccuracy: 75.79 4.2 Model Built Using Chi-Squared Features # evaluation of a model fit using chi squared input features from pandas import read_csv from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] # format all fields as string X = X.astype(str) return X, y # prepare input data def prepare_inputs(X_train, X_test): oe = OrdinalEncoder() oe.fit(X_train) X_train_enc = oe.transform(X_train) X_test_enc = oe.transform(X_test) return X_train_enc, X_test_enc # prepare target def prepare_targets(y_train, y_test): le = LabelEncoder() le.fit(y_train) y_train_enc = le.transform(y_train) y_test_enc = le.transform(y_test) return y_train_enc, y_test_enc # feature selection def select_features(X_train, y_train, X_test): fs = SelectKBest(score_func=chi2, k=4) fs.fit(X_train, y_train) X_train_fs = fs.transform(X_train) X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs # load the dataset X, y = load_dataset(\u0026#39;breast-cancer.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # prepare input data X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) # prepare output data y_train_enc, y_test_enc = prepare_targets(y_train, y_test) # feature selection X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc) # fit the model model = LogisticRegression(solver=\u0026#39;lbfgs\u0026#39;) model.fit(X_train_fs, y_train_enc) # evaluate the model yhat = model.predict(X_test_fs) # evaluate predictions accuracy = accuracy_score(y_test_enc, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả:\nAccuracy: 74.74 4.3 Model Built Using Mutual Information Features # evaluation of a model fit using mutual information input features from pandas import read_csv from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import mutual_info_classif from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # load the dataset def load_dataset(filename): # load the dataset as a pandas DataFrame data = read_csv(filename, header=None) # retrieve numpy array dataset = data.values # split into input (X) and output (y) variables X = dataset[:, :-1] y = dataset[:,-1] # format all fields as string X = X.astype(str) return X, y # prepare input data def prepare_inputs(X_train, X_test): oe = OrdinalEncoder() oe.fit(X_train) X_train_enc = oe.transform(X_train) X_test_enc = oe.transform(X_test) return X_train_enc, X_test_enc # prepare target def prepare_targets(y_train, y_test): le = LabelEncoder() le.fit(y_train) y_train_enc = le.transform(y_train) y_test_enc = le.transform(y_test) return y_train_enc, y_test_enc # feature selection def select_features(X_train, y_train, X_test): fs = SelectKBest(score_func=mutual_info_classif, k=4) fs.fit(X_train, y_train) X_train_fs = fs.transform(X_train) X_test_fs = fs.transform(X_test) return X_train_fs, X_test_fs # load the dataset X, y = load_dataset(\u0026#39;breast-cancer.csv\u0026#39;) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # prepare input data X_train_enc, X_test_enc = prepare_inputs(X_train, X_test) # prepare output data y_train_enc, y_test_enc = prepare_targets(y_train, y_test) # feature selection X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc) # fit the model model = LogisticRegression(solver=\u0026#39;lbfgs\u0026#39;) model.fit(X_train_fs, y_train_enc) # evaluate the model yhat = model.predict(X_test_fs) # evaluate predictions accuracy = accuracy_score(y_test_enc, yhat) print(\u0026#39;Accuracy: %.2f\u0026#39; % (accuracy*100)) Kết quả:\nAccuracy: 76.84 Theo các kết quả thu được từ 3 lần thí nghiệm thì có thể thấy rằng Mutual Information mang lại kết quả tốt nhất. Để kết quả được tin cậy hơn, bạn có thể sử dụng k-Fold Cross-Validation thay vì chỉ chia Train/Test thông thường.\n5. Kết luận Bài thứ 2 về chủ đề Feature Selection, mình đã giới thiệu 2 kỹ thuật Chi-Squared Statistic và Mutual Information áp dụng cho Input Variables dạng Categorical.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nBài tiếp theo sẽ là các kỹ thuật Feature Selection áp dụng cho Input Variables dạng Numerical. Mời các bạn đón đọc.\n6. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/83_data_prepeation_for_ml_feature_selection_2/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Feature Selection - Phần 2 - Select Categorical Input Features"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 8 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài đầu tiên về Feature Selection. Trong bài này, chúng ta sẽ chuyển tiếp sang tìm hiểu một nhiệm vụ khác của Data Preparation, đó là Feature Selection.\n1. Feature Selection là gì? Feature Selection là quá trình giảm số lượng features khi phát triển các ML model. Việc làm này xuất phát từ 2 nguyên nhân chính:\n Giảm thời gian tính toán, huấn luyện mô hình. Tăng hiệu năng của các mô hình.  Feature Selection, đôi khi hay bị hiểu nhầm thành Dimensionality Reduction. Thực ra, chúng đều là các công việc phải thực hiện trong quá trình chuẩn bị dữ liệu cho các ML model. Chúng vừa có sự giống nhau, vừa có sự khác nhau:\n Giống: Về bản chất, 2 cái đều giảm số lượng chiều của dữ liệu, từ cao xuống thấp. Khác: Feature Selection giảm chiều dữ liệu bằng cách loại bớt một số features không liên quan đi, chỉ giữ lại các features liên quan nhất đến mục đích cần dự đoán. Trong khi đó, Dimensionality Reduction giảm chiều dữ liệu bằng cách ánh xạ tất cả các features hiện có sang một miền không gian có số chiều nhỏ hơn miên không gian ban đầu. Đối với Feature Selection, các features còn lại đều là các features cũ, nằm trong số các features ban đầu. Còn đối với Dimentionality Reduction, các features còn lại đều là các features mới hoàn toàn.  2. Các phương pháp thực hiện Feature Selection Có 2 phương pháp chính để thực hiện Feature Selection là: Supervised và Unsupervised.\n2.1 Supervised Đây là phương pháp Feature Selection mà có sử dụng đến Target Variable (nhãn). Nó lại được chia thành 3 loại nhỏ hơn:\na, Intrinsic Là các thuật toán thực hiện Feature Selection trong quá trình huấn luyện mô hình.\nb, Filter Loại này sẽ lựa chọn một tập nhỏ các features dựa trên mối quan hệ của các features đó với Target Variable. Để xác định mối quan hệ này, chúng ta có thể sử dụng một số phương pháp thống kê khác nhau (sẽ được đề cập chi tiết bên dưới). Nói chung, đây là cách thức được sử dụng rất rộng rãi để thực hiện Feature Selection.\nc, Wrapper Wraper tạo ra nhiều tập con của các features, mỗi tập con đó được sử dụng để huấn luyện một mô hình riêng. Kết quả cuối cùng là tổng hợp của các models đó.\n2.2 Unsupervised Đây là phương pháp Feature Selection mà không sử dụng đến Target Variable.\n3. Các phương pháp thống kê sử dụng cho Feature Selection Người ta thường sử dụng các phép đo thống kê kiểu tương quan giữa các biến đầu vào và đầu ra để làm cơ sở cho việc lựa chọn các features. Mỗi một kiểu dữ liệu sẽ có các phép đo tương ứng khác nhau. Có 2 kiểu dữ liệu thường gặp là numerical (integer, float, \u0026hellip;) và categorical (boolean, ordinal, nominal, \u0026hellip;).\nChúng ta định nghĩa 2 khái niệm sau để thuận tiện sử dụng trong các bài viết thuộc chuỗi chủ đề Data Preparation này:\n Input Variable: Là tất cả các features có thể được sử dụng để huấn luyện mô hình. Có thể gọi bằng những cái tên Input Variables, Features, Input Features. Ouput Variable: Là kết quả được dự đoán bởi mô hình. Nó còn có thể được gọi bằng cái tên khác là Target Variable, Target.  Sơ đồ cây dưới đây khái quát các phương pháp thống kê tương ứng với các kiểu của Input/Output Variable.\n Chúng ta sẽ lần lượt tìm hiểu những thuật toán này trong các bài viết tiếp theo.\nNãy giờ chúng ta mới chỉ đề cập đến phương pháp thống kê cho dữ liệu thuộc một trong hai kiểu Numerical hoặc Categorical. Nhưng trong thực tế, bộ dữ liệu của chúng ta có thể bao gồm cả 2 loại đó. Khi gặp tình huống như vậy, chúng ta phải sử dụng thuật toán ColumnTransformer. Chúng ta sẽ tìm hiểu thuật toán này trong các bài về sau trong cùng chủ đề Data Preparation này.\n4. Một số câu hỏi thường gặp 4.1 Làm cách nào để chọn ra được những Input Variables cần thiết? Trả lời:\n Sắp xếp các Input Variables theo thứ tự giảm dần của Score hoặc Percentage (cụ thể Score và Percentage là gì thì phụ thuộc vào thuật toán sử dụng). Lựa chọn các Input Variables mong muốn sử dụng một trong hai classes của thư viện Scikit-learn: SelectKbest (sử dụng cho Score), SelectPercentile (sử dụng cho Percentage).  4.2 Làm thế nào biết được phương pháp Feature Selection nào là tốt nhất? Trả lời: Không thể biết trước. Bạn phải dựa vào kinh nghiệm, sự hiểu biết về bài toán, cũng như ưu/nhược điểm của các thuật toán. Nếu vẫn không thể chọn được thì sử dụng phương án thử-sai, tức chọn một số phương pháp khả dĩ, rồi so sánh kết quả của chúng với nhau.\n5. Kết luận OK, bài đầu tiên về Feature Selection chỉ ngắn như vậy thôi. Mình đã giới thiệu khái quát qua về Feature Selection để cho các bạn có cái nhìn tổng thể về nó. Trong các bài tiếp theo, mình sẽ đi chi tiết vào từng phương pháp. Mời các bạn đón đọc.\n6. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/82_data_prepeation_for_ml_feature_selection_1/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Feature Selection - Phần 1 - Giới thiệu chung"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 7 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 4 về chủ đề Missing Data. Trong bài này, chúng ta sẽ tìm hiểu phương pháp tiếp theo để giải quyết vấn đề Missing Data, đó là phương pháp Iterative Imputation.\n1. Iterative Imputation Iterative Imputation là quá trình trong đó mỗi feature được mô hình hóa như một hàm của các features khác. Mỗi feature được xác định một cách tuần tự, lần lượt, cho phép các features đã xác định trước đó được sử dụng như làm một phần đầu vào của mô hình trong việc dự đoán các features tiếp theo. Quá trình này được lặp lại nhiều lần, cho phép các ước tính luôn được cải thiện. Cách tiếp cận này được gọi chung là Fully Conditional Specification (FCS) hoặc Multivariate Imputation by Chained Equations (MICE).\nVề mặt lý thuyết, các thuật toán Regressions đều có thể sử dụng để ước lượng các Missing Data. Nhưng thực tế thì Linear Regression được sử dụng phổ biến vì tính đơn giản của nó. Số lượng vòng lặp cũng không cần quá lớn, thường là 10. Cuối cùng, thứ tự các features được ước lượng tuần tự cũng cần được xem xét. Có thể là từ các features có ít đến các features có nhiều Missing Data, hoặc là ngược lại.\n2. Thực hành Iterative Imputation Chúng ta sẽ thực hành phương pháp Iterative Imputation trên bộ dữ liệu Horse Colic Dataset giống như bài trước.\nThư viện Scikit-learn cung cấp lớp IterativeImputer giúp chúng ta dễ dàng thực hiện Iterative Imputation.\n2.1 IterativeImputer và Data Transform Cũng giống như SimpleImputer và kNNImputer, khi sử dụng IterativeImputer, nó sẽ tạo ra một phiên bản khác của tập dữ liệu ban đầu mà ở đó, các Missing Data đã được thay thế bởi các giá trị sinh ra từ thuật toán ước lượng (Data Transform). Các bước thực hiện Data Transform sử dụng IterativeImputer như sau:\na, Khai báo một Instance của IterativeImputer Khi khởi tạo Instance cho IterativeImputer cần quan tâm đến 3 tham số truyền vào:\n  Thuật toán ước lượng (estimator): mặc định là \u0026lsquo;BayesianRidge\u0026rsquo;.\n  Thứ tự thực hiện ước lượng (imputation_order): mặc định là \u0026lsquo;ascending\u0026rsquo;, tức là từ các features có ít đến các features có nhiều Missing Data. Ngoài ra còn có các giá trị khác như: descending - từ các features có nhiều đến các features có ít Missing Data, roman - từ trái sang phải, arabic - từ phải sang trái, random - ngẫu nhiên.\n  Số lượng vòng lặp (max_iter): mặc định là 10.\n  ... # define imputer imputer = IterativeImputer(estimator=BayesianRidge(), imputation_order= \u0026#39;ascending\u0026#39;) b, Ước lượng giá trị cho Missing Data trên tập dữ liệu ... # fit on the dataset imputer.fit(X) c, Tạo ra Transform Data ... # transform the dataset Xtrans = imputer.transform(X) Để kiểm chứng lại hiệu quả làm việc của IterativeImputer, chúng ta sẽ áp dụng lên tập dữ liệu Horse Colic Dataset, xem trước và sau khi áp dụng IterativeImputer có gì thay đổi:\n# iterative imputation transform for the horse colic dataset from numpy import isnan from pandas import read_csv from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39;, header=None, na_values=\u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # summarize total missing print(\u0026#39;Missing: %d\u0026#39; % sum(isnan(X).flatten())) # define imputer imputer = IterativeImputer() # fit on the dataset imputer.fit(X) # transform the dataset Xtrans = imputer.transform(X) # summarize total missing print(\u0026#39;Missing: %d\u0026#39; % sum(isnan(Xtrans).flatten())) Kết quả:\nMissing: 1605 Missing: 0 Ban đầu có 1605 Missing Data. Sau khi áp dụng IterativeImputer, số lượng Missing Data giảm về 0, chứng tỏ rằng nó đã thành công trong việc xóa bỏ Missing Data.\n2.2 IterativeImputer và Model Evaluation Phần này chúng ta sẽ áp dụng Iterative Imputation vào việc mô hình hóa thuật toán RandomForest trên tập dữ liệu Horse Colic Dataset. k-Fold Cross-validation và Pipleline cũng sẽ được sử dụng tương tự như bài trước.\n# evaluate iterative imputation and random forest for the horse colic dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39;, header=None, na_values=\u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # define modeling pipeline model = RandomForestClassifier() imputer = IterativeImputer() pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39;, imputer), (\u0026#39;m\u0026#39;, model)]) # define model evaluation cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # evaluate model scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) print(\u0026#39;Mean Accuracy: %.3f(%.3f)\u0026#39; % (mean(scores), std(scores))) Kết quả thực hiện:\nMean Accuracy: 0.871 (0.052) 2.3 Tuning imputation_order parameter Như đã trình bày ở phần trên, imputation_order là một tham số quan trọng ảnh hưởng đến độ chính xác của IterativeImputer. Chúng ta không thể biết chính xác giá trị phù hợp nhất của nó đối với mỗi bộ dữ liệu. Cách dễ nhất để tìm ra nó là thử-sai, tức là chọn các giá trị có thể có của nó và thử lần lượt từng giá trị đó xem giá trị nào tốt nhất. Code dưới đây đánh giá độ chính xác trung bình của RandomForest cho mỗi trường hợp giá trị của imputation_order.\n# compare iterative imputation strategies for the horse colic dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from matplotlib import pyplot # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39;, header=None, na_values=\u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # evaluate each strategy on the dataset results = list() strategies = [\u0026#39;ascending\u0026#39;, \u0026#39;descending\u0026#39;, \u0026#39;roman\u0026#39;, \u0026#39;arabic\u0026#39;, \u0026#39;random\u0026#39;] for s in strategies: # create the modeling pipeline pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39;, IterativeImputer(imputation_order=s)), (\u0026#39;m\u0026#39;, RandomForestClassifier())]) # evaluate the model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # store results results.append(scores) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (s, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=strategies, showmeans=True) pyplot.show() Kết quả chạy:\n Độ chính xác trung bình và độ lệch chuẩn.  \u0026gt;ascending 0.871 (0.054) \u0026gt;descending 0.868 (0.051) \u0026gt;roman 0.866 (0.054) \u0026gt;arabic 0.864 (0.052) \u0026gt;random 0.867 (0.049) Theo kết quả này, độ chính xác khi imputation_order = \u0026lsquo;ascending\u0026rsquo; cao nhất, nhưng độ lệch chuẩn cũng khá cao. imputation_order = \u0026lsquo;arabic cho ra độ chính xác thấp nhất, độ lệch chuẩn cao gần nhất.\n Phân phối kết quả   Mức độ phân phối trong trường hợp imputation_order = \u0026lsquo;ascending\u0026rsquo; là lớn nhất.\nKết hợp các nhận xét trên, có thể nhận định imputation_order = \u0026lsquo;ascending\u0026rsquo; là giá trị tối ưu của bài toán này.\n2.4 Tuning max_iter parameter Ngoài imputation_order thì max_iter cũng là 1 tham số ảnh hưởng đến độ chính xác của IterativeImputer. Code dưới đây đánh giá độ chính xác trung bình của RandomForest cho mỗi trường hợp giá trị của max_iter, từ 1 đến 20.\n# compare iterative imputation number of iterations for the horse colic dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from matplotlib import pyplot # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39;, header=None, na_values=\u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # evaluate each strategy on the dataset results = list() strategies = [str(i) for i in range(1, 21)] for s in strategies: # create the modeling pipeline pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39;, IterativeImputer(max_iter=int(s))), (\u0026#39;m\u0026#39;, RandomForestClassifier())]) # evaluate the model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # store results results.append(scores) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (s, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=strategies, showmeans=True) pyplot.show() Kết quả chạy:\n Độ chính xác trung bình và độ lệch chuẩn.  \u0026gt;1 0.869 (0.052) \u0026gt;2 0.873 (0.053) \u0026gt;3 0.869 (0.052) \u0026gt;4 0.876 (0.053) \u0026gt;5 0.874 (0.051) \u0026gt;6 0.873 (0.051) \u0026gt;7 0.870 (0.050) \u0026gt;8 0.870 (0.050) \u0026gt;9 0.866 (0.047) \u0026gt;10 0.874 (0.059) \u0026gt;11 0.870 (0.050) \u0026gt;12 0.867 (0.049) \u0026gt;13 0.873 (0.053) \u0026gt;14 0.873 (0.055) \u0026gt;15 0.871 (0.056) \u0026gt;16 0.869 (0.050) \u0026gt;17 0.874 (0.052) \u0026gt;18 0.869 (0.056) \u0026gt;19 0.876 (0.053) \u0026gt;20 0.874 (0.055) Theo kết quả này, độ chính xác khi max_iter = 4 hoặc 19 cao nhất.\n Phân phối kết quả   Mức độ phân phối trong trường hợp max_iter = 4\u0026rsquo; nhỏ hơn so với max_iter = 4.\nKết hợp các nhận xét trên, có thể nhận định max_iter = 19' là giá trị tối ưu của bài toán này.\n2.5 Sử dụng IterativeImputer Transform khi dự đoán dữ liệu mới. Chúng ta sẽ sử dụng các kết quả phân tích từ phần trên để tạo ra model, sau đó dự dự đoán trên một mẫu dữ liệu mới. Code hoàn chỉnh như sau:\n# compare iterative imputation number of iterations for the horse colic dataset # iterative imputation strategy and prediction for the horse colic dataset from numpy import nan from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer from sklearn.pipeline import Pipeline # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39;, header=None, na_values=\u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # create the modeling pipeline pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39;, IterativeImputer(max_iter=19, imputation_order=\u0026#39;ascending\u0026#39;)), (\u0026#39;m\u0026#39;, RandomForestClassifier())]) # fit the model pipeline.fit(X, y) # define new data row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 00000, 00000, 2] # make a prediction yhat = pipeline.predict([row]) # summarize prediction print(\u0026#39;Predicted Class: %d\u0026#39; % yhat[0]) Kết quả thực hiện:\nPredicted Class: 2 Chú ý quan trọng là Missing Data trong mẫu dữ liệu mới phải được đánh dấu là nan thì model mới có thể hiểu được.\n3. Kết luận Hôm nay, chúng ta đã tìm hiểu về phương pháp Iterative Imputation trong việc giải quyết vấn đề Missing Data. Đây là một phương pháp phức tạp hơn 3 phương pháp trước đó, nhưng thường mang lại hiệu quả cao hơn.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ chuyển sang tìm hiểu một vấn đề mới của quá trình Data Prepration, đó là Feature Selection. Mời các bạn đón đọc.\n4. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/81_data_prepeation_for_ml_data_cleaning_missing_data_4/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Missing Data - Phần 4 - Iterative Imputation"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 6 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 3 về chủ đề Missing Data. Trong bài này, chúng ta sẽ tìm hiểu phương pháp tiếp theo để giải quyết vấn đề Missing Data, đó là phương pháp kNN Imputation.\n1. k-Nearest Neighbor (kNN) Imputation Nhắc đến kNN, chắc hẳn mọi người đều biết đó là một thuật toán supervised-learning đơn giản nhất (nhưng lại có hiệu quả đối với một số trường hợp) trong Machine Learning. Khi training, thuật toán này không học một điều gì từ dữ liệu training (*đây cũng là lý do thuật toán này được xếp vào loại lazy learning, mọi tính toán được thực hiện khi nó cần dự đoán kết quả của dữ liệu mới. kNN có thể áp dụng được vào cả hai loại của bài toán Supervised Learning là Classification và Regression. Nó cũng được gọi là một thuật toán Instance-based hay Memory-based Learning.\nĐể giải quyết vấn đề Missing Data, một cách hiệu quả là sử dụng một mô hình để dự đoán giá trị cho Missing Data đó, dựa vào những giá trị tồn tại trong tập dữ liệu. Về lý thuyết, chúng ta có thể sử dụng bất kỳ thuật toán ML Classification/Regression nào để thực hiện việc Imputation cho Missing Data, nhưng thực tế chứng minh rằng kNN mang lại hiệu quả tốt hơn, cả về khía cạnh độ chính xác và mức độ phức tạp khi thực hiện.\nViệc cấu hình cho kNN thường bao gồm việc lựa chọn 2 giá trị là loại metric đo khoảng cách giữa các mẫu dữ liệu (Euclidean, Cosine, \u0026hellip;) và số lượng mẫu (k) lân cận với mẫu cần xác định giá trị/lớp.\n2. Thực hành kNN Imputation Chúng ta sẽ thực hành phương pháp kNN Imputation trên bộ dữ liệu Horse Colic Dataset giống như bài trước.\nThư viện Scikit-learn cung cấp lớp KNNImputer giúp chúng ta dễ dàng thực hiện kNN Imputation.\n2.1 KNNImputer và Data Transform Cũng giống như SimpleImputer, khi sử dụng KNNImputer, nó sẽ tạo ra một phiên bản khác của tập dữ liệu ban đầu mà ở đó, các Missing Data đã được thay thế bởi các giá trị sinh ra từ thuật toán kNN (Data Transform). Các bước thực hiện Data Transform sử dụng KNNImputer như sau:\na, Khai báo một Instance của KNNImputer Khi khởi tạo Instance cho KNNImputer cần quan tâm đến 3 tham số truyền vào:\n  Số lượng mẫu dữ liệu lân cận (n_neighbors)\n  Loại khoảng cách (metric): mặc định là \u0026lsquo;nan_euclidean\u0026rsquo;, tức là Euclidean nhưng bỏ qua các Missing Data*)\n  Trọng số (weight): sử dụng trọng số giữa các mẫu dữ liệu lân cận khi tính khoảng cách. Giá trị mặc định là \u0026lsquo;uniform\u0026rsquo;, tức là tất cả trọng số đều bằng nhau. Để sát sao hơn, ta có thể sử dụng giá trị \u0026lsquo;distance\u0026rsquo;, tức là mẫu dữ liệu nào càng gần mẫu dữ liệu cần dự đoán thì trọng số càng cao.\n  ... # define imputer imputer = KNNImputer(n_neighbors=5, weights= \u0026#39;distance\u0026#39; , metric= \u0026#39;nan_euclidean\u0026#39;) b, Tính toán giá trị cho Missing Data trên tập dữ liệu ... # fit on the dataset imputer.fit(X) c, Tạo ra Transform Data ... # transform the dataset Xtrans = imputer.transform(X) Để kiểm chứng lại hiệu quả làm việc của KNNImputer, chúng ta sẽ áp dụng lên tập dữ liệu Horse Colic Dataset, xem trước và sau khi áp dụng KNNImputer có gì thay đổi:\n# knn imputation transform for the horse colic dataset from numpy import isnan from pandas import read_csv from sklearn.impute import KNNImputer # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39; , header=None, na_values= \u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # summarize total missing print(\u0026#39;Missing: %d\u0026#39; % sum(isnan(X).flatten())) # define imputer imputer = KNNImputer() # fit on the dataset imputer.fit(X) # transform the dataset Xtrans = imputer.transform(X) # summarize total missing print(\u0026#39;Missing: %d\u0026#39; % sum(isnan(Xtrans).flatten())) Kết quả:\nMissing: 1605 Missing: 0 Ban đầu có 1605 Missing Data. Sau khi áp dụng KNNImputer, số lượng Missing Data giảm về 0, chứng tỏ rằng nó đã thành công trong việc xóa bỏ Missing Data.\n2.2 KNNImputer và Model Evaluation Phần này chúng ta sẽ áp dụng kNN Imputation vào việc mô hình hóa thuật toán RandomForest trên tập dữ liệu Horse Colic Dataset. k-Fold Cross-validation và Pipleline cũng sẽ được sử dụng tương tự như bài trước.\n# evaluate knn imputation and random forest for the horse colic dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.impute import KNNImputer from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39;, header=None, na_values=\u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # define modeling pipeline model = RandomForestClassifier() imputer = KNNImputer(n_neighbors=5, weights= \u0026#39;distance\u0026#39; , metric= \u0026#39;nan_euclidean\u0026#39;) pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39;, imputer), (\u0026#39;m\u0026#39;, model)]) # define model evaluation cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # evaluate model scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) print(\u0026#39;Mean Accuracy: %.3f(%.3f)\u0026#39; % (mean(scores), std(scores))) Kết quả thực hiện:\nMean Accuracy: 0.871 (0.051) 2.3 Tuning n_neighbors parameter Như đã trình bày ở phần trên, n_neighbors là một tham số quan trọng ảnh hưởng đến độ chính xác của kNN. Chúng ta không thể biết chính xác giá trị phù hợp nhất của nó đối với mỗi bộ dữ liệu. Cách dễ nhất để tìm ra nó là thử-sai, tức là chọn một khoảng giá trị của nó và thử lần lượt từng giá trị trong khoảng đó xem giá trị nào tốt nhất. Code dưới đây đánh giá độ chính xác trung bình của kNN cho mỗi trường hợp giá trị của n_neighbors thay đôi từ 1 đến 21.\n# compare knn imputation strategies for the horse colic dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.impute import KNNImputer from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from matplotlib import pyplot # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39;, header=None, na_values=\u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # evaluate each strategy on the dataset results = list() strategies = [str(i) for i in range(21): for s in strategies: # create the modeling pipeline pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39;, KNNImputer(n_neighbors=int(s))), (\u0026#39;m\u0026#39;, RandomForestClassifier())]) # evaluate the model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(pipeline, X, y, scoring=\u0026#39;accuracy\u0026#39;, cv=cv, n_jobs=-1) # store results results.append(scores) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (s, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=strategies, showmeans=True) pyplot.show() Kết quả chạy:\n Độ chính xác trung bình và độ lệch chuẩn.  \u0026gt;1 0.868 (0.048) \u0026gt;2 0.861 (0.047) \u0026gt;3 0.859 (0.051) \u0026gt;4 0.863 (0.052) \u0026gt;5 0.864 (0.056) \u0026gt;6 0.866 (0.060) \u0026gt;7 0.872 (0.056) \u0026gt;8 0.864 (0.056) \u0026gt;9 0.868 (0.053) \u0026gt;10 0.868 (0.053) \u0026gt;11 0.864 (0.053) \u0026gt;12 0.862 (0.051) \u0026gt;13 0.861 (0.049) \u0026gt;14 0.868 (0.051) \u0026gt;15 0.862 (0.054) \u0026gt;16 0.863 (0.048) \u0026gt;17 0.863 (0.054) \u0026gt;18 0.862 (0.057) \u0026gt;19 0.866 (0.051) \u0026gt;20 0.869 (0.055) \u0026gt;21 0.866 (0.057) Theo kết quả này, độ chính xác khi n_neighbors = 7 cao nhất, nhưng độ lệch chuẩn cũng khá cao. n_neighbors = 1 cho ra độ chính xác tương đối cao, độ lệch chuẩn gần nhỏ nhất.\n Phân phối kết quả   Mức độ phân phối trong trường hợp n_neighbors = 1 là nhỏ nhất.\nKết hợp các nhận xét trên, có thể nhận định n_neighbors = 1 là giá trị tối ưu của bài toán này.\n2.4 Sử dụng KNNImputer Transform khi dự đoán dữ liệu mới. Chúng ta sẽ sử dụng các kết quả phân tích từ phần trên để tạo ra model, sau đó dự dự đoán trên một mẫu dữ liệu mới. Code hoàn chỉnh như sau:\n# knn imputation strategy and prediction for the horse colic dataset from numpy import nan import joblib from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.impute import KNNImputer from sklearn.pipeline import Pipeline # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39;, header=None, na_values=\u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # create the modeling pipeline pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39;, KNNImputer(n_neighbors=1, weights=\u0026#39;distance\u0026#39;)), (\u0026#39;m\u0026#39;, RandomForestClassifier())]) # fit the model pipeline.fit(X, y) # save pipeline as model file joblib.dump(pipeline, \u0026#39;model.mod\u0026#39;) # load model from file model = joblib.load(\u0026#39;model.mod\u0026#39;) # define new data row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 00000, 00000, 2] # make a prediction yhat = model.predict([row]) # summarize prediction print(\u0026#39;Predicted Class: %d\u0026#39; % yhat[0]) Kết quả thực hiện:\nPredicted Class: 2 Chú ý quan trọng là Missing Data trong mẫu dữ liệu mới phải được đánh dấu là nan thì model mới có thể hiểu được.\n3. Kết luận Hôm nay, chúng ta đã tìm hiểu về phương pháp kNN Imputation trong việc giải quyết vấn đề Missing Data. Đây là một phương pháp đơn giản vì nó chỉ dựa trên các mẫu dữ liệu lân cận để tìm ra giá trị mới thay thế cho Missing Data. Cũng giống như Statistical Imputation, kNN Imputation tỏ ra hiệu quả cao trong một số trường hợp cụ thể.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ tìm hiểu về phương pháp tiếp theo trong việc xử lý Missing Data, đó là Iteratove Imputation. Mời các bạn đón đọc.\n4. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/80_data_prepeation_for_ml_data_cleaning_missing_data_3/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Missing Data - Phần 3 - kNN Imputation"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 5 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài thứ 2 về chủ đề Missing Data. Trong bài này, chúng ta sẽ tìm hiểu phương pháp tiếp theo để giải quyết vấn đề Missing Data, đó là phương pháp Statistical Imputation.\nImputation, nói chung là các phương pháp thay thế các Missing Data bằng một giá trị khác. Có 3 phương pháp Imputation phổ biến là Statistical Imputation, KNN Imputation, và Iterative Imputation. Phương pháp đầu tiên sẽ được tìm hiểu trong bài này.\n1. Statistic Imputation Đây là phương pháp sử dụng các giá trị thống kê để thay thế cho Missing Data. Ưu điểm của nó là đơn giản, tính toán nhanh. Một số phương án thay thế Missing Data bằng giá trị thống kê có thể sử dụng ở đây là:\n Thay thế Missing Data trong 1 cột bằng giá trị trung bình của cột chứa Missing Data đó. Thay thế Missing Data trong 1 cột bằng giá trị trung vị của cột chứa Missing Data đó. Thay thế Missing Data trong 1 cột bằng giá trị xuất hiện nhiều nhất (mode) trong cột chứa Missing Data đó. Thay thế Missing Data trong 1 cột bằng giá trị là hằng số khác.  2. Thực hàng Statistical Imputation Chúng ta sẽ thử thực hành phương pháp Statistical Imputation trên một bộ dữ liệu cụ thể.\n2.1 Giới thiệu bộ dữ liệu Horse Colic Dataset Horse Colic Dataset là bộ dữ liệu mô tả các đặc điểm y học của những con ngựa bị đau bụng và cho ra kết quả là chúng còn sống hay đã chết. Có tổng cộng 300 con ngựa, mỗi con có 26 đặc điểm được ghi nhận. Thông tin chi tiết về các trường dữ liệu được trình bày trong file này. Đây là bài toán Binary Classification, kết quả dự đoán là 1 nếu con ngựa còn sống và 2 nếu con ngựa đã chết. Tập dữ liệu có nhiều giá trị bị thiếu được đánh dấu bằng một ký tự dấu chấm hỏi (“?”).\n2,1,530101,38.50,66,28,3,3,?,2,5,4,4,?,?,?,3,5,45.00,8.40,?,?,2,2,11300,00000,00000,2 1,1,534817,39.2,88,20,?,?,4,1,3,4,2,?,?,?,4,2,50,85,2,2,3,2,02208,00000,00000,2 2,1,530334,38.30,40,24,1,1,3,1,3,3,1,?,?,?,1,1,33.00,6.70,?,?,1,2,00000,00000,00000,1 1,9,5290409,39.10,164,84,4,1,6,2,2,4,4,1,2,5.00,3,?,48.00,7.20,3,5.30,2,1,02208,00000,00000,1 ... Mình sẽ tiến hành đọc bộ dữ liệu này vào một Dataframe, thay thế Missing Data bằng giá trị NaN, sau đó đếm số lượng Missing Data trong mỗi cột kèm theo phần trăm tương ứng:\n# summarize the horse colic dataset from pandas import read_csv # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39; , header=None, na_values= \u0026#39;?\u0026#39;) # summarize the first few rows print(dataframe.head()) # summarize the number of rows with missing values for each column for i in range(dataframe.shape[1]): # count number of rows with missing values n_miss = dataframe[[i]].isnull().sum() perc = n_miss / dataframe.shape[0] * 100 print(\u0026#39;\u0026gt; %d, Missing: %d(%.1f%%)\u0026#39; % (i, n_miss, perc)) Kết quả thực hiện:\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 0 2.0 1 530101 38.5 66.0 28.0 3.0 3.0 NaN 2.0 5.0 4.0 4.0 NaN NaN NaN 3.0 5.0 45.0 8.4 NaN NaN 2.0 2 11300 0 0 2 1 1.0 1 534817 39.2 88.0 20.0 NaN NaN 4.0 1.0 3.0 4.0 2.0 NaN NaN NaN 4.0 2.0 50.0 85.0 2.0 2.0 3.0 2 2208 0 0 2 2 2.0 1 530334 38.3 40.0 24.0 1.0 1.0 3.0 1.0 3.0 3.0 1.0 NaN NaN NaN 1.0 1.0 33.0 6.7 NaN NaN 1.0 2 0 0 0 1 3 1.0 9 5290409 39.1 164.0 84.0 4.0 1.0 6.0 2.0 2.0 4.0 4.0 1.0 2.0 5.0 3.0 NaN 48.0 7.2 3.0 5.3 2.0 1 2208 0 0 1 4 2.0 1 530255 37.3 104.0 35.0 NaN NaN 6.0 2.0 NaN NaN NaN NaN NaN NaN NaN NaN 74.0 7.4 NaN NaN 2.0 2 4300 0 0 2 \u0026gt; 0, Missing: 1 (0.3%) \u0026gt; 1, Missing: 0 (0.0%) \u0026gt; 2, Missing: 0 (0.0%) \u0026gt; 3, Missing: 60 (20.0%) \u0026gt; 4, Missing: 24 (8.0%) \u0026gt; 5, Missing: 58 (19.3%) \u0026gt; 6, Missing: 56 (18.7%) \u0026gt; 7, Missing: 69 (23.0%) \u0026gt; 8, Missing: 47 (15.7%) \u0026gt; 9, Missing: 32 (10.7%) \u0026gt; 10, Missing: 55 (18.3%) \u0026gt; 11, Missing: 44 (14.7%) \u0026gt; 12, Missing: 56 (18.7%) \u0026gt; 13, Missing: 104 (34.7%) \u0026gt; 14, Missing: 106 (35.3%) \u0026gt; 15, Missing: 247 (82.3%) \u0026gt; 16, Missing: 102 (34.0%) \u0026gt; 17, Missing: 118 (39.3%) \u0026gt; 18, Missing: 29 (9.7%) \u0026gt; 19, Missing: 33 (11.0%) \u0026gt; 20, Missing: 165 (55.0%) \u0026gt; 21, Missing: 198 (66.0%) \u0026gt; 22, Missing: 1 (0.3%) \u0026gt; 23, Missing: 0 (0.0%) \u0026gt; 24, Missing: 0 (0.0%) \u0026gt; 25, Missing: 0 (0.0%) \u0026gt; 26, Missing: 0 (0.0%) \u0026gt; 27, Missing: 0 (0.0%) Có thể thấy rằng, chỉ có cột 1, 2, 23, 24, 25, 25, 27 là không có Missing Data. Các cột còn lại đều có. Đặc biệt, cột cột 15 và 21, số lượng Missing Data chiếm phần lớn.\n2.2 Thực hiện Statistical Imputation Thư viện Scikit-learn cung cấp lớp SimpleImputer giúp chúng ta thực hiện Statistical Imputation rất dễ dàng.\na, SimpleImputer và Data Transform Đầu tiên, khai báo một Instance của lớp SimpleImputer, truyền vào loại thống kê muốn sử dụng: mean, median, mode, \u0026hellip;\n# define imputer imputer = SimpleImputer(strategy= \u0026#39;mean\u0026#39;) Tiếp theo, dùng imputer vừa khai báo fit trên tập dữ liệu để tính toán trung bình của mỗi cột.\n... # fit on the dataset imputer.fit(X) Cuối cùng, mang imputer áp dụng lên toàn bộ tập dữ liệu để tạo ra một phiên bản mới của tập dữ liệu, trong đó, tất cả Missing Data được thay thể bởi giá trị trung bình của cột chứa nó.\n... # transform the dataset Xtrans = imputer.transform(X) Code đầy đủ áp dụng vào tập dữ liệu Horse Colic Dataset như sau:\n# statistical imputation transform for the horse colic dataset from numpy import isnan from pandas import read_csv from sklearn.impute import SimpleImputer # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39; , header=None, na_values= \u0026#39;?\u0026#39; ) # split into input and output elements. #23 is label column data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # summarize total missing print( \u0026#39;Missing: %d\u0026#39; % sum(isnan(X).flatten())) # define imputer imputer = SimpleImputer(strategy= \u0026#39;mean\u0026#39; ) # fit on the dataset imputer.fit(X) # transform the dataset Xtrans = imputer.transform(X) # summarize total missing print( \u0026#39;Missing: %d\u0026#39; % sum(isnan(Xtrans).flatten())) Kết quả thực thi:\nMissing: 1605 Missing: 0 Ban đầu, có 1605 Missing Data. Sau khi thực hiện Statistical Imputation, không còn Missing Data nữa.\nb, SimpleImputer và Model Evaluation Phần này, chúng ta sẽ thực hiện đầy đủ việc áp dụng Statistical Imputation vào mô hình hóa thuật toán RandomForest trên tập dữ liệu Horse Colic Dataset theo phương pháp k-Fold Cross-validation.\nHãy nhớ lại cách tiếp cận chuẩn khi thực hiện Data Preparation mà chúng ta đã đề cập ở bài đầu tiên. Cụ thể, với bài toán này, các bước thực hiện sẽ như sau:\n Chia tập dữ liệu thành 2 tập Train và Test theo kiểu k-Fold Cross-validation. Tính toán giá trị Statistic trên mỗi cột của tập Train. Áp dụng phần tính toán đó vào cả 2 tập Train và Test.  Để thực hiện các công viêc này một cách đơn giản, mình sẽ sử dụng lớp Pipeline của Scikit-learn. Có thể hiểu đó là một cách thực hiện các bước tuần tự nhau theo một thứ tự được quy định trước. Như ví dụ dưới đây, Pipeline gồm 2 bước: bước 1 là khai báo SimpleImputer, bước 2 là khai báo RandomForest model:\n... # define modeling pipeline model = RandomForestClassifier() imputer = SimpleImputer(strategy= \u0026#39;mean\u0026#39;) pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39; , imputer), (\u0026#39;m\u0026#39; , model)]) Code đây đủ của bài toán như dưới đây:\n# evaluate mean imputation and random forest for the horse colic dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39; , header=None, na_values= \u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # define modeling pipeline model = RandomForestClassifier() imputer = SimpleImputer(strategy=\u0026#39;mean\u0026#39;) pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39; , imputer), (\u0026#39;m\u0026#39; , model)]) # define model evaluation cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # evaluate model scores = cross_val_score(pipeline, X, y, scoring= \u0026#39;accuracy\u0026#39; , cv=cv, n_jobs=-1) print(\u0026#39;Mean Accuracy: %.3f(%.3f)\u0026#39; % (mean(scores), std(scores))) Pipeline được đánh giá sử dụng 3 lần lặp của 10-Fold Cross-validation. Độ chính xác trung bình đo được là khoảng 86.4%, một kết quả tương đối tốt.\nMean Accuracy: 0.864 (0.054) Chú ý rằng kết quả thực hiện của bạn có thể không giống ở đây. Nên chạy thử vài lần để tính kết quá trung bình.\nc, So sánh các loại Statistical Imputation khác nhau Như đã nói ở trên, Statistical Imputation có thể sử dụng Mean, Median, Mode, Constant \u0026hellip; để thay thế cho Missing Data. Câu hỏi đặt ra là làm sao biết được cái nào là tốt nhất đối với bài toán của chúng ta? Câu trả lời là chúng ta không thể khẳng định ngay được, mà phải thông qua phép thử-sai mới có được kết quả chính xác.\nCode dưới đây thực hiện áp dụng 4 loại Statistical Imputation khác nhau cho bài toán ở mục b, sau đó so sách các độ chính xác trung bình của mỗi loại với nhau:\n# compare statistical imputation strategies for the horse colic dataset from numpy import mean from numpy import std from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from matplotlib import pyplot # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39; , header=None, na_values= \u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # evaluate each strategy on the dataset results = list() strategies = [\u0026#39;mean\u0026#39; , \u0026#39;median\u0026#39; , \u0026#39;most_frequent\u0026#39; , \u0026#39;constant\u0026#39;] for s in strategies: # create the modeling pipeline pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39; , SimpleImputer(strategy=s)), (\u0026#39;m\u0026#39;, RandomForestClassifier())]) # evaluate the model cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(pipeline, X, y, scoring= \u0026#39;accuracy\u0026#39; , cv=cv, n_jobs=-1) # store results results.append(scores) print(\u0026#39;\u0026gt;%s%.3f(%.3f)\u0026#39; % (s, mean(scores), std(scores))) # plot model performance for comparison pyplot.boxplot(results, labels=strategies, showmeans=True) pyplot.show()  Kết quả so sánh độ chính xác trung bình và độ lệch chuẩn:  \u0026gt;mean 0.868 (0.055) \u0026gt;median 0.870 (0.055) \u0026gt;most_frequent 0.873 (0.056) \u0026gt;constant 0.880 (0.048) Khá ngạc nhiên là Statictical Imputation với hằng số 0 lại cho kết quả cao nhất, 88%.\n Kết quả so sánh sự phân phối kết quả:   Quan sát đồ thị box-plot ta thấy Statistical Imputation với hằng số 0 có độ phân tán kết quả thấp nhất giữa các lần thực hiện của nó.\nTừ 2 nhận xét trên, có thể kết luận rằng Statistical Imputation với hằng số 0 là chiến thuật phù hợp nhất đối với bài toán của chúng ta.\nd, Sử dụng SimpleImputer Transform khi dự đoán dữ liệu mới Sau khi đã biết được loại Statistical Imputation phù hợp nhất, chúng ta mong muốn sử dụng nó để thực hiện việc dự đoán trên mẫu dữ liệu mới. Điều này có thể đạt được bằng cách định nghĩa một Pipeline, fitting nó trên toàn bộ dữ liệu, sau đó gọi hàm predict() và truyền vào mẫu dữ liệu mới cần dự đoán nhãn.\n# constant imputation strategy and prediction for the horse colic dataset from numpy import nan import joblib from pandas import read_csv from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline # load dataset dataframe = read_csv(\u0026#39;horse-colic.csv\u0026#39;, header=None, na_values=\u0026#39;?\u0026#39;) # split into input and output elements data = dataframe.values ix = [i for i in range(data.shape[1]) if i != 23] X, y = data[:, ix], data[:, 23] # create the modeling pipeline pipeline = Pipeline(steps=[(\u0026#39;i\u0026#39;, SimpleImputer(strategy=\u0026#39;constant\u0026#39;)), (\u0026#39;m\u0026#39;, RandomForestClassifier())]) # fit the model pipeline.fit(X, y) # save pipeline as model file joblib.dump(pipeline, \u0026#39;model.mod\u0026#39;) # load model from file model = joblib.load(\u0026#39;model.mod\u0026#39;) # define new data row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 00000, 00000, 2] # make a prediction yhat = model.predict([row]) # summarize prediction print(\u0026#39;Predicted Class: %d\u0026#39; % yhat[0]) Chạy code trên thu được kết quả:\nPredicted Class: 2 Chú ý quan trọng là Missing Data trong mẫu dữ liệu mới phải được đánh dấu là nan thì model mới có thể hiểu được.\n3. Kết luận Hôm nay, chúng ta đã tìm hiểu về phương pháp Statistical Imputation trong việc giải quyết vấn đề Missing Data. Đây là một phương pháp đơn giản, dễ thực hiện, và tỏ ra hiệu quả cao trong một số trường hợp cụ thể.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ tìm hiểu về phương pháp tiếp theo trong việc xử lý Missing Data, đó là kNN Imputation. Mời các bạn đón đọc.\n4. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/79_data_prepeation_for_ml_data_cleaning_missing_data_2/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Missing Data - Phần 2 - Statistical Imputation"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 4 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML và là bài đầu tiên về Missing Data. Trong bài này, chúng ta sẽ tìm hiểu về vấn đề Missing Data.\n1. Missing Data là gì? Missing Data là hiện tượng thiếu hụt một vài giá trị trong tập dữ liệu. Những vị trí thiếu đó có thể được thể hiện bởi số 0, số âm, khoảng trắng, hoặc một ký tự đặc biệt nào đó (dấu ? chẳng hạn).\nNguyên nhân dẫn đến hiện tượng này có thể là do có sai sót trong quá trình thu thập dữ liệu, hoặc dữ liệu bị hỏng (corruption) trong quá trình lưu trữ và trao đổi.\nPhần lớn các thuật toán ML không thể làm việc được với Missing Data, hoặc nếu có thì kết quả cũng không đáng tin cậy. Vì thế, chúng ta cần loại bỏ vấn đề này trước khi thực hiện các bước mô hình hóa dữ liệu.\n2. Nhận diện và đánh dấu Missing Data Trước tiên, bạn hãy tải bộ dữ liệu Diabetes tại đây để thực hành trong phần này.\nĐể nhận diện Missing Data, các hay dùng nhất là sử dụng các hàm thống kê. Xem code dưới đây:\n# load and summarize the dataset from pandas import read_csv # load the dataset dataset = read_csv( \u0026#39;pima-indians-diabetes.csv\u0026#39; , header=None) # summarize the dataset print(dataset.describe()) Đoạn code trên đọc Diabetes dataset và hiển thị các thông tin tóm tắt thống kê của nó:\n0 1 2 3 4 5 6 7 8 count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958 std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000 75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000 Quan sát kết quả hiển thị bên trên, ta có thể thấy rằng có một vài cột có chứa giá trị 0 (thể hiện ở việc giá trị nhỏ nhất của cột đó là 0). Theo kiến thức chuyên môn trong lĩnh vực y học về bệnh tiểu đường thì 0 là giá trị không hợp lệ ở đây. Điều đó có nghĩa là, tại các vị trí có giá trị 0 chính là Missing Data.\nCụ thể hơn, Missing Data nằm ở các cột [1, 2, 3, 4, 5] tương ứng với [Plasma glucose concentration, Diastolic blood pressur, Triceps skinfold thickness, 2-Hour serum insulin, Body mass index]. Tại cột đầu tiên, giá trị 0 là hợp lệ.\nThử xác nhận lại điều này bằng cách kiểm tra 20 hàng đầu tiên của bộ dữ liệu:\n# load the dataset and review rows from pandas import read_csv # load the dataset dataset = read_csv( \u0026#39;pima-indians-diabetes.csv\u0026#39; , header=None) # summarize the first 20 rows of data print(dataset.head(20)) Kết quả:\n0 1 2 3 4 5 6 7 8 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 5 5 116 74 0 0 25.6 0.201 30 0 6 3 78 50 32 88 31.0 0.248 26 1 7 10 115 0 0 0 35.3 0.134 29 0 8 2 197 70 45 543 30.5 0.158 53 1 9 8 125 96 0 0 0.0 0.232 54 1 10 4 110 92 0 0 37.6 0.191 30 0 11 10 168 74 0 0 38.0 0.537 34 1 12 10 139 80 0 0 27.1 1.441 57 0 13 1 189 60 23 846 30.1 0.398 59 1 14 5 166 72 19 175 25.8 0.587 51 1 15 7 100 0 0 0 30.0 0.484 32 1 16 0 118 84 47 230 45.8 0.551 31 1 17 7 107 74 0 0 29.6 0.254 31 1 18 1 103 30 38 83 43.3 0.183 33 0 19 1 115 70 30 96 34.6 0.529 32 1 Khi làm việc với dữ liệu có Missing Data, chúng ta nên đánh dấu (mark) chúng bằng một giá trị là NaN, bởi vì các thư viện Pandas, Numpy, Scikit-learn đều bỏ qua giá trị này trong các tính toán của chúng. Để làm việc này, ta sử dụng hàm replace() của Pandas.\n# example of review data with missing values marked with a nan from numpy import nan from pandas import read_csv # load the dataset dataset = read_csv( \u0026#39; pima-indians-diabetes.csv \u0026#39; , header=None) # replace \u0026#39; 0 \u0026#39; values with \u0026#39; nan \u0026#39; dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan) # summarize the first 20 rows of data print(dataset.head(20)) Kết quả hiển thị:\n0 1 2 3 4 5 6 7 8 0 6 148.0 72.0 35.0 NaN 33.6 0.627 50 1 1 1 85.0 66.0 29.0 NaN 26.6 0.351 31 0 2 8 183.0 64.0 NaN NaN 23.3 0.672 32 1 3 1 89.0 66.0 23.0 94.0 28.1 0.167 21 0 4 0 137.0 40.0 35.0 168.0 43.1 2.288 33 1 5 5 116.0 74.0 NaN NaN 25.6 0.201 30 0 6 3 78.0 50.0 32.0 88.0 31.0 0.248 26 1 7 10 115.0 NaN NaN NaN 35.3 0.134 29 0 8 2 197.0 70.0 45.0 543.0 30.5 0.158 53 1 9 8 125.0 96.0 NaN NaN NaN 0.232 54 1 10 4 110.0 92.0 NaN NaN 37.6 0.191 30 0 11 10 168.0 74.0 NaN NaN 38.0 0.537 34 1 12 10 139.0 80.0 NaN NaN 27.1 1.441 57 0 13 1 189.0 60.0 23.0 846.0 30.1 0.398 59 1 14 5 166.0 72.0 19.0 175.0 25.8 0.587 51 1 15 7 100.0 NaN NaN NaN 30.0 0.484 32 1 16 0 118.0 84.0 47.0 230.0 45.8 0.551 31 1 17 7 107.0 74.0 NaN NaN 29.6 0.254 31 1 18 1 103.0 30.0 38.0 83.0 43.3 0.183 33 0 19 1 115.0 70.0 30.0 96.0 34.6 0.529 32 1 Sau khi đã đánh dấu Missing Data với giá trị NaN, ta có thể tính toán được số lượng Missing Data trong mỗi cột bằng hàm isnull() kết hợp với hàm sum() như dưới đây:\n# example of marking missing values with nan values from numpy import nan from pandas import read_csv # load the dataset dataset = read_csv( \u0026#39;pima-indians-diabetes.csv\u0026#39; , header=None) # replace \u0026#39; 0 \u0026#39; values with \u0026#39; nan \u0026#39; dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan) # count the number of nan values in each column print(dataset.isnull().sum()) Kết quả thực hiện:\n0 0 1 5 2 35 3 227 4 374 5 11 6 0 7 0 8 0 dtype: int64 Ta thấy cột thứ 3 và 4 có rất nhiều Missing Data, trong khi cột thứ 1 và thứ 5 chỉ có một vài Missing Data.\n3. Các phương pháp xử lý Missing Data Có rất nhiều cách để xử lý loại bỏ vấn đề Missing Data. Chúng ta sẽ đi sâu, tìm hiểu 4 phương pháp phổ biến nhất.\nTrước khi đi vào tìm hiểu các phương pháp đó, chúng ta sẽ thực hiện mô hình hóa một thuật toán với dữ liệu chứa Missing Data để xem kết quả ra sao. Kết quả này, sau đó được đem ra so sánh với kết quả mô hình hóa cùng thuật toán trên dữ liệu đã loại bỏ Missing Data theo từng phương pháp.\nMình chọn mô hình hóa thuật toán LDA sử dụng 3-Fold Cross-validation. Code thực hiện như sau:\n# example where missing values cause errors from numpy import nan from pandas import read_csv from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score # load the dataset dataset = read_csv(\u0026#39;pima-indians-diabetes.csv\u0026#39; , header=None) # replace \u0026#39; 0 \u0026#39; values with \u0026#39; nan \u0026#39; dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan) # split dataset into inputs and outputs values = dataset.values X = values[:,0:8] y = values[:,8] # define the model model = LinearDiscriminantAnalysis() # define the model evaluation procedure cv = KFold(n_splits=3, shuffle=True, random_state=1) # evaluate the model result = cross_val_score(model, X, y, cv=cv, scoring= \u0026#39;accuracy\u0026#39;) # report the mean performance print(\u0026#39;Accuracy: %.3f\u0026#39; % result.mean()) Kết quả thực hiện:\nValueError: Input contains NaN, infinity or a value too large for dtype(\u0026#39;float64\u0026#39;). Có lỗi xảy ra đúng như mình đã nói từ đầu, phần lớn các thuật toán ML không thể làm việc được với Missing Data. OK, giờ là lúc chúng ta đi vào tìm hiểu chi tiết từng phương pháp xử lý Missing Data.\n3.1 Xóa các hàng/cột chứa Missing Data Đây là phương pháp đơn giản nhất để loại bỏ Missing Data. Tuy nhiên, nếu số lượng Missing Data quá nhiều mà ta lại xóa hết di thì phần còn lại sẽ không đủ để mô hình hóa dữ liệu đó. Thông thường, nếu tỉ lệ Missing Data nhỏ hơn 5% trên tổng số thì ta nên xóa chúng.\nĐể xóa hàng/cột chứa Missing Data, ta sẽ sử dụng hàm dropna() của Pandas. Tham số axis=0/1 chỉ ra là xóa hàng/cột, mặc định là xóa hàng. Tham số inplace=True/False chỉ ra việc lưu/không lưu thay đổi vào Dataframe ban đầu, mặc định là False. Code thực hiện như sau:\n# example of removing rows that contain missing values from numpy import nan from pandas import read_csv # load the dataset dataset = read_csv(\u0026#39;pima-indians-diabetes.csv\u0026#39; , header=None) # summarize the shape of the raw data print(dataset.shape) # replace \u0026#39; 0 \u0026#39; values with \u0026#39; nan \u0026#39; dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan) # drop rows with missing values dataset.dropna(inplace=True) # summarize the shape of the data with missing rows removed print(dataset.shape) Kết quả thực hiện:\n(768, 9) (392, 9) Từ 768 hàng, sau khi xóa các hàng có chứa Missing Data, chỉ còn lại 392 hàng.\nChúng ta sẽ thực hiện lại việc mô hình hóa thuật toán LDA trên tập dữ liệu đã loại bỏ Missing Data:\n# evaluate model on data after rows with missing data are removed from numpy import nan from pandas import read_csv from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score # load the dataset dataset = read_csv(\u0026#39;pima-indians-diabetes.csv\u0026#39; , header=None) # replace \u0026#39; 0 \u0026#39; values with \u0026#39; nan \u0026#39; dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan) # drop rows with missing values dataset.dropna(inplace=True) # split dataset into inputs and outputs values = dataset.values X = values[:,0:8] y = values[:,8] # define the model model = LinearDiscriminantAnalysis() # define the model evaluation procedure cv = KFold(n_splits=3, shuffle=True, random_state=1) # evaluate the model result = cross_val_score(model, X, y, cv=cv, scoring= \u0026#39;accuracy\u0026#39;) # report the mean performance print( \u0026#39;Accuracy: %.3f\u0026#39; % result.mean()) Kết quả:\nAccuracy: 0.781 Như vậy là chúng ta đã mô hình hóa thành công thuật toán LDA sau khi đã xóa bỏ Missing Data trong bộ dữ liệu của chúng ta.\n3. Kết luận Bài hôm nay cũng đã khá dài,mình sẽ kết thúc tại đây. Chúng ta đã cùng nhau tìm hiểu về hiện tượng Missing Data là gì? tác hại của nó ra sao? Cách nhận biết và thống kê nó. Chúng ta cũng đã tìm hiểu được một phương pháp đơn giản nhất để loại bỏ Missing Data, đó là xóa các hàng/cột có chứa Missing Data.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ tìm hiểu về phương pháp tiếp theo trong việc xử lý Missing Data, đó là Statisticcal Imputation. Mời các bạn đón đọc.\n4. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/78_data_prepeation_for_ml_data_cleaning_missing_data_1/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Missing Data - Phần 1 - Giới thiệu chung"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 3 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML. Trong bài này, chúng ta sẽ tìm hiểu về Outlier Data, mà tiếng việt của chúng ta gọi là dữ liệu ngoại lệ.\n1. Outlier Data là gì? Đôi khi làm việc với dữ liệu, chúng ta bắt gặp các mẫu rất lạ, khác nhiều so với những mẫu khác trong cùng bộ dữ liệu. Sự khác nhau đó có thể là về kiểu dữ liệu, phạm vi giá trị, phân phối dữ liệu, \u0026hellip; Số lượng những mẫu lạ thường không lớn, và chúng đó được gọi là Outlier Data, hay \u0026ldquo;dữ liệu ngoại lệ\u0026rdquo;. Việc tồn tại Outlier Data có thể gây nhiễu, làm cho việc mô hình hóa trở nên khó khăn hơn. Giải pháp xóa bỏ Outlier Data, trong hầu hết các trường hợp, giúp cải thiện đáng kể hiệu quả của các ML model.\nOutlier Data được sinh ra do một trong các nguyên nhân chủ yếu sau:\n Xảy ra lỗi trong quá trình đo đạc, thu thập dữ liệu. Dữ liệu bị hư hỏng (currption) trong quá trình lưu trữ hoặc chuyển đổi. Dữ liệu bị tác động bởi các yếu tố ngẫu nhiên bên ngoài.  Trong thực tế, không có định nghĩa chính xác của Outlier Data, bởi vì nó phụ thuộc vào bài toán cụ thể. Đối với bài toán này, đó có thể là Outlier Data, nhưng đối với bài toán kia, nó lại có thể là giá trị hợp lệ. Để quyết định chính xác, cần phải tham khảo thêm ý kiến của chuyên gia trong lĩnh vực đó.\n2. Các phương pháp nhận diện Outlier Data Trước khi tìm hiểu các phương pháp giúp nhận diện ra Outlier Data, chúng ta sẽ sinh ra một bộ dữ liệu giả để thực hành cho phần này.\nDữ liệu sinh ra bao gồm 10.000 số ngẫu nhiên được phân phối theo Gaussian, có Mean là 50 và STD là 5. Sẽ có một vài số trong tập dữ liệu này nằm ở khá xa so với Mean, chúng ta sẽ coi đó như là Outlier Data.\nCode thực hiện như sau:\n# generate gaussian data from numpy.random import seed from numpy.random import randn from numpy import mean from numpy import std # seed the random number generator seed(1) # generate univariate observations data = 5 * randn(10000) + 50 # summarize print( \u0026#39; mean=%.3fstdv=%.3f\u0026#39; % (mean(data), std(data))) Chạy code trên ta có Output:\nmean=50.049 stdv=4.994 2.1 Phương pháp sử dụng độ lệch chuẩn - STD Nếu biết trước rằng tập dữ liệu mà ta đang làm việc tuân theo phân phối Gaussian hoặc gần với Gaussian thì chúng ta có thể sử dụng STD để xác định và loại bỏ Outlier Data. Bởi vì, Gaussian có đặc điểm là:\n Các mẫu nằm trong phạm vi STD tính từ Mean sẽ chiếm khoảng 68% tổng số mẫu trong tập dữ liệu. Các mẫu nằm trong phạm vi 2*STD tính từ Mean sẽ chiếm khoảng 95% tổng số mẫu trong tập dữ liệu.' Các mẫu nằm trong phạm vi 3*STD tính từ Mean sẽ chiếm khoảng 99.7% tổng số mẫu trong tập dữ liệu.  Ví dụ, nếu tập dữ liệu có Mean là 50 và STD là 5 thì tổng số mẫu có giá trị trong khoảng [45;55] sẽ chiếm 68% tổng số mẫu.\nBiết được như vậy rồi, chúng ta sẽ tính Mean và STD, sau đó định nghĩa Outlier Data là các mẫu nằm ngoài phạm vi của STD (hoặc 2STD, hoặc 3STD) tính từ Mean. Code dưới đây sẽ hiện thực phương pháp này:\n# identify outliers with standard deviation from numpy.random import seed from numpy.random import randn from numpy import mean from numpy import std # seed the random number generator seed(1) # generate univariate observations data = 5 * randn(10000) + 50 # calculate summary statistics data_mean, data_std = mean(data), std(data) # define outliers là 3*STD cut_off = data_std * 3 lower, upper = data_mean - cut_off, data_mean + cut_off # identify outliers outliers = [x for x in data if x \u0026lt; lower or x \u0026gt; upper] print( \u0026#39; Identified outliers: %d\u0026#39; % len(outliers)) # remove outliers outliers_removed = [x for x in data if x \u0026gt;= lower and x \u0026lt;= upper] print( \u0026#39; Non-outlier observations: %d\u0026#39; % len(outliers_removed)) Chạy code trên, đầu tiên nó sẽ in ra số lượng Outliers, sau đó là số lượng mẫu sau khi đã loại bỏ Outliers.\nIdentified outliers: 29 Non-outlier observations: 9971 Bộ dữ liệu mà chúng ta đang sử dụng trong ví dụ này chỉ có 1 chiều (univariate), nhưng phương pháp này hoàn toàn có thể mở rộng ra được với dữ liệu nhiều chiều.\n2.2 Phương pháp sử dụng khoảng tứ phân vị - Interquartile Range a, Nhắc lại một chút về trung bình, trung vị, tứ phân vị. Giả sử ta có dãy số sau: 6, 5, 8, 7, 12, 13, 15, 14, 2, 200, 1. Câu hỏi đặt ra là tìm giá trị trung bình, trung vị, tứ phân vị của dãy số đó.\n Giá trị trung bình  Giá trị trung bình chính là tổng của tất cả các số, chia cho số lượng số, ở đây số lượng số là 11 số, như vậy giá trị trung bình cộng sẽ là:\n$Mean = \\frac{6+5+8+7+12+13+15+14+2+200+1}{11} = 25.72$\n  Giá trị trung vị  Bước 1: Sắp xếp dãy số ở trên theo thứ tự tăng dần, ta được kết quả:1, 2, 5, 6, 7, 8, 12, 13, 14, 15, 200\nBước 2: Trung vị là giá trị đứng ở vị trí giữa trong một dãy số đã được sắp xếp có thứ tự. Trước và sau trị số trung vị sẽ có 50% quan sát. Dãy số ở trên có 11 số (1, 2, 5, 6, 7, 8, 12, 13, 14, 15, 200) =\u0026gt; Số ở chính giữa là 8 sẽ chia đôi bộ số làm 2, bên trái nó có 5 số, bên phải nó có 5 số, =\u0026gt; 8 chính là số trung vị của tập hợp ở trên( trung vị cũng còn được gọi là tứ phân vị thứ nhì).\nĐiểm khác nhau cơ bản của giá trị trung vị trong việc mô tả dữ liệu so với giá trị trung bình là nó không bị sai lệch bởi một tỷ lệ nhỏ các giá trị cực lớn hoặc cực nhỏ (outliers), và do đó nó cung cấp một đại diện tốt hơn về giá trị đặc trưng.\nTrung vị của một danh sách hữu hạn các số là số ở giữa, khi các số đó được liệt kê theo thứ tự từ nhỏ nhất đến lớn nhất. Nếu tập dữ liệu có số lượng quan sát lẻ, thì tập ở giữa được chọn. Ví dụ: danh sách bảy số sau 1, 3, 3, 6, 7, 8, 9 có giá trị trung vị là số 6. Nếu tập hợp có số lượng quan sát là chẵn thì không có giá trị giữa. Khi đó, giá trị trung vị thường được xác định là giá trị trung bình của hai giá trị giữa. Ví dụ, tập dữ liệu 1, 2, 3, 4, 5, 6, 8, 9 có giá trị trung vị là 4.5 nghĩa là (4 + 5) / 2.\n Giá trị tứ phân vị  Điểm tứ phân vị (interquartile) là giá trị bằng số phân chia một nhóm các kết quả quan sát bằng số thành bốn phần, mỗi phần có số liệu quan sát bằng nhau(=25% số kết quả quan sát). Tứ phân vị có 3 giá trị, đó là tứ phân vị thứ nhất (Q1), thứ nhì (Q2) và thứ ba (Q3). Ba giá trị này chia một tập hợp dữ liệu (đã sắp xếp theo thứ tự từ từ bé đến lớn) thành 4 phần có số lượng quan sát đều nhau.\nXem lại dãy số 11 số ở trên của chúng ta (1, 2, 5, 6, 7, 8, 12, 13, 14, 15, 200):\nGiá trị tứ phân vị thứ nhất Q1 bằng trung vị phần dưới, phần dưới là các số (1, 2, 5, 6, 7), là số 5.\nGiá trị tứ phân vị thứ hai Q2 chính bằng giá trị trung vị, là số 8.\nGiá trị tứ phân vị thứ ba Q3 bằng trung vị phần trên (12, 13, 14, 15, 200), là số 14.\nb, Mô tả phương pháp Trong thực tế, dữ liệu của chúng ta hiếm khi nào tuân theo phân phối Gaussian. Vì thế, phương pháp sử dụng Mean và STD không thể sử dụng được trong những trường hợp đó. Lúc này, tứ phân vị sẽ phát huy tác dụng. Phương pháp Interquartile Range, viết tắt là IQR, bao gồm các bước sau:\n Bước 1:  Tính toán sai số giữa tứ phân vị thứ 3 và tứ phân vị thứ nhất: IQR = Q3 - Q1\n Bước 2:  Tính toán giá trị cut_off bằng cách nhân IQR với hệ số k. Giá trị của k thể hiện mức độ Outlier của dữ liệu. Giá trị thông thường của nó là 1.5: cut_off = IQR * k.\n Bước 3:  Tính toán giá trị giới hạn trên và giới hạn dưới của Outlier: lower, upper = Q1 - cut_off, Q3 + cut_off.\n Bước 4:  Các mẫu có giá trị nằm ngoài khoảng giới hạn bởi [lower; upper] được coi là Outlier.\nCode thực hiện phương pháp này như sau:\n# identify outliers with interquartile range from numpy.random import seed from numpy.random import randn from numpy import percentile # seed the random number generator seed(1) # generate univariate observations data = 5 * randn(10000) + 50 # calculate interquartile range Q1, Q3 = percentile(data, 25), percentile(data, 75) iqr = Q3 - Q1 print( \u0026#39; Interquartile: Q1=%.3f, Q3=%.3f, IQR=%.3f\u0026#39; % (Q1, Q3, iqr)) # calculate the outlier cutoff cut_off = iqr * 1.5 lower, upper = Q1 - cut_off, Q3 + cut_off # identify outliers outliers = [x for x in data if x \u0026lt; lower or x \u0026gt; upper] print( \u0026#39; Identified outliers: %d\u0026#39; % len(outliers)) # remove outliers outliers_removed = [x for x in data if x \u0026gt;= lower and x \u0026lt;= upper] print( \u0026#39; Non-outlier observations: %d\u0026#39; % len(outliers_removed)) Kết quả chạy code trên, đầu tiên sẽ in ra giá trị của tứ phân vị thứ nhất và thứ 3, sau đó là số lượng Outliers, và cuối cùng là số lượng mẫu còn lại trong tập dữ liệu sau khi đã xóa đi các Outliers:\nInterquartile: Q1=46.685, Q3=53.359, IQR=6.674 Identified outliers: 81 Non-outlier observations: 9919 Đối với tập dữ liệu nhiều chiều, cách tiếp cận của phương pháp này cũng hoàn toàn tương tự.\n2.3 Phương pháp sử dụng Local Outlier Factor (LOF) LOF là một kỹ thuật khai thác ý tưởng về việc sử dụng các mẫu lân cận để phát hiện ngoại lệ. Mỗi mẫu sẽ được gán cho một giá trị Score thể hiện mức độ cô lập hoặc khả năng nó có thể là Outlier dựa trên quy mô của vùng lân cận của nó. Những mẫu có giá trị Score lớn nhất có nhiều khả năng là Outliers.\nThư viện Scikit-learn cung cấp lớp LocalOutlierFactor giúp chúng ta đơn giản hóa việc thực hiện phương pháp này.\nDưới đây, chúng ta sẽ sử dụng bộ dữ liệu Boston Housing Dataset để huấn luyện mô hình LinearRegression theo 2 cách: không xóa bỏ Outliers và có xóa bỏ Outliers sử dụng phương pháp LOF. Kết quả của 2 cách đó sẽ được mang ra so sánh với nhau.\na, Cách 1 # evaluate model on the raw dataset from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error # load the dataset df = read_csv( \u0026#39;housing.csv\u0026#39; , header=None) # retrieve the array data = df.values # split into input and output elements X, y = data[:, :-1], data[:, -1] # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # fit the model model = LinearRegression() model.fit(X_train, y_train) # evaluate the model yhat = model.predict(X_test) # evaluate predictions mae = mean_absolute_error(y_test, yhat) print( \u0026#39;MAE: %.3f\u0026#39; % mae) Kết quả thực hiện:\nMAE: 3.417 b, Cách 2 # evaluate model on training dataset with outliers removed from pandas import read_csv from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.neighbors import LocalOutlierFactor from sklearn.metrics import mean_absolute_error # load the dataset df = read_csv( \u0026#39; housing.csv \u0026#39; , header=None) # retrieve the array data = df.values # split into input and output elements X, y = data[:, :-1], data[:, -1] # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # summarize the shape of the training dataset print(X_train.shape, y_train.shape) # identify outliers in the training dataset lof = LocalOutlierFactor() yhat = lof.fit_predict(X_train) # select all rows that are not outliers mask = yhat != -1 X_train, y_train = X_train[mask, :], y_train[mask] # summarize the shape of the updated training dataset print(X_train.shape, y_train.shape) # fit the model model = LinearRegression() model.fit(X_train, y_train) # evaluate the model yhat = model.predict(X_test) # evaluate predictions mae = mean_absolute_error(y_test, yhat) print( \u0026#39;MAE: %.3f\u0026#39; % mae) Kết quả thực hiện:\n(339, 13) (339,) (305, 13) (305,) MAE: 3.356 Số lượng mẫu giảm từ 339 -\u0026gt; 305 sau khi xóa bỏ Outliers theo phương pháp LOF. Giá trị MAE giảm từ 3.417 -\u0026gt; 3.356. Đó là một sự cải thiện hiệu suất đáng kể của model.\nNgoài LocalOutlierFactor, Scikit-learn còn cung cấp 1 lớp khác là IsolationForest (tất nhiên là thuật toán cũng khác) để loại bỏ Outliers. Cách sử dụng thì 2 cách hoàn toàn giống nhau. Bạn có thể thử thay LocalOutlierFactor bằng IsolationForest vào code trên rồi chạy lại xem kết quả như thế nào?\n3. Kết luận Kết thúc bài thứ 3 trong chuỗi bài viết về chủ đề Data Preparation cho ML model. Trong bài này, chúng ta đã tìm hiểu về Outliers và phương pháp xử lý chúng.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ tiếp tục tìm hiểu về một vấn đề khác của Data Cleaning, đó là nhận diện và xử lý vấn đề dữ liệu bị thiếu - Missing Data. Mời các bạn đón đọc.\n4. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/77_data_prepeation_for_ml_data_cleaning_outlier/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Outlier"},{"categories":["Machine Learning","Data Preparation"],"contents":"Bài thứ 2 trong chuỗi các bài viết về chủ đề Data Preparation cho các mô hình ML. Trong bài này, chúng ta sẽ bàn về nhiệm vụ Data Cleaning, tiếng việt gọi là làm sạch dữ liệu.\nData Cleaning là một bước rất quan trong trong bất kỳ dự án ML nào. Trong dữ liệu dạng bảng (tabular data), có nhiều kỹ thuật phân tích thống kê và trực quan hóa dữ liệu khác nhau mà bạn có thể sử dụng để khám phá dữ liệu của mình nhằm xác định các thao tác làm sạch dữ liệu mà bạn có thể muốn thực hiện.\nTrước khi chuyển sang các phương pháp phức tạp, có một số thao tác rất cơ bản mà bạn có thể nên thực hiện trên mọi dự án học máy. Những điều này cơ bản đến mức chúng thường bị các kỹ sư AI/Ml dày dạn kinh nghiệm bỏ qua, nhưng lại rất quan trọng đến mức nếu bỏ qua, các mô hình có thể hoạt động một cách thiếu chính xác hoặc không thể sử dụng được.\nChúng ta sẽ sử dụng 2 bộ dữ liệu Oil Spill Dataset và Iris Flowers Dataset để thực hành. Chúng đều là những bộ dữ liệu tiêu chuẩn cho việc học tập ML.\n1. Nhận diện và xóa các features chỉ chứa 1 giá trị (tất cả giá trị trong các hàng của 1 cột đều bằng nhau) Các features mà chỉ có một giá trị duy nhất chắc chắn không có ý nghĩa cho việc mô hình hóa. Những features như vậy được gọi là zero-variance, bởi vì nếu chúng ta đo phương sai của chúng, sẽ thu được giá trị là 0.\nHình dưới đây là ví dụ về features kiểu này:\n 1.1 Sử dụng Numpy Để phát hiện ra các features này, chúng ta có thể sử dụng hàm unique() của Numpy, nó sẽ cho ta biết số lượng của các giá trị duy nhất trong mỗi cột. Ví dụ: 2 2 -\u0026gt; 1, 2 3 -\u0026gt; 2, 4 5 4 -\u0026gt; 2, \u0026hellip;\n# summarize the number of unique values for each column using numpy from numpy import loadtxt from numpy import unique # load the dataset data = loadtxt( \u0026#39; oil-spill.csv \u0026#39; , delimiter= \u0026#39; , \u0026#39; ) # summarize the number of unique values in each column for i in range(data.shape[1]): print(i, len(unique(data[:, i])) Chạy code trên sẽ in ra kết quả như sau:\n0 238 1 297 2 927 3 933 4 179 5 375 6 820 7 618 8 561 9 57 10 577 11 59 12 73 13 107 14 53 15 91 16 893 17 810 18 170 19 53 20 68 21 9 22 1 23 92 24 9 25 8 26 9 27 308 28 447 29 392 30 107 31 42 32 4 33 45 34 141 35 110 36 3 37 758 38 9 39 9 40 388 41 220 42 644 43 649 44 499 45 2 46 937 47 169 48 286 49 2 Nhìn vào đây, ta có thể thấy ngay rằng cột 22 chỉ chứa 1 giá trị duy nhất. Vì thế, nó nên được loại bỏ khỏi dataset.\n1.2 Sử dụng Pandas Một cách làm khác là sử dụng hàm nunique() của Pandas như dưới đây:\n# summarize the number of unique values for each column using numpy from pandas import read_csv # load the dataset df = read_csv( \u0026#39; oil-spill.csv \u0026#39; , header=None) # summarize the number of unique values in each column print(df.nunique()) Chạy code trên sẽ thu được kết quả tương tự như Numpy.\nĐể xóa những cột chỉ chứa 1 giá trị duy nhất, ta thực hiện như sau:\n# delete columns with a single unique value from pandas import read_csv # load the dataset df = read_csv( \u0026#39; oil-spill.csv \u0026#39; , header=None) print(df.shape) # get number of unique values for each column counts = df.nunique() # record columns to delete to_del = [i for i,v in enumerate(counts) if v == 1] print(to_del) # drop useless columns df.drop(to_del, axis=1, inplace=True) print(df.shape) Kết quả thực hiện:\n(937, 50) [22] (937, 49) 1.3 Sử dụng Scikit-learn Thư viện Sckit-learn cung cấp lớp VarianceThreshold giúp ta nhận ra các cột chỉ có 1 giá trị dựa vào tính chất của phương sai. Nếu tất cả các giá trị trong một cột đều bằng nhau thì phương sai của chúng bằng 0. Cách sử dụng như sau:\n# example of applying the variance threshold for feature selection from pandas import read_csv from sklearn.feature_selection import VarianceThreshold # load the dataset df = read_csv( \u0026#39;oil-spill.csv\u0026#39; , header=None) # split data into inputs and outputs data = df.values X = data[:, :-1] y = data[:, -1] print(X.shape, y.shape) # define the transform transform = VarianceThreshold(threshold=0) # transform the input data X_sel = transform.fit_transform(X) print(X_sel.shape) Một instance của lớp VarianceThreshold được tạo với tham số threshold=0 được cung cấp (đây là giá trị mặc định, bạn có thể bỏ qua không cần chỉ ra nếu muốn sử dụng giá trị mặc định này). Sau đó, nó dược fit và áp dụng vào tập dữ liệu thông qua hàm fit_transform() để tạo ra một phiên bản đã biến đổi của dữ liệu mà ở đó, các cột có phương sai không lớn hơn giá trị threshold sẽ bị xóa bỏ.\nKết quả chạy code trên như sau:\n(937, 49) (937,) (937, 48) Trước, 50 cột. Sau khi biến đổi còn lại 49 cột.\nBan đầu, dataset có 50 cột, cột số 22 chỉ chứa 1 giá trị duy nhất nên bị xóa, còn lại 49 cột trong dataset.\n2. Xem xét các cột có rất ít giá trị (rất nhiều giá trị trong các hàng của 1 cột trùng nhau) Quan sát lại kết quả từ phần 1, ta thấy rằng có những cột có rất ít giá trị. Ví dụ, các cột 45, 32, và 38 chỉ có 2,4, và 9 giá trị không trùng nhau, tương ứng theo thứ tự. Những cột mà có ít giá trị như thế này, theo lý thuyết nên có dạng Ordinal hoặc Categorical. Tuy nhiên, trong trường hợp này, Oil Spill Dataset chỉ chứa giá trị dạng số nên hiện tượng này có thể coi là bất thường. Near-zero là tên gọi được đặt cho chúng bởi vì nếu tính toán thì giá trị phương sai của chúng rất gần 0.\n2.1 Sử dung Numpy Để có cái nhìn rõ hơn, chúng ta sẽ tính phần trăm số lượng giá trị không trùng của mỗi cột theo tổng số hàng và in ra những cột có phần trăm nhỏ hơn 1:\n# summarize the percentage of unique values for each column using numpy from numpy import loadtxt from numpy import unique # load the dataset data = loadtxt( \u0026#39; oil-spill.csv \u0026#39; , delimiter= \u0026#39; , \u0026#39; ) # summarize the number of unique values in each column for i in range(data.shape[1]): num = len(unique(data[:, i])) percentage = float(num) / data.shape[0] * 100 if percentage \u0026lt; 1: print( \u0026#39; %d, %d, %.1f%%\u0026#39; % (i, num, percentage)) Kết quả:\n21, 9, 1.0% 22, 1, 0.1% 24, 9, 1.0% 25, 8, 0.9% 26, 9, 1.0% 32, 4, 0.4% 36, 3, 0.3% 38, 9, 1.0% 39, 9, 1.0% 45, 2, 0.2% 49, 2, 0.2% 11 cột trong tổng số 50 cột có giá trị phần trăm không lớn hơn 1. Điều này, ko có nghĩa là chúng ta nên xóa bỏ chúng ngay lập tức như đối với các cột chỉ có 1 giá trị, mà chúng ta nên xem xét thực hiện các lựa chọn sau:\n Chuyển sang dạng Ordinal Chuyển sang dạng Categorical Xóa bỏ  Hai lựa chọn đầu có thể căn cứ vào thông tin về dataset để quyết định có thực hiện hay không? Thực hiện các lựa chọn một cách độc lập và so sánh kết quả của model trong mỗi trường hợp với nhau xem lựa chọn nào mang lại kết quả tốt nhất.\n2.2 Sử dụng Pandas Code dưới đây sử dụng Pandas để tính toán và xóa bỏ 11 cột có phần trăm không lớn hơn 1:\n# delete columns where number of unique values is less than 1% of the rows from pandas import read_csv # load the dataset df = read_csv( \u0026#39; oil-spill.csv \u0026#39; , header=None) print(df.shape) # get number of unique values for each column counts = df.nunique() # record columns to delete to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) \u0026lt; 1] print(to_del) # drop useless columns df.drop(to_del, axis=1, inplace=True) print(df.shape) Kết quả thực hiện:\n(937, 50) [21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49] (937, 39) Trước, 50 cột. Xóa các cột [21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49]. Còn lại, 39 cột.\n2.3 Sử dụng Scikit-learn Vẫn theo tính chất của phương sai, ta biết rằng, những cột mà có rất ít giá trị thì phương sai của chúng rất nhỏ, chỉ lớn hơn 0.0 một chút xíu. Vì thế, tương tự như mục 1.3, ta có thể sử dụng lớp VarianceThreshold() với giá trị threshold \u0026gt; 0.0 để loại bỏ những cột đó.\n# explore the effect of the variance thresholds on the number of selected features from numpy import arange from pandas import read_csv from sklearn.feature_selection import VarianceThreshold from matplotlib import pyplot # load the dataset df = read_csv( \u0026#39; oil-spill.csv \u0026#39; , header=None) # split data into inputs and outputs data = df.values X = data[:, :-1] y = data[:, -1] print(X.shape, y.shape) # define thresholds to check thresholds = arange(0.0, 0.55, 0.05) # apply transform with each threshold results = list() for t in thresholds: # define the transform transform = VarianceThreshold(threshold=t) # transform the input data X_sel = transform.fit_transform(X) # determine the number of input features n_features = X_sel.shape[1] print( \u0026#39; \u0026gt;Threshold=%.2f, Features=%d\u0026#39; % (t, n_features)) # store the result results.append(n_features) # plot the threshold vs the number of selected features pyplot.plot(thresholds, results) pyplot.show() Trong ví dụ trên, ta cho threshold nhận giá trị thay đổi từ 0.0 đến 0.55, với bước nhảy 0.05. Kết quả thực hiện:\n(937, 49) (937,) \u0026gt;Threshold=0.00, Features=48 \u0026gt;Threshold=0.05, Features=37 \u0026gt;Threshold=0.10, Features=36 \u0026gt;Threshold=0.15, Features=35 \u0026gt;Threshold=0.20, Features=35 \u0026gt;Threshold=0.25, Features=35 \u0026gt;Threshold=0.30, Features=35 \u0026gt;Threshold=0.35, Features=35 \u0026gt;Threshold=0.40, Features=35 \u0026gt;Threshold=0.45, Features=33 \u0026gt;Threshold=0.50, Features=31 Ta thấy, ban đầu có 50 cột, và số lượng cột giảm dần khi tăng giá trị của threshold. Đồ thị sau thể hiện mối quan hệ giữa threshold và số lượng cột còn lại trong dataset sau khi biến đổi.\n 3. Nhận diện và xóa các hàng trùng nhau Các hàng có dữ liệu giống hệt nhau gần như là vô nghĩa đối với quá trình mô hình hóa, thậm chí là gây ra những tác động tiêu cực trong quá trình đánh giá mô hình. Bởi vì, cùng một dữ liệu nhưng có thể xuất hiện trong cả 2 tập Train và Test. Về mặt xác suất, chúng ta có thể coi sự trùng lặp như là 1 sự điều chỉnh phân phối dữ liệu và nó có thể có 1 chút lợi ích nếu bạn muốn mô hình của mình thiên vị theo hướng có lợi cho bạn (purposefully bias). Tuy nhiên, trong hầu hết trường hợp, ML model được mong đợi hoạt động một cách minh bạch, chính xác nhất có thể. Vì thế, một cách hiển nhiên, chúng ta cần tìm ra và xóa bỏ chúng khỏi tập dữ liệu trước khi tiến hành huấn luyện mô hình.\nĐể kiểm tra dữ liệu trùng lặp trong các hàng, ta có thể sử dụng dung hàm dupplicate() của Pandas như sau:\n# locate rows of duplicate data from pandas import read_csv # load the dataset df = read_csv( \u0026#39;iris.csv\u0026#39; , header=None) # calculate duplicates dups = df.duplicated() # report if there are any duplicates print(dups.any()) # list all duplicate rows print(df[dups]) Kết quả thực hiện:\nTrue 0 1 2 3 4 34 4.9 3.1 1.5 0.1 Iris-setosa 37 4.9 3.1 1.5 0.1 Iris-setosa 142 5.8 2.7 5.1 1.9 Iris-virginica Đầu tiên, kết quả kiểm tra xem có dự trùng lặp dữ liệu hay không được báo cáo. Trong trường hợp này là có (True). Tiếp theo sau là danh sách các hàng bị trùng dữ liệu với 1 hàng nào đó phía trên. Cụ thể, dữ liệu ở hàng 34 bị trùng với 1 hàng nào đó từ 0-\u0026gt;33, hàng 37 bị trùng với 1 hàng nào đó từ 0-\u0026gt;36, hàng 142 bị trùng với 1 hàng nào đó từ 0-\u0026gt;141.\nBiết được kết quả kiểm tra trùng lặp dữ liệu rồi, chúng ta sẽ tiến hành xóa chúng đi, sử dụng hàm drop_duplicates() của Pandas như sau:\n# delete rows of duplicate data from the dataset from pandas import read_csv # load the dataset df = read_csv( \u0026#39;iris.csv\u0026#39; , header=None) print(df.shape) # delete duplicate rows df.drop_duplicates(inplace=True) print(df.shape) Kết quả thực hiện:\n(150, 5) (147, 5) Ban đầu, 150 hàng. Sau khi xóa các hàng trùng nhau, còn lại 147 hàng.\n4. Kết luận Mình sẽ kết thúc bài này tại đây. Trong bài này, chúng ta đã khám phá một số lỗi cơ bản của dữ liệu và phương pháp xử lý chúng.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ tiếp tục tìm hiểu về một vấn đề khác của Data Cleaning, đó là nhận diện và xử lý dữ liệu Outlier . Mời các bạn đón đọc.\n5. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/76_data_prepeation_for_ml_data_cleaning_basic/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Data Cleaning - Các xử lý cơ bản"},{"categories":["Machine Learning","Data Preparation"],"contents":"Series các bài viết tiếp theo, mình sẽ tập trung vào chủ đề chuẩn bị dữ liệu (Data Preparation) để huấn luyện các mô hình ML (DP4ML). Đây có thể coi là bước quan trọng nhất để tạo ra được một ML model có độ chính xác cao. Có rất nhiều kiến thức, thuật toán, kỹ thuật có thể được áp dụng ở phần này. Mình sẽ cùng các bạn đi từng bước, khám phá dần dần để giúp bạn dễ dàng nắm bắt và áp dụng những gì học được vào giải quyết các bài toán của bạn.\nBài đầu tiên, mình sẽ chủ yếu nhắc lại một số kiến thức, khái niệm chung về ML, cũng như công đọan Data Preparation.\n1. Giới thiệu chung 1.1 Các bước xây dựng Machine Learning model Mỗi dự án ML nói chung đều bao gồm một số bước thực hiện giống nhau (cùng chiến lược), nhưng khác nhau vì dữ liệu cụ thể của mỗi dự án. Vì thế mà các kỹ thuật xử lý tại mỗi bước cũng sẽ khác nhau đối với từng dự án (khác chiến thuật). Điều này làm cho mỗi dự án học máyMLw trở thành duy nhất. Không ai có thể cho bạn biết kết quả tốt nhất là gì hay làm thế nào để đạt được nó. Bạn phải thiết lập một giá trị hiệu suất làm điểm tham chiếu, sau đó so sánh với hiệu suất từ tất cả các mô hình mà bạn có thể xây dựng được theo các cách thức, thuật toán khác nhau. Dựa vào các kết quả so sánh đó mà bạn có thể kết luận được rằng mô hình nào là tốt nhất, tối ưu nhất trên tập dữ liệu bạn có. Tất nhiên, bạn không hề đơn độc trong quá trình làm việc của mình. Các kiến thức, kỹ thuật đã được đúc kết, bạn có thể lấy ra sử dụng rất dễ dàng. Việc của bạn chỉ là khi nào thì lấy ra kỹ thuật nào mà thôi.\nChiến lược của các dự án ML có thể được tóm tắt trong 4 bước như sau: Định nghĩa vấn đề, Chuẩn bị dữ liệu, Đánh giá mô hình và Hoàn thiện mô hình.\na, Bước 1 - Định nghĩa vấn đề Bước này liên quan đến việc tìm hiểu đầy đủ về dự án để chọn lựa thuật toán, và chỉ tiêu đánh giá phù hợp. Ví dụ, đó là bài toán phân loại (classification) - đánh giá bằng độ chính xác hay hồi quy (regression) - đánh giá bằng chỉ số MSE? Nó cũng liên quan đến việc định hướng thu thập dữ liệu, là tiền đề cho bước tiếp theo.\nb, Bước 2 - Thu thập dữ liệu Khi đã nhận thức rõ ràng vấn đề cần giải quyết, chúng ta sẽ bắt tay vào việc tìm kiếm, thu thập dữ liệu. Dữ liệu có thể đến từ nhiều nguồn với các định dạng khác nhau. Vì thế, cần phải có kế hoạch lưu trữ, xử lý, quản lý cho phù hợp.\nc, Bước 3 - Chuẩn bị dữ liệu Bước này liên quan đến việc chuyển đổi dữ liệu thô đã được thu thập thành một dạng có thể được sử dụng trong mô hình hóa. Các kỹ thuật tiền xử lý dữ liệu thường đề cập đến việc bổ sung, xóa hoặc chuyển đổi dữ liệu thành các dạng khác nhau. Bạn cũng nên hỏi ý kiến chuyên gia tại bước này để tìm ra những thông tin hữu ích nhất trong đống dữ liệu của mình.\nd, Bước 4 - Lựa chọn và đánh giá mô hình Bước này liên quan đến việc lựa chọn và đánh giá các mô hình học máy trên tập dữ liệu của bạn. Có rất nhiều thuật toán ML, và bạn thường không thể biết chính xác thuật toán nào sẽ giải quyết tốt cho bài toán của bạn. Các thông thường là xây dựng một mô hình làm cơ sở (base model) tham chiếu cho các mô hình khác. Các mô hình sau đó phải có giá trị metrics lớn hơn Base Model mới được xem xét sử dụng. Đối với những tập dữ liệu lớn, có thể chia nhỏ ra thành nhiều phần, sử dụng chiến thuật k_Folds để có thể nhanh chóng đạt được kết quả đánh giá.\ne, Bước 5- Hoàn thiện mô hình Sau khi có kết quả đánh giá và lựa chọn được mô hình phù hợp nhất với bài toán, ta tiến hành huấn luyện mô hình đó trên toàn bộ tập dữ liệu lớn. Model được huấn luyện xong và đáp ứng được các yêu cầu cụ thể đặt ra có thể được mang đi tích hợp vào các hệ thống khác để sử dụng. Quá trình này có thể được chia ra thành nhiều công đoạn nhỏ hơn như Giám sát, Cập nhật, \u0026hellip;\nTiếp theo, mình sẽ đi chi tiết vào bước 3.\n1.2 Chuẩn bị dữ liệu - Data Preparation là gì? Như chúng ta đã biết, các ML model không thể sử dụng trực tiếp dữ liệu thô sau khi được thu thập. Một số lý do là:\n Các thuật toán ML yêu cầu dữ liệu phải ở dạng số - Numbers Dữ liệu thô có thể chứa các loại nhiễu, lỗi có thể gây ra ảnh hưởng tiêu cực đến quá trình học của các mô hình ML  Do đó, dữ liệu thô cần phải trải qua một bước tinh chỉnh trước khi đưa vào mô hình để huấn luyện. Quá trình này thường được gọi tên là Data Preparation. Một số tên khác cũng có thể được sử dụng là Data Wrangling, Data Cleaning, Data Preprocessing, Feature Engineering, \u0026hellip; mặc dù những cái tên này chính xác hơn là để chỉ các công việc cần hoàn thành trong quá trình Data Preparation.\nĐi vào chi tiết hơn, các công việc cần phải thực hiện trong quá trình Data Preparation là:\na, Data Cleaning Nhận diện và sửa các lỗi như thiếu hụt (missing) dữ liệu, dữ liệu bất thường (outlier), dữ liệu trùng lặp (dupplicate), \u0026hellip;\n b, Feature Selection Tìm ra mỗi liên hệ giữa các features và nhãn cần dự đoán của bài toán. Từ đó bỏ đi các features có sự liên quan ít đi để mô hình có thể tập trung vào những features có giá trị.\n c, Data Transforms Thay đổi kiểu, phạm vi và phân phối của dữ liệu.\n d, Feature Engineering Tạo ra các features mới từ dữ liệu hiện có.\ne, Dimensionality Reduction Ánh xạ dữ liệu lên một không gian mới để giảm kích thước của dữ liệu.\n Mỗi công việc này đều đòi hỏi những kỹ năng cụ thể, hiểu biết về công cụ, kỹ thuật áp dụng để có thể cho ra kết quả tốt nhất. Chúng ta sẽ đi chi tiết trong các bài sau.\n1.3 Làm thế nào để biết nên áp dụng kỹ thụật Data Preparation nào? Đứng trước một tập dữ liệu thô vừa được thu thập, bạn phân vân không biết nên làm như thế nào? bắt đầu từ đâu? Dưới đây là một vài gợi ý:\n Tìm hiễu kỹ càng yêu cầu bài toán và thông tin về dữ liệu thô  Việc này là bước khởi đầu để bạn có cái nhìn tổng thể về dữ liệu, giúp bạn trả lời các câu hỏi: dữ liệu có đúng/đủ không? liệu có sai sót gì trong quá trình thu thập không? \u0026hellip; Từ đó, giúp bạn loại đi được những dữ liệu không phù hợp và ý niệm về việc cần phải có dữ liệu tinh ở dạng như thế nào?\n Sử dụng phương pháp thống kê, phân tích  Các công cụ thống kê như Mean, Standard Deviation, Sum, Count, \u0026hellip; có thể giúp bạn nhận ra dữ liệu có bị thiếu hay không? Đang được phân phối như thế nào? Ở dạng Numbers hay Categories, \u0026hellip; Biết được những điều này rồi, bạn chỉ cần chọn kỹ thuật xử lý tương ứng.\n Sử dụng phương pháp trực quan hóa dữ liệu  Biểu diễn lên đồ thị cũng là 1 cách hiệu quả để hiểu hơn về dữ liệu. Các vấn đề như mức độ tương quan giữa các features, có tồn tại hay không dữ liệu Outlier, \u0026hellip; đều có thể nhận biết được thông qua đồ thị.\n2. Phân tích các chiến lược cho Data Preparation 2.1 Naive Approach Data Preparation là quá trình chuyển đổi dữ liệu thô thành một dạng thích hợp để mô hình hóa. Một cách tiếp cận đơn giản (Naive Approach) là áp dụng biến đổi trên toàn bộ tập dữ liệu trước khi phân chia thành tập Train và Test để huấn luyện và đánh giá hiệu suất của mô hình. Điều này có thể dẫn đến một vấn đề gọi là rò rỉ dữ liệu (Data Leakage), trong đó thông tin của dữ liệu trong tập Test bị rò rỉ vào tập Train. Việc đó làm cho sự ước tính độ chính xác về hiệu suất của mô hình khi đưa ra dự đoán trên dữ liệu mới sẽ không còn đáng tin cậy nữa.\nCode ví dụ cho chiến lược này như sau:\n# naive approach to normalizing the data before splitting the data and evaluating the model from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # define dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7) # standardize the dataset scaler = MinMaxScaler() X = scaler.fit_transform(X) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # fit the model model = LogisticRegression() model.fit(X_train, y_train) # evaluate the model yhat = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print( \u0026#39;Accuracy: %.3f\u0026#39; % (accuracy*100)) Đoạn code trên thực hiện các việc: tạo dữ liệu giả, chuẩn hóa dữ liệu, chia dữ liệu thành 2 tập Train và Test, huấn luyện và đánh giá mô hình.\nKết quả chạy thu được độ chính xác là 84.848%. Khá cao, nhưng ta biết rằng cách tiếp cận này tiềm ẩn nguy cơ như đã phân tích nên ta không nên sử dụng kết quả này.\n2.2 Standard Approach Chiến lược chuẩn của Data Preparation sẽ như sau:\n Chia dữ liệu thành 2 tập Train và Test Huấn luyện (Fit) các thuật toán của Data Preparation chỉ trên tập Train Áp dụng các thuật toán của Data Preparation đã huấn luyện vào cả 2 tập Train và Test Huấn luyện model trên tập Train Đánh giá model trên tập Test  Code ví dụ:\n# correct approach for normalizing the data after the data is split before the model is evaluated from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # define dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7) # split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # define the scaler scaler = MinMaxScaler() # fit on the training dataset scaler.fit(X_train) # scale the training dataset X_train = scaler.transform(X_train) # scale the test dataset X_test = scaler.transform(X_test) # fit the model model = LogisticRegression() model.fit(X_train, y_train) # evaluate the model yhat = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, yhat) print( \u0026#39;Accuracy: %.3f\u0026#39; % (accuracy*100)) Đọan code này thực hiện các công việc: tạo dữ liệu giả, chia dữ liệu thành 2 tập Train và Test, chuẩn hóa dữ liệu, huấn luyện và đánh giá mô hình. Kết quả chạy code cho ra Accuracy: 84.455.\nChú ý: Đối với cách phân chia dữ liệu theo kiểu k-Fold, những phân tích về 2 chiến lược trên vẫn đúng. Mình có cả code ví dụ cho phần này, bạn có thể xem trên github của mình.\n3. Kết luận Kết thúc bài đầu tiên về chủ đề Data Preparation cho các ML model. Chúng ta đã được ôn lại các bước để thực hiện huấn luyện một mô hình ML, chi tiết các công việc cần thực hiện và các chiến lược cho phần Data Preparation.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ đi sâu vào các kỹ thuật giải quyết công việc Data Cleaning . Mời các bạn đón đọc.\n4. Tham khảo [1] Jason Brownlee, \u0026ldquo;Data Preparation for Machine Learning\u0026rdquo;, Book: https://machinelearningmastery.com/data-preparation-for-machine-learning/.\n","permalink":"https://tiensu.github.io/blog/75_data_prepeation_for_ml_foundation/","tags":["Machine Learning","Data Preparation"],"title":"DP4ML - Những kiến thức cơ bản"},{"categories":["Recommender System"],"contents":"Đây là bài thứ 3 trong chuỗi bài viết về Recommender System. Trong bài này, chúng ta sẽ thực hành xây dựng một RS sử dụng kỹ thuật Deep Learning.\nNgày nay, Deep Learning đã đi sâu vào rất nhiều lĩnh vực, giải quyết được rất nhiều bài toán khó trong thực tế, từ Computer Vision cho đến Natual Language Processing. Recommender System cũng được hưởng lợi từ sự phát triển mạnh mẽ đó. Có thể nói rằng, với sự tham gia của Deep Learning, các RS hiện đại ngày nay như Youtube, Amazon, Netflix, Facebook, \u0026hellip; đã đạt được State-of-Art, bỏ xa các phương pháp truyền thống trong cuộc đua mang lại lợi ich thiết thực cho các doanh nghiệp.\n1. Đặt vấn đề\nMột website chiếu phim muốn thu hút nhiều User hơn. Họ muốn bạn - 1 Data Scientist giúp họ xây dựng một RS để đề xuất các bộ phim phù hợp cho người dùng mỗi khi họ ghé vào website của họ. Sau khi xem xét kỹ yêu cầu bài toán và dữ liệu hiện có, bạn quyết định xây dựng RS sử dụng kỹ thuật Deep Learning.\n2. Chuẩn bị dữ liệu\nDownload dữ liệu tại đây.\nCó rất nhiều thông tin về các bộ phim được lưu rải rác trong các file khác nhau: tên, thể loại, diễn viên, đạo diễn, năm sản xuất, quốc gia sản xuất, đường dẫn xem phim, \u0026hellip; Bộ dataset này được thu thập từ năm 1995 đến năm 2015 và được cung cấp bởi MovieLends. Ở bài này, chúng ta chỉ tập trung vào thông tin Rating trong file rating.csv.\n 3. Data Preprocessing\nVì kích thước của bộ dữ liệu này khá lớn, nên nếu máy tính của bạn không đủ mạnh thì sẽ không thể xử lý hết được, hoặc là sẽ mất rất nhiều thời gian. Máy tính của mình là Core i7, 16GB RAM, GTX Gefore 1660i (6GB), mình đã thử chạy toàn bộ dataset nhưng không nổi. Vì thế, mình quyết định chỉ sử dụng 30% trong tổng số dữ liệu để thực hành. Nếu bạn có máy tính cấu hình mạnh hơn, bạn có thể thử với toàn bộ tập dữ liệu này.\n Với 30% dữ liệu, chúng ta có 6.027.314 hàng trong DataFrame. Mỗi hàng tương ứng với một Rating tạo bởi một User đối với 1 bộ phim (item). Số lượng Users tham gia vào viêc Rating là 41.547. Nói chung thì dữ liệu như này vẫn là khá lớn.\n 3. Train-Test Split\nCùng với Rating, trong file rating.csv còn có cột timestamp chứa thông tin về thời gian thực hiện Rating của User. Dựa vào thông tin này, chúng ta sẽ tiến hành chia tập dữ liệu của chúng ta thành 2 phần Train và Test theo chiến lược leave-one-out. Nếu bạn chưa biết thì leave-one-out là cách chia dữ liệu theo thời gian, tức là sử dụng những dữ liệu ở thời điểm gần hiện tại làm tập Test, còn lại làm tập Train. Xem ví dụ dưới đây:\n Cách chia này thường được sử dụng cho các bài toán RS và Time Series. Nếu thực hiện như cách truyền thống là xáo trộn ngẫu nhiên rồi chia thì sẽ không công bằng vì chúng ta có thể đang sử dụng những dữ liệu gần đây để huấn luyện và những dữ liệu trước đó để kiểm tra. Điều này dẫn đến rò rỉ dữ liệu với xu hướng nhìn trước và hiệu suất của mô hình được đào tạo sẽ không thể tổng quát hóa thành hiệu suất trong thế giới thực.\nCode dưới đây thực hiện leave-one-out:\n 4. Convert Dataset into Implicit Dataset\nCác bài toán RS trong thực tế thường được giải quyết sử dụng dữ liệu dạng Implicit vì dữ liệu dạng này dễ dàng thu thập được từ người dùng thông qua Log hoạt động của hệ thống. Trong bài này, chúng ta cung sẽ sử dụng Implicit Dataset để huấn luyện model.\nTuy nhiên, tập MovieLends chỉ có dữ liệu dạng Explicit (chính là Rating của User đối với Item). Để chuyển sang dạng Implicit, chúng ta sẽ tiến hành chuyển đổi các giá trị Rating thành \u0026ldquo;1\u0026rdquo; (positive class), ngụ ý rằng User có tương tác với Item đó. Thay vì cố gắng dự đoán giá trị của Rating, ta sẽ dự đoán liệu User có tương tác với Item đó hay không? Tương tác ở đây có thể hiểu là hành vi Click vào nút Like của Item, mua Item, hay xem Item. Những Items này sẽ được đề xuất cho User, làm tăng khả năng mua hàng của User.\n Sau khi thực hiện việc chuyển đổi Rating thành \u0026ldquo;1\u0026rdquo; thì chúng ta mới chỉ có Positive class, cần phải có thêm Negative class nữa mới có thể huấn luyện được model. Negative class (0) ở đây được giả định là User không tương tác, không quan tâm đến Item. Mặc dù điều giả định này không hoàn toàn đúng, vì có thể User chưa biết đến Item đó, nhưng nó vẫn tỏ ra khá hiệu quả trong thực tế.\nCode dưới đây sinh ra 4 Negative Samples tương ứng với mỗi hàng trong DataFrame. Tỉ lệ Positive : Negative là 1:4 được chọn tùy ý. Bạn có thể thử theo ý bạn, nhưng cá nhân mình thấy thì tỉ lệ này làm cho model hoạt động khá ổn trong thực tế.\n Sử dụng Pytorch, ta viết lại đoạn code trên vào class Dataset để thuận tiện cho việc huấn luyện model như sau:\n 5. Định nghĩa Deep Learning model\nĐã có một số kiến trúc Deep Learning model được đề xuất cho bài toán RS. Trong số đó, đề xuất của He và cộng sự có tên là Neural Collaborative Filtering (NFC) được sử dụng phổ biến hơn cả bởi tính đơn giản của nó. Chúng ta cũng sẽ sử dụng nó trong bài này.\nKiến trúc của NFC được mô tả như sau:\n Inputs của NFC model là các vectors dạng One-hot Encoded của User và Item. Các vectors này được đưa qua các lớp Embeddings để sinh ra các Embedded vectors tương ứng. Embedded vectors sau đó được kết hợp lại với nhau trước khi đi qua một vài lớp Fully Connected, chuyển thành vector dự đoán (output vector). Hàm Sigmoid được áp dụng cho Ouput vector sinh ra xác suất của mỗi class: Positive và Negative. Như ví dụ trong sơ đồ kiến trúc NFC model thì xác suất mỗi class Positive : Negative tương ứng là 0.8:0.2. Từ đó có thể kết luận là User được dự đoán là có tương tác với Item đó vì 0.8 \u0026gt; 0.2.\nSử dụng Pytorch Lightning, ta định nghĩa NFC model như sau:\n Pytorch Lightning là một dạng lightweight của Pytorch, nó đóng gói các hàm của Pytorch ở mức high-level, giúp đơn giản hóa code và tăng tốc độ xử lý. Chi tiết hơn vê Pytorch Lightning, bạn có thể tham khảo trong link mình để bên trên.\n5. Huấn luyện Deep Learning model\nNFC model được huấn luyện như sau:\n  Thời gian huấn luyện khá lâu, mình phải chờ gần một ngày mới hoàn thành.\n6. Đánh giá Deep Learning model\nĐể đánh giá các RS model, chúng ta sẽ sử dụng metric Hit Ration @10 như đã đề cập trong bài trước.\nCode thực hiện như sau:\n Kết quả cuối cùng:\n Hit Ration @10 mới chỉ đạt 0.58, vẫn còn khá khiêm tốn. Có lẽ model cần phải được huấn luyện thêm nhiều epochs nữa. Con số 0.58 có nghĩa là nếu đề xuất một Item nào đó cho 100 Users thì 58 người có tương tác với Item đó.\n7. Kết luận\nNhư vậy là chúng ta đã kết thúc chuỗi bài viết về Recommender System tại đây. Hi vọng từ những điều mình chia sẻ sẽ giúp ích cho các bạn trong quá trình học tập và công tác của mình.\nToàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ chuyển sang một chủ đề hoàn toàn mới, Data Preparation for Machine Learning. Mời các bạn đón đọc.\n8. Tham khảo\n[1] james Loy, \u0026ldquo;Deep Learning based Recommender Systems\u0026rdquo;, Available online: https://towardsdatascience.com/deep-learning-based-recommender-systems-3d120201db7e (Accessed on 10 Jul 2021).\n[2] Abhijit Roy, \u0026ldquo;Introduction To Recommender Systems- 2: Deep Neural Network Based Recommendation Systems\u0026rdquo;, Available online: https://towardsdatascience.com/introduction-to-recommender-systems-2-deep-neural-network-based-recommendation-systems-4e4484e64746 (Accessed on 05 Jul 2021).\n","permalink":"https://tiensu.github.io/blog/74_recommender_system_3/","tags":["Recommender System"],"title":"Thực hành xây dựng RS sử dụng Deep Learning"},{"categories":["Recommender System"],"contents":"Đây là bài thứ 2 trong chuỗi bài viết về Recommender System. Trong bài này, chúng ta sẽ thực hành xây dựng một RS sử dụng phương pháp Content-Based Filtering.\n1. Đặt vấn đề\nMột website bán sách muốn tăng doanh số bán hàng. Họ muốn bạn - 1 Data Scientist giúp họ xây dựng một RS để đề xuất các cuốn sách cho người dùng mỗi khi họ ghé vào website của họ. Sau khi xem xét kỹ yêu cầu bài toán và dữ liệu hiện có, bạn quyết định xây dựng RS theo phương pháp Content-Based Filtering.\nSơ đồ các bước tiến hành sẽ như sau:\n Ở đây, phần RS Engine sẽ bao gồm 2 nhiệm vụ:\n Nhiệm vụ 1: Sử dụng Cosine Similarity để tìm ra danh sách 10 cuốn sách có mức độ tương tự gần với cuốn sách mà ta đưa vào RS nhất. Nhiệm vụ 2: Sử dụng những hiểu biết về lĩnh vực sách để đưa ra gợi ý khi đưa vào RS một cuốn sách, làm cho kết quả mang tính đa dạng hơn.  2. Chuẩn bị dữ liệu\nDownload dữ liệu tại đây.\nThông tin về sách đựợc lưu trong các files: books.csv, book_tags.csv, tags.csv. Chúng ta sẽ khám phá từng file:\n    Ta thấy rằng có tổng cộng 10.000 cuốn sách, và trường book_tags của mỗi cuốn sách chứa đựng nhiều về cuốn sách đó. Vì thế, chúng ta sẽ sử dụng thông tin này đề xây dựng RS.\nVì các thông tin book_id, tag_id, tag_name, \u0026hellip; nằm rải rác ở các file csv khác nhau nên ta sẽ tiến hành gom chúng lại với nhau.\n     Đến đây, chúng ta đã có đầy đủ thông tin cần thiết về các cuốn sách trong một DataFrame là books.\n3. Transform Data\nNhư đã phân tích ở trên, chúng ta sẽ sử dụng thông tin tag_name làm cơ sở cho việc tính toán mức độ tương tự giữa các cuốn sách.\nHãy xem thử, tag_name đang như thế nào?\n Chúng ta cần phải chuyển đổi các thông tin này từ dạng Text sang dạng Number để làm Input cho việc tính toán. Việc này được gọi là Word Embedding. Mỗi cuốn sách sẽ được chuyển đổi tương ứng thành một Vector, gọi là Vector Embedding. Có rất nhiều thuật toán giúp chúng ta thực hiện nhiệm vụ này: CountVectorizing, TF-IDF, Glove, Word2Vec, BERT, \u0026hellip; Trong bài này, chúng ta sẽ sử dụng TF-IDF trong thư viện Scikit-learn như dưới đây:\n TFIDF (Term Frequency - Inverse Document Frequency) tính toán mức độ quan trọng của mỗi từ dưạ trên tần suất xuất hiện và mối liên hệ với các từ khác trong toàn bộ đoạn văn bản. Vector Embedding của mỗi cuốn sách là một hàng trong TF-IDF vector.\n4. Xây dựng RS Engine\nNhư đã nói ở bên trên, sẽ có 2 nhiệm vụ trong bước này:\n Nhiệm vụ 1:  Chúng ta cần tính mức độ tương tự nhau giữa các Vector Embedding của các cuốn sách. Như đã giới thiệu ở bài trước, chúng ta có thể sử dụng Cosine Similarity hoặc Euclidean Distance. Mình sẽ chọn Cosine Similarity để thực hiện. Thư viện Scikit-learn đã hỗ trợ chúng ta việc này.\n Nếu zoom to lên một phần của ma trân Cosine Similarity ta sẽ thấy như sau:\n Giá trị tại điểm giao nhau giữa hàng và cột trong ma trận này chính là thể hiện mức độ tương tự nhau của 2 cuốn sách nằm trên hàng và cột đó.\nTiếp theo, chúng ta sẽ viết code để lấy ra được danh sách các cuốn sách được đề xuất khi đưa vào RS tên một cuốn sách:\n   Chúng ta sẽ kiểm tra luôn kết quả của nhiệm vụ 1. Ta đưa vào RS một cuốn sách có tên The Fellowship of the Ring (The Lord of the Rings, #1), kết quả nhận được như sau:\n Chúng ta nhận đuọc một danh sách gồm 10 cuốn sách có mức độ tương tự lớn nhất với cuốn sách mà ta đưa vào cho RS. Mình cũng thử lên Amazon, thử chọn mua cuốn sách cùng tên thì thật thú vị là Amazon cũng gọi ý cho mình 10 cuốn sách gần giống như kết quả của RS mà ta vừa xây dựng. Chứng tỏ rằng RS của chúng ta hoạt động khá hiệu quả.\n  Nhiệm vụ 2:  Nếu có hiểu biết về lĩnh vực sách, chúng ta sẽ thấy rằng những người mà quan tâm đến thể loại Fantasy cũng thường quan tâm đến một số thể loại khác như Science Fiction, Technology, Entrpreneurship, Biograpies, \u0026hellip; Như vậy, để cho đa dạng hơn trong kết quả của RS, chúng ta có thể xử lý như sau: Lọc ra 10 cuốn sách có mức độ tương tự với cuốn sách đưa vào cao nhất để gơi ý, nhưng các cuốn sách đó nằm trên các Tags(Categories) khác nhau.\nGiả sử chúng ta lấy ra các cuốn sách có Tag là Non-Fiction, Economics, Entrepreneurial từ danh sách các cuốn sách sắp xếp giảm dần theo mức độ tương tự với cuốn sách The Fellowship of the Ring (The Lord of the Rings, #1), code như sau:\n Amazon cũng thi thoảng đưa thêm tính chất Diversiry (đa dạng) này vào trong RS của họ, thậm chí còn cho phép User bật/tắt tính năng này. Ví dụ dưới đây là kết quả gợi ý nếu người dùng tắt tính năng Diversity:\n Nếu User bật tính năng Diversity lên thì kết quả sẽ thay đổi như sau:\n Trong 5 cuốn sách được gợi ý thì có 3 cuốn giữ nguyên như khi tắt Diversity, còn 2 cuốn mới xúât hiện, khác Category. Đó chính là kết quả của Diversity.\n5. Kết luận\nNhư vậy là chúng ta đã cùng nhau thực hành code một RS theo phương pháp Content-Based Filtering, sử dụng bộ dataset về sách. Toàn bộ code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ tìm hiểu cách xây dựng RS bằng phương pháp sử dụng Deep Learning. Mời các bạn đón đọc.\n6. Tham khảo\n[1] Parul Pandey, \u0026ldquo;The Remarkable World of Recommender Systems\u0026rdquo;, Available online: https://www.topbots.com/diversity-through-recommendation-systems/ (Accessed on 05 Jul 2021).\n[2] James Loy, \u0026ldquo;Deep Learning based Recommender Systems\u0026rdquo;, Available online: https://towardsdatascience.com/deep-learning-based-recommender-systems-3d120201db7e (Accessed on 05 Jul 2021).\n","permalink":"https://tiensu.github.io/blog/73_recommender_system_2/","tags":["Recommender System"],"title":"Thực hành xây dựng RS bằng phương pháp Content-Based Filtering"},{"categories":["Recommender System"],"contents":"Chúng ta sẽ chuyển sang tìm hiểu một chủ đề mới, đó là các hệ thống khuyến nghị - Recommender System (RS).\nNgày nay, RS rất phổ biến trong cuộc sống của chúng ta. Khi xem Youtube, lướt Facebook, mua hàng trên Amazon hay Tiki, xem phim trên Netflix, chúng ta đều được đưa ra các gợi ý về những thứ mà chúng ta có thể quan tâm\u0026hellip; Có thể nói, RS là ứng dụng quan trọng nhất của AI vào lĩnh vực giải trí và Maketing.\nTrong chuỗi 3 bài tiếp theo ngay sau đây, chúng ta sẽ cùng tìm hiểu kỹ càng hơn về các thuật toán mà RS sử dụng, đồng thời sẽ thực hành một số thuật toán đó. Các bài viết bao gồm:\n Bài 1: Giới thiệu các phương pháp truyền thống xây dựng Recommender System bài 2: Xây dựng Recommender System bằng phương pháp Content-Based Filtering Bài 3: Xây dựng Recommender System sử dụng Deep Learning  1. Phân loại thông tin sử dụng trong RS\nTrong RS, có 2 đối tượng quan trọng nhất là khách hàng (Users) và sản phẩm (Items). Thông tin về 2 đối tượng này được sử dụng làm Input Data để tạo nên các RS. Dựa vào tính chất của chúng mà ta có thể phân loại như sau:\n Explicit Information: Là những thông tin được thể hiện một các rõ ràng, tường mình. Đó có thể là các thông tin về sự tương tác giữa Users-Items như hành vi click và nút Like/Dislike hoặc cho điểm (ratting) Items. Hoặc thông tin về bản thân Users/Items (User Profile - Item Profile). Implicit Information: Ngược lại với Explicit Information, đây là các thông tin mà không được thể hiện một các cụ thể, rõ ràng. Đó là khi User truy cập, tìm kiếm, bình luận, \u0026hellip; và một Item cụ thể. Từ đó, hệ thống sẽ ghi nhận rằng User đang quan tâm đến Item đó.  2. Các phương pháp RS truyền thống\nHai phương pháp RS phổ biến mà có thể bạn đã biết là: Content-Based Filtering và Collaborative Filtering. Ngoài ra, có thể kết hợp 2 phương pháp này lại, tạo ra một phương pháp mới gọi là Hybrid Filtering.\n 2.1 Phương pháp Content-Based Filtering\nĐây là phương pháp đơn giản nhất, hầu như chỉ dựa trên thông tin User Profile và Item Profile. Ý tưởng cơ bản của nó là nếu một User A đã từng quan tâm đến một Item X trong quá khứ thì khả năng cao là A cũng sẽ quan tâm đến Item Y nếu Y tương tự X.\n User Profile được xây dựng dần dần theo thời gian sử dụng Explicit Information.\nMột số Metrics đo độ Tương tự (Similarity) giữa 2 Items\nĐể đánh giá mức độ Similarity giữa 2 Items, chúng ta có thể sử dụng một trong các Metrics sau:\n Cosin Similarity:  Công thức tính Cosin Similarity giữa 2 vector A và B như sau:\n$sim(A,B) = cos(\\theta) = \\frac{A.B}{\\begin{Vmatrix}A\\end{Vmatrix} \\begin{Vmatrix}B\\end{Vmatrix}}$\n Trong đó, $A$ và $B$ là 2 Profile Vector của 2 Items, $\\theta$ là góc giữa chúng.\nCosin Similarity có giá trị nằm trong khoảng [-1,1]. Càng gần 1 thì 2 Items càng giống nhau. Chúng ta có thể chọn n-Items từ danh sách giảm dần mức độ Similarity hoặc lấy các Items mà mức độ Similarity lớn hơn giá trị ngưỡng.\n Euclidean Distance:  Công thức tính Euclidean Distance giữa 2 vector $A(x_1, x_2, \u0026hellip;, x_n)$ và $B(y_1, y_2, \u0026hellip;, y_n)$ như sau:\n$ED(A,B) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2}$\n Euclidean Distance có giá trị \u0026gt; 0 và càng nhỏ thì 2 vectors càng giống nhau. So với Cosin Similarity, việc tính toán Euclidean Distance khá chậm, đặc biệt là trong trường hợp số chiều của các vectors lớn.\n2.2 Phương pháp Collaborative Filtering\nPhương pháp này sử dụng mối quan hệ giữa User-User, Item-Item và User-Item để đưa ra các đề xuất.\n Collaborative Filtering được chia thành 2 phương pháp:\n  Memory-based: Phương pháp này còn được gọi là thuật toán Collaborative Filtering dựa trên vùng lân cận. Các vùng lân cận này có thể được xác định theo một trong hai cách:\n User-based Collaborative Filtering: Ý tưởng là phân chia các Users tương tự nhau vào chung một nhóm. Nếu một User bất kỳ trong nhóm thích một Item nào đó thì Item đó sẽ được đề xuất cho toàn bộ các Users khác trong nhóm đó. Item-Item Collaborative Filtering: Ý tưởng là phân chia Items tương tự nhau vào chung một nhóm. Nếu một User thích bất kỳ một Item nào trong nhóm đó thì tất cả các Item còn lại trong cùng nhóm sẽ được đề xuất cho User đó. Nghe qua ý tưởng thì có vẻ Item-Item Collaborative Filtering rất giống với ý tưởng của phương pháp Content-Based Filtering. Và quả thực đúng là như vậy. Tuy nhiên, sự khác nhau ở đây là cách thức để tìm ra các Items tương tự nhau. Đối với Content-Based Filtering, ta sử dụng chính đặc điểm (Profile) của các Items để tính toán mức độ Similarity giữa chúng. Còn đối với Item-Item Collaborative Filtering, độ Similarity được tính dựa trên Utility Matrix mà sẽ được đề cập dưới đây.    Model-based: Phương pháp này sử dụng các thuật toán ML như PCA, Clustering, SVD, Matrix Factorisation, \u0026hellip; để dự đoán các giá trị Rating còn thiếu của User đối với Item.\n  Tất cả các cách/phương pháp của Collaborative Filtering đề phải sử dụng Utility Matrix, được tạo thành từ bảng dữ liệu User-Item Ratings như dưới đây.  Bảng User-Item Ratings chỉ chứa các Users và Items mà có sự tương tác với nhau, còn Utility Matrix thì chứa tất các các Users, Items trong hệ thống. Mỗi Cell trong Utility Matrix thể hiện giá trị Rating của User và Item tương ứng. Vấn đề lớn nhất của Collaborative Filtering là số lượng Rating thường nhỏ hơn rất nhiều so với số lượng User và Item. Điều này dẫn đến Utility Matrix bị thiếu rất nhiều gíá trị trong các Cells. Vì lẽ đó mà Utility Matrix còn được gọi là ma trận thưa (Sparsity Matrix).\nTa có công thức tính độ thưa như sau: $Sparsity = \\frac{No of Ratings}{No of Cells}$\n Nếu giá trị của Sparsity \u0026gt; 0.5 thì chúng ta không nên sử dụng Collaborative Filtering mà nên xem xét sử dụng các phương pháp khác.\nChi tiết hơn về 2 phương pháp Content-Based Filtering và Collaborative Filtering, các bạn có thể đọc thêm tại đây và đây.\n2.3 Phương pháp Hybrid\nPhương pháp này có tên gọi khác là Ensemble, tức là kết hợp 2 phương pháp Content-Based Filtering và Collaborative Filtering.  Content-Based Filtering và Collaborative Filtering, mỗi phương pháp đều có ưu/nhược điểm riêng của mỗi loại. Và trong một số trường hợp, việc kết hợp chúng lại sẽ mang đến hiệu quả tốt hơn. Netflix chính là một ví dụ sử dụng phương pháp Hybrid này.\n3. Đánh giá Recommender System\nCác ML model thường sử dụng Accuracy metric (đối với bài toán Classification) và RMSE metric (đối với bài toán Regression) để đánh giá model. Tuy nhiên, đối với RS model, các metrics này đều tỏ ra không hiệu quả.\nĐể đưa ra được một metric phù hợp hơn, đầu tiên chúng ta hãy xem thử Netflix và Amazon đang đề xuất cho User những gì?   Dễ nhận thấy, điểm mấu chốt ở đây là User không cần tương tác với tất cả các Items trong danh sách đề xuất mà chỉ cần tương tác với ít nhất một Item trong số đó. Và khi điều này xảy ra, tức là RS model đã làm việc đúng (hitted).\nDo vậy, các xây dựng metric đánh giá RS model sẽ như sau:\n Đối với mỗi User, chọn ngẫu nhiên 99 Items mà User đó không tương tác. Kết hợp 99 Items đó với một Test Item (Item mà User đã tương tác). Ta có 100 Items. Chạy RS model trên 100 Items đó rồi sắp xếp theo thứ tự giảm dần xác suất mà User đó có tương tác với mỗi Item. Lựa chọn 10 Items có xác suất lớn nhất trong danh sách đã sắp xếp. Nếu Test Item nằm trong số 10 Items đã chọn thì ta nói RS model đã làm việc đúng (tính là một Hit). Lặp lại quá trình trên cho toàn bộ Users. Ta định nghĩa giá trị Hit Ratio @10 như sau:  $Hit Ration @10= \\frac{No Of Hits}{No Of Users}$\n Hit Ration @10 chính là metric đánh giá RS model của chúng ta.\n4. Kết luận\nNhư vậy là chúng ta đã kết thúc bài đầu tiên trong chuỗi 4 bài tìm hiểu về Recommender System. Qua bài này, chúng ta đã hiểu được phần nào rõ hơn về bài toán RS, cũng như một số phương pháp truyền thống để xây dựng RS.\nTrong bài tiếp theo, chúng ta sẽ thực hành xây dựng một RS bằng phương pháp Content-Based Filtering. Mời các bạn đón đọc.\n5. Tham khảo\n[1] Parul Pandey, \u0026ldquo;The Remarkable World of Recommender Systems\u0026rdquo;, Available online: https://www.topbots.com/overview-of-recommender-systems/ (Accessed on 25 Jun 2021).\n[2] James Loy, \u0026ldquo;Deep Learning based Recommender Systems\u0026rdquo;, Available online: https://towardsdatascience.com/deep-learning-based-recommender-systems-3d120201db7e (Accessed on 25 Jun 2021).\n","permalink":"https://tiensu.github.io/blog/72_recommender_system_1/","tags":["Recommender System"],"title":"Giới thiệu các phương pháp truyền thống xây dựng Recommender System"},{"categories":["Audio Classification","Speech Recognition","Speech-to-Text"],"contents":"Đây là bài cuối cùng trong chuỗi 5 bài về Audio Deep Learning. Trong bài này, chúng ta sẽ tìm hiểu về bài toán Automatic Speech Recognition (ASR) hay Speech-to-Text: kiến trúc, cách thức làm việc, \u0026hellip;\nCó lẽ chúng ta không còn quá xa lạ với một số ứng dụng như Siri, Alexa, Google Home, Cortana, \u0026hellip; Chúng đều là các ứng dụng dựa trên bài toán ASR.\n1. Speech to Text\nNhư chúng ta đều biết, tiếng nói của con người là một thứ tồn tại hàng ngày xung quanh chúng ta. Trong một số trường hợp, chúng ta cần phải chuyển đổi những giọng nói đó thành văn bản (transcribe) đề có thể xử lý tốt hơn. Ví dụ: ghi chép nội dụng trong một cuộc họp, tổng đài hỗ trợ tự động, \u0026hellip;\nTương tự như bài toán Audio Classification, đối với bài toán Speech-to-Text cũng yêu cầu dữ liệu huấn luyện gồm 2 phần:\n Input Features (X): các đoạn audio thu âm giọng nói của người. Target Labels (Y): các đoạn văn bản tương ứng với các đoạn audio X.    ASR model sẽ nhận vào X và đưa ra dự đoán Y.\n2. Data Preprocessing\nCác bước tiền xử lý dữ liệu của bài toán này cũng tương tự như bài trước.  2.1 Load Audio Files\n Input Data là các file audio thu âm giọng nói của người dưới dạng .mp3, .wav, \u0026hellip; Sử dụng một trong các thư viện Librosa, Torchaudio, \u0026hellip; để đọc vào các Input Data, chuyển chúng thành các mảng Numpy 1D. Mỗi giá trị trong mảng này chính là giá trị của biên độ tại các thời điểm lấy mẫu. Kích thước của mảng phụ thuộc vào giá trị của Tần số lấy mẫu (Sampling Rate). Ví dụ: nếu Sampling Rate = 44100Hz, thì cứ mỗi giây sẽ có 44100 giá trị của biên độ được lấy. Nếu file Audio dài 2s thì mảng Numpy của nó sẽ có kích thước là 88200.  2.2 Normalization = Convert to uniform dimensions: sample rate, channels, and duration\n Các Input Data có thể rất khác nhau về độ dài thời gian, số kênh (Mono hoặc Stereo), Sampling Rate, \u0026hellip; Điều này dẫn đến kích thước của mảng Numpy cũng sẽ rất khác nhau. Bởi vì các DL model đều yêu cầu Input Features có cùng kích thước nên chúng ta phải chuân hóa, sao cho tất cả các Input Data có cùng giá trị của mỗi đại lượng kể trên. Nếu chất lượng Input Data quá tệ, có thể xem xét áp dụng một số thuật toán Denoise để loại bớt nhiễu.  2.3 Mel Spectrogram\nMảng Numpy của các file Audio (gọi là Raw Audio) được chuyển sang Mel Spectrogram. Mel Spectrogram chứa đầy đủ thông tin của Audio trong cả miền thời gian và không gian và thể hiện chúng dưới dạng ảnh phổ.\n2.4 Data Augmentation\nChúng ta có thể áp dụng một số kỹ thuật Augmentation để làm phong phú thêm dữ liệu huấn luyện model. Có 2 thời điểm thích hợp mà ta có thể sử dụng các kỹ thuật này:\n Thời điểm dữ liệu ở dạng Raw Audio: Một số kỹ thuật là Time Shift, Pitch Shift, Time Stretch. Thời điểm dữ liệu ở dạng Mel Spectrogram: Một số kỹ thuật là: Time Mask, Frequency Mask.  2.5 MFCC\nMFCC là một dạng \u0026ldquo;nén\u0026rdquo; của Mel Spectrogram, được chứng mình rằng phù hợp hơn Mel Spectrogram đối với dữ liệu là giọng nói của người.\n2.6 Prepare Target Label\nĐể chuẩn bị nhãn cho model, chúng ta cần xây dựng một từ điển (Vocabulary) các ký tự trong các Target Labels Y, sau đó chuyển mỗi ký tự đó sang ID tương ứng của chúng trong từ điển.\n3. Kiến trúc\nCó một vài kiến trúc DL model khác nhau cho ASR, trong đó có 2 kiến trúc phổ biến là:\n3.1 CRNN-CTC Tương tự như bài toán OCR, kiến trúc này cũng bao gồm một mạng CNN để trích xuất đặc trưng (feature maps) từ MFCC, mạng RCNN để xử lý các Feature Maps dạng chuỗi, và cuối cùng là CTC Loss để decode ra các ký tự cụ thể. Baidu\u0026rsquo;s Deep Speech là một model nổi bật sử dụng kiến trúc này.\nCách thức hoạt động của nó như sau:\n  Mạng CNN (thường là ResNet) xử lý các MFCC Images và Output ra Feature Maps của mỗi Image đó.    Tiếp theo, mạng RNN (thường là Bidirectional LSTM) xử lý các Feature Maps như là một chuỗi các TimeSteps/Frames liên tiếp, mỗi TimeStep này tương ứng với một/nhiều ký tự ở chuỗi đầu ra mong muốn.    Một Linear Layer kết hợp với Softmax, sử dụng Output từ LSTM để sinh ra xác suất của mỗi ký tự, tương ứng với mỗi TimeStep.    Ngoài ra, cũng có thể có một vài Linear Layers khác nằm giữa CNN và RNN để chuyển đổi kích thước giữa 2 mạng cho phù hợp với nhau.\n  CTC Loss: Xem lại bài này\n  3.2 RNN Kiến trúc này chỉ sử dụng mạng RNN SeqSeq, sử dụng Input là các slices được chia ra từ MFCC. Google\u0026rsquo;s Listen Attend Spell (LAS) là một model nột bật sử dụng kiến trúc này.\n4. Metrics - Word Error Rate (WER)\nSau khi dựng được kiến trúc và huấn luyện model, bước tiếp theo cần làm là đánh giá model đó xem nó hoạt động hiệu quả như thế nào. Mỗi dạng bài toán đều có các Metrics để đánh giá model khác nhau. Đối với bài toán ASR, Word Error Rate (WER) thường được sử dụng. Về bản chất, WER dựa vào việc so sánh kết quả dự đoán của model và nhãn thực tế theo từng ký tự (Character by Character) hoặc từ (Word by Word).\nSự khác nhau giữa kết quả dự đoán và nhãn thực tế có thể là các ký tự/từ có trong nhãn nhưng khồn có trong dự đoán (gọi là Deletion), hoặc ký tự/từ có trong dự đoán nhưng không có trong nhãn (gọi là Insertion), hoặc ký tự/từ có trong dự đoán/nhãn nhưng trong nhãn/dự đoán, nó bị biến đổi thành ký tự/từ khác mà vẫn giữ nguyên ý nghĩa (gọi là Substitution).  Công thức tính WER khá đơn giản, nó là tỉ lệ giữa số lượng các ký tự/từ khác nhau (cả 3 loại) và tổng số ký tự/từ.\nVí dụ, trong hình bên trên thì: $WER = \\frac{Detetion + Insertion + Substitution}{\\sum{Characters/Words}} = \\frac{1 + 1 + 1}{6} = 0.5$\n 5. Beam Search\nTrong cách làm việc của CTC Decoder, mặc dù không nói rõ nhưng chúng ta đều ngầm hiểu rằng ký tự với xác sủất cao nhất sẽ được chọn. Cách này được gọi bằng cái tên Greedy Search.\nMột sự thay thế khác là Beam Seach, mà trong một số trường hợp mang lại kết quả tốt hơn so với Greedy Search, đặc biệt là trong các bài toán NLP. Nếu có thể thì bạn hãy thử nó trong các dự án của mình.\n6. Mở rộng\nNói chung, sau khi đánh giá model, WER đã thỏa mãn yêu cầu (\u0026gt; giá trị ngưỡng, 0.8 chẳng hạn) thì bài toán ASR coi như đã được giải quyết xong. ASR mặc dù đã sinh ra văn bản từ giọng nói nhưng nó không hiểu gì về ngữ nghĩa của văn bản đó cả. Kết quả của ASR thường được sử dụng làm Input cho các bài toán khác trong lĩnh vực NLP: sinh văn bản mới, phân loại văn bản, tự động trả lời, \u0026hellip;\n7. Kết luận\nNhư vậy là chúng ta đã kết thúc bài thứ 5 tại đây. Qua bài này, chúng ta đã hiểu được phần nào rõ hơn về bài toán ASR, từ kiến trúc cho đến cách làm việc\nTrong bài tiếp theo, chúng ta sẽ chuyển sang một chủ đề mới, đó là bài toán Recommender System. Mời các bạn đón đọc.\n8. Tham khảo\n[1] Ketan Doshi, \u0026ldquo;Audio Deep Learning Made Simple: Automatic Speech Recognition (ASR), How it Works\u0026rdquo;, Available online: https://towardsdatascience.com/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706 (Accessed on 05 Jun 2021).\n[2] Scott Duda, \u0026ldquo;Urban Environmental Audio Classification Using Mel Spectrograms\u0026rdquo;, Available online: https://scottmduda.medium.com/urban-environmental-audio-classification-using-mel-spectrograms-706ee6f8dcc1 (Accessed on 05 Jun 2021).\n","permalink":"https://tiensu.github.io/blog/71_audio_deep_learning_part_5/","tags":["Audio Classification","Speech Recognition","Speech-to-Text"],"title":"Tìm hiểu bài toán Automatic Speech Recognition (ASR)"},{"categories":["Audio Classification","Speech Recognition","Speech-to-Text"],"contents":"Đây là bài thứ 4 trong chuỗi 5 bài về Audio Deep Learning. Trong bài này, chúng ta sẽ code thực hành huấn luyện một mô hình phân loại Audio. Chúng ta sẽ đi tuần tự từng bước, từ việc chuẩn bị dữ liệu, xây dựng kiến trúc model, huấn luyện và đánh giá model. Cuối cùng là sử dụng model đã huấn luyện để dự đoán. Một số hướng mở rộng để nâng cao độ chính xác của model cũng sẽ được đưa ra bàn thảo.\n1. Luồng hoạt động của bài toán Audio Classification\nTương tự như bài toán Image Classification hay Text Classification, bài toán Audio Classification thông thường sẽ bao gồm các bước xử lý chính như sau:\n Dữ liệu Audio được chuyển sang dạng Spectrogram (hoặc Mel Spectrogram, hoặc MFCC). Spectrogram Image được đưa qua mạng CNN để tạo ra Feature Maps. Sử dụng Feature Maps làm đầu vào cho bộ phân lớp (FC, SVM, \u0026hellip;) để cho ra kết qủa dự đoán.   2. Chuẩn bị dữ liệu\n2.1 Download dữ liệu\nChúng ta sẽ sử dụng bộ dữ liệu Urban Sound 8K trong bài này. Nó bao gồm 10 classes là các loại âm thanh khác nhau như tiếng chó sủa, tiếng còi báo động, tiếng mát khoan, \u0026hellip;\nSau khi tải về, chúng ta thấy bộ dữ liệu này gồm 2 phần:\n Các Audio files trong 10 sub-folders có tên từ fold1 đến fold10. Trong mỗi sub-folders đó đều chứa các Audio files. Ví dụ: fold1/103074-7-1-0.wav. Độ dài của mỗi file Audio khoảng 4s. File UrbanSound8K.csv chứa thông tin về mỗi Audio files trong bộ dữ liệu: Tên file, nhãn, \u0026hellip; Nhãn của mỗi file Audio được quy định là theo ID từ 0 đến 9.  2.2 Tiền xử lý dữ liệu\nMọi thông tin về dataset đều nằm trong file UrbanSound8K.csv (metadata file), vì vậy, trước tiên chúng ta đọc nó lên dể xem nó chứa những thông tin gì:\n# ---------------------------- # Prepare training data from Metadata file # ---------------------------- import pandas as pd from pathlib import Path data_path = \u0026#39;/home/sunt/Downloads/UrbanSound8k\u0026#39; # Read metadata file metadata_file = download_path + \u0026#39;/UrbanSound8K.csv\u0026#39; df = pd.read_csv(metadata_file) df.head() # Construct file path by concatenating fold and file name df[\u0026#39;relative_path\u0026#39;] = \u0026#39;/fold\u0026#39; + df[\u0026#39;fold\u0026#39;].astype(str) + \u0026#39;/\u0026#39; + df[\u0026#39;slice_file_name\u0026#39;].astype(str) # Take relevant columns # df = df[[\u0026#39;relative_path\u0026#39;, \u0026#39;classID\u0026#39;]] df.head()   Tiếp theo, chúng ta sẽ thực hiện một số bước tiền xử lý (pre-processing) dữ liệu để sẵn sàng đưa vào model huấn luyện. Quá trình Pre-processing sẽ đuợc thực hiện một cách tự động, cùng lúc với việc đọc các files Audio (thực hiện lúc runtime). Nói một cách dễ hiểu là đọc files Audio đến đâu, thực hiện Pre-processing và đưa vào model huấn luyện đến đó chứ không phải đọc xong toàn bộ files Audio rồi mới thực hiện các bước kia. Cách làm này cũng tương tự như cách chúng ta thường làm với dữ liệu Image, bởi vì cả 2 loại dữ liệu này thường tương đối lớn, nếu đọc hết một lần thì sẽ rất tốn bộ nhớ. Tất nhiên, vì model yêu cầu nhận vào dữ liệu theo từng Batch nên chúng ta cũng sẽ đọc và tiền xử lý dữ liệu theo từng Batch. Như vậy thì trong bộ nhớ lúc nào cũng chỉ có tối đa một Batch dữ liệu, tránh được việc tràn bộ nhớ. Giá trị của Batch, gọi là Batch_size có thể lớn hoặc nhỏ tùy theo kích thước bộ nhớ máy tính xử lý của bạn.\n Ở bài này, mình sẽ sử dụng Pytorch và thư việ torchaudio để thực hiện. Code cho từng bước lần lượt như sau:\n Đọc Audio files  import math, random import torch import torchaudio from torchaudio import transforms from IPython.display import Audio class AudioUtil(): # ---------------------------- # Load an audio file. Return the signal as a tensor and the sample rate # ---------------------------- @staticmethod def open(audio_file): sig, sr = torchaudio.load(audio_file) return (sig, sr)  Convert to two channels  Một vài files Audio có thể ở dạng một kênh (mono), trong khi đó một số khác ở dạng hai kênh (stereo). Bởi vì model chỉ chấp nhận dữ liệu có cùng kích thước nên chúng ta sẽ chuyển đổi tất cả sang dạng stereo:\n# ---------------------------- # Convert the given audio to the desired number of channels # ---------------------------- @staticmethod def rechannel(aud, new_channel): sig, sr = aud if (sig.shape[0] == new_channel): # Nothing to do return aud if (new_channel == 1): # Convert from stereo to mono by selecting only the first channel resig = sig[:1, :] else: # Convert from mono to stereo by duplicating the first channel resig = torch.cat([sig, sig]) return ((resig, sr))  Standardize sampling rate  Tương tự bước thứ 2, các files Audio có thể có Sample Rate khác nhau (48000Hz, 44100Hz, \u0026hellip;). Chúng ta phải đưa tất cả về cùng 1 giá trị của Sample Rate:\n# ---------------------------- # Since Resample applies to a single channel, we resample one channel at a time # ---------------------------- @staticmethod def resample(aud, newsr): sig, sr = aud if (sr == newsr): # Nothing to do return aud num_channels = sig.shape[0] # Resample first channel resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:]) if (num_channels \u0026gt; 1): # Resample the second channel and merge both channels retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:]) resig = torch.cat([resig, retwo]) return ((resig, newsr))  Resize to the same length  Tiếp tục, chúng ta sẽ thay đổi chiều dài của tất cả các files Audio về chung một giá trị max_length. File có chiều dài nhỏ hơn max_length sẽ được kéo dài bằng cách thêm vào khoảng im lặng - slience. File có chiều dài lớn hơn max_length sẽ được cắt bớt đi.\n# ---------------------------- # Pad (or truncate) the signal to a fixed length \u0026#39;max_ms\u0026#39; in milliseconds # ---------------------------- @staticmethod def pad_trunc(aud, max_ms): sig, sr = aud num_rows, sig_len = sig.shape max_len = sr//1000 * max_ms if (sig_len \u0026gt; max_len): # Truncate the signal to the given length sig = sig[:,:max_len] elif (sig_len \u0026lt; max_len): # Length of padding to add at the beginning and end of the signal pad_begin_len = random.randint(0, max_len - sig_len) pad_end_len = max_len - sig_len - pad_begin_len # Pad with 0s pad_begin = torch.zeros((num_rows, pad_begin_len)) pad_end = torch.zeros((num_rows, pad_end_len)) sig = torch.cat((pad_begin, sig, pad_end), 1) return (sig, sr)  Data Augmentation: Time Shift  Đến đây, chúng ta đã coi như thực hiện Pre-processing xong dữ liệu thô của Audio. Chúng ta có thể áp dụng kỹ thuật Augmentation ở tại bước này, cụ thể là Time-Shift.\n# ---------------------------- # Shifts the signal to the left or right by some percent. Values at the end # are \u0026#39;wrapped around\u0026#39; to the start of the transformed signal. # ---------------------------- @staticmethod def time_shift(aud, shift_limit): sig,sr = aud _, sig_len = sig.shape shift_amt = int(random.random() * shift_limit * sig_len) return (sig.roll(shift_amt), sr) Ngoài Time-Shift, vẫn còn một số kỹ thuật Augmentation khác có thể áp dụng ở đây. Bạn có thể xem lại ở bài số 3 tại đây.\n Convert to Mel Spectrogram  Dữ liệu Audio thô sau đó sẽ được chuyển sang dạng Mel Spectrogram. Bạn có thể xem lại lý thuyết về Mel Spectrogram và lý do cần chuyển sang Mel Spectrogram ở bài số 2 tại đây\n# ---------------------------- # Generate a Spectrogram # ---------------------------- @staticmethod def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None): sig,sr = aud top_db = 80 # spec has shape [channel, n_mels, time], where channel is mono, stereo etc spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig) # Convert to decibels spec = transforms.AmplitudeToDB(top_db=top_db)(spec) return (spec)  Data Augmentation: Time and Frequency Masking  Tiếp tục áp dụng thêm một số kỹ thuật Augmentation nữa đối với Mel Spectrogram. Lần này là SpecAugment với 2 phương pháp Time and Frequency Masking.\n# ---------------------------- # Augment the Spectrogram by masking out some sections of it in both the frequency # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent # overfitting and to help the model generalise better. The masked sections are # replaced with the mean value. # ---------------------------- @staticmethod def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1): _, n_mels, n_steps = spec.shape mask_value = spec.mean() aug_spec = spec freq_mask_param = max_mask_pct * n_mels for _ in range(n_freq_masks): aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value) time_mask_param = max_mask_pct * n_steps for _ in range(n_time_masks): aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value) return aug_spec 3. Định nghĩa Data Set và Data Loader\nĐể đưa dữ liệu vào cho model để huẩn luyện, trong Pytorch, chúng ta cần 2 Objects:\n Dataset object: Sử dụng tất cả các hàm Pre-processing đã định nghĩa ở trên.  from torch.utils.data import DataLoader, Dataset, random_split import torchaudio # ---------------------------- # Sound Dataset # ---------------------------- class SoundDS(Dataset): def __init__(self, df, data_path): self.df = df self.data_path = str(data_path) self.duration = 4000 self.sr = 44100 self.channel = 2 self.shift_pct = 0.4 # ---------------------------- # Number of items in dataset # ---------------------------- def __len__(self): return len(self.df) # ---------------------------- # Get i\u0026#39;th item in dataset # ---------------------------- def __getitem__(self, idx): # Absolute file path of the audio file - concatenate the audio directory with # the relative path audio_file = self.data_path + self.df.loc[idx, \u0026#39;relative_path\u0026#39;] # Get the Class ID class_id = self.df.loc[idx, \u0026#39;classID\u0026#39;] aud = AudioUtil.open(audio_file) # Some sounds have a higher sample rate, or fewer channels compared to the # majority. So make all sounds have the same number of channels and same  # sample rate. Unless the sample rate is the same, the pad_trunc will still # result in arrays of different lengths, even though the sound duration is # the same. reaud = AudioUtil.resample(aud, self.sr) rechan = AudioUtil.rechannel(reaud, self.channel) dur_aud = AudioUtil.pad_trunc(rechan, self.duration) shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct) sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None) aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2) return aug_sgram, class_id  DataLoader object: Sử dụng Dataset object để lấy ra từng dữ liệu, gom thành từng Batch trước khi đưa cho model học.  Toàn bộ dữ liệu sẽ được chia thành 2 phần train/validation theo tỷ lệ 80/20, sau đó được sử dụng để tạo ra các DataLoader.  from torch.utils.data import random_split myds = SoundDS(df, data_path) # Random split of 80:20 between training and validation num_items = len(myds) num_train = round(num_items * 0.8) num_val = num_items - num_train train_ds, val_ds = random_split(myds, [num_train, num_val]) # Create training and validation data loaders train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True) val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False) Mỗi Batch sẽ bao gồm 2 Tensors, một là Mel Spectrogram và một là nhãn tương ứng. Các Batch được lấy ngẫu nhiên từ tập train thông qua các Epoch. Kích thước của Batch sẽ là: (batch_size, num_chanels, Mel freq_bands, time_steps).  Nếu chúng ta thử Visualize một Data Sample trong một Batch lên sẽ được như sau:  Ta có thể thấy các đường sọc ngang, dọc. Đó là kết quả của việc áp dụng các kỹ thuật SpecAugment.\nDữ liệu bây giờ đã sẵn sàng để đưa cho model học tập.\nToàn bộ quá trình Pre-precessing thông qua Dataset và DataLoader được thể hiện như trong hình dưới đây:  4. Tạo model\nBởi vì dữ liệu huấn luyện là Mel Spectrogram có dạng Image nên chúng ta sẽ xây dựng model bằng cách kết hợp một vài lớp CNN để trích xuất đặc trưng của ảnh và một vài lớp FC làm nhiệm vụ phân loại.  import torch.nn.functional as F from torch.nn import init # ---------------------------- # Audio Classification Model # ---------------------------- class AudioClassifier (nn.Module): # ---------------------------- # Build the model architecture # ---------------------------- def __init__(self): super().__init__() conv_layers = [] # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)) self.relu1 = nn.ReLU() self.bn1 = nn.BatchNorm2d(8) init.kaiming_normal_(self.conv1.weight, a=0.1) self.conv1.bias.data.zero_() conv_layers += [self.conv1, self.relu1, self.bn1] # Second Convolution Block self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) self.relu2 = nn.ReLU() self.bn2 = nn.BatchNorm2d(16) init.kaiming_normal_(self.conv2.weight, a=0.1) self.conv2.bias.data.zero_() conv_layers += [self.conv2, self.relu2, self.bn2] # Second Convolution Block self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) self.relu3 = nn.ReLU() self.bn3 = nn.BatchNorm2d(32) init.kaiming_normal_(self.conv3.weight, a=0.1) self.conv3.bias.data.zero_() conv_layers += [self.conv3, self.relu3, self.bn3] # Second Convolution Block self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) self.relu4 = nn.ReLU() self.bn4 = nn.BatchNorm2d(64) init.kaiming_normal_(self.conv4.weight, a=0.1) self.conv4.bias.data.zero_() conv_layers += [self.conv4, self.relu4, self.bn4] # Linear Classifier self.ap = nn.AdaptiveAvgPool2d(output_size=1) self.lin = nn.Linear(in_features=64, out_features=10) # Wrap the Convolutional Blocks self.conv = nn.Sequential(*conv_layers) # ---------------------------- # Forward pass computations # ---------------------------- def forward(self, x): # Run the convolutional blocks x = self.conv(x) # Adaptive pool and flatten for input to linear layer x = self.ap(x) x = x.view(x.shape[0], -1) # Linear layer x = self.lin(x) # Final output return x # Create the model and put it on the GPU if available model = nn.Parallel(AudioClassifier()) device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = myModel.to(device) # Check that it is on Cuda next(model.parameters()).device Ở đây mình có máy tính 2 GPU nên mình sẽ sử dụng đồng thời cả 2 GPU đó để quá trình huấn luyện diễn ra nhanh hơn.\n5. Training\nCó được model rồi, chúng ta cần định nghĩa Optimizer, Loss function, Learning Rate schedule, \u0026hellip; Tất cả có trong hàm Training như sau:\n# ---------------------------- # Training Loop # ---------------------------- def training(model, train_dl, num_epochs): # Tensorboard writer = SummaryWriter() # Loss Function, Optimizer and Scheduler criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(),lr=0.001) scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=int(len(train_dl)), epochs=num_epochs, anneal_strategy=\u0026#39;linear\u0026#39;) # Repeat for each epoch for epoch in range(num_epochs): running_loss = 0.0 correct_prediction = 0 total_prediction = 0 # Repeat for each batch in the training set for i, data in enumerate(train_dl): # Get the input features and target labels, and put them on the GPU inputs, labels = data[0].to(device), data[1].to(device) # Normalize the inputs inputs_m, inputs_s = inputs.mean(), inputs.std() inputs = (inputs - inputs_m) / inputs_s # Zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() scheduler.step() # Keep stats for Loss and Accuracy running_loss += loss.item() # Get the predicted class with the highest score _, prediction = torch.max(outputs,1) # Count of predictions that matched the target label correct_prediction += (prediction == labels).sum().item() total_prediction += prediction.shape[0] #if i % 10 == 0: # print every 10 mini-batches # print(\u0026#39;[%d, %5d] loss: %.3f\u0026#39; % (epoch + 1, i + 1, running_loss / 10)) # Print stats at the end of the epoch num_batches = len(train_dl) avg_loss = running_loss / num_batches avg_acc = correct_prediction/total_prediction writer.add_scalar(\u0026#34;Loss/train\u0026#34;, avg_loss, epoch) writer.add_scalar(\u0026#34;Acc/train\u0026#34;, avg_acc, epoch) print(f\u0026#39;Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}\u0026#39;) # Save model torch.save(model.state_dict(), \u0026#39;model.pt\u0026#39;) print(\u0026#39;Finished Training\u0026#39;) Chúng ta sẽ tiến hành huấn luyện model với 100 epochs:\nnum_epochs=100 training(myModel, train_dl, num_epochs) Sau 100 epochs, chúng ta thu được kết quả:\nEpoch: 0, Loss: 2.22, Accuracy: 0.19 Epoch: 1, Loss: 2.10, Accuracy: 0.27 ... Epoch: 98, Loss: 0.31, Accuracy: 0.90 Epoch: 99, Loss: 0.31, Accuracy: 0.90 Finished Training Và đồ thị Training Loss, Training Acc trên Tensorboard:   6. Inference\nTiếp theo, chúng ta sử dụng model đã lưu đề tiến hành dự đoán và đánh giá độ chính xác trên tập Test.\n# ---------------------------- # Inference # ---------------------------- def inference (model, test_dl): correct_prediction = 0 total_prediction = 0 # Disable gradient updates with torch.no_grad(): for data in test_dl: # Get the input features and target labels, and put them on the GPU inputs, labels = data[0].to(device), data[1].to(device) # Normalize the inputs inputs_m, inputs_s = inputs.mean(), inputs.std() inputs = (inputs - inputs_m) / inputs_s # Get predictions outputs = model(inputs) # Get the predicted class with the highest score _, prediction = torch.max(outputs,1) # Count of predictions that matched the target label correct_prediction += (prediction == labels).sum().item() total_prediction += prediction.shape[0] acc = correct_prediction/total_prediction print(f\u0026#39;Accuracy: {acc:.2f}, Total items: {total_prediction}\u0026#39;) # Run inference on trained model with the validation set load best model weights model_inf = nn.DataParallel(AudioClassifier()) device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model_inf = model_inf.to(device) model_inf.load_state_dict(torch.load(\u0026#39;model.pt\u0026#39;)) model_inf.eval() inference(model_inf, val_dl) Kết quả:\nAccuracy: 0.90, Total items: 1746 Trên tập Test, độ chính xác vẫn đạt đuọc 90%. Điều này chứng tỏ model của chúng ta hoạt động khá tốt, không bị hiện tượng Overfitting.\n7. Hướng mở rộng\nKết quả đạt được của chúng ta đã khá tốt rồi, tuy nhiên vẫn còn một số hướng có khả năng sẽ làm cho kết quả tốt hơn. Nếu gặp bài toán Audio Classification trong thực tế, bạn có thể thử áp dụng những cách này xem độ chính xác của model có được cải thiện thêm không nhế.\n7.1 Thay thế kiến trúc CNN model khác\nỞ đây, chúng ta đang tự xây dựng kiến trúc CNN model. Như bạn đã biết, có khá nhiều kiến trúc CNN model kinh điển cho bài toán Image Classification như VGG, ResNet, InceptionNet, \u0026hellip; Hãy thử với các kiến trúc này hoặc huấn luyện từ đầu hoặc sử dụng Transfer Learing \u0026hellip;\n7.2 Thay đổi Audio Features\nNhư bài trước đã phân tích, Audio Features có thể ở dạng Mel Spectrogram hoặc MFCCs. Bài này chúng ta đã sử dụng Mel Spectrogram rồi, còn lại MFCC là dành cho các bạn thử.\n7.3 Mel Spectrogram Hyper-parameters Tuning\nĐể tạo ra Mel Spectrogram, chúng ta cần cung cấp một số Hyper-parameters. Giá trị của các Hyper-parameters này ảnh hưởng ít nhiều đến Mel Spectrogram được tạo ra, từ đó ảnh hưởng đến kết quả của model. Hãy thử Tune các Hyper-parameters của Mel Spectrogram xem sao nhé. Tham khảo thêm tại đây.\n7.4 Áp dụng phương pháp k-Fold Cross Validation\nk-Fold Cross Validation vẫn là một trong những phương pháp khá hiệu quả đổi với các bài toán có sự mất cân bằng về dữ liệu. Dataset sử dụng trong bài này cũng được phân ra thành 10 folds, ngụ ý rằng nên sử dụng phương pháp k-Fold Cross Validation đối với nó để có được kết quả tốt hơn. Code cho phương pháp này mình cũng đã viết nhưng chưa có thời gian chạy thử để so sánh.\n8. Kết luận\nBài thứ tư trong chuỗi các bài viết về Audio Deep Learning này, chúng ta đã cùng nhau thực hiện code hoàn chỉnh bài toán Audio Classification. Một số hướng tiếp cận mở rộng cũng được đưa ra để các bạn nghiên cứu thêm.\nSource code bài này mình để ở đây.\nTrong bài thứ 5 tiếp theo, chúng ta sẽ thảo luận về bài toán Speech Recognition. Mời các bạn đón đọc.\n9. Tham khảo\n[1] Ketan Doshi, \u0026ldquo;Audio Deep Learning Made Simple (Part 3): Data Preparation and Augmentation\u0026rdquo;, Available online: https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5 (Accessed on 05 Jun 2021).\n[2] Scott Duda, \u0026ldquo;Urban Environmental Audio Classification Using Mel Spectrograms\u0026rdquo;, Available online: https://scottmduda.medium.com/urban-environmental-audio-classification-using-mel-spectrograms-706ee6f8dcc1 (Accessed on 05 Jun 2021).\n","permalink":"https://tiensu.github.io/blog/70_audio_deep_learning_part_4/","tags":["Audio Classification","Speech Recognition","Speech-to-Text"],"title":"Thực hành huấn luyện Audio Classification model"},{"categories":["Audio Classification","Speech Recognition","Speech-to-Text"],"contents":"Đây là bài thứ 3 trong chuỗi 5 bài về Audio Deep Learning. Trong bài này, chúng ta sẽ tiến thêm một bước sâu hơn là tìm cách nâng cao chất lượng của Mel Spectrogram thông qua tinh chỉnh các Hyper-parameters của nó. Đồng thời, chúng ta cũng sẽ tìm hiểu một số kỹ thuật làm giàu dữ liệu Audio. Cả 2 vấn đề này đều quan trọng, góp phần nâng cao hiệu quả của các Deep Learning model.\n1. Spectrograms Optimization with Hyper-parameter tuning\nỞ bài trước, chúng ta đã tìm hiểu về Mel Spectrogram và cách tạo ra nó sử dụng thư viện libsora. Thực chất, để tạo ra được Mel Spectrogram, chúng ta cần phải chỉ ra một số tham số cho nó. Nếu không được cung cấp, các giá trị mặc định sẽ được sử dụng. Để tạo ra được Mel Spectrogram thực sự tốt cho Deep Learning model, chúng ta cần điều chỉnh các tham số này tùy theo từng loại dữ liệu.\nTrước hết, chúng ta cần hiểu một số khái niệm sau:\n1.1 Discrete Fourier Transform (DFT) và Fast Fourier Transform (FFT)\nDFT và FFT là 2 cách để tính toán FFT. Sử dụng DFT thường mất nhiều thời gian hơn nên trong thực tế, FFT được sử dụng nhiều hơn. Về bản chất, FFT là một biến đổi khác của DFT.\nSử dụng FFT tuy nhanh nhưng nó lại chỉ đưa ra một cách khái quát về tất cả các thành phần tần số có trong toàn bộ chuỗi thời gian của Audio. Nó không chỉ ra được cách mà các thành phần tần số thay đổi như thế nào theo thời gian.\n1.2 Short-time Fourier Transform (STFT)\nĐể có được thông tin về sự biến đổi của các thành phần tần số qua thời gian, chúng ta phải sử dụng thuật toán Short-time Fourier Transform (STFT). STFT là một biến thể của Fourier Transform mà ở đó, các Audio Signal được chia nhỏ thành các phần bằng nhau, sử dụng Sliding Time window và Sliding Frequency window. Sau đó, áp dụng FFT trên mỗi phần đó rồi kết hợp các kết quả lại với nhau thành một kết quả cuối cùng. Cách làm này cho phép STFT có khả năng lấy được thông tin về sự thay đổi của các thành phần tần số theo thời gian.  Ví dụ, ta có một Audio Signal có độ dài 1 phút chứa các tần số từ 0Hz đến 10000Hz (theo Mel Scale). Thuật toán STFT để tạo ra Mel Spectrogram sẽ làm như sau:\n Chọn kích thước của Time-Window là 3s, ta có 20 Time-Sections. Chia khoảng tần số thành 10 bands: 0-1000Hz, 1000-2000Hz, \u0026hellip;, 9000-10000Hz.  Kết quả ta thu được một màng 2D Numpy có kích thước (10,20), ở đó:\n Mỗi cột đại diện cho STFT của một Time-Section. Mỗi hàng đại diện cho giá trị của biên độ trong một Band.  Mỗi hàng/cột ở đây tương ứng với mỗi hàng/cột của Mel Spectrogram.\nThử in ra Type và Shape của Mel Spectrogram:\n#Spectrogram is a 2D numpy array print(type(mel_sgram), mel_sgram.shape) # \u0026lt;class \u0026#39;numpy.ndarray\u0026#39;\u0026gt; (128, 194) Ta thấy Mel Spectrogram là một Numpy Array có kích thước (128, 194).\n1.3 Mel Spectrogram Hyperparameters\nDưới đây là danh sách các Hyperparameters của Mel Spectrogram được sử dụng trong thư viện librosa (các thư viện khác cũng có các hyperparameters tương tự):\n  Frequency Bands:\n fmin: tần số nhỏ nhất fmax: tần số lớn nhất n_mels: số lượng Frequency Bands, hay chiều cao của Mel Spectrogram.    Time Sections\n n_fft: kích thước Time-Window, hay chiều dài mỗi Time-Section. hop_length: Có thể hiểu nó tương tự như Stride trong CNN, tức là số bước trượt/nhảy tính theo đơn vị Hop của Time-Window (xem hình minh họa bên trên)  Các tham số này có thể điều chỉnh được và chúng ta nên điều chỉnh nó tùy theo từng bộ dữ liệu để có kết quả tốt nhất.\n  2. Mel Frequency Cepstral Coeficients (MFCC)\nMFCC là một dạng thể hiện khác của Audio Data, được biến đổi từ Mel Spectrogram. Cụ thể MFCC nén các Frequency Bands từ Mel Spectrogram tương ứng với các mức tần số thông thường của giọng nói con người. Chính vì thê mà Mel Spectrogram phù hợp với đa số các loại Audio, còn MFFC thì phù hợp hơn với âm thanh phát ra từ miệng chúng ta.\nimport sklearn import librosa import librosa.display # Load the audio file AUDIO_FILE = \u0026#39;./7061-6-0-0.wav\u0026#39; samples, sample_rate = librosa.load(AUDIO_FILE, sr=None) mfcc = librosa.feature.mfcc(samples, sr=sample_rate) # Center MFCC coefficient dimensions to the mean and unit variance mfcc = sklearn.preprocessing.scale(mfcc, axis=1) librosa.display.specshow(mfcc, sr=sample_rate, x_axis=\u0026#39;time\u0026#39;) print (f\u0026#39;MFCC is of type {type(mfcc)} with shape {mfcc.shape}\u0026#39;) # MFCC is of type \u0026lt;class \u0026#39;numpy.ndarray\u0026#39;\u0026gt; with shape (20, 194)   Ta thấy, Mel Spectrogram có kích thước (128,194), còn MFFC có kích thước (20, 194).\n3. Audio Data Augmentation\nChúng ta đã không còn xa lạ với khái niệm Data Augmentaion và lợi ích của nó trong các bài toán CV, NLP. Đối với Auio, chúng ta cũng làm tương tự. Có thể áp dụng Augmentaion đối với Raw Audio hoặc Mel Spectrogram.\n3.1 Spectrogram Augmentation\nCác phép biến đổi của Image khong thể áp dụng trực tiếp vào cho Mel Spectrogram được vì nó sẽ biến đổi hoàn toàn Sound Signal, tạo ra các Sound Signal mới hoàn toàn. Thay vào đó, chúng ta sẽ sử dụng phương pháp SpecAugment với 2 kỹ thuật chính:\n Frequency Masks: Ngẫu nhiên Mask một khoảng liên tục của Frequency ngang theo trục X. Time Masks: Ngẫu nhiên Mask một khoảng liên tục của Time dọc theo trục Y.   3.2 Raw Audio Augmentation\nMột số kỹ thuật:\n  Time Shift: dịch chuyển Audio sang trái/phải một đoạn ngẫu nhiên    Pitch Shift: Thay đổi ngẫu nhiên các tần số thành phần của Audio    Time Stretch: Tăng/giảm tốc độ của âm thanh một cách ngẫu nhiên.    Add Noise: Thêm nhiễu ngẫu nhiên vào Audio    4. Kết luận\nBài thứ ba trong chuỗi các bài viết về Audio Deep Learning này, chúng ta biết thêm về một số Hyperparameters của Mel Spectrogram cần tune để thu được kết quả tốt nhất. Chúng ta cũng biết MFFC là gì và khi nào thì sử dụng nó. Cuối cùng, chúng ta biết các phương pháp, kỹ thuật để tăng cường dữ liệu Audio, giúp Deep Learning models có thể học được tốt hơn.\nSource code bài này mình để ở đây.\nỞ bài tiếp theo, chúng ta sẽ áp dụng những kỹ thuật đã tìm hiểu ngày hôm nay vào bài toán Audio Classification. Mời các bạn đón đọc.\n5. Tham khảo\n[1] Ketan Doshi, \u0026ldquo;Audio Deep Learning Made Simple (Part 3): Data Preparation and Augmentation\u0026rdquo;, Available online: https://towardsdatascience.com/audio-deep-learning-made-simple-part-3-data-preparation-and-augmentation-24c6e1f6b52 (Accessed on 30 May 2021).\n[2] Wikipedia, \u0026ldquo;Discrete Fourier transform\u0026rdquo;, Available online: https://en.wikipedia.org/wiki/Discrete_Fourier_transform_cepstrum (Accessed on 30 May 2021).\n[3] Wikipedia, \u0026ldquo;Fast Fourier transform\u0026rdquo;, Available online: https://en.wikipedia.org/wiki/Fast_Fourier_transform (Accessed on 30 May 2021).\n[4] Wikipedia, \u0026ldquo;Short-time Fourier transform\u0026rdquo;, Available online: https://en.wikipedia.org/wiki/Short-time_Fourier_transform (Accessed on 30 May 2021).\n[5] Springer, \u0026ldquo;MFCC Features\u0026rdquo;, Available online: https://link.springer.com/content/pdf/bbm%3A978-3-319-49220-9%2F1.pdf (Accessed on 30 May 2021).\n","permalink":"https://tiensu.github.io/blog/69_audio_deep_learning_part_3/","tags":["Audio Classification","Speech Recognition","Speech-to-Text"],"title":"Audio Data Preparation - Một số kỹ thuật nâng cao"},{"categories":["Audio Classification","Speech Recognition","Speech-to-Text"],"contents":"Đây là bài thứ 2 trong chuỗi 5 bài về Audio Deep Learning. Trong bài này, chúng ta sẽ tìm hiểu cách xử lý dữ liệu Audio bằng các thư viện của Python. Chúng ta cũng tìm hiểu về Mel Spectrogram, một dạng biến đổi từ Spectrogram giúp Deep Learning model học tốt hơn.\n1. Audio File Formats and Python Libraries\nBắt đầu từ âm thanh chúng ta nghe được trong thực tế ở dạng tín hiệu tương tự, nó được số hóa và lưu lại theo các định dạng khác nhau: .mp3, .wav, .wma, .aac, .flac, \u0026hellip;\nPython có một số thư viện xử lý dữ liệu Audio rất tốt. Nổi bật nhất là 2 thư viện Librosa và Scipy. Nếu bạn sử dụng Pytorch thì có thư viện torchaudio, sử dụng Tensorflow thì có tư viện tf.audio. Cả 2 đều khá tiện dụng, được xây dựng để xử lý riêng cho dữ liệu Audio.\nThử đọc một file Audio bằng thư viện librosa:\n# Load the audio file with librosa import librosa AUDIO_FILE = \u0026#39;./7061-6-0-0.wav\u0026#39; samples, sample_rate = librosa.load(AUDIO_FILE, sr=None) Tương tự với thư viện scipy:\n# Load the audio file with scipy from scipy.io import wavfile sample_rate, samples = wavfile.read(AUDIO_FILE) Kết quả đọc Audio file được thể hiện trên đồ thị như sau:  Nếu sử dụng Jupyter Notebook, chúng ta có thể nghe được file Audio đó:\nfrom IPython.display import Audio Audio(AUDIO_FILE)   2. Audio Signal Data\nNhư chúng ta đã biết từ bài trước đó, Audio Data có được bằng cách lấy mẫu từ Sound Analog Signal theo một chu kỳ thời gian và đo đặc giá trị của biên độ tại mỗi thời điểm lấy mẫu đó. Audio Data được lưu lại thành file theo một trong các định dạng nén (.mp3, .wav, \u0026hellip;). Khi đọc lên bằng các thư viện xử lý, nó được giải nén và chuyển thành một Numpy Array. Mảng dữ liệu này là giống nhau cho dù Audio Data được lưu dưới bất kỳ định dạng nào.\nTrong bộ nhớ, Audio có thể coi là một chuỗi các giá trị của biên độ theo thời gian. Ví dụ, nếu tần số lấy mẫu là 16800Hz thì cứ 1s Audio sẽ có 16800 giá trị biên độ.\nprint (\u0026#39;Example shape \u0026#39;, samples.shape, \u0026#39;Sample rate \u0026#39;, sample_rate, \u0026#39;Data type\u0026#39;, type(samples)) print (samples[22400:22420])   Khoảng giá trị của biên độ được quy định bởi thông số bit-length. Ví dụ, bit-length bằng 16 có nghĩa là biên độ có thể có giá trị trong khoảng từ 0 đến $2^{16}-1$. Bit-length càng lớn thì chất lượng của Audio càng tốt.\n3. Spectrograms\nDữ liệu Audio hiếm khi được đưa trực tiếp vào các Deep Learning model để huấn luyện. Thay vào đó, chúng thường được chuyển sang dạng Spectrogram như trong bài trước chúng ta đã đề cập.\nTa sẽ vẽ Spectrogram lên đồ thị:\nsgram = librosa.stft(samples) librosa.display.specshow(sgram)   Đây là dạng nguyên thủy của Spectrogram. Rõ ràng, chúng ta không thể thấy rõ được các thông tin về tần số, biên độ mà Spectrogram thể hiện. Điều này được giải thích là do khả năng nhận thức âm thanh của con người. Hầu hết những âm thanh mà chúng ta nghe được đều tập trung xung quanh một dải tần số và biên độ khá hẹp.\n4. Mel Spectrogram\nĐể giải quyết vấn đề này, Spectrogram được chuyển sang một dạng mới, gọi là Mel Spectrogram mà ở đó:\n Tần số được thay thế bằng giá trị Logarithmic của nó, gọi là Mel Scale. Biên độ được thay thế bằng giá trị Logarithmic của nó, gọi là Decibel Scale.  Hình dưới đây cho ta biết các giá trị Decibel Scale của một số loại âm thanh khác nhau.\n Chúng ta thử vẽ lại Spectrogram ở trên, thay thế tần số bằng Mel Scale:\n# use the mel-scale instead of raw frequency sgram_mag, _ = librosa.magphase(sgram) mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sample_rate) librosa.display.specshow(mel_scale_sgram)   Ta thấy thông tin đã xuất hiện rõ ràng hơn. Tiếp tục sử dụng Decibel Scale thay cho biên độ để tạo thành Mel Spectrogram:\n# use the decibel scale to get the final Mel Spectrogram mel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min) librosa.display.specshow(mel_sgram, sr=sample_rate, x_axis=\u0026#39;time\u0026#39;, y_axis=\u0026#39;mel\u0026#39;) plt.colorbar(format=\u0026#39;%+2.0fdB\u0026#39;)   Đến đây thì thông tin của Audio đã được thể hiện rất rõ ràng trên hình ảnh của Mel Spectrogram.\n5. Kết luận\nBài thứ hai trong chuỗi các bài viết về Audio Deep Learning này, chúng ta học cách sử dụng một số thư viện của Python để xử lý Audio Data. Chúng ta cũng tìm hiểu về Mel Spectrogram và cách tạo ra nó. Sử dụng Mel Spectrogram thay thế cho Spectrogram thông thường sẽ mang lại hiệu quả hơn khi đưa vào các Deep Learning model để huấn luyện.\nSource code bài này mình để ở đây.\nỞ bài tiếp theo, chúng ta sẽ tìm hiểu một số kỹ thuật nâng cao, cải thiện Audio Data để giúp Deep Learning models học tốt hơn. Mời các bạn đón đọc.\n6. Tham khảo\n[1] Ketan Doshi, \u0026ldquo;Audio Deep Learning Made Simple (Part 2): Why Mel Spectrograms perform better\u0026rdquo;, Available online: https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505 (Accessed on 25 May 2021).\n[2] Wikipedia, \u0026ldquo;Mel-frequency cepstrum\u0026rdquo;, Available online: https://en.wikipedia.org/wiki/Mel-frequency_cepstrum (Accessed on 25 May 2021).\n","permalink":"https://tiensu.github.io/blog/68_audio_deep_learning_part_2/","tags":["Audio Classification","Speech Recognition","Speech-to-Text"],"title":"Xử lý dữ liệu Audio trong Python. Tìm hiểu về Mel Spectrogram"},{"categories":["Audio Classification","Speech Recognition","Speech-to-Text"],"contents":"Bên cạnh Images, Text thì Sound cũng là một dạng dữ liệu mà chúng ta thường gặp trong đời sống hàng ngày. Trong chuỗi 5 bài tiếp theo, chúng ta sẽ tìm hiểu chi tiết về bài toán Audio Classification và Speech Recognition (Speech to Text).\n Bài 1: Giới thiệu chung về bài toán Audio Clasifidation Bài 2: Mel Spectrograms Bài 3: Feature Optimization and Augmentation Bài 4: Xây dựng Audio Classification model bằng PyTorch Bài 5: Tìm hiểu bài toán Speech Recognition (Speech to Text)  Nội dung của bài đầu tiên bao gồm một số lý thuyết chung về Audio, các ứng dụng liên quan đến Sound. Chúng ta cũng tìm hiểu về Spectrogram và một số kỹ thuật, kiến trúc mô hình để làm việc với Sound.\n1. Kiến thức chung về Sound Signal\nTừ hồi học phổ thông chúng ta đã biết, Sound là một dạng tín hiệu được sinh ra từ sự thay đổi áp suất không khí, bắt nguồn từ một dao động cơ học nào đó. Cường độ của sự thay đổi áp suất này có thể đo được, và nó chính là biên độ (Applitude) của Sound Signal.\nSound Signal thường lặp đi lặp lại theo một chu kỳ T, đồ thị của nó có dạng sóng.  Giá trị nghịch đảo của chu kỳ T, ký hiệu là f, gọi là tần số của Sound Signal. Nó thể hiện mức độ dao động của Signal trong thời gian 1s (bằng số đỉnh của Signal trong 1s). Đơn vị của f là Hertz.\nTrong thực tế, đồ thị của Sound Signal thường không đơn giản dạng Sin như vậy, mà phức tạp hơn rất nhiều. Tuy nhiên, chúng vẫn có dạng sóng và có chu kỳ. Ví dụ, đồ thị của một dụng cụ âm nhạc như dưới đây:  Nhìều Sound Signal có thể được tổng hợp thành một Sound Signal duy nhất.\nVề mặt cảm thụ sinh học, mỗi Sound Signal có một đặc trưng riêng, gọi là âm sắc (timbre). Tai người có thể phân biệt được các Sound khác nhau dựa vào âm sắc của các Sound đó.\n2. Số hóa (Digitize) Sound Signal\nNguyên bản, Sound Signal là một dạng tín hiệu tương tự (Analog Signal) liên tục theo thời gian. Tuy nhiên, để thuận lợi trong việc lưu trữ, xử lý và truyền tải, Sound Signal được chuyển sang dạng Số (Digital Signal). Việc chuyển đổi này phải đảm bảo không làm mất mát quá nhiều thông tin so với tín hiệu gốc, và từ tín hiệu đã chuyển đổi có thể dễ dàng khôi khục lại gần như nguyên vẹn tín hiệu ban đầu. Số hóa Sound Signal được thực hiện bằng cách lấy giá trị biên độ của nó tại các vị trí cách đều nhau trong mỗi chu kỳ.  Mỗi vị trí như vậy được gọi là một mẫu (Sample). Ta có khái niệm Tần số lấy mẫu (Sample Rate) là số lượng mẫu trong 1s.\nMột câu hỏi đặt ra là giá trị của Sample Rate là bao nhiêu là hợp lý. Hai nhà khoa học Nyquist và Shannon đã đồng thời, độc lập đưa ra một định lý, gọi là Định lý lấy mẫu Nyquist–Shannon, về việc xác đinh giá trị của Sample Rate. Định lý phát biểu như sau:\nMột hàm số tín hiệu x(t) không chứa bất kỳ thành phần tần số nào lớn hơn hoặc bằng một giá trị fm có thể biểu diễn chính xác bằng tập các giá trị của nó với chu kỳ lấy mẫu T = 1/(2fm).\nTừ định lý này có thể dễ dàng suy ra, Tần số lấy mẫu phải thoả mãn điều kiện $f_s$ ≥ $2f_m$. Tần số giới hạn $f_s/2$ này được gọi là tần số Nyquist và khoảng $(-f_s/2; f_s/2)$ gọi là khoảng Nyquist. Thực tế, tín hiệu trước khi lấy mẫu sẽ bị giới hạn bằng một bộ lọc để tần số tín hiệu nằm trong khoảng Nyquist.\nĐịnh lý Nyquist–Shannon được áp dụng cho tín hiệu nói chung chứ không phải chỉ riêng tín hiệu âm thanh.\nĐến đây, chúng ta cần phân biệt được 2 khái niệm Audio và Sound. Sound có nguồn gốc là các dao động cơ học lan truyền trong các môi trường đàn hồi (rắn, lỏng, khí), còn Audio được sinh ra từ các thiết bị điện tử thông qua quá trình lấy mẫu, ghi âm, \u0026hellip;\n3. Chuẩn bị dữ liệu Audio cho Deep Learning model\nHãy nhớ lại khoảng 10 năm trước, khi mà các kỹ thuật Deep Learning còn chưa phát triển, chúng ta phải sử dụng các phương pháp thống kê, xử lý truyền thống để có thể trích xuất được các đặc trưng làm đầu vào cho các thuật toán Machine Learning. Cụ thể, đối với lĩnh vực Computer Vision, chúng ta sử dụng các kỹ thuật xử lý ảnh như phát hiện biên, ngưỡng nhị phân, \u0026hellip; Đối với lĩnh vực NLP, chúng ta sử dụng phương pháp N-gram, Term Frequency, \u0026hellip;\nDữ liệu Audio cũng không ngoại lệ, chúng ta phải thực hiện rất nhiều công việc tay chân, sữ dụng các phương pháp phân tích ngữ âm, âm vị, \u0026hellip; để có thể tạo được các vectors đặc trưng của chúng.\nNgày nay, với sự trợ giúp của Deep Learning, việc trích xuất đặc trưng để chuẩn bị dữ liệu cho các mô hình học máy trở nên đơn giản hơn rất nhiều. Riêng đối với dữ liệu Audio, chúng ta sẽ chuyển chúng sang dạng Image và sử dụng kiến trúc CNN kinh điển để xử lý chúng. Từ Audio chuyển sang Image, điều này nghe có vẻ rất kỳ lạ, nhưng thực tế đó lại là cách làm rất bình thường trong các bài toán xử lý dữ liệu Audio. Để hiểu rõ hơn về vấn đề này, trước tiên chúng ta cần tìm hiểu Spectrum và Spectrogram.\nHình dưới đây thể hiện Spectrum của một đoạn nhạc. Trục tung là giá trị biên độ, trục hoành là giá trị tần số của mỗi tín hiệu thành phần.  Tần số có giá trị nhỏ nhất được gọi là tần số Cơ bản. Các tần số khác là bội số của tần số cơ bản được gọi là Sóng hài (harmonics). Ví dụ, nếu tần số cơ bản là 200Hz, thì các sóng hài của nó là 400Hz, 600Hz, \u0026hellip;\n3.1 Spectrum\nNhư đã nói ở trên, một Sound Signal trong thực tế thường là sự tổng hợp của nhiều tín hiệu thành phần khác nhau. Ví dụ, tiếng nói của chúng ta bao gồm cả các tạp âm (Noise) xung quanh. Mỗi tín hiệu thành phần đó lại có tần số khác nhau, và do vậy, tổng hợp các tần số thành phần ta có tần số của Sound Signal.\nSpectrum chính là tập hợp các tần số của các tín hiệu thành phần tạo nên Sound Signal của chúng ta.\n3.2 Miền thời gian và miền Tần số\nMỗi Sound Signal đều có 2 miền giá trị: Thời gian và Tần số. Trong mỗi miền đó, Sound Signal được thể hiện theo cách khác nhau.   Thể hiện trong miền thời gian Trong miền thời gian, Sound Signal mô tả sự thay đổi của biên độ theo thời gian. Biên độ nằm trên trục tung và thời gian nằm trên trục hoành (xem đồ thị bên trên). Thể hiện trong miền tần số Trong miền tần số, Sound Signal mô tả sự thay đổi của biên độ theo tần số. Biên độ nằm trên trục tung và tần số nằm trên trục hoành (xem đồ thị bên trên).  3.3 Spectrogram\nỞ mục 3.2, ta đã nói đến mỗi liên hệ giữa biên độ với thời gian và biên độ với tần số. Hơn thế nữa, tần số và và thời gian cũng có mối liên hệ với nhau. Đồ thị thể hiện mối liên hệ này gọi là Spectrogram, trong đó, trục X thể hiện thời gian và trục Y thể hiện tần số. Nói một cách dễ hiểu thì Spectrogram thể hiện sự thay đổi của tần số theo thời gian. Không chỉ có vậy, độ lớn của biên độ cũng được Spectrogram thể hiện thông qua màu sắc. Màu sắc càng sáng thì thì biên độ càng lớn và ngược lại.\nTrong 2 hình bên dưới, hình thứ nhất thể hiện Soung Signal trong miền thời gian. Tại t = 600000, biên độ có giá trị lớn nhất. Hình thứ 2 là Spectrogram của Sound Signal đó. Tương ứng với giá trị lớn nhất của biên độ là vùng màu sắc sáng nhất trên Spectrogram.\n Có thể nói, Spectrogram là cách thể hiện tốt nhất của Sound Signal, dưới dạng một hình ảnh, bởi vì nó mang đầy đủ thông tin về thời gian, tần số và biên độ. Và do vậy, Spectrogram được sử dụng làm dữ liệu đầu vào cho các Deep Learning model, như một bức ảnh thông thường.\nSpectrogram được sinh ra bằng cách áp dụng phép biến đổi Fourier lên một Signal để phân tách Signal đó thành các tần số thành phần. Có lẽ đa số chúng ta đã quên mất cách tính Fourier bằng tay như thế nào, mặc dù chúng ta đã được học ở bậc đại học. Rất may là có một số thư viên của Python sẽ giúp chúng ta thực hiện Fourier Transform một cách dễ dàng.\n4. Audio Deep Learning models\n Hầu hết các Deep Learning models đều sử dụng Spectrograms để làm đặc trưng cho Audio. Luồng xử lý sẽ như sau:\n Chuyển đổi Raw Audio sang Spectrogram. Áp dụng một số kỹ thuật tăng cường dữ liệu. Các kỹ thuật này cũng có thể áp dụng cho Raw Audio. Bước này không bắt buộc. Xây dựng CNN model và huấn luyện nó. Output của CNN là các Feature Maps. Tùy vào bài toán cụ thể mà chúng ta sinh ra các dạng Output khác nhau.  Audio Classification: Cho Feature Maps đi qua một bộ phân lớp (FC, SVM, \u0026hellip;) để sinh ra nhãn cho Audio Speech to Text: Cho Feature Maps đi qua RNN để sinh ra Text tương ứng với Audio.    5. Một số bài toán mà Deep Learning có thể giải quyết đối với dữ liệu Audio\nAudio luôn tồn tại xung quanh chúng ta: giọng người nó, tiếng kêu động vật, tiếng máy móc hoạt động, \u0026hellip; Áp dụng Deep Learning, chúng ta có thể giải quyết được một số bài toán sau:\n5.1 Audio Classification\nĐây có lẽ là bài toán phổ biến nhất của Audio. Input là 1 đoạn Audio, Output là nhãn của nó.  Một số ứng dụng thực tế của bài toán này:\n Phát hiện sự hư hỏng của máy móc thông qua tiếng kêu khi hoạt động bình thường và khi hư hỏng. Phát hiện trộm dựa vào phát hiện tiếng đập vỡ cữa kính Phát hiện bệnh thông qua tiếng ho, tiếng thở \u0026hellip;  5.2 Audio Separation and Segmentation\nBài toán này liên quan đến viêc tách riêng các tín hiệu thành phần từ tín hiệu gốc ban đầu.  Một số ví dụ ứng dụng thực tế:\n Tách riêng giọng của mỗi người trong 1 cuộc họp. Tách riêng âm thanh của một loại nhạc cụ trong một buổi hòa nhạc. Tách giọng ca sĩ khỏi bài hát.  5.3 Music Generation and Music Transcription\nCũng giống như GAN hay Language Model, một số Deep Learning model có thể sinh ra các bản nhạc theo thể loại, loại nhạc cụ hay thậm chí là phong cách của một nhạc sĩ cụ thể. Đó gọi là Music Generation.\nỞ chiều ngược lại, từ một bản nhạc dạng Audio, Deep Learning model có thể dịch ngược lại thành Text, kèm theo giai điệu, node nhạc, \u0026hellip;  5.4 Voice Recognition\nThực chất, đây cũng là bài toán Audio Classification nhưng được áp dụng cho giọng nói của con người.  Một số ứng dụng thực tế của bài toán này:\n Phân biệt giọng nam và giọng nữ Nhận diện tên người thông qua giọng nói của họ Nhận diện cảm xúc thông qua giọng nói của họ \u0026hellip;  5.5 Speech to Text and Text to Speech\nĐối với giọng nói của con người, chúng ta có thể đi sâu hơn một chút. Đó là không chỉ nhận dạng người nói là ai mà còn có thể hiểu được người đó nói gì. Điều này được gọi là bài toán Speech to Text, tức chuyển Audio thành văn bản.  Theo hướng ngược lại, chúng ta có bài toán Text to Speech, hay Speech Synthesic, tức là chuyển đổi một văn bản thành dạng Audio.\nCả 2 bài toán này đều có rất nhiều ứng dụng hữu ích trong cuộc sống, ví dụ như trợ lý ảo thông mình Siri, Alexa, Cortana hay Google Home.\n6. Kết luận\nBài đầu tiên trong chuỗi các bài viết về Audio Deep Learning này, chúng ta đã tìm hiểu ở mức khái quát lý thuyết về Sound Signal, các số hóa Sound Signal, cách tạo dữ liệu cho Deep Learning model sử dụng Spectrogram. Cuối cùng, chúng ta cũng điểm qua một số bài toán cụ thể mà Deep Learning có thể giải quyết đối với dữ liệu Audio.\nỞ bài tiếp theo, mình sẽ cùng các bạn tìm hiểu chi tiết về Mel Spectrogram, một dạng biến đổi khác từ Spectrogram mà có thể giúp cho các Deep Learning model học tốt hơn. Mời các bạn đón đọc.\n7. Tham khảo\n[1] Ketan Doshi, \u0026ldquo;Audio Deep Learning Made Simple (Part 1): State-of-the-Art Techniques\u0026rdquo;, Available online: https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504 (Accessed on 20 May 2021).\n[2] Wikipedia, \u0026ldquo;Định lý lấy mẫu Nyquist–Shannon\u0026rdquo;, Available online: https://vi.wikipedia.org/wiki/%C4%90%E1%BB%8Bnh_l%C3%BD_l%E1%BA%A5y_m%E1%BA%ABu_Nyquist%E2%80%93Shannon (Accessed on 20 May 2021).\n[3] Wikipedia, \u0026ldquo;Fourier transform\u0026rdquo;, Available online: https://en.wikipedia.org/wiki/Fourier_transform (Accessed on 20 May 2021).\n[4] Wikipedia, \u0026ldquo;Spectrogram\u0026rdquo;, Available online: https://en.wikipedia.org/wiki/Spectrogram (Accessed on 20 May 2021).\n","permalink":"https://tiensu.github.io/blog/67_audio_deep_learning_part_1/","tags":["Audio Classification","Speech Recognition","Speech-to-Text"],"title":"Giới thiệu chung về các bài toán Sound/Audio sử dụng Deep Learning"},{"categories":["Object Detection"],"contents":"Object Detection là một trong những bài toán rất phổ biến trong lĩnh vực Computer Vision. Ngày nay, với sự giúp đỡ của các thuật toán Deep Learning, bài toán này đã được giải quyết khá tốt. Các thuật toán nối tiếng có thể kể ra ở đây là: R-CNN, Fast R-CNN, Faster R-CNN, YOLOv1, YOLOv2, YOLOv3, YOLOv4, YOLOv5, SSD, EfficientDet. Trong bài này, hãy cùng nhau tóm tắt lại các thuật toán Deep Learning đó nhé!\n1. Các thuật toán Object Detection\n1.1 R-CNN\nĐây có thể coi là thuật toán Deep Learning đầu tiên giải quyết bài toán Computer Vision. Trước đó, cũng có một vài thuật toán khác (không phải Deep Learning), như Exhaustive Search, \u0026hellip; Nhược điểm chung của các thuật toán này là chúng yêu cầu tài nguyên tính toán rất lớn, thời gian xử lý cũng rất lâu.\nĐể giải quyết những yếu điểm đó, R-CNN đề xuất sử dụng phương pháp Selective Search để trích xuất thông tin từ khoảng 2000 khu vực trên bức ảnh (mỗi khu vực được gọi là Region Proposal). Các thông tin được trích xuất sau đó sẽ được đưa qua một mạng CNN để xác định vị trí cũng như phân loại đối tượng.  Mặc dù vậy, R-CNN vẫn cần trung bình khoảng 50s để xử lý một bức ảnh. Nếu ảnh có nhiều đối tượng thì thời gian xử lý còn lâu hơn nữa.\n1.2 Fast R-CNN\nFast R-CNN ra đời để giải quyết hạn chế của R-CNN. Thay vì chia bức ảnh thành các Region Proposals và đưa chúng vào mạng CNN như R-CNN, Fast R-CNN đưa toàn bộ bức ảnh vào mạng CNN một lần để sinh ra Feature Map. Từ Feature Map này, các Region Proposals mới được nhận diện và đưa vào mạng FC. Cuối cùng, Softmax được sử dụng để dự đoán nhãn cho mỗi đối tượng và tính toán tọa độ các Bouding Boxs của chúng.  Fast R-CNN nhanh hơn R-CNN vì mạng CNN chỉ hoạt động một lần trên mỗi bức ảnh.\n1.3 Faster R-CNN\nTương tự như Fast R-CNN, toàn bộ bức ảnh cũng được đi qua mạng CNN đẻ sinh ra Feature Map. Điểm khác ở đây là Faster R-CNN sử dụng một Region Proposal Network (RPN) thay vì Selective Search để sinh ra các Region Proposals. Tiếp đó, ROI Pooling nhận đầu vào là các Region Proposals đó để sinh ra nhãn dự đoán và tọa độ của Bounding Box cho mỗi đối tượng trong ảnh.  So sánh về mặt thời gian xử lý giữa 3 thuật toán trong hộ R-CNN, có sự giảm dần từ R-CNN -\u0026gt; Fast R-CNN -\u0026gt; Faster R-CNN.  1.4 YOLO - You Look Only Once\nCác thuật toán trong họ R-CNN có thể được phân loại là nhóm Two Stages Detector, bởi vì cách làm việc của chúng bao gồm 2 bước (2 stages). Đầu tiên, chúng lựa chọn ROI (Region of Interest) trong bức ảnh. Sau đó, chúng phân loại các ROI đó sử dụng mạng CNN. Đó chính là nguyên nhân làm cho tốc độ thực thi của những thuật toán đó tương đối chậm.\nYOLO là một thuật toán Single Stage Detecto (Single Shot Detector), nghĩa là chúng sẽ dự đóan nhãn và vị trí của đối tượng trong toàn bộ bức ảnh chỉ với một lần chạy thuật toán duy nhất. Và tất nhiên, cách làm việc này giúp cho thời gian xử lý của YOLO rất nhanh, phù hợp với các ứng dụng cần chạy Realtime.\nCách làm viêc của YOLO có thể tóm tắt như sau:\n Chia bức ảnh thành các Cells. Ví dụ: 19x19, 13x13, \u0026hellip; Mỗi Cell chịu trách nhiệm dự đoán ra b Bounding Box. b = 3, 5, 7, \u0026hellip; Ouput của việc dự đoán bao gồm:  Tọa độ trung tâm của Bounding Box ($b_x, b_y$) Chiều rộng của Bounding Box ($b_w$) Chiều cao của Bounding Box ($b_h$) Nhãn của đối tượng (nếu có) trong Bounding Box (c) Xác suất có đối tượng trong Bounding Box ($p_c$)     Hầu hết các Cell và Bounding Box không chứa đối tượng. Quy định một giá trị ngưỡng cho $p_c$ để loại bỏ bớt những Bounding Box này. Cách này gọi là NMS (Non-Max Suppression).  Darknet github phổ biến nhất của YOLO. Bạn có thể sử dụng nó cho các dự án của mình.\nTiếp sau thành công của YOLOv1, các phiên bản tiếp theo YOLOv2, YOLOv3, YOLOv4, YOLOv5 lần lượt ra đời, phiên bản sau kế thừa ưu điểm và hạn chế nhược điểm của phiên bản trước đó.\n  YOLOv1:\n 24 Conv layers, 2 FC layers -\u0026gt; tổng cộng có 26 layers. Nhược điểm chính là khả năng phát hiện những đối tượng kích thước nhỏ rất kém. Chi tiết xem tại đây    YOLOv2:\n Thêm BN layers -\u0026gt; tổng cộng có 30 layers. Có thêm 3 Anchor Boxes. Không có FC layers. Kích thước của Training Image được Scale trong khoảng 320-608. Mỗi đối tượng có thể có nhiều nhãn. Vẫn gặp khó khăn khi phát hiện những đối tượng có kích thước nhỏ. Chi tiết xem tại đây    YOLOv3:\n Tổng cộng 106 layers. Thay đối cách tính Loss Function. Sử dụng 9 Anchor Boxes. Phát hiện đối tượng kích thước nhỏ khá tốt. Chi tiết xem tại đây    YOLOv4:\n Sử dụng CSPDARKNET53 model làm Backbone. Neck — Spatial pyramid pooling and Path Aggregation Network Head — Class subnet and Box subnet, as well as in YOLOv3 Hỗ trợ rất nhiều Frameworks: TensorFlow, OpenCV, OpenVINO, PyTorch, TensorRT, ONNX, CoreML, etc. So với YOLOv3,mAP và FPS của YOLOv4 tăng tương ứng 10% và 12%. So với EfficientDet, YOLOv4 nhanh hơn khoảng 2 lần. Chi tiết xem tại đây       YOLOv5:  Ra đời chỉ vài ngày sau YOLOv4 Có 4 phiên bản khác nhau: YOLOv5s, YOLOv5m, YOLOv5l, YOLOv5x. Độ chính xác tăng dần và tốc độ giảm dần theo thứ tự đó. So sánh với YOLOv4:  \u0026ldquo;If you’re a developer looking to incorporate near realtime object detection into your project quickly, YOLOv5 is a great choice. If you’re a computer vision engineer in pursuit of state-of-the-art and not afraid of a little more custom configuration, YOLOv4 in Darknet continues to be most accurate.\u0026rdquo; CSPDarknet53s-YOSPP gets 19.5% faster model inference speed and 1.3% higher AP than YOLOv5l.      1.5 SSD - Single Shot Detector\nSSD cũng là một thuật toán thuộc nhóm Single Stage Detector. Ảnh đầu vào được cho đi qua mạng VGG-16 để trích xuất Feature Maps với các hệ số Scale khác nhau. Tiếp theo, nó dự đoán nhãn và vị trí của đối tượng sử dụng Conv layers. Output là một tập các Bounding Box và xác suất có đối tượng bên trong đó. Cuối cùng áp dụng NMS để loại bỏ đi những Bounding Box không phù hợp.\nChi tiết về SSD, bạn có thể tham khảo ở đây\n Khá ngạc nhiên là mặc dù ra đời từ năm 2015 nhưng đến nay vẫn không có thêm phiên bản mới nào của SSD, trong khi YOLO thì ra phiên bản mới liên tục.\nVề tốc độ xử lý và độ chính xác thì nói chung SSD ngang ngửa với YOLOv3 và chỉ chịu thua YOLOv4. Các bạn có thể xem so sánh chi tiết ở đây, và ở đây nữa\nNgoài các thuật toán kể trên, còn có 1 số thuật toán Object Detection khác như EfficientDet, RatinaNet, \u0026hellip; Bạn có thể đọc thêm về những thuật toán này từ các nguồn trên Internet. Đến thời điểm hiện tại, ý kiến của cá nhân mình thì YOLOv4 đang là State-of-Art cho bài toán Object Detection, cả về tốc độ và độ chính xác. Mình luôn áp dụng YOLOv4 đầu tiên trong các dự án về Object Detection của mình, nếu có vấn đề gì thì mới chuyển qua thuật toán khác.\n2. Metrics đánh giá Object Detection model\nĐể đánh giá một Classification model, ta thường sử dụng các Metrics: Precision, Recall, F1-Score, Accuracy.\nĐể đánh giá một Regression model, ta thường sử dụng các Metrics: Root Mean Square (RMS), Mean Square Error (MSE), Mean Absoluate Error (MAE), \u0026hellip; Nhược\nĐể đánh giá một Object Detection model, ta dùng Metrics nào?\nĐể trả lời câu hỏi này, hãy nhớ lại 2 mục đích của một Object Detection model là:\n Classification: Phân loại đối tượng trong bức ảnh thuộc vào lớp nào? Localization: Xác định chính xác vị trí của đối tượng trong bức ảnh.  Vì vậy, Metrics đánh giá một Object Detection model cần phải kết hợp cả 2 mục tiêu này. Hiện nay, Mean Average Precision (mAP) được sử dụng rộng rãi để đánh giá các Object Detection model. Về bản chất, mỗi một lớp (class - nhãn) trong bài Object Detection model có một giá trị AP riêng, mAP là trung bình của các giá trị AP đó. Vì AP được tính cho mỗi lớp nên trong cách tính AP dưới đây, bạn có thể thấy rằng ta chỉ sử dụng kết quả dự đoán Bounding Box mà không sử dụng kết quả dự đoán nhãn của model.\n2.1 Intersection over Union - IoU\nTrước tiên, cần hiểu khái niệm IoU. Như cái tên của nó đã gợi nhớ, IoU là tỉ số giữa phần giao nhau và phần hợp nhau của 2 Bounding Box. Ground Trust(GT - Bounding Box mà ta vẽ bằng tay khi đánh nhãn đối tượng) Bounding Box và Bounding Box mà model dự đoán ra được.  Giá trị IoU nằm trong khoảng [0;1]. IoU = 0 ngụ ý rằng model dự đoán sai hoàn toàn, IoU = 1 ngụ ý rằng model dự đoán chính xác hoàn toàn.\nThông thường, sẽ có một giá trị ngưỡng (threshold) của IoU để xác định Bounding Box dự đoán ra có được chấp nhận hay không? Threshold thường là 0.5 hoặc 0.75. Nếu IoU \u0026gt; Threshold, Bounding Box là hợp lệ và ngược lại.\n2.2 Precision và Recall\nPrecision và Recall được xác định thông qua công thức: $Precision = \\frac{TP}{TP + FP}$\n$Recall = \\frac{TP}{TP + FN}$\n Sử dụng IoU và Threshold, ta có thể xác định được TP, FP, TN, FN như sau:\n Nếu IoU \u0026gt;= Threshold, Bounding Box được coi là TP - True Positive. Nếu IoU \u0026lt; Threshold, Bounding Box được coi là FP - False Positive. Nếu có đối tượng trong bức ảnh nhưng model không phát hiện được (không dự đoán được Bounding Box) \u0026ndash;\u0026gt; FN - False Negative. Mọi phần trong bức ảnh mà không có đối tượng và model cũng dự đoán là không có đối tượng \u0026ndash;\u0026gt; TN - True Negative.  Precision and Recall được tính cho mỗi đối tượng trong bức ảnh. Chúng ta cũng cần xem xét đến giá trị Confidence Score trả ra từ model. Các Bounding Box có Confidence lớn hơn một giá trị ngưỡng được coi là Positive và ngược lại.\n2.3 Tính toán mAP\n Bước 1 - Vẽ đồ thị Precision - Recall Thể hiện 2 giá trị Precision và Recall trên đồ thị PR, Precision trên trục y, Recall trên trục x.    Mong đợi rằng đồ thị PR (đường màu xanh) sẽ có dạng đơn điệu giảm dần từ trái sang phải theo giá trị của Precision. Nếu không, chúng ta sẽ phải sử dụng giá trị interpolation - nội suy của Pricsion (đường màu đỏ) - $p_{interp}$. Giá trị $interp_p$ tại mỗi vị trí bằng giá trị lớn nhất của Precision ở bên phải của vị trí đó. Vì AP được tính bằng trung bình của 11 điểm $p_{interp}$ nên ta thể hiện 11 điểm trên đồ thị PR, tương ứng với Recall từ 0.0, 0.1, \u0026hellip;, 1.0\n  Bước 2 - Tính AP $AP = \\frac{1}{11}\\sum_{r \\in {0,0.1,0.2,...,1}}p_{interp}(r) = \\frac{1}{11} (1 + 0.8 + 0.6 + 0.5 + 0.5 + 0.5 + 0.45 + 0.48 + 0.48 + 0.35 + 0.2) = 0.53$\n   Bước 3 - Tính mAP mAP được tính bằng trung bình của tất cả các AP của mỗi class. $mAP = \\frac{1}{n}\\sum_{i=1}^n AP_i$\n   Trong đó, $n$ là số class mà model có thể phát hiện.\n3. Kết luận\nBài này, mình đã giới thiệu đến các bạn những kiến thức khái quát về các thuật toán Object Detection và một Metrics mAP để đánh giá các thuật toán đó. Hi vọng các bạn sẽ có được cái nhìn toàn diện để có thể lựa chọn được thuật toán phù hợp với bài toán của mình. Để hiểu sâu hơn về mỗi thuật toán đó, mình khuyên các bạn nên đọc kỹ bài báo gốc của nó và bắt tay vào train/test lại chúng trên bộ dữ liệu của mình. Có rất nhiều hướng dẫn trên Internet giúp các bạn làm điều này.\nỞ bài tiếp theo, chúng ta sẽ chuyển qua tìm hiểu về bài toán Sematics Segmentation. Mời các bạn đón đọc.\n4. Tham khảo\n[1] Renu Khandewal, \u0026ldquo;Evaluating performance of an object detection model\u0026rdquo;, Available online: https://towardsdatascience.com/evaluating-performance-of-an-object-detection-model-137a349c517b (Accessed on 12 May 2021).\n[2] Jędrzej Świeżewski, \u0026ldquo;YOLO Algorithm and YOLO Object Detection: An Introduction\u0026rdquo;, Available online: https://appsilon.com/object-detection-yolo-algorithm/ (Accessed on 12 May 2021).\n[3] Analytics Vidhya, \u0026ldquo;A Review of Object Detection Models\u0026rdquo;, Available online: https://medium.com/analytics-vidhya/a-review-of-object-detection-models-f575c515655cl (Accessed on 12 May 2021).\n","permalink":"https://tiensu.github.io/blog/66_object_detection_summary/","tags":["Object Detection"],"title":"Tóm tắt các thuật toán Object Detection"},{"categories":["OCR","CTC","Text Recognition"],"contents":"1. CRNN Model\nConvolutional Recurrent Neural Network (CRNN) là một kiến trúc được thiết kế chuyên biệt để giải quyết nhiệm vụ Text Recognition trong bài toán OCR. Xuất hiện trong bài báo An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition xuất bản năm 2015, cho đến nay, nó vẫn được coi là một trong những model hiệu quả nhất cho việc thực hiện Text Recognition.  CRNN, đúng như cái tên của nó, là sự kết hợp giữa CNN và RNN. Đây là một sự cộng sinh rất hợp lý bởi vì nhiệm vụ của nó là nhận một bức ảnh đầu vào và cho ra một văn bản chứa trong bức ảnh đó. Nhắc đến xử lý ảnh thì CNN chắc chắn không thể thiếu, và xử lý văn bản thì RNN cũng là ứng cử viên nặng ký. Kiến trúc của CRNN chia thành 3 phần rõ rệt.\n1.1 Convolutional Layers\nẢnh đầu vào được cho đi qua các lớp Conv, sinh ra các Feature Maps. Các Feature Maps sau đó lại được chia ra thành một chuỗi của các Feature Vectors (các TimeSteps), gọi là Feature Sequence.  1.2 Recurrent Layers\nFeature Sequence được đưa vào các lớp Bidirectional LSTM, sinh ra một chuỗi các ký tự (Seq2Seq), mà mỗi một ký tự tương ứng với một TimeStep trong Feature Sequence. Về lý thuyết thì đây chính là văn bản đầu ra cần xác định. Tuy nhiên, Feature Maps không phải lúc nào cũng được chia chính xác thành các Feature Vectors, mà mỗi Feature Vector chứa đúng 1 ký tự cần nhận diện, nên chuỗi đầu ra của LSTM cũng rất lộn xộn: trùng lặp, không có ký tự, \u0026hellip;  1.3 Transcription Layers\nNhiệm vụ của phần này là xử lý đầu ra của Recurrent Layers, sắp xếp lại các ký tự, loại bỏ các lỗi tồn tại (alignment) để đưa ra kết quả cuối cùng.  Một số hướng tiếp cận của Transcription Layers như sau:\n Cách 1 - Quy định mỗi ký tự tương ứng với một số cố định TimeSteps. Ví dụ, chúng ta có 10 TimeSteps, tương ứng với Output là State thì ta sắp xếp các TimeSteps như sau: \u0026ldquo;S, S, T, T, A, A, T, T, E, E\u0026rdquo;. Cách này đơn giản nhưng chỉ đúng khi kích thước, kiểu chữ, \u0026hellip; của văn bản không đổi. Cách 2 - Đánh nhãn cho mỗi TimeStep như hình bên dưới, rồi huấn luyện model sử dụng dữ liệu này.    Cách này có vẻ chính xác hơn, nhưng nhược điểm là tốn rất nhiều thời gian cho việc tạo dữ liệu.\n Cách 3 - Sử dụng Connectionist Temporal Classification (CTC)  CTC ra đời từ năm 2006 để giải quyết vấn đề Alignment (chính là vấn đề ánh xạ từ Output của RNN ra kết quả cuối cùng) cho các bài toán OCR, Speech Recognition, \u0026hellip; một cách hiệu quả. Chúng ta sẽ tìm hiểu chi tiết về nó trong phần 2 dưới đây.\nĐể nâng cao hiệu quả của CRNN, chúng ta có thể thêm vào một lớp Attention ở giữa hai lớp CNN và RNN. Cơ chế hoàn toàn giống như trong bài Tìm hiểu cơ chế Attention trong mô hình Seq2Seq.  2. Connectionist Temporal Classification\nCTC đơn giản là một Loss Function được sử dụng để huấn luyện các Deep Learning model. Mục tiêu của nó là tìm ra cách ánh xạ (alignment) giữa một Input X và Output Y. Nó không yêu cầu Aligned Data (dữ liệu được gắn nhãn cụ thể cho mỗi TimeStep), bởi vì nó có thể đưa ra được xác suất cho mỗi khả năng Align từ X sang Y. Nó chỉ yêu cầu đầu vào là một hình ảnh (ma trận Feature của hinh ảnh) và đoạn Text tương ứng với hình ảnh đó.  Thực chất, bản thân CTC cũng ko biết chính xác cách Alignment giữa X và Y. Nó làm việc theo kiểu Work Arround, tức là nếu đưa cho nó X thì nó sẽ trả lại cho ta tất cả các khả năng của Y, kèm theo xác suất chính xác của mỗi khả năng đó. Chúng ta cần huấn luyện model sao cho:  Trong đó $Y^*$ là kết quả đầu ra cuối cùng mà ta mong đợi, nó càng giống với nhãn (Ground Trust - GT) càng tốt.\nQuá trình làm việc của CTC bao gồm 3 bước:\n2.1 Encoding Text\nLý tưởng thì mỗi TimeStep sẽ tương ứng với một ký tự. Nhưng nếu không phải vậy thì sao, nếu một ký tự tồn tại trong cả 2 TimeSteps thì sao? Khi đó, kết quả sẽ xuất hiện các ký tự trùng nhau. CTC giải quyết vấn đề này bằng cách gộp tất cả các ký tự trùng nhau thành một. Ví dụ: ttiien ssuu -\u0026gt; tien su.\nTuy nhiên, nếu làm như vậy thì những từ mà bản thân nó có các ký tự trùng nhau thì sao? Ví dụ: hello, sorry, \u0026hellip; Để tiếp tục xử lý vấn đề, CTC sử dụng một ký tự giả, gọi là blank và ký hiệu là \u0026ldquo;-\u0026rdquo;. Trong khi mã hóa Text, nếu gặp 2 ký tự trùng nhau, CTC sẽ chèn thêm kí tự blank vào giữa chúng. Ví dụ: meet -\u0026gt; mm-ee-ee-t, mmm-e-ee-tt. Trong quá trình Decoding Text, nếu gặp ký tự blank này thì CTC hiểu rằng phải giữ lại cả 2 ký tự 2 bên ký hiệu đó.\n2.2 Loss Calculate\nLoss được tính toán cho mỗi Training Sample (một cặp ảnh và GT Text tương ứng). Nó là tổng tất cả các Scores của tất cả các khả năng Alignments của GT Text. Giả sử chúng ta có một ma trận Score là Ouput của CRNN như sau:  Ma trận này gồm 2 TimeSteps, và GT Text có 3 ký tự: a, b và ký tự blank (-). Tổng Score tại mỗi TimeStep bằng 1. Giả sử:\n Các khả năng Alignment của ký tự a là: aaa, a\u0026ndash;, a-, aa-, -aa, \u0026ndash;a \u0026ndash;\u0026gt; Score của *a = 0.4x0.3x0.4 + 0.4x0.7x0.6 + 0.4x0.7 + 0.4x0.3x0.6 + 0.1x0.3x0.4 + 0.1x0.7x0.4 = 0.608. * \u0026ndash;\u0026gt; Loss = $-\\log_{10}0.6084 = 0.216$ Các khả năng Alignment của ký tự b là: bbb, b\u0026ndash;, b-, bb-, -bb, \u0026ndash;b \u0026ndash;\u0026gt; Score của b = 0.5x0.0x0.0 + 0.5x0.7x0.6 + 0.5x0.7 + 0.5x0.0x0.6 + 0.1x0.0x0.0 + 0.1x0.7x0.0 = 0.56 \u0026ndash;\u0026gt; *Loss = * $\\log_{10}0.56 = 0.25$ Các khả năng Alignment của ký tự blank là: \u0026ndash;, \u0026mdash; -\u0026gt; Score của blank = 0.1x0.7 + 0.1x0.7x0.6 = 0.112 \u0026ndash;\u0026gt; Loss = $-\\log_{10}0.112 = 0.95$  Tổng Loss = 0.216 + 0.25 + 0.95 = 1.416.\nChúng ta cần tối thiểu hóa giá trị Loss này trong quá trình huấn luyện model, sử dụng thuật toán Backpropagation và SGD.\n2.3 Decoding Text\nQuá trình Decoding một Unseen Image diễn ra như sau:\n Tìm đường đi tối ưu nhất từ Score Matrix bằng cách chọn các ký tự có Score cao nhất tại mỗi TimeStep. Xóa bỏ các ký tự trống, ký tự trùng lặp.  Xem thử ví dụ sau:  Có 3 ký tự là a. b, - và 5 TimeSteps. Các ký tự trên đường đi tối ưu nhất là: aaa-b. Sau khi loại bỏ ký tự trùng và khoảng trắng ta được ab.\n3. Kết luận\nBài này, mình đã giới thiệu đến các bạn những kiến thức khái quát về CRNN model, về CTC Loss dùng cho nhiệm vụ Text Recognition của bài toán OCR: kiến trúc, cách làm việc \u0026hellip;\nSouce Code ví dụ của CRNN + CTC, các bạn có thể tham khảo tại đây.\nỞ bài tiếp theo, chúng ta sẽ chuyển qua tìm hiểu về bài toán Object Detection. Mời các bạn đón đọc.\n4. Tham khảo\n[1] B.Shi, X.Bai, C.Yao, An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition. arXiv preprint arXiv:1507.05717.\n[2] TheAILearner, \u0026ldquo;CTC – Problem Statement\u0026rdquo;, Available online: https://theailearner.com/2021/03/10/ctc-problem-statement/ (Accessed on 07 May 2021).\n[3] Harald Scheidl, \u0026ldquo;An Intuitive Explanation of Connectionist Temporal Classification\u0026rdquo;, Available online: https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c (Accessed on 07 May 2021).\n[4] Siddhant, \u0026ldquo;Explanation of Connectionist Temporal Classification\u0026rdquo;, Available online: https://sid2697.github.io/Blog_Sid/algorithm/2019/10/19/CTC-Loss.html (Accessed on 07 May 2021).\n","permalink":"https://tiensu.github.io/blog/65_ocr_crnn_model_ctc_loss_function/","tags":["OCR","CRNN","CTC","Text Recognition"],"title":"Text Recognition với CRNN và CTC"},{"categories":["OCR","Text Detection"],"contents":"Trước khi có sự ra đời của học sâu trong bài toán Text Detection, hầu hết các phương pháp đều khó thực hiện trong các tình huống phức tạp của dữ liệu thực tế. Các phương pháp này sử dụng những kỹ thuật xử lý ảnh truyền thống và thủ công, chúng thường có nhiều giai đoạn và kết thúc với hiệu suất tổng thể không thực sự thuyết phục. Trong bài viết này, chúng ta sẽ tìm hiểu một thuật toán dựa trên học sâu (EAST) để phát hiện văn bản bằng một mạng nơron duy nhất.\n1. Giới thiệu EAST\nEAST - pipeline được giới thiệu trong bài báo EAST: An Efficient and Accurate Scene Text Detector vào năm 2017. Nó có thể phát hiện Text theo cả 2 cấp độ: Words và Line. Nó cũng không hạn chế quá nhiều điều kiện của ảnh đầu vào, ảnh có thể xoay, mờ, nhiễu (nói vậy không có nghĩa là ta có thể bỏ qua bước Pre-processing Image); Text trên ảnh có thể thuộc một trong 2 loại: Structure và Unstructure. Tại thời điểm ra đời, EAST vượt trội hơn hẳn so với các phương pháp trước đó.\n2. Kiến trúc của EAST\n Kiến trúc tổng thể của EAST gồm 3 phần:\n  Feature Extractor Stem - Phần này làm nhiệm vụ trích xuất đặc trưng từ ảnh đầu vào theo 4 cấp độ, từ $f_1$ đến $f_4$. Trong bài báo, tác giả sử dụng 2 Pre-trained model là VGG16 và PVANet cho thí nghiệm của mình.\n  Feature Merging Branch - Các Features từ phần trên được cho đi qua các lớp Unpool, sau đó được tập hợp lại, cuối cùng là đi qua lần lượt các lớp 1x1 Conv, 3x3 Conv. Mục đích của việc làm này là để EAST có thể dự đoán được Text ở những khu vực nhỏ.    Output Layer - Ouput Layer chứa Score Map và Geometry Map. Score Map cho ta biết xác suất xuất hiện Text trong một khu vực, còn Geometry Map chứa thông tin về tọa độ Bounding Box của khu vực đó. Bounding Box có thể được đưa ra dưới dạng Rotated Box (tọa độ của điểm top-left, chiều dài, chiều rộng, góc quay) hoặc Quadrangle (đầy đủ tọa độ 4 điểm của hình chữ nhật).\n  3. Loss Function\nLoss Function của EAST là tổng Loss của Score Map và Geometry Map. $L = L_s + \\lambda_gL_g$\n Trong đó, $L_s$ là Loss Function của Score Map, còn $L_g$ là Loss Function của Geomatry Map. Hằng số $\\lambda_g$ là trọng số thể hiện mức độ quan trong của $L_g$, nó nhận giá trị trong khoảng [0,1]. Trong bài báo thì tác giả đang sử dụng $\\lambda_g$ = 1.\n4. Non-max Suppression Mergin Stage\nOutput của Geomatry Map đối với một khu vực có thể có rất nhiều, Chỉ một Output lớn hơn giá trị ngưỡng quy định bởi Locality-Aware NSM mới được giữ lại.\n5. Sử dụng EAST model\nCó khá nhiều Source Code của EAST trên Github. Trong phần này, mình sẽ sử dụng Github của argman để làm thực nghiệm.\n5.1 Clone EAST Repository\n$ git clone https://github.com/argman/EAST.git   5.2 Compile LANMS\nEAST sử dụng Locality-Aware NMS (LSNMS) thay vì NSM chuẩn, trong Github này, LSNMS được viết bằng C++. Để nó có thể làm việc được với Python, chúng ta cần biên dịch nó thành thư viện .so\n Cài đặt công cụ cần thiết:  $ sudo apt-get install build-essential   Mở file init.py trong thư mục lanms, và Comment Out 2 dòng 7 và 8 như hình:    Chạy lệnh sau để biên dịch\n  $ cd EAST/lanms/ $ make 5.3 Test EAST model\nTrước tiên cần phải có EAST model. Nếu có đủ dữ liệu của riêng mình, chúng ta có thể huấn luyện nó lại từ đầu như hướng dẫn trên Github argman. Ở đây, mình chọn Pre-trained EAST model, download tại đây.\n Chuyển đổi Code từ TF 1.xx sang TF 2.xx Source Code trên Github được viết bằng Tensorflow phiên bản 1.xx. Thời điểm hiện tại, hầu hết các model mới đều chuyển sang sử dụng Tensorflow phiên bản 2.xx. Để chạy được với Tensorflow 2.xx, chúng ta cần làm thêm 1 bước chuyển đổi Code từ TF 1.xx sang TF 2.xx.  $ tf_upgrade_v2 --intree . --outtree tf2_version --reportfile report.txt Source Code mới tương thích với TF 2.xx được di chuyển hết vào thư mục tf2_version. Tuy nhiên, vẫn còn 1 lỗi mà chưa thể chuyển đổi tự động được, ta phải sửa bằng tay.\nDòng 60, file nets/resnet_v1.py: from tensorflow.contrib import slim -\u0026gt; import tf_slim as slim\nDòng 43, file nets/resnet_utils.py: slim = tf.contrib.slim -\u0026gt; import tf_slim as slim\nTất nhiên, ta phải cài thêm thư viện tf_slim: pip install tf_slim.\n Chạy EAST model  Thực hiện lệnh sau:\n$ cd tf2_version $ python eval.py --test_data_path=training_samples/ --gpu_list=0 --checkpoint_path=east_icdar2015_resnet_v1_50_rbox/ --output_dir=outputs/ Ở đây, ta dùng cách ảnh trong thư mục training_samples để test, kết quả được lưu ra thư mục outputs.\n Hiện tại, EAST đã được tích hợp sẵn trong OpenCV phiên bản 4.xx. Chúng ta có thể dễ dàng sử dụng nó mà không cần phải làm nhiều bước dài dòng như thế này. Tham khảo cách sử dụng EAST trong OpenCV tại đây.\n6. Kết luận\nBài này, mình đã giới thiệu đến các bạn những kiến thức khái quát về EAST model, dùng cho nhiệm vụ Text Detection của bài toán OCR: ưu/nhược điểm, kiến trúc, cách làm việc và cách sử dụng nó từ Source Code trên Github.\nỞ bài tiếp theo, mình giới thiệu về model CRNN và CTC để thực hiện Text Recognition. Mời các bạn đón đọc.\n7. Tham khảo\n[1] X.Zhou, C.Yao, H.Wen, et al, EAST: An Efficient and Accurate Scene Text Detector. arXiv preprint arXiv:1704.03155.\n[2] TheAILearner, \u0026ldquo;Efficient and Accurate Scene Text Detector (EAST)\u0026rdquo;, Available online: https://theailearner.com/2019/10/19/efficient-and-accurate-scene-text-detector-east/ (Accessed on 06 May 2021).\n[3] TheAILearner, \u0026ldquo;Implementation of EAST\u0026rdquo;, Available online: https://theailearner.com/2021/03/10/implementation-of-east/ (Accessed on 06 May 2021).\n","permalink":"https://tiensu.github.io/blog/64_ocr_text_detection_east/","tags":["OCR","Text Detection"],"title":"Text Detection với EAST"},{"categories":["Data Imbalance"],"contents":"1. Thế nào là Data Imbalance?\nMất cân bằng dữ liệu (Data Imbalance) là một vấn đề thường xuyên gặp phải đối với các bộ dữ liệu trong thực tế. Nó xảy ra khi số lượng mẫu dữ liệu (hình ảnh, bản ghi, \u0026hellip;) của mỗi lớp có sự khác nhau lớn. Nguyên nhân của vấn đề này thường do đặc thù từng bài toán. Trong khi các mẫu dữ liệu của lớp này rất dễ dàng thu thập được thì ngược lại, các mẫu dữ liệu của lớp kia rất khó để tập hợp, hoặc phải mất một thời gian rất dài. Ví dụ, các bài toán về dự đoán lỗi trong giao dịch ngân hàng, bài toán về phân loại bệnh nhân ung thư, \u0026hellip;\nMất cân bằng dữ liệu ảnh hưởng rất lớn đến kết quả dự đoán của model trong bài toán phân loại. Model thường có xu hướng dự đoán kết quả nghiêng về lớp có nhiều dữ liệu hơn (bias model).\n2. Giải quyết vấn đề Data Imbalance\nĐể giải quyết vấn đề này, cách làm triệt để và chính xác nhất, tất nhiên là thu thập thêm dữ liệu cho các lớp có lượng dữ liệu ít hơn. Tuy nhiên, như đã nói ở phần nguyên nhân, việc có đủ lượng dữ liệu cần thiết của một số lớp là rất khó, thậm chí không thể thực hiện được. Thay vào đó, chúng ta sẽ sử dụng một số phương pháp để hạn chế ảnh hưởng của vấn đề này.\nMình sẽ trình bày các phương pháp hạn chế ảnh hưởng của Data Imbalance thông qua một ví dụ cụ thể.\n2.1 Chuẩn bị dữ liệu\nDữ liệu được sử dụng trong bài này là tập NIH Chest X-ray\n11. Tham khảo\n[1] Krizhevsky, A., Sutskever, I., \u0026amp; Hinton, G. E. (2017). Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6), 84-90.\n[2] Simonyan, K., \u0026amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.\n[3] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., \u0026hellip; \u0026amp; Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).\n[4] He, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n[5] Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., \u0026amp; Houlsby, N. (2019). Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370, 6(2)\n[6] Huang, G., Liu, Z., Van Der Maaten, L., \u0026amp; Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).\n[7] Tan, M., \u0026amp; Le, Q. V. (2019). Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946.\n[8] Xie, Q., Luong, M. T., Hovy, E., \u0026amp; Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).\n[9] Pham, H., Xie, Q., Dai, Z., \u0026amp; Le, Q. V. (2020). Meta pseudo labels. arXiv preprint arXiv:2003.10580.\n[10] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., \u0026amp; Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).\n[11] Nikolas Adaloglou, \u0026ldquo;Best deep CNN architectures and their principles: from AlexNet to EfficientNet\u0026rdquo;, Available online: https://theaisummer.com/cnn-architectures/ (Accessed on 02 May 2021).\n","permalink":"https://tiensu.github.io/blog/ddd_data_imbalance_in_classification_task/","tags":["Data Imbalance"],"title":"Mất cân bằng dữ liệu - làm sao để giải quyết?"},{"categories":["OCR"],"contents":"1. OCR là gì?\nOCR, viết tắt của Optical Character Recognition là một phương pháp chuyển đổi các văn bản, ký tự xuất hiện trong hình ảnh, hay các tài liệu scaned thành định dạnh mà máy tính có thể hiểu được. Từ đó, chúng ta có thể dễ dàng chỉnh sửa, tìm kiếm, và thực hiện rất nhiều công việc khác. Nếu đưa cho máy tính một hình ảnh chứa các văn bản, ký tự thì máy tính chỉ coi đó là một bức ảnh đại diện bởi ma trận các giá trị của từng pixel trong ảnh đó.\n2. Ứng dụng của OCR\nOCR có rất nhiều ứng dụng trong thực tế. Có thể kể ra đây một số ví dụ sau:\n- Automatic Data Entry - Tự động nhập liệu\nĐây có lẽ là ứng dụng phổ biến và quan trọng nhất của OCR. Trước đây, đối với những số liệu trong một hình ảnh hay tài liệu scan, để đưa vào máy tính xử lý, con người phải nhập thủ công bằng tay. Việc này rất mất thời gian và nhàm chán. Ngày nay, với sự hỗ trợ của OCR, quá trình này diễn ra hoàn toàn tự động, nhanh chóng, dễ dàng, độ chính xác cao.  - Nhận diện biển số xe\nÁp dụng trong các bãi đỗ xe, tự động nhận diện biển số giúp giảm thời gian quản lý cho cả người lái xe và nhân công bảo vệ.  - Xe tự lái\nOCR giúp xe tự động nhận diện biển số để đi theo đúng chỉ dẫn.  - Book Scanning\nOCR giúp chuyển sách giấy thành sách điện từ một cách dễ dàng.  Và còn rất rất nhiều ứng dụng khác nữa.\n3. OCR Pipeline\nOCR hoạt động theo một Pipeline như sau:  3.1 Image Pre-processing\nĐây là bước tiền xử lý hình ảnh trước khi đưa vào cho model học tập. Các images có thể bị mờ, bị nhiễu, bị lệch, \u0026hellip; Nếu để nguyên như vậy đưa vào model thì kết quả sẽ rất kém. Nhiệm vụ của Image Pre-processing là cố gắng loại bỏ những lỗi như vậy.  3.2 Text Detection\nNhư cái tên đã chỉ ra, nhiệm vụ của bước này là tìm ra khu vực trong hình ảnh chứa ký tự/văn bản.  Có 2 kiểu ký tự/văn bản trong hình ảnh mà bài toán OCR có thể giải quyết:\n Ký tự/văn bản có cấu trúc: Là những hình ảnh tương đối rõ ràng, background cố định, font chữ cố định, màu sắc cố định, ký tự/văn bản được tổ chức ngay ngắn theo hàng/cột, \u0026hellip; Ví dụ, trang sách.    - Ký tự/văn bản phi cấu trúc: Là những hình ảnh có ký tự/văn bản xuất hiệu không sự thống nhất về màu sắc, vị trí, kiểu chữ, \u0026hellip; Ví dụ: bảng quảng cáo.  Rõ ràng, giải quyết kiểu thứ 2 khó hơn rất nhiều so với kiểu thứ nhất.\nĐể thực hiện nhiệm vụ Text Detection, có thể tiếp cận theo 3 cách:\n Cách 1 - Phát hiện từng ký tự một (Character-by-Character). Cách 2 - Phát hiện từng từ một (Word-by-Word). Cách 3 - Phát hiện từng dòng một (Line-by-Line).   Nhìn chung, hầu hết các hệ thống OCR đều sử dụng cách tiếp cận thứ 2 hoặc 3. Cách 1 chậm và độ chính xác thấp hơn.\nVề mặt kỹ thuật, có 2 cách có thể sử dụng:\n Cách 1- Sử dụng các kỹ thuật xử lý ảnh cơ bản (truyền thống) Cách này sử dụng các bộ lọc (filters) để tách rời các ký tự ra khỏi nền của bức ảnh, sau đó áp dụng các kỹ thuật phát hiện biên, Contours để thu được vị trí của từng ký tự riêng rẽ. Một số cái tên điển hình sử dụng nguyên lý này là Stroke Width Transform (SWT), Maximally Stable Regions (MSER).  Trong điều kiện tương đối lý tưởng, dữ liệu sạch sẽ, ít nhiễu thì phương pháp này tỏ ra khá hiệu quả, độ chính xác cao và dễ thực hiện. Tuy nhiên, trong thực tế, rất khó để đảm bảo những điều kiện như vậy.\n Cách 2 - Sử dụng kỹ thuật Deep Learning Sử dụng DL, nói chung là hiệu quả hơn rất nhiều so với phương pháp bên trên, bởi vì chúng có khả năng học từ dữ liệu nên không bị ảnh hưởng quá nhiều bởi các yếu tố môi trường. Một số model nổi bật là Connectionist Text Proposal Network (CTPN), Efficient and Accurate Scene Text Detector (EAST), \u0026hellip; Các Object Detection models khác như SSD, Yolo, Faster RCNN cũng có thể thực hiện được nhiệm vụ này.  Ở cách 2 này, nếu chia nhỏ hơn nữa thì có thể thành: Simplified pipeline và Multi-steps.  Hai model CTPN và EAST đều thuộc nhóm Simplified Pipeline. Trong các bài tiếp theo, chúng ta sẽ tìm hiểu kỹ hơn EAST model.\n3.3 Text Recognition\nCác ký tự/văn bản trong từng khu vực phát hiện ở bước bên trên sẽ được nhận diện cụ thể ở bước này.  Tương tự như Text Detection, ở đây cũng có 2 phương pháp giải quyết là dùng kỹ thuật xử lý ảnh cơ bản và dùng kỹ thuật Deep Learning.\nĐối với cách thứ nhất, sau khi tách riêng được từng ký tự ra khỏi nền, sẽ cho chúng đi qua mội bộ phân lớp để nhận diện. Cách này xử lý ở mức Characters, phụ thuộc nhiều vào kết quả của Text Detection và môi trường nên độ chính xác không cao.\nCách thứ 2 hiện nay đã chứng tỏ được tính ưu việt của nó so với cách thứ nhất. Hai model nổi bật sử dụng DL cho nhiệm vụ này là:\n CRNN - Connectionist Temporal Classification (CTC) based. Attention-based.  Trong các bài tiếp theo, chúng ta sẽ tìm hiểu kỹ hơn về 2 model này.\n3.4 Restructing\nỞ bước cuối cùng này, ký tự/văn bản sau khi nhận dạng xong sẽ được sắp xếp lại đúng theo vị trí của nó như trong hình ảnh bản đầu. Mục đích của việc làm này là để thuận tiện trong việc trích chọn ra các thông tin cần thiết, dựa vào vị trí tương đối của chúng với nhau.  4. Kết luận\nBài này, mình đã giới thiệu đến các bạn những kiến thức tổng quát về bài toán OCR: OCR là gì, các bước thực hiện như thế nào, có các phương pháp gì, ưu/nhược điểm của từng phương pháp.\nỞ bài tiếp theo, mình giới thiệu về model EAST để thực hiện Text Detection. Mời các bạn đón đọc.\n5. Tham khảo\n[1] TheAILearner, \u0026ldquo;Optical Character Recognition: Introduction and its Applications\u0026rdquo;, Available online: https://theailearner.com/2021/03/10/optical-character-recognition-introduction-and-its-applications/ (Accessed on 04 May 2021).\n[2] TheAILearner, \u0026ldquo;Optical Character Recognition Pipeline\u0026rdquo;, Available online: https://theailearner.com/2019/05/28/optical-character-recognition-pipeline/ (Accessed on 04 May 2021).\n[3] TheAILearner, \u0026ldquo;OOptical Character Recognition Pipeline: Text Detection\u0026rdquo;, Available online: https://theailearner.com/2021/01/28/optical-character-recognition-pipeline-text-detection/ (Accessed on 04 May 2021).\n[4] TheAILearner, \u0026ldquo;Optical Character Recognition Pipeline: Text Recognition\u0026rdquo;, Available online: https://theailearner.com/2019/05/29/optical-character-recognition-pipeline-text-recognition/ (Accessed on 04 May 2021).\n","permalink":"https://tiensu.github.io/blog/63_ocr_introduction/","tags":["OCR"],"title":"Giới thiệu bài toán OCR"},{"categories":["CNN"],"contents":"Năm 2012, Alexnet ra đời với độ chính xác trên tập dữ liệu ImageNet được công bố là 63.3%. Từ đó đến nay, trải qua gần 9 năm phát triển, có rất nhiều kiến trúc mới của CNN nối tiếp nhau ra đời, cái sau tốt hơn cái trước. Thời điểm hiện tại, EfficientNet có lẽ là kiến trúc đạt được độ chính xác trên ImageNet cao nhất, lên đến hơn 90% khi huấn luyện bằng phương pháp Teacher-Student.  Bài viết này, mục đích là nhìn lại toàn bộ quá trình phát triển đó của CNN, không chỉ đưa ra số liệu, bảng biểu, đồ thị mà còn tóm tắt lại nguyên lý cơ bản của mỗi kiến trúc CNN.\nSimone Bianco, năm 2018, đã đưa ra một tóm tắt về Top Performing CNNs Model, thể hiện như hình dưới đây:  Trong hình trên, trục Y thể hiện độ chính xác của model trên tập ImageNet, trục X (Floating Point Operations Per Second - FLOPS) chỉ ra mức độ phức tạp của model. Bán kính của vòng tròn càng lớn, model càng có nhiều tham số. Từ tổng kết này, rõ ràng rằng không phải cứ có nhiều tham số thì độ chính xác sẽ cao hơn.\n1. Một số thuật ngữ sử dụng trong bài\nĐể tránh làm các bạn bối rối khi theo dõi bài viết, mình sẽ giải thích trước một số thuật ngữ được sử dụng ở đây:\n Wider network - Network có nhiều Feature Maps (Filters). Deeper network - Network có nhiều Convolutional layers. High Resolution network - Network nhận Input Image có độ phân giải lớn (Spatial resolutions).    2. AlexNet: ImageNet Classification with Deep Convolutional Neural Networks (2012)\nAlexnet được tạo thành từ 5 Conv Layers, bắt đầu từ 11x11 kernel, giảm dần đến 3x3 kernel. Nó là kiến trúc CNN đầu tiên sử dụng Max-Pooling layers, ReLU Activation function, và Dropout. Alexnet được sử dụng cho bài toán phân loại hình ảnh, số lượng nhãn lên đến 1000. Đó là một điều rất bất ngờ tại thời điểm bấy giờ.\nChúng ta có thể tạo ra Alexnet chỉ với khoảng 35 dòng Pytorch code:\nclass AlexNet(nn.Module): def __init__(self, num_classes: int = 1000) -\u0026gt; None: super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: x = self.features(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.classifier(x) return x Alexnet cũng là mô hình đầu tiên huấn luyện thành công trên tập ImageNet, đạt được Top-5 Error Rate là 15.3%.\n3. VGG (2014)\nKiến trúc VGG xuất hiện trong bài báo Very Deep Convolutional Networks for Large-Scale Image Recognition vào năm 2014. Đây là nghiên cứu đầu tiên cung cấp bằng chứng không thể phủ nhận rằng chỉ cần thêm nhiều lớp Conv trong kiến trúc sẽ tăng hiệu quả quả model. Tuy nhiên, giả định này chỉ đúng đến một thời điểm nhất định. Các tác giả của bài báo chỉ sử dụng các Filters có kích thước 3x3, trái ngược lại với Alexnet. Ảnh đầu vào để huấn luyện model là ảnh RGB có kích thước 224x224.\nVGG được đặc trưng bởi sự đơn giản của nó, chỉ sử dụng các lớp Conv với Kernel 3 × 3 xếp chồng lên nhau theo chiều sâu ngày càng tăng. Việc giảm kích thước được xử lý bằng cách sử dụng Max-pooling. Ba Fully-Connected layers, trong đó 2 lớp đầu, mỗi lớp có 4.096 nodes, lớp còn lại có 1000 nodes (tương ứng với 1000 classes), được theo sau bởi một bộ phân loại Softmax.  Có 2 phiên bản của VGG thường hay được sử dụng là VGG16 và VGG19. Các con số 16, 19 chỉ ra số Weights layers của mỗi model (cột D và E trong bảng trên). Ở thời điểm năm 2014 thì 16 và 19 layers được xem là rất deep rồi. Bây giờ thì chúng ta có kiến trúc ResNet có số lượng layers từ 50-200.\nVGG có 2 nhược điểm:\n Thời gian huấn luyện rất lâu nếu bạn ko có GPU. Dung lượng của model sau khi huấn luyện xong rất lớn (VGG16 là khoảng 533MB, còn VGG19 khoảng 574MB). Điều này làm cho VGG khó triển khai trên các thiết bị có bộ nhớ khiêm tốn.  VGG vẫn thi thoảng được sử dụng trong một số ứng dụng như Image Classification, Feature Extraction, \u0026hellip; nhưng nhìn chung thì các kiến trúc nhỏ nhẹ (SqueezeNet, GoogleNet, \u0026hellip;) vẫn được ưu chuộng hơn.\n4. InceptionNet/GoogleNet (2014)\nSau VGG, bài báo Going Deeper with Convolutions viết bởi Christian Szegedy cũng tạo ra một bước đột phá lớn. Bài báo ra đời xuất phát từ suy nghĩ rằng việc tăng độ sâu của model không phải các duy nhất làm cho nó tốt hơn. Tại sao không mở rộng model trong khi vẫn cố gắng duy trì sự tính toán ở mức độ ổn định?\nKiến trúc của GoogleNet bao gồm nhiều Inception Module, mỗi Module hoạt động như một multi-level feature extractor (bộ trích xuất đặc trưng nhiều tầng) bằng cách sử dụng các Filters có kích thước khác nhau: 1x1, 3x3, 5x5. Output của các Filters sau đó được tổng hợp lại trước khi đưa vào Module tiếp theo.  Filter 1x1 đặt trước cac Filters 3x3 và 5x5 để giảm số lượng Input Channel, từ đó giảm giúp chi phí tính toán của kiến trúc GoogleNet.\n4.1 Inception V2\nTrong bài báo năm 2014 thì kiến trúc này có tên là GoogleNet, đến bài báo Rethinking the Inception Architecture for Computer Vision (2015), với một chút cải tiến để tăng hiệu quả, nó được đặt tên là Inception V2 và Inception V3.\nSự cải tiến của Inception V2 so với GoogleNet thể hiện ở 2 điểm:\n  Thay thế Filter 5x5 bằng 2 Filters 3x3 chồng lên nhau, mục đích là để tăng tốc độ xử lý vì theo lý thuyết, thời gian để một Filter 5x5 tính toán băng 2.78 lần so với Filter 3x3.    Tách Filter nxn thành 1xn và nx1. Ví dụ, với Filter 3x3 sẽ tương đương với 1x3 và 3x1. Theo thực nghiệm thì việc làm này sẽ giảm được khoảng 33% chi phí tính toán.    4.2 Inception V3\nInception V3 tiếp tục cải tiển từ Inception V2:\n Sử dụng RMSProp Optimizer. Thêm Filter 7x7. Sử dụng BatNorm sau các FC layers. Sử dụng Label Smoothing.  Dung lượng của Inception V3 khá nhỏ so với VGG, chỉ khoảng 96MB.\nCode thực hiện GoogleNet bằng Pytorch như sau:\nimport torch import torch.nn as nn class InceptionModule(nn.Module): def __init__(self, in_channels, out_channels): super(InceptionModule, self).__init__() relu = nn.ReLU() self.branch1 = nn.Sequential( nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0), relu) conv3_1 = nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0) conv3_3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1) self.branch2 = nn.Sequential(conv3_1, conv3_3,relu) conv5_1 = nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0) conv5_5 = nn.Conv2d(out_channels, out_channels, kernel_size=5, stride=1, padding=2) self.branch3 = nn.Sequential(conv5_1,conv5_5,relu) max_pool_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) conv_max_1 = nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0) self.branch4 = nn.Sequential(max_pool_1, conv_max_1,relu) def forward(self, input): output1 = self.branch1(input) output2 = self.branch2(input) output3 = self.branch3(input) output4 = self.branch4(input) return torch.cat([output1, output2, output3, output4], dim=1) model = InceptionModule(in_channels=3,out_channels=32) inp = torch.rand(1,3,128,128) print(model(inp).shape) 5. ResNet: Deep Residual Learning for Image Recognition (2015)\nTừ sau khi VGG ra đời, người ta đã từng nghĩ rằng cứ thêm nhiều lớp Conv thì model sẽ hoạt động tốt hơn. Nhiều người trong số họ cũng thử tiến hành các thực nghiệm với số lớp Conv nhiều hơn của VGG. Tuy nhiên, tất cả đều gặp phải một vấn đề, đó là Vanishing Gradient.\nResNet ra đời đã giải quyết được phần nào vấn đề này. Ý tưởng của nó là đưa vào trong kiến trúc của mình các Identity Shortcut Connection hay Skip Connection, để sử dụng thông tin của các layers trước đó cho layer hiện tại. Nhờ vậy mà hạn chế được hiện tượng Vanishing Gradient khi số lớp Conv tăng lên.  Với việc áp dụng ý tưởng này, số lớp Conv của Resnet có thể tăng đến con sô 150 lớp (Resnet-150).  Torchvision cung cấp sẵn một số Pre-trained của các phiên bản Resnet, bạn có thể import trực tiếp vào và sử dụng chúng.\nimport torchvision pretrained = True # A lot of choices :P model = torchvision.models.resnet18(pretrained) model = torchvision.models.resnet34(pretrained) model = torchvision.models.resnet50(pretrained) model = torchvision.models.resnet101(pretrained) model = torchvision.models.resnet152(pretrained) model = torchvision.models.wide_resnet50_2(pretrained) model = torchvision.models.wide_resnet101_2(pretrained) Bản thân mình không thích từ Skip Connection hay từ dịch nghĩa bỏ qua kết nối vì thực tế ResNet có bỏ qua kết nối nào đâu (nhìn vào hình minh họa thấy rất rõ ràng). Chẳng qua là nó thêm đường tắt, bắc cầu từ các lớp Conv trước đó đến chính nó. Do vây, dùng từ Shortcut Connection mới chính xác, phản ánh đúng bản chất của ResNet.\n6. DenseNet: Densely Connected Convolutional Networks (2017)\nDenseNet tiếp tục giải quyết vấn đề cố hữu khi sử dụng nhiều lớp Conv, đó là Vanishing Gradient.  Đối với mạng CNN truyền thống (VGG, \u0026hellip;) thì Input của một lớp chính là Ouput của lớp ngay trước đó. $x_i = H_i(x_{i-1})$\n ResNet mở rộng hành vi này bằng cách thêm vào thông tin của một lớp trước đó nữa (không nhất thiết là lớp ngay trước mà có thể trước vài lớp) thông qua Shortcut Connection. $x_i = H_i(x_{i-1} + x_{i-n})$\n DenseNet tiếp tục mở rộng, nó tổng hợp thông tin của tất cả các lớp trước đó làm Input cho lớp hiện tại. $x_i = H_i([x_0, x_1, ..., x_{i-1}])$\n Tạo DenseNet model trong Torchvision như sau:\nimport torchvision model = torchvision.models.DenseNet( growth_rate = 16, # how many filters to add each layer (`k` in paper) block_config = (6, 12, 24, 16), # how many layers in each pooling block num_init_features = 16, # the number of filters to learn in the first convolution layer (k0) bn_size= 4, # multiplicative factor for number of bottleneck (1x1 cons) layers drop_rate = 0, # dropout rate after each dense conv layer num_classes = 30 # number of classification classes ) print(model) # see snapshot below Ban đầu, DenseNet được đề xuất để sử dụng cho bài toán Image Classification, nhưng về sau nó còn được sử dụng cho rất nhiều bài toán khác, như thống kê dưới đây.  7. Big Transfer (BiT): General Visual Representation Learning (2020)\nBiT là một biến thể của ResNet. Cả ba phiên bản của nó (small, medium và large) đều dựa trên ResNet152. BiT-large sử dụng ResNet152x4 và được huấn luyện trên tập JFT chứa khoảng 300M hình ảnh đã đánh nhãn, lớn hơn rất nhiều so với ImageNet.\nĐóng góp lớn nhất của kiến trúc này là việc sử dụng các Normalization Layers. Tác giả đã sử dụng Group Normalization và Weight Standardization thay vì Batch Normalization.  8. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (2020)\nEfficientNet được đề xuất bởi Mingxing Tan và Quoc V. Le tại Google trong bài báo EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. Kết quả nghiên cứu của các tác giả chỉ ra rằng nó đạt được dộ chính xác tốt hơn nhiều so với các kiến trúc CNN trước đó.  Ý tưởng của EfficientNet là thay vì tìm ra một kiến trúc tối ưu từ đầu thì nó xuất phát từ một Base model F, sau đó dần dần mở rộng, cải tiến nó dần lên.\nTuy nhiên, hãy nhớ lại một số vấn đề cần chú ý trong các kiến trúc từ trước đến giờ khi Scale-up từng thành phần riêng lẻ (Individual Scaling):\n Deeper Network có thể nắm bắt được nhiều các Features phức tạp hơn nhưng rất khó huấn luyện do vấn đề Vanishing Gradient. Wider Network có thể nắm bắt được nhiều các Featureschi tiết hơn nhưng cũng khó huấn luyện do vấn đề Saturate Gradient. High Resolution Network cũng có thể nắm bắt được nhiều các Featureschi tiết hơn nhưng độ chính xác giảm dần khi gặp những hình ảnh có độ phân giải thấp hơn.  Rút kinh nghiệm từ những vấn đề trên, EfficientNet tiến hành Scale-up đồng thời cả 3 thành phần, gọi là Compound Scaling.\nĐể tìm ra các hệ số Scale-up cho 3 thành phần đó, các nhà nghiên cứu đã sử dụng phương pháp chia tỷ lệ kết hợp. Grid-search được áp dụng để tìm mối quan hệ giữa các chiều có tỷ lệ khác nhau của Base-model trong điều kiện hạn chế tài nguyên cố định. Sử dụng chiến lược này, tác giả đã tìm được các hệ số tỷ lệ thích hợp cho mỗi chiều để có thể tăng lên. Từ các hệ số này, Base-model có thể được Scale-upe lên theo kích thước mong muốn.  Việc áp dụng Compound Scaling rõ ràng đã cải thiện được hiệu quả đáng kể so với Individual Scaling.  9. Noisy Student Training: Self-training with Noisy Student improves ImageNet classification (2020)\nXuất hiện sau EfficientNet một thời gian ngắn, Noisy Student Training đưa ra một phương pháp huấn luyện mới, sử dụng EfficientNet làm kiến trúc nền tảng. làm tăng đáng kể độ chính xác trên tập dữ liệu ImageNet.\nPhương pháp này bao gồm 4 bước như sau:\n Bước 1 - Huấn luyện một Teacher model trên tập dữ liệu đã được gán nhãn (tập A). Bước 2 - Sử dụng Teacher model để sinh ra nhãn cho 300M ảnh chưa có nhãn (pseudo labels) (tập B) Bước 3 - Huấn luyện Student model trên tổng dữ liệu (tập A và B). Bước 4 - Lặp lại bước 1 bằng cách coi Student model như là Teacher model.  Về mặt lý thuyết, Student model sẽ hiệu quả hơn Teacher model vì nó được huấn luyện trên nhiều dữ liệu hơn. Ngoài ra, một lượng lớn nhiễu (Noise) cũng được thêm vào trong quá trình huấn luyện Student model để giúp nó học hiệu quả hơn từ tập B.\nMột số kỹ thuật như Dropout, Data Augmentation, \u0026hellip; cũng được áp dụng.  10. Meta Pseudo-Labels (2021)\nQuay lại phương pháp Noisy Student Training, một vấn đề phát sinh là nếu như các Pseudo Labels không chính xác thì Student model sẽ không thể cải thiện được so với Teacher model, thậm chí là tồi hơn. Vấn đề này được gọi bằng cái tên Sự xác nhận sai lệch trong gán nhãn giả (confirmation bias in pseudo-labeling).\nĐể khác phục vấn đề này, một ý tưởng mới xuất hiện, đó là thiết kế một cơ chế phản hồi từ Student model đến Teacher model, để Teacher model sinh ra nhãn đúng hơn.  Bằng cách này, cả Teacher và Student đều tham gia vào quá trình huấn luyện cùng nhau, giúp nhau học tập tốt hơn - Together to better (Ý tưởng của phương pháp này nghe hơi giống với cách thức mà model GAN hoạt động nhỉ, :D).\n11. Kết luận\nBảng sau so sánh các kiến trúc CNN đã trình bày từ đầu đến giờ, về các khía canh: số lượng tham số, độ chính xác trên tập ImageNet và năm công bố.  Có thể rút ra một số nhận xét sau:\n Model DenseNet có ít tham số nhất, model BiT-L có nhiều tham số nhất. Model Meta Pseudo Labels đạt được độ chính xác cao nhất trên tập ImageNet. Không phải cứ nhiều tham số hơn thì độ chính xác cao hơn.  Trong bài này, chúng ta đã cũng nhau nhìn lại chặng đường phát triển của các kiến trúc CNN thông qua một số models tiêu biểu. Hầu hết các models đều được cung cấp dưới dạng Pre-trained trong Keras hay Torchvision. Bạn có thể thử sử dụng chúng và Fine-tune trên tập dữ liệu của bạn để so sánh và đánh giá kết quả, giữa các models với nhau và so với việc tạo và huấn luyện model từ đầu.\nTrong 3 bài tiếp theo, mình sẽ giới thiệu về bài toán OCR và hướng dẫn huấn luyện OCR model. Mời các bạn đón đọc.\n11. Tham khảo\n[1] Krizhevsky, A., Sutskever, I., \u0026amp; Hinton, G. E. (2017). Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6), 84-90.\n[2] Simonyan, K., \u0026amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.\n[3] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., \u0026hellip; \u0026amp; Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).\n[4] He, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n[5] Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., \u0026amp; Houlsby, N. (2019). Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370, 6(2)\n[6] Huang, G., Liu, Z., Van Der Maaten, L., \u0026amp; Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).\n[7] Tan, M., \u0026amp; Le, Q. V. (2019). Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946.\n[8] Xie, Q., Luong, M. T., Hovy, E., \u0026amp; Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).\n[9] Pham, H., Xie, Q., Dai, Z., \u0026amp; Le, Q. V. (2020). Meta pseudo labels. arXiv preprint arXiv:2003.10580.\n[10] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., \u0026amp; Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).\n[11] Nikolas Adaloglou, \u0026ldquo;Best deep CNN architectures and their principles: from AlexNet to EfficientNet\u0026rdquo;, Available online: https://theaisummer.com/cnn-architectures/ (Accessed on 02 May 2021).\n","permalink":"https://tiensu.github.io/blog/62_cnn_architecture_summary/","tags":["CNN"],"title":"CNN - Một hành trình phát triển từ 63.3% đến 90.2%"},{"categories":["RNN","LSTM","Attention","Transformer","BERT"],"contents":"Cuối năm 2018, các nhà nghiên cứu tại Google AI Language đã công bố một mô hình với tên gọi BERT trong bài báo BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Kế thừa kiến trúc của Transformers đã từng công bố trước đó, BERT đạt được hiệu quả rất cao so với các mô hình khác trong các tác vụ của NLP. Từ đó, có rất nhiều phiên bản của BERT ra đời, tập trung giải quyết các bài toán NLP trong từng lĩnh vực, ngôn ngữ cụ thể. Có thể nói Transformers và BERT đã mở ra một kỷ nguyên mới cho lĩnh vực AI nói chung và NLP nói riêng.\nKỹ thuật chính mà BERT sử dụng là sử dụng chỉ phần Encoder Stack của Transformers và áp dụng phương pháp huấn luyện 2 chiều (bidirectional training). Trước đó thì các Language Model thường chỉ huấn luyện 1 chiều (nondirectional training), từ trái qua phải hoặc từ phải qua trái. Các tác giả của bài báo đã chỉ ra rằng, bằng việc sử dụng bidirection training có thể thu được nhiều thông tin về ngữ nghĩa của mỗi từ trong Input Sequence hơn so với nondirectional training.\nTrong bài này, chúng ta sẽ cùng tìm hiểu về nó.\n1. Tại sao lại cần BERT?\nCũng như trong Computer Vision (*CV) trong những thách thức lớn nhất trong NLP là thiếu dữ liệu đào tạo. Nhìn chung, có rất nhiều dữ liệu văn bản có sẵn, nhưng nếu chúng ta muốn giải quyết các bài toán đặc thù của mình thì chúng ta phải tự tạo các bộ dữ liệu dành riêng cho bài toán đó. Việc đó quả thực mất rất nhiều thời gian. Để giúp thu hẹp khoảng cách về dữ liệu này, các nhà nghiên cứu đã phát triển các kỹ thuật khác nhau để đào tạo các mô hình biểu diễn ngôn ngữ có mục đích chung bằng cách sử dụng vô số văn bản trên các website (Pre-trained model). Sau đó, các mô hình Pre-trained này có thể được tinh chỉnh trên các tập dữ liệu nhỏ hơn dành riêng cho nhiệm vụ cụ thể. Cách tiếp cận này cải thiện độ chính xác lớn rất nhiều so với việc huấn luyện model từ đầu trên các bộ dữ liệu nhỏ. Kỹ thuật này được gọi bằng cái tên Transfer Learning giống như trong CV. BERT là một bổ sung gần đây cho kỹ thuật này trong việc tạo ra các Pre-trained model của NLP. Các Pre-trained BERT model có thể dễ dàng tải về miễn phí, sau đó được sử dụng hoặc là để trích xuất Features từ dữ liệu văn bản như đề cập trong bài báo ELMO, hoặc Fine-tune trên tập dữ liệu riêng của một nhiệm vụ cụ thể.\n2. Ý tưởng của BERT\nHãy nói về Language Modeling. Nhiệm vụ của nó là \u0026ldquo;tìm từ tiếp theo trong câu\u0026rdquo;. Ví dụ, trong câu sau: The woman went to the store and bought a ...\n Language Modeling có thể hoàn thành việc này bằng cách đưa ra một dự đoán rằng, 80% từ còn thiếu là \u0026ldquo;bag\u0026rdquo; và 20% là \u0026ldquo;water\u0026rdquo;. Cách làm việc của Language Modeling là nhìn vào Input Sequence từ trái qua phải hoặc từ phải qua trái, gọi là Unidirection, hoặc là từ cả 2 phía, gọi là Bidirection. Các cách tiếp cận này khá hiệu quả đối với bài toán dự đoán từ tiếp theo, như ví dụ trên.\nĐến với BERT, nó cũng có thể được coi là một Language Modeling, nhưng cách làm việc của nó có một vài điểm khác với trước đó:\n BERT dựa trên kiến trúc của Transformers, tức là cũng tiếp cận Input Sequence theo cả 2 hướng, nhưng tại cùng một thời điểm (trước đó là lần lượt từ trái qua phải rồi từ phải qua trái, hoặc ngược lại). Cách này có thể gọi là Nondirection. Hơn nữa, việc sử dụng kiến trúc của Tranformers làm cho BERT có thể hiểu ngữ nghĩa của cả câu tốt hơn (xem lại bài viết về Transformers) so với kiến trúc LSTM/GRU. Thay vì dự đoán từ tiếp theo, BERT sử dụng một kỹ thuật mới, gọi là Mask LM (MLM). Ý tưởng là \u0026ldquo;mask\u0026rdquo; ngẫu nhiên một số từ trong câu và sau đó cố gắng dự đoán chúng.  Để tạo ra Word Embedding, chúng ta có thể sử dụng một trong 2 phương pháp: Context-Free hoặc Context-Based.\n Trong Context-Based - lại được chia thành 3 cách: Unidirection, Bidirection và Nondirection. Context-Free- kiểu như Word2Vec hay Glove, sinh ra các Word Embedding dựa hoàn toàn vào từ điển từ (Vocabulary Dictionary).  Ví dụ:\n Từ bank trong câu bank account sẽ có Word Embedding giống hệt với từ bank trong câu bank of the river nếu sử dụng Context-Free. Trong câu I accessed the bank account, nếu sử dụng Unidirection thì Word Embedding của từ bank sẽ được sinh ra dựa trên các từ I accessed the. Còn nếu sử dụng Bidirection/Nondirection thì sẽ dựa trên các từ I accessed the \u0026hellip; account.  3. Cách làm việc của BERT\nKiến trúc của BERT kế thừa kiến trúc của Transformers. Kiến trúc đầy đủ của Transformers bao gồm 2 thành phần:\n Encoder Stack - nhận Input Sequence và sinh ra Context Vector đại diện cho Input Sequence đó. Decoder Stack - sinh ra Output Sequence dựa vào Context Vector.  Bởi vì nhiệm vụ của BERT là sinh ra Vector đại diện (Sequence Embedding hay Context Vector) của câu nên nó chỉ cần phần Encoder Stack.\nCũng giống như Transformers, BERT cũng yêu cầu Positional Encoding thêm vào Input Sequence. Ngoài ra, nó còn yêu cầu thêm một số thành phần khác:\n Token Embedding: Một CLS Token được thêm vào tại vị trí đầu tiên của Input Sequence, và SEP Token được thêm vào cuối mỗi câu trong Input Sequence. Segment Embedding: Một ký hiệu chỉ ra từ nào thuộc về câu nào nếu trong Input Sequence có nhiều câu. Positional Encoding: Chỉ ra vị trí của từ trong Input Sequence.    Để huấn luyện BERT model, chúng ta sử dụng 2 chiến lược như sau:\n3.1 Masked LM (MLS)\nÝ tưởng của LMS khá đơn giản: Lựa chọn ngẫu nhiên 15% các từ trong Input Sequence, thay thế chúng bằng [MASK] token, sau đó toàn bộ qua BERT model để dự đoán các từ được Masked dựa trên mỗi liên hệ trước sau với các từ còn lại. Về mặt kỹ thuật, các bước để thực hiện dự đoán như sau:\n Thêm một lớp Classification ngay sau Ouput của Encoder Stack. Nhân Ouput của Encoder Stack với Embedding Matrix để chuyển chúng sang miền của Vocabulary. Tính toán xác suất của mỗi từ trong Vocabulary với hàm Softmax.    Ý tưởng này tuy đơn giản dễ thực hiện nhưng lại gặp phải một vấn đề. Đó là BERT model sẽ chỉ dự đoán khi gặp [MASK] token trong Input Sequence, trong khi đó, chúng ta muốn nó phải dự đoán trong bất cứ trường hợp nào, có hoặc không có [MASK] token. Để giải quyết vấn đề này, trong số 15% số từ được lựa chọn ngẫu nhiên kia:\n 80% được thay thế bằng [MASK] token. 10% được thay thế bằng token ngẫu nhiên. 10% còn lại được giữ nguyên, không thay đổi.  Trong khi huấn luyện BERT model, Loss Function chỉ áp dụng đối với các dự đoán của Masked token và bỏ qua dự đoán của các Non-masked tokens khác. Thời gian huấn luyện cũng lâu hơn khá nhiều so với các model sử dụng phương pháp Nondirection training.\n3.2 Next Sentence Prediction (NSP)\nĐể hiểu được mối quan hệ giữa 2 câu, BERT sử dụng một kỹ thuật gọi là NSP. Trong quá trình huần luyện, một cặp câu được đưa vào, model sẽ học để dự đoán câu thứ 2 trong cặp câu Input đó có phải là câu tiếp theo của câu thứ nhất hay không? (ý tưởng nghe cũng khá giống với Siamese Network trong CV, :D).\nCụ thể, dữ liệu huấn luyện sẽ có 50% số cặp là liên tiếp (câu thứ 2 là tiếp theo của câu thứ nhất), 50% số cặp còn lại là 2 câu rời rạc nhau.  Để dự đoán xem câu thứ 2 có phải là tiếp theo của câu thứ nhất hay không, model sẽ làm như sau:\n Toàn bộ Input Sequence được đưa qua model. Ouput của [CLS] token được chuyển về vector kích thước 2x1 thông qua một lớp Classification đơn giản. Tính toán xác suất của mỗi nhãn sử dụng hàm Softmax.  MLM và NSP được sử dụng song song trong quá trình huấn luyện BERT model, với mục tiêu là tối thiểu hóa Loss Function kết hợp của cả 2 chiến lược đó. Đây là một ví dụ của câu nói nổi tiếng - Together to better.\n4. Một số thông tin về Pre-trained BERT model\n Có 2 phiên bản của Pre-trained BERT model:  BERT-Base: 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters BERT-Large: 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters    Thời gian huấn luyện mỗi phiên bản như sau:   Nếu có đủ dữ liệu huấn luyện, thời gian huấn luyện lâu hơn sẽ cho độ chính xác cao hơn. Ví dụ, đối với chiến lược Mask LM, BERT-Base model đạt được độ chính xác cao hơn 1% khi huấn luyện 1M Steps, so với 500K Steps (cùng batch_size).    6. Cách sử dụng BERT để Fine-tune model\nBERT có thể sử dụng cho rất nhiều tác vụ trong NLP:\n Classification task: Thêm một Classification layer ngay sau Ouput của Encoder Stack cho [CLS] token. Question Answering task: Một Q\u0026amp;A model nhận một câu hỏi và nhiệm vụ của nó là tìm ra câu trả lời trong Corpus. Sử dụng BERT, nó có thể được huấn luyện bằng cách học từ các cặp câu trong Input Sequence để dự đoán xem cặp câu đó có phải là một cặp câu hỏi - trả lời hay không? Named Entity Recognition (NER): Model nhận một câu và được yêu cầu là đánh dấu các dạng Entities khác nhau (Person, Organization, Date, \u0026hellip;) xuất hiện trong câu đó. Sử dụng BERT, model này có thể được huấn luyện bằng cách cho Ouput vector của mỗi Token đi qua một Classification layer để dự đoán xem đó có phải là một Entity hay không?  Trong quá trình Fine-tune, hầu hết các Hyper-parameters của BERT model được giữ nguyên. Các tác giả của bài báo đã đưa ra một số chỉ dẫn về các Hyper-parameters cần quan tâm, thay đổi để đạt được kết quả tốt nhất.\n Dropout – 0.1 Batch Size – 16, 32 Learning Rate (Adam) – 5e-5, 3e-5, 2e-5 Number of epochs – 3, 4  Các bạn có thể đọc chi tiết trong Section 3.5 \u0026amp; 4 của bài báo đó.\n7. Một số Pre-trained BERT model cho các tác vụ (lĩnh vực) cụ thể\nBERT là mã nguồn mở, có nghĩa là bất kỳ ai cũng có thể sử dụng nó. Google tuyên bố rằng người dùng có thể huấn luyện một hệ thống Question\u0026amp;Answering chỉ mất khoảng 30 phút nếu sử dụng TPU trên Cloud và vài giờ nếu sử dụng GPU. Nhiều tổ chứcc, nhóm nghiên cứu và các nhóm riêng biệt của Google đang tinh chỉnh kiến trúc mô hình BERT để tối ưu hóa hiệu quả của nó hoặc chuyên môn hóa nó cho một số nhiệm vụ nhất định. Một số ví dụ bao gồm:\n patentBERT - BERT model fine-tuned thực hiện nhiệm vụ phân loại các sáng kiến theo các nhóm khác nhau. docBERT - BERT model fine-tuned thực hiện nhiệm vụ phân loại các văn bản nói chung. bioBERT - BERT model fine-tuned thực hiện tạo Vector Embedding cho các tài liệu trong lĩnh vực sinh học. VideoBERT - BERT model fine-tuned thực hiện gán nhãn Video trên Youtube. SciBERT - BERT model fine-tuned thực hiện tạo Vector Embedding cho các tài liệu trong lĩnh vực khoa học. G-BERT - BERT model fine-tuned kết hợp với phương pháp Hierarchical Representations để đưa ra các khuyến nghị trong lĩnh vực y học. TinyBERT by Huawei - Phiên bản rút gọn của BERT để chạy nhanh hơn. DistilBERT by HuggingFace - Tương tự TinyBERT. PhoBERT - BERT model fine-tuned dành riêng cho các bài toán NLP sử dụng tiếng Việt, được công bố bởi VinAI.  8. Kết luận\nBERT chắc chắn là một bước đột phá trong việc giải quyết các bài toán NLP. Nó cho phép tiếp cận và tinh chỉnh nhanh các tham số, layers để áp dụng vào một loạt các bài toán thực tế . Trong bài này, chúng ta đã mô tả một số đặc điểm chính của BERT mà không đi quá sâu về mặt toán học. Nếu bạn muốn tìm hiểu tường tận, chi tiết hơn, bạn nên tìm đọc bài báo gốc của tác giả. Một tài liệu tham khảo hữu ích khác là mã nguồn BERT và các Pre-trained model của nó, bao gồm 103 ngôn ngữ và được nhóm nghiên cứu phát hành rộng rãi dưới dạng mã nguồn mở.\nỞ bài tiếp theo, có lẽ mình sẽ quay lại một số chủ đề của Computer Vision. Mời các bạn đón đọc.\n9. Tham khảo\n BERT paper Samia Rani Horev Yashu Seth Ben Lutkevich  ","permalink":"https://tiensu.github.io/blog/61_bert/","tags":["RNN","LSTM","Attention","Transformer","BERT"],"title":"BERT - Bidirectional Encoder Representations from Transformers"},{"categories":["RNN","LSTM","Attention","Transformer"],"contents":"Nếu bạn là dân ngoại đạo, bạn cũng có thể đã từng nghe về Transformers. Đó là một bộ phim bom tấn, liên tục lập kỷ lục phòng vé tại thời điểm nó ra mắt. Tuy nhiên, Transformers mình muốn nói ở đầy là một AI model. Được giới thiệu lần đầu vào năm 2017 trong bài báo Attention is all you need, cũng như bộ phim kia, nó cũng lập tức gây chấn động cộng động NLP lúc bấy giờ bởi hiệu năng của nó hơn hẳn so với các kiến trúc mô hình tồn tại trước đó trong các thử nghiệm được công bố.\n1. So sánh Transformers và họ hàng nhà RNN (LSTM, GRU)\nĐầu tiên, chúng ta thử so sánh Transformers với họ hàng RNN để thấy được ưu điểm của nó, và hiểu tại sao nó lại được yêu mến đến vậy.\nNhư chúng ta đã biết, kiến trúc Encoder-Decoder với RNN truyền thống tồn tại 2 nhược điểm:\n Không có khả năng tận dụng hết thông tin ngữ nghĩa của Input và Targer Sequence, đặc biệt là trong trường hợp Input Sequence có chiều dài lớn (Với cơ chế Attention, khả năng này có tốt hơn một chút nhưng vẫn chưa đủ). Vì phải xử lý tuần tự từng TimeStep một nên thời gian tính toán rất lâu.  Transformers giải quyết được 2 nhược điểm đó bằng cách:\n Sử dụng cơ chế Self-Attention (nhiều tầng Self-Attentions) để nắm bắt tốt hơn thông tin ngữ nghĩa của Input và Targer Sequence. Xử lý song song tất cả các TimeStep cùng một lúc \u0026ndash;\u0026gt; Giảm được rất nhiều thời gian tính toán.    2. Kiến trúc và thành phần của Transformers\nTransformer tỏ ra vượt trội trong việc xử lý dữ liệu văn bản vốn có tính chất tuần tự. Nó lấy một chuỗi văn bản làm đầu vào và tạo ra một chuỗi văn bản khác. Ví dụ như bài toán Machine Translation hay Text Summarization.\nVề thành phần cấu tạo, Transformers bao gồm một nhóm các bộ Encoders (Encoder stack) và một nhóm các bộ Decoder (Decoder stack). Ngoài ra còn có các thành phần Embedding, Encoding, Mask, \u0026hellip; khác để xử lý dữ liệu đầu vào và đầu ra.  2.1 Positional Encoder\nViệc xử lý đồng thời tất cả cá từ trong câu một lượt mang lại khả năng tính toán nhanh chóng cho Transformers, nhưng nó lại vô tình làm mất thông tin về vị trí của các từ trong câu đó. Để khắc phục vấn đề này, thông tin về vị trí của từ được mã hóa thành Positional Encoding (PE) vector để làm đầu vào cho Transformers.  PE được tính như sau: $PE_{(pos,2i)} = sin(\\frac{pos}{1000^{\\frac{2i}{d}}})$\n$PE_{(pos,2i+1)} = cos(\\frac{pos}{1000^{\\frac{2i}{d}}})$\n Trong đó, $pos$ là vị trí của từ trong câu, còn $i$ là chỉ số của các phần tử trong PE vector, $d$ là số chiều của Work/Token Embedding (cũng bằng với kích thước của PE).  Trong khi $d$ cố định thì $pos$ và $i$ thay đổi. PE cuối cùng là tổng hợp của tất cả các PE với sự thay đổi của $pos$ và $i$ đó.   Chi tiết thêm về Positional Encoding, các bạn có thể tham khảo tại đây.\n2.2 Masking\n Mục đích chính của Masking là che giấu (Mask) đi phần thông tin của Token phía sau, chỉ cho phép Decoder sử dụng thông tin của Token hiện tại và trước đó khi tạo Ouput. Ví dụ như trong một kỳ thi, ta cần phải che giấu đáp án đi, chỉ cho phép thí sinh sử dụng kiến thức của họ để làm bài.\nXem xét cách tính Output của Self-Attention như hình dưới đây:  Có 4 bước, chúng ta sẽ đi chi tiết mỗi bước.\n Bước 1    Ma trận Query (Q) được tạo từ Input (X) và ma trận trọng số của Q ($W^Q$). Tương tự cho Key (K) và Value (V).\nX có kích thước là (2,4), trong đó 2 là chiều dài của Input Sequence (số từ trong câu), 4 là số chiều của Word Embedding của 1 từ.\n$W^Q$ có kích thước là (4,4), trong đó 4 là số chiều của Word Embedding của 1 từ, 4 là do chúng ta giả sử $W^Q$ là ma trận vuông cho dễ xử lý. Thực tế có thể phức tạp hơn.\nKết quả, các ma trận Q, V, K có kích thước (2, 4).\n Bước 2    Đây chính là công thức tính Output của Self-Attention.\n Bước 3    Ta biểu diễn ma trận $I = Q \\times K^T$ bởi các giá trị A, B, C, D. Nhận thấy rằng, A chỉ phụ thuộc vào Embedding Token ở vị trí đầu tiên: $A = q_1 \\times k_1 = x_1*W^Q \\times k_1$).\nTrong khi đó, B phụ thuộc vào Embedding ở vị trí thứ nhất và thứ hai: ($B = q_1 \\times k_2 = x_1W^A \\times x_2W^K$).\n($x_1, x_2$ là các Embedding của Token thứ nhất và thứ hai).\n Bước 4    Làm tương tự cho ma trận F. Ta cũng nhận thấy B' phụ thuộc vào Embedding Token của cả hai vị trí 1 và 2. Để ngăn chặn điều này, ta thêm vào Mask như sau:  Ở đó:   Ta được:  Và cuối cùng là:  Như vậy, sau khi thêm Mask vào thì Output F của Decoder đã ko còn xuất hiện thành phần Embedding Token ở vị trí số 2.\n2.3 Scaled Dot-Product Attention\nĐây chính các phép tính toán của Self-Attention.  Lần lượt từng bước như sau:\n  Matmul - phép toán Matrix Dot-Product giữa ma trận Query và chuyển vị của ma trận Key. $MatMul(Q,K) = Q.K^T$\n   Scale - Ouput của phép toán Dot-Product có thể là một giá trị rất lớn, có thể làm rối loạn hoạt động của hàm Softmax. Do vậy, ta Scale chúng bằng cách chia cho hệ số $\\sqrt(d_k)$.\n  Mask - như đã đề cập ở mục 2.2.\n  Softmax - Đưa giá trị về một phân phối xác suất trong khoảng [0,1]. $Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt(d_k)})V$\n   2.4 Multi-Head Self-Attention\n Xem lại bài trước.\n2.5 Point-Wise Feed Forward Network and Residual Dropout\n Point-Wise Feed Forward Network Block, về cơ bản là một phép biến đổi tuyến tính hai lớp được sử dụng giống nhau trong toàn bộ kiến trúc mô hình, thường là sau các khối Attention.\nĐể áp dụng Regularization, một Dropout được áp dụng tại đầu ra của mỗi Sub-layer, trước khi nó được đưa vào làm Input cho Sub-layer tiếp theo.\n3. Huấn luyện mô hình Transformers\nDữ liệu huấn luyện bao gồm 2 thành phần: Input Sequence và Target Sequence. Quá trình huấn luyện diễn ra như sau:\n Bước 1 - Input Sequence được biến đổi thành Embedding, cùng với Positional Encoding để đưa vào Encoder Stack. Bước 2 - Encoder Stack xử lý và đưa ra một Vector đại diện của Input Sequence, gọi là Context Vector. Vector này sau đó được đưa sang cho Decoder Stack. Bước 3 - Target Sequence được bổ sung thêm tiền tố START_ (*còn gọi là start-of-sentence token*), chuyển đổi thành Embedding, cùng với Positional Decoding, đưa vào Decoder Stack. Bước 4 - Decoder Stack xử lý, sinh ra một Vector đại diện của Target Sequence. Bước 5 - Output Layer chuyển Vector đại diện này một Output Sequence. Bước 6 - Transformers Loss Function so sánh Output Sequence với Target Sequence. Loss sẽ được sử dụng để sinh ra Gradients để cập nhật trọng số mô hình trong quá trình Back-propagation.    Cách thức huấn luyện như thế này còn được gọi với cái tên là Teacher Forcing. Target Sequence ở đây đóng vai trò là Teacher để Force mô hình hoạt động đúng như mong muốn. Cá nhân mình thấy, nó khá giống với phương pháp Supervise Learning mà chúng ta đã quen thuộc.\n4. Sử dụng mô hình Transformers để dự đoán (Inference) 3 Quá trình dự đoán của Transformers có một chút khác biệt so với quá trình huấn luyện của nó. Chúng ta chỉ có Input Sequence, không có Target Sequence. Mục đích của Inference là sinh ra Output Sequence từ Input Sequence.\nTrong mô hình Seq2Seq, Output của Decoder Stack được sinh ra trong một vòng lặp, Ouput từ TimeStep trước được đưa vào làm Input của TimeStep tiếp theo. Vòng lặp kết thúc khi Output là Token kết thúc (_END). Còn trong mô hình Transformers, tại mỗi TimeStep, toàn bộ Output tại các thời điểm trước đó được đưa vào làm Input cho thời điểm tiếp theo, thay vì chỉ sử dụng Output cuối cùng.\nToàn bộ quá trình Inference diễn ra như sau:\n Bước 1 - Input Sequence đuọc chuyển thành Embedding, cùng với Positional Decoding, đưa vào Encoder Stack. Bước 2 - Encoder Stack xử lý và tạo ra Context Vector. Vector này được chuyển sang cho Decoder Stack. Bước 3 - Phía Decoder Stack, sử dụng một Input Sequence rỗng (*chỉ có một Start Token - START_*), chuyển sang Embedding, cùng với Positional Encoding, đưa vào Decoder Stack. Bước 4 - Decoder Stack xử lý, cùng với Context Vector từ Encoder Stack, sinh ra Vector đại diện của Output Sequence. Bước 5 - Output Layer biến đổi Vector đại diện này thành Output Sequence. Bước 6 - Từ cuối cùng trong Output Sequence đặt vào vị trí thứ 2 (sau Start Token) của Input Sequence của Decoder Stack. Sau đó, Intput Sequence mới này lại được đưa vào Decoder Stack. Bước 7 - Lặp lại từ bước 4-6 cho đến khi bắt gặp từ cuối cùng trong Ouput Sequence là End Token (_END).    5. Ứng dụng của Transformers\nTransformers được sử dụng rất rộng rãi ở hầu hết các bài toán trong lĩnh vực NLP. Với mỗi bài toán, chúng ta sẽ sử dụng một biến thể khác của Transformers.\n  Language models - Đây là bài toán sinh ra từ mới cho câu (sáng tác nhạc, làm thơ, viết truyện, \u0026hellip;). Ở đây, chỉ thành phần Encoder Stack của Transformers được sử dụng, như là một bộ trích xuất đặc trưng (Sequence Embedding) của Input Sequence. Đầu ra của Encoder Stack được đưa vào Language Model để cho ra một xác suất cho mỗi từ trong từ điển. Từ có xác suất cao nhất là kết quả cuối cùng.    Text Classification - Đây là bài toán phân loại văn bản thành các chủ đề, nhãn, \u0026hellip; khác nhau. Tương tự Language Model, chúng ta cũng chỉ sử dụng phần Encoder Stack cho ứng dụng này. Đầu ra của Encoder Stack được đưa vào bộ phân lớp, cho ra xác suất của từng nhãn. Nhãn có xác suất cao nhất sẽ được công nhận là kết quả chung cuộc.    Seq2Seq models - Đây là lớp bài toán bao gồm Machine Translation, Text Summarization, Question-Answering, Named Entity Recognition, Speech Recognition, \u0026hellip;\n  Kiến trúc đầy đủ của Transformers được sử dụng trong các ứng dụng này.\n5. Kết luận\nTrong bài này, chúng ta đã cùng nhau tìm hiểu khá chi tiết về kiến trúc Transformers, cũng như các ưu/nhược điểm và ứng dụng của nó.\nỞ bài tiếp theo, mình sẽ giới thiệu về của BERT, một state of the art language model for NLP. Mời các bạn đón đọc.\n6. Tham khảo\n Ketan Doshi Ketan Doshi datascience Rohan Jagtap Samuel Kiebaum Attention is all you need  ","permalink":"https://tiensu.github.io/blog/60_transformer/","tags":["RNN","LSTM","Attention","Transformer"],"title":"Transformers - Nhưng không phải là kẻ hủy diệt ..."},{"categories":["RNN","LSTM","Attention","Transformer"],"contents":"Kiến trúc Transformer với xương sống là Self-Attention đã làm mưa làm gió trong cộng đồng NLP trong 1-2 năm gần đây. Nó đạt được độ chính xác rất cao trong hầu hết các bài toán NLP. Trong bài này, hãy cùng nhau tìm hiểu kỹ hơn về cơ chế Self-Attention, làm tiền đề cho việc tìm hiểu kiến trúc Transformer ở bài sau.\n1. Self-Attention là gì?\nChúng ta đều biết rằng Word Embedding là vector đại diện cho ngữ nghĩa của một từ trong câu. Những từ mà có nghĩa tương tự nhau thì vector của chúng cũng sẽ gần giống nhau và ngược lại. Tuy nhiên, trong một câu, ý nghĩa của các từ riêng lẻ không đại diện cho cả câu đó. Ví dụ, xét câu: The bank of a river. Hai từ bank và river, nếu tách riêng thì có ý nghĩa hoàn toàn khác nhau, và nếu mang ý nghĩa đó của chúng vào câu thì sai hoàn toàn.\nCơ chế Self-Attention được đề xuất trong bài báo Attention is all you need có thể giải quyết tốt vấn đề này. Ý tưởng làm việc của nó là so sánh các từ với nhau đôi một, bao gồm cả chính nó (self) để tìm ra mức độ quan trọng của mỗi từ mà nó nên chú ý tới (thể hiện qua trọng số).\n2. Minh họa cách làm việc của Self-Attention\nCơ chế Attention có thể được hiểu như là sự kết hợp giữa một Query và một cặp Key-Value để cho ra một Output. Công thức tính như sau: $Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) \\times V$\n Trong đó, $\\frac{1}{\\sqrt(d_k)}$ là hệ số tỷ lệ. $d_k$ là số chiều của Key.\n2.1 Bước 1 - Chuẩn bị Input\nGiả sử, chúng ta có 3 Inputs (3 từ trong Input Sequence), mỗi Input là một vector 4 chiều như sau:   2.2 Bước 2 - Khởi tạo trọng số (Weight) cho Key, Query và Value\nMỗi một Input sẽ được đại diện bởi 3 đại lượng: key, query và value có số chiều tùy ý. Giả sử, 3 đại lượng này có số chiều là 3. Để tạo ra chúng, ta cần khởi tạo các trọng số cho từng đại lượng. Vì Input có số chiều là 4, (Key, Query, Value) có số chiều là 3 nên các Weights phải có kích thước 4x3. Chúng thường có giá trị nhỏ, được khởi tạo ngẫu nhiên sử dụng một trong số các phân phối Gaussian, Xavier, Kaiming, \u0026hellip;\n  Weight cho Key:    Weight cho Query:    Weight cho Value:    2.3 Bước 3 - Tính Key, Query, Value\nKey, Query, Value đạt được bằng cách nhân (dot product) Input với trọng số tương ứng của chúng.\n Key:      Query:      Value:     Trong thực tế, một Bias Vector có thể được thêm vào khi tính các giá trị Key, Query và Value.\n2.4 Bước 4 - Tính Attention Scores\n Để tính Attention Scores cho Input1, ta nhân (dot product) Query của nó với tất cả các Keys.  Từ bước 4 đến bước 7, mình chỉ tính cho Input1 làm đại diện. Hai Inputs còn lại được tính tương tự.\n2.5 Bước 5 - Tính Softmax Để đơn giản, mình bỏ qua hệ số tỉ lệ $\\frac{1}{\\sqrt(d_k)}$  Đưa Attention Scores qua hàm Softmax ta được:  2.6 Bước 6 - Tính Weighted Values\n Softmaxed Attention Score nhân với tất cả các Values ta được Weighted Values.  2.7 Bước 7 - Tính tổng Weighted Values\n Tính tổng Weighted Values theo kiểu element-wise ta được Output1 vector:  [2.0, 7.0, 1.5] chính là bộ trọng số thể hiện sự tương quan của Input1 với từng Input (bao gồm chính nó).\n2.8 Bước 8 - Lặp lại từ bước 4-7 cho Input2 \u0026amp; Input3\nChúng ta đã tính xong Output1, lặp lại các bước từ 4-7 đối với Input2 \u0026amp; Input3 ta được Output2 \u0026amp; Output3.  Chú ý - Kích thước của Query và Key phải luôn bằng nhau để có thể thực hiện được dot product, còn kích thước của Value có thể khác. Kich thước của Output sẽ giống với kích thước của Value.\n3. Multi-Head Self-Attention\nTrong kiến trúc của Transformer, mỗi một Self-Attention module được gọi là một Head. Việc sử dụng nhiều Heads đồng thời gọi là Multi-Head.  Mỗi Head nhận vào một Input $x$ (Token Embedding và Positional Decoding) và cho ra: $Head_i = Attention_i(x) = softmax(\\frac{Q_iK_i^T}{\\sqrt(d_k)})V_i$\n Trong đó, $d_k = d_k$(trong trường hợp một Head) /(số lượng Head).\nSau khi có được các Output của từng Head, ta sẽ tổng hợp chúng lại thành 1 Output duy nhất. $MultiHead(Q,K,V) = Concat(Head_1, Head_2, ..., Head_h)W^0$\n $W^0$ là ma trận có chiều rộng bằng với chiều rộng của ma trận Input, mục đích sử dụng của nó là để đưa kích thước của Output về bằng với kích thước của Input.\n4. So sánh Attention và Self-Attention\nNếu bạn vẫn còn mơ hồ giữa Attention và Self-Attention thì mình sẽ liệt kê những điểm khác nhau giữa chúng cho bạn.\n Attention thường sử dụng kết hợp với RNN/LSTM/GRU để cải thiện hiệu năng của mô hình hiện tại. Self-Attention thay thế hoàn toàn RNN/LSTM/GRU. Attention thường xuất hiện trong kiến trúc có đủ 2 thành phần Encoder và Decoder để truyền thông tin giữa chúng. Ngược lại,Self-Attention thường chỉ áp dụng trong phạm vi một thành phần, hoặc Encoder hoặc Decoder. Attention chỉ có thể sử dụng 1 lần trong một kiến trúc mô hình, trong khi đó, Self-Attention có thể áp dụng nhiều lần (VD: 18 lần trong Transformer). Self-Attention, như tên gọi, làm nhiệm vụ mô hình hóa mối quan hệ giữa các từ trong một cùng 1 chuỗi (Query, Key, Value xuất phát từ cùng 1 nguồn), còn Attention thì là 2 chuỗi khác nhau. Attention có thể kết nối 2 kiểu Input Sequence khác nhau (VD: text \u0026amp; image), Self-Attention thì chỉ làm việc với 1 loại. Cơ chế Multi-Head thường áp dụng cho Self-Attention. Nhưng về mặt lý thuyết, nó cũng có thể được sử dụng cho Attention. Tương tự, Query/Key/Value thường sử dụng đối với Self-Attention, nhưng nó cũng có thể được áp dụng cho Attention.  5. Kết luận\nTrong bài này, chúng ta đã cùng nhau tìm hiểu khá chi tiết về cơ chế Self-Attention cũng như khái niệm Multi-Head Self-Attention.\nỞ bài tiếp theo, mình sẽ giới thiệu về mô hình Transformer. Mời các bạn đón đọc.\n6. Tham khảo\n Praphul Singh Raimi Karim datascience Attention is all you need  ","permalink":"https://tiensu.github.io/blog/59_self-attention/","tags":["RNN","LSTM","Attention","Transformer"],"title":"Self-Attention và Multi-head Sefl-Attention trong kiến trúc Transformer"},{"categories":["RNN","LSTM","Attention"],"contents":"Trong bài này, mình sẽ giải thích qua về kiến trúc Encoder-Decoder với mô hình Seq2Seq. Sau đó, chúng ta sẽ tìm hiểu chi tiết về cơ chế Attention áp dụng trong kiến trúc đó.\n1. Giới thiệu mô hình Sequence to Sequence (Seq2Seq)\nMô hình Seq2Seq được giới thiệu lần đầu vào năm 2014 bởi Google. Mục đích của nó là ánh xạ một Input Sequence Data có chiều dài cố định thành một Output Sequence Data có chiều dài cố định. Chiều dài của 2 Sequence Data không nhất thiết phải giống nhau. Ví dụ khi dịch câu có 5 từ What are you doing now? từ tiếng Anh sang câu có 7 ký tự 今天你在做什麼？ trong tiếng Trung Quốc.\nMô hình Seq2Seq có thể giải quyết các bài toán sau:\n Text Summarization - Đây là bài toán tóm tắt nội dung của một văn bản dài thành một đoạn văn bản ngắn hơn. Kể từ khi được Google giới thiệu năm 2014, nó đã trở nên khá phổ biến. Machine Translation - Dịch văn bản giữa các ngôn ngữ khác nhau. Google Translate chính là một sản phẩm của bài toán này. Image/Video Captioning - Đưa cho máy tính một bức ảnh hoặc một video, nó sẽ trả lại cho bạn một (hoặc một vài) câu miêu tả nội dung của bức ảnh / Video đó. Mình đang nghĩ rằng phần thi đầu tiên của kỳ thi TOEIC (phần thi miêu tả tranh) có thể chính là một ứng dụng thực tế của bài toán này. Speech Recognition - Bài toán trong lĩnh vực Audio, còn được gọi là Speech To Text, tức chuyển đổi âm thanh thành văn bản. Music Generation - Đây là một bài toán rất thú vị, máy tính có thể sáng tác nhạc cho bạn. Nghe chắc sẽ rất ngầu! :D Recommendation Engine - Hệ thống khuyến nghị có lẽ đã không còn xa lạ với mọi người. Có rất nhiều các để tạo ra nó, và mô hình Seq2Seq với kiến trúc Encoder-Decoder cũng là một trong số đó, cho kết quả rất khả quan. Chatbot - Hay còn gọi là hệ thống Question-Answer. Siri hay Alexa là ví dụ thực tế.  Các bài toán kể trên đều có chung một đặc điểm là chúng sử dụng dữ liệu ở dạng chuỗi (tuần tự), bao gồm nhiều TimeSteps. Đó có thể là văn bản, âm thanh, tín hiệu, \u0026hellip; Ví dụ đối với văn bản thì mỗi TimeStep có thể hiểu là một từ trong văn bản đó.\n2. Kiến trúc của mô hình Seq2Seq\nMô hình Seq2Seq bao gồm 2 thành phần: Encoder và Decoder. Mỗi một thành phần bao gồm nhiều NN Layers xếp chồng lên nhau (stack). NN Layer có thể là CNN, RNN, LSTM. GRU, \u0026hellip; Trong bài này, mình sẽ lấy ví dụ là LSTM.  2.1 Quá trình huấn luyện\nTrong quá trình huấn luyện, mỗi thành phần sẽ thực hiện nhiệm vụ như sau:\n  Encoder - Đọc vào toàn bộ Input Sequence, lần lượt từng TimeStep tại các LSTM Cell. Tại TimeStep $t$, Output ra của các Cell là Hidden State ($h_t$) và Cell State ($C-t$), gọi chung là Internal State. Internal State của TimeStep trước được sử dụng cùng với Input của TimeStep hiện tại để làm đầu vào cho Cell hiện tại. Internal State ($h_0, c_0$) được khởi tạo ngẫu nhiên. Internal State của Cell cuối cùng của Encoder được sử dụng làm đầu vào cho Decoder. Chi tiết về Internal State của LSTM Cell, bạn có thể xem lại bài này của mình.\n  Decoder - Đọc vào toàn bộ Target Sequence, lần lượt từng TimeStep. Khác với Encoder, Target Sequence được thêm vào tiền tố START_ và hậu tố _END để chỉ ra điểm bắt đầu và kết thúc của nó. Internal State ban đầu ($h_0, s_0$) của Decoder được khởi tạo bằng với Intern State của Cell cuối cùng trong Encoder. Tại mỗi TimeStep $t$, Decoder sẽ đọc vào một từ trong văn bản Target Sequence, cho ra ra một từ dự đoán ($y'_t$) và Internal State ($h_t, c_t$). Internal State này cũng sẽ được sử dụng cho TimeStep tiếp theo, còn $y'_t$ sẽ được dùng để tính toán lỗi với Target Sequence, sau đó Backpropagation sẽ cập nhật lại các trọng số của model theo lỗi đó. Internal State ở Cell cuối cùng của Decoder được loại bỏ vì không dùng đến.\n   2.2 Quá trình dự đoán\nQuá trình dự đoán của Encoder vẫn giống như quá trình huấn luyện nó. Còn đối với Decoder, quá trình dự đoán diễn ra như sau:\n Internal State ban đầu ($h_0, s_0$) của Decoder được khởi tạo bằng với Intern State của Cell cuối cùng trong Encoder. Input của Decoder luôn bắt đầu bằng START_. LSTM Cell của Decoder sinh ra mỗi từ tại mỗi TimeStep. Internal State của mỗi TimeStep được sử dụng cho TimeStep tiếp theo. Từ dự đoán sinh ra tại mỗi TimeStep ($y'_t$) được chuyển thành Input cho TimeStep tiếp theo. Quá trình dự đoán kết thúc khi Decoder dự đoán ra $y'_t$ là _END.   3. Hạn chế của mô hình Seq2Seq với kiến trúc Encoder-Decoder\nKiến trúc Encoder-Decoder làm việc rất hiệu quả đối với Input Sequence có chiều dài nhỏ, nhưng hiệu năng sẽ giảm dần khi kích thước của Input Sequence tăng lên. Giả sử, Encoder nhận vào một Input Sequence {$x_1, x_2, \u0026hellip;, x_n$} và mã hóa thành các vectors có chiều dài cố định {$h_1, h_2, \u0026hellip;, h_n$}, gọi là Hidden State hay Context Vector. Chỉ có Context Vector cuối cùng $h_n$ mới được sử dụng cho bộ Decoder để dự đoán Output, dẫn đến thông tin của toàn bộ Input Sequence không được sử dụng đầy đủ (mất thông tin). Attention xuất hiện như là một giải pháp hữu hiệu để giải quyết vấn đề này.\n4. Giới thiệu Attention\nAttention là một kỹ thuật được Bahdanau et al., 2014 và Luong et al., 2015 giới thiệu trong các bài báo của họ. Ý tưởng của nó là cho phép Decoder sử dụng thông tin của toàn bộ Input Sequence, nhưng chỉ tập trung vào những phần quan trọng tại mỗi TimeStep. Nói một cách cụ thể và dễ hiểu hơn, Attention thực chất là cơ chế tạo ra một Context Vector bằng cách tính trung bình có trọng số của toàn bộ Internal State của Input Sequence trong bộ Encoder: $c_i = \\sum_{j=1}^n\\alpha_{ij}h_j$\n  Trong đó:\n $\\alpha_{ij}$ là trọng số của TimeStep $j$ của Decoder và TimeStep $i$ của Encoder. Nói cách khác, Output thứ $j$ của Decoder nên chú ý một lượng $alpha_{ij}$ đến Input thứ $i$ của Encoder. $h_i$ là Hidden State tại TimeStep $i$ của Encoder. $n$ là chiều dài của Input Sequence.  $\\alpha_{ij}$ được tính bằng cách lấy Softmax của Attention Score ($e_{ij}$): $\\alpha_{ij} = softmax(e_{ij}) = \\frac{exp(e_{ij})}{\\sum_{k=1}^m exp(e_{ik}}$\n$e_{ij} = f(s_{i-1}, h_j) = AlignScore(s_{i-1}, h_j)$\n Trong đó:\n $h_{i-1}$ là Hidden State tại TimeStep $i-1$ của Decoder. $s_j$ là Hidden State tại TimeStep $j$ của Encoder.  Context Vector $c_{ij}$ sau đó được sử dụng để Decoder tính ra Output $y_i$.\n 5. Bahdanau Attention \u0026amp; Luong Attention\nNhư bên trên đã giới thiệu, hai nhóm tác giả đã giới thiệu 2 loại Attention khác nhau, gọi là Bahdanau Attention và Luong Attention.  Xét về nguyên lý hoạt động thì 2 dạng Attention này đều giống nhau. Sự khác nhau của chúng nằm ở kiến trúc và cách tính toán của mỗi loại.\n5.1 Bahdanau Attention\n Bahdanau Attention còn được gọi là Additive Attention, được tạo ra bởi Dzmitry Bahdanau trong bài báo vào năm 2014. Mục tiêu của nó là cải thiện hiệu năng của mô hình Seq2Seq bằng cách thay đổi đầu vào của Decoder với các thông tin từ Input Sequence. Các bước tiến hành như sau:\n Tạo Encoder Hidden State - Encoder sinh ra Hidden State tại mỗi TimeStep. Tính toán Alignment Score giữa Decoder Hidden State ở TimeStep trước đó với mỗi Encoder Hidden State. Chú ý rằng, Encoder Hidden State ở TimeStep cuối cùng được sử dụng như là Decoder Hidden State ở TimeStep đầu tiên. Tính toán Softmax của Alignment Score - Giá trị của Alignment Score ở bước trên được đưa về khoảng giá trị [0,1] bằng cách sử dụng hàm Softmax. Tính toán Context Vector - Encoder Hidden State và Alignment Score tương ứng của nó được nhân với nhau để tạo thành Context Vector cho mỗi TimeStep. Tính toán Output của Decoder - Các Context Vectors được cộng lại với nhau, rồi cộng với vào Decoder Output và Decoder Hidden State tại TimeStep trước đó, để sinh ra Decoder Output mới tại TimeStep hiện tại. Lặp lại bước 2-5 đối với mỗi TimeStep của Decoder đến tận khi Decoder Output là _END hoặc chiều dài của Output Sequence đặt đến giá trị tối đa quy định trước.    5.2 Luong Attention\n Luong Attention được đề xuất bởi Thang Luong trong bài báo của anh ấy và đồng nghiệp. Nó còn có tên khác là Multiplicative Attention, kế thừa từ Bahdanau Attention. Hai điểm khác biết chủ yếu giữa Luong Attention và Bahdanau Attention là:\n Cách tính toán Alignment Score. Có 3 phương pháp tính Aligment Score trong Luong Attention so với 1 phương pháp của Bahdanau Attention. Vị trí của Attention trong kiến trúc Encoder-Decoder.  Các bước thực hiện Luong Attention như sau:\n Tạo Encoder Hidden State - Encoder sinh ra Hidden State tại mỗi TimeStep. Tạo Decoder Hidden State - Decoder Hidden State và Decoder Output của TimeStep trước đó được đưa qua Decoder RNN Cell để sinh ra Decoder Hidden State tại TimeStep hiện tại. Tính toán Alignment Score - Sử dụng Decoder Hidden State ở bước trên và Encoder Hidden State để tính Alignment Score. Tính toán Softmax của Alignment Score - Giá trị của Alignment Score ở bước trên được đưa về khoảng giá trị [0,1] bằng cách sử dụng hàm Softmax. Tính toán Context Vector - Encoder Hidden State và Alignment Score tương ứng của nó được nhân với nhau để tạo thành Context Vector. Tính toán Output của Decoder - Context Vector được cộng vào Decoder Output và Decoder Hidden State tại TimeStep trước đó, để sinh ra Decoder Output mới tại TimeStep hiện tại. Lặp lại bước 2-6 đối với mỗi TimeStep của Decoder đến tận khi Decoder Output là _END hoặc chiều dài của Output Sequence đặt đến giá trị tối đa quy định trước.  Như chúng ta thấy, thứ tự các bước của Luong Attention khác so với Bahdanau Attention.\n6. Global/Soft Attention \u0026amp; Local/Hard Attention\nPhụ thuộc vào việc có bao nhiêu Encoder Hidden State tham gia vào quá trình tạo Context Vector cho Decoder mà chúng ta có thể chia Attention thành 2 loại: Global/Soft Attention và Local/Hard Attention.  6.1 Global/Soft Attention\nGlobal Attention, tên khác là Soft Attention là loại Attention mà ở đó toàn bộ Encoder Hidden State đều được sử dụng để tính toán Context Vector tại mỗi TimeStep.  6.2 Local/Hard Attention\nGlobal Attention có một nhược điểm là nó yêu cầu tài nguyên tính toán khá lớn, nhất là đối với các bài toán mà Input Sequence có chiều dài lớn. Đó chính là lý do Local/Hard Attention ra đời. Nó giải quyết vấn đề của Global Attention bằng cách chỉ sử dụng một số lượng nhất định Encoder Hidden State thay vì tất cả.  7. Mở rộng của Attention (Extended Attention)\nCác loại Attention mà chúng ta nói từ đầu đến giờ chỉ hoạt động với kiến trúc Encoder-Decoder (có đủ 2 thành phần Encoder và Decoder), tức là phải có cả Input Sequence và Target Sequence như trong bài toán Machine Translation hay Text Summarization. Để áp dụng vào bài toán mà chỉ có một thành phần Encoder (chỉ có Input Sequence, không có Target Sequence) hoặc ngược lại, như Text Classification, chúng ta phải sử dụng các dạng mở rộng của Attention. Có 3 loại Extended Attention là: Self-Attention, Multi-head Self-Attention và Hierarchical Attention.  Chúng ta sẽ tìm hiểu kỹ hơn về Self-Attention và Multi-head Sefl-Attention trong bài tiếp theo. Còn Hierachical Attention, các bạn đọc thêm tại đây.\n8. Một số dạng Alignment Score Function\nBảng dưới đây tổng hợp một số dạng Alignment Score Function:  9. Ví dụ về cách làm việc của Attention\nTrong phần này, chúng ta sẽ minh họa cách làm việc của Attention thông qua một ví dụ trực quan để có thể hiểu rõ hơn về nó.\n9.1 Bước 1 - Chuẩn bị Encoder Hidden State\nGiả sử chúng ta có 4 Encoder Hidden States (màu xanh) và Decoder Hidden State đầu tiên (màu vàng).  9.2 Bước 2 - Tính Alignment Score\nTính Aligment Score, sử dụng Dot Product Function giữa Decoder Hidden State và Encoder Hidden States (xem mục 8).   Theo kết quả trên, chúng ta đạt được Alignment Score cao nhất là 60 tại TimeStep thứ 2 của Encoder (Hidden State là [5,0,1]). Điều này có nghĩa là Output tiếp theo của Decoder sẽ chịu ảnh hưởng nhiều của Hidden State này.\n9.3 Bước 3 - Cho Alignment Score qua Softmax Function\nTiếp theo, chúng ta đưa Alignment Scores đi qua hàm Softmax, thu được các giá trị trong khoảng [0,1].   9.4 Buớc 4 - Tính Context Vector của mỗi TimeStep\nVector Context được tính bằng cách nhân Encoder Hidden State với Alignment Score (đã đi qua hàm Softmax) tương ứng của nó.   9.5 Bước 5 - Tính tổng của các Context Vector\nCác Context Vector tại mỗi TimeStep được cộng lại với nhau, tạo thành 1 Context Vector chung cho toàn bộ Input Sequence.   9.6 Bước 6 - Sử dụng Context Vector cho Decoder\nĐến đây, ta đã được Context Vector đầy đủ của toàn bộ Input Sequence. Chúng ta sẽ đưa nó vào Decoder để sử dụng tạo ra Output mới.  10. Kết luận\nTrong bài này, chúng ta đã cùng nhau tìm hiểu khá chi tiết về cơ chế Attention áp dụng cho mô hình Seq2Seq với kiến trúc Encoder-Decoder.\nỞ bài tiếp theo, mình sẽ tiếp tục giới thiệu về Self_Attention và Multi-head Sefl-Attention. Hiểu được 2 lại Attention này là điều kiện tiền để để chúng ta có thể tiếp tục với mô hình Transformer. Mời các bạn đón đọc.\n11. Tham khảo\n Keshav Bhandari Raimi Karim Anusha Lihala floydhub  ","permalink":"https://tiensu.github.io/blog/58_attention/","tags":["RNN","LSTM","Attention"],"title":"Tìm hiểu cơ chế Attention trong mô hình Seq2Seq"},{"categories":["RNN","LSTM"],"contents":"Thực ra, các bài toán về Nartual Language Processing(NLP) không phải là thế mạnh của mình. Từ trước đến giờ, mình chủ yếu làm các bài toán về Computer Vision(CV). Tuy nhiên, trong thời gian gần đây, cộng đồng nói rất nhiều về Attention, Transformer, BERT, \u0026hellip;. Đó là những kỹ thuật tiên tiến mới ra đời, giúp giải quyết rất nhiều tác vụ khó của NLP, đạt đến độ State of Art. Hoạt động trong lĩnh vực AI đã lâu, mình không muốn đứng ngoài dòng chảy công nghệ đó. Mặc dù bên CV, mình còn rất nhiều thứ muốn viết, nhưng trong một số bài viết sắp tới mình muốn \u0026ldquo;đổi gió\u0026rdquo; một chút. Mình sẽ viết về các kỹ thuật sử dụng trong NLP, sau khi đã dành ra kha khá thời gian để tìm hiểu về chúng. Mục đích viết vẫn là để ghi nhớ và chia sẻ với mọi người. Các kỹ thuật mình dự định viết sẽ bao gồm: RNN, LSTM, Transformer, BERT, Attention, Seq2Se2, \u0026hellip;\nBài đầu tiên trong chủ đề NLP, mình sẽ cùng các bạn tìm hiểu về RNN.\nNếu như các bên CV có CNN thì bên NLP có RNN. CNN chuyên xử lý dữ liệu có kiến trúc dạng lưới (grid), (VD: Images, \u0026hellip;), còn RNN chuyên xử lý dữ liệu dạng chuỗi (sequential).\n1. Overview\n1.1 Kiến trúc RNN\nGiả sử:\n $X_t \\in R^{n\\times d}$ là Input tại Time-Step $t$, $h_t \\in R^{n\\times h}$ là Hidden State tại Time-Step $t$, $W_{xh} \\in R^{d\\times h}$ là ma trận trọng số của Hidden State tại Time-Step $t$, $b_h \\in R^{1\\times h}$ là hệ số Bias của Hidden State tại Time-Step $t$, $h_{t-1}$ là Hidden State tại Time-Step $t-1$, $W_{hh} \\in R^{h\\times h}$ là ma trận trọng số của Hidden State tại Time-Step $t-1$, $W_{hq} \\in R^{h\\times q}$ là ma trận trọng số của Output Layer tại Time-Step $t$ $b_q \\in R^{1\\times h}$ là hệ số Bias của Output Layer tại Time-Step $t$, $\\phi$ là hàm kích hoạt.   Khi đó:\n  Trạng thái của Hidden Layer tại Time_Step $t$ sẽ là: $h_t = \\phi(X_tW_{xh} + h_{t-1}W_{hh} + b_h)$\n   Giá trị đầu ra tại Time-Step $t$ sẽ là: $O_t = h_tW_{hq} + b_q$\n   1.2 Ưu điểm, nhược điểm của RNN\nƯu điểm:\n Có khả năng xử lý dữ liệu đầu vào ở bất kỳ độ dài nào. Kích thước mô hình không tăng theo kích thước đầu vào. Việc huấn luyện mô hình có sử dụng thông tin ở Time-Step trước đó. Các hệ số của mô hình (weight và bias) được chia sẻ theo thời gian.  Nhược điểm:\n Việc xử lý, tính toán mất khá nhiều thời gian. Thông tin từ các Time-Step ở xa không được duy trì tốt. Không thể xem xét bất kỳ đầu vào nào trong tương lai cho trạng thái hiện tại.  1.3 Một số loại mô hình RNN\n  Dạng 1: One-to-One    Dạng 2: One-to-Many  Ứng dụng: Music generation\n  Dạng 3: Many-to_One  Ứng dụng: Sentiment classification\n  Dạng 4: Many-to-Many (Input Data và Ouput có chiều dài bằng nhau)  Ứng dụng: Name entity recognition\n  Dạng 4: Many-to-Many (Input Data và Ouput có chiều dài khác nhau)  Ứng dụng: Machine translation\n  1.4 Loss Function\nLoss Function của RNN bằng tổng các Loss tại các Time-Step:  1.5 Backpropagation through time\nBackpropagation được thực hiện tại mỗi Time-Step. Tại Time-Step T, đạo hàm của Loss Function $L$ đối với ma trận trọng số $W$ được cho bởi công thức:\n 2. Xử lý vấn đề Long Term Dependences\nBình thường thì RNN sử dụng 1 trong 3 Activation Function là: Sigmoid, Tanh và ReLU.  Cũng giống như các mô hình khác, hiện tượng Vanishing \u0026amp; Exploring Gradient cũng xảy ra ở trong mô hình RNN. Điều này dẫn đến việc RNN không có khả năng ghi nhớ trạng thái của các Time-Step ở xa. Nguyên nhân của các hiện tượng này là do việc thực hiện phép nhân Gradient theo hàm mũ, làm cho nó tăng/giảm một các đột biến khi số lượng Layers tăng lên.\nSử dụng ReLU chúng ta đã giải quyết được khá tốt vấn đề Vanishing Gradient. Còn đối với Exploring Gradient, chúng ta có thể sử dụng Gradient Clipping.  Ý tưởng của Gradient Clipping cũng tương tự như ReLU, nó đưa ra một giới hạn, và Gradient chỉ có thể nhỏ hơn hoặc bằng giới hạn đó.\n3. Long Short Team Memory (LSTM)\nLong Short Team Memory (LSTM)) là một phiên bản cải tiến của RNN, giúp dễ dàng ghi nhớ dữ liệu quá khứ trong bộ nhớ của nó. Vấn đề Vanishing Gradient của RNN được giải quyết tốt hơn với LSTM. LSTM rất phù hợp giải quyết các bài toán phân loại, xử lý và dự đoán chuỗi thời gian có độ dài không xác định. Mô hình LSTM cũng được huấn luyện bằng cách thuật toán Backpropagation.\nKiến trúc mạng LSTM bao gồm nhiều Layers, mỗi Layers được cấu tạo bởi nhiều đơn vị nhỏ gọi là Cell. Mỗi Cell được đại diện bởi 2 bộ nhớ: Cell State ($C$) và Hidden State ($h$).\n- Hidden State - h,H: Bộ nhớ ngắn hạn (working memory), chỉ lưu thông tin của Cell ngay trước Cell hiện tại. Tồn tại trong cả RNN và LSTM. Hidden State cũng chính là Ouput của RNN/LSTM Cell.\n- Cell State - C: Bộ nhớ dài hạn (long-term memory, memory cell), lưu thông tin của nhiều Cells trong quá khứ. Chỉ tồn tại trên LSTM.\nXét 1 Cell hiện tại ($C_t, h_t$) trong LSTM. Luồng dữ liệu trong Cell này tuần tự đi qua 3 cổng như sau:\n- Forget gate: Cổng này quyết định thông tin nào từ Cell trước đó ($C_{t-1}$) nên được giữ lại hoặc ném đi. Thông tin từ Hidden State trước đó ($h_{t-1}$) và thông tin từ Input hiện tại ($x_t$) được đưa qua hàm Sigmoid, cho ra một giá trị trong khoảng từ 0 đến 1. Giá trị này càng gần 0 nghĩa là thông tin ít quan trọng (*trường hợp = 0 thì có thể bỏ qua - Forget*), giá trị càng gần 1 nghĩa là thông càng tin quan trọng.  $f_t = \\sigma(W_f\\cdot[h_{t-1},x_t] + b_f) = \\sigma(W_{xf}X_t + W_{hf}h_{t-1} + b_f)$\n  Input gate: Cổng này quyết định giá trị nào từ Hidden State trước đó ($h_{t-1}$) và thông tin từ Input hiện tại ($x_t$) sẽ được đi vào Cell hiện tại. Để làm được việc này, nó sử dụng 2 hàm Sigmoid và Tanh. Tương tự như Forget Gate, thông tin từ Hidden State trước đó ($h_{t-1}$) và thông tin từ Input hiện tại ($x_t$) được đưa qua hàm Sigmoid, cho ra một giá trị trong khoảng từ 0 đến 1. Giá trị này càng gần 0 nghĩa là thông tin càng ít quan trọng , giá trị càng gần 1 nghĩa là thông tin càng quan trọng. Hàm Tanh tạo ra một vector ứng cử của Cell State hiện tại ($\\widetilde{C_t}$). Output của 2 hàm này được nhân với nhau tính toán Cell State hiện tại.  $i_t = \\sigma(W_i\\cdot[h_{t-1},x_t] + b_i) = \\sigma(W_{xi}X_t + W_{hi}h_{t-1} + b_i)$\n$\\widetilde{C_t} = tanh(W_c\\cdot[h_{t-1},x_t] + b_c) = tanh(X_tW_{xc} + h_{t-1}W_{hc} + b_c)$\n   Đến đây, ta đã có thể tính được Cell State hiện tại:\n Chú giả về các phép toán: $\\otimes$: Element-wise multiplication\n$\\oplus$: Element-wise addition\n Chúng ta nhân trạng thái của Cell trước đó với $f_t$ rồi cộng với $i_t * \\widetilde{C_t}$ $C_t = f_t * C_{t-1} + i_t * \\widetilde{C_t}$\n - Output gate: Cổng này quyết định thông tin nào sẽ được output ra ngoài ($h_t$). Vẫn vẫn giống như 2 cổng bên trên, thông tin từ Hidden State trước đó ($h_{t-1}$) và thông tin từ Input hiện tại ($x_t$) được đưa qua hàm Sigmoid, cho ra một giá trị trong khoảng từ 0 đến 1. Giá trị này càng gần 0 nghĩa là thông tin càng ít quan trọng , giá trị càng gần 1 nghĩa là thông tin càng quan trọng. Tiếp đó, Cell State được cho qua hàm Tanh để đưa giá trị của Cell State về miền [-1,1].  Output của Cell được tính như sau: $o_t = \\sigma(W_o\\cdot[h_{t-1},x_t] + b_o) = \\sigma(W_{xo}X_t + W_{ho}h_{t-1} + b_o)$\n$h_t = o_t * tanh(C_t)$\n 4. Kết luận\nTrong bài này, chúng ta đã tóm tắt sơ lược về RNN \u0026amp; LSTM. Mặc dù LSTM tiên tiến hơn RNN nhưng nó vẫn còn tồn tại một số nhược điểm.\nTrong bài tiếp theo, chúng ta sẽ cùng tìm hiểu về cơ chế Attention và kiến trúc Transformer. Mời các bạn đón đọc.\n5. Tham khảo\n stanford colah aditi-mittal d2l.ai  ","permalink":"https://tiensu.github.io/blog/57_rnn_summary/","tags":["RNN","LSTM"],"title":"Tóm tắt về RNN \u0026 LSTM"},{"categories":["MLOps"],"contents":"Nếu bạn là người theo nghiệp Code được vài năm, bạn chắc đã từng trải qua cảm giác bực bội, khó chịu khi phải đọc code của một dự án có từ trước đó. Nó được viết một cách cẩu thả, tùy tiện, không theo một quy định hay tổ chức nào cả. Người ta gọi đó là Source Code không có tính Maintainance. Là một Coder có trách nhiệm, có đạo đức, chúng ta nên viết code làm sao cho người đến sau, khi đọc code của bạn có thể nhanh chóng hiểu được vấn đề. \u0026ldquo;Người đến sau\u0026rdquo; ở đây có thể là chính bạn, vì sau một thời gian thì có khi chính bạn cũng quên hết những gì mình từng viết. Việc viết code một cách cẩn thận còn làm giảm rủi ro phát sinh các lỗi tiềm tàng trong quá trình sử dụng về sau này.\nBất kỳ dự án phần mềm nào cũng đều phải có quy định về cách tổ chức Source Code trong dự án, ngay từ khi bắt đầu dự án. Các dự án về AI cũng không ngoại lệ. Trong bài này, chúng ta sẽ cùng xem xét một cách tổ chức Source Code cho các dự án AI. Và nếu bạn thấy phù hợp thì có thể áp dụng cho các dự án của bạn.\n1. Project Structure\nMột dự án được coi là tổ chức tốt khi nó có tính module. Tức là mỗi chức năng của dự án được tách riêng ra thành các thành phần khác nhau. Đó gọi là nguyên lý Separation of Concerns. Theo cách này, chúng ta có thể dễ dàng chỉnh sửa, nâng cấp, bổ sung mỗi chức năng mà không ảnh hưởng quá nhiều đến các chức năng khác. Hơn thế nữa, nó còn làm tăng khả năng tái sử dụng code hiệu quả, hạn chế việc code bị trùng lặp không cần thiết. Từ đó, toàn bộ Source Code của dự án sẽ gọn gàng, sáng sủa, dễ nâng cấp, hạn chế lỗi, \u0026hellip; hơn rất nhiều.\nĐối với các dự án về AI, một cách tổ chức Source Code dự án có thể như sau:  Trước hết, chúng ta cần phân biệt module và package. Module chỉ đơn giản là các file chứa Python code và chúng có thể được Import lẫn nhau. Trong khi đó, Package là một thư mục chứa nhiều Modules hoặc các Sub-packages. Để có thể Import được Package, cần phải có một file init.py trong Package đó. File này có thể rỗng.\nNhư vậy, trong cách tổ chức này, chúng ta có 8 Packages khác nhau:\n configs: Chứa các thông tin cấu hình của hệ thống, có thể thay đổi trong quá trình phát triển và sử dụng. Ví dụ: Các Hyper-parameters, các đường dẫn đến Dataset, Model, các thông số trong kiến trúc của Model, các thông số để huấn luyện Model, \u0026hellip; Một cấu hình ví dụ đơn giản như sau:  CFG = { \u0026#34;data\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;oxford_iiit_pet:3.*.*\u0026#34;, \u0026#34;image_size\u0026#34;: 128, \u0026#34;load_with_info\u0026#34;: True }, \u0026#34;train\u0026#34;: { \u0026#34;batch_size\u0026#34;: 64, \u0026#34;buffer_size\u0026#34;: 1000, \u0026#34;epoches\u0026#34;: 20, \u0026#34;val_subsplits\u0026#34;: 5, \u0026#34;optimizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;adam\u0026#34; }, \u0026#34;metrics\u0026#34;: [\u0026#34;accuracy\u0026#34;] }, \u0026#34;model\u0026#34;: { \u0026#34;input\u0026#34;: [128, 128, 3], \u0026#34;up_stack\u0026#34;: { \u0026#34;layer_1\u0026#34;: 512, \u0026#34;layer_2\u0026#34;: 256, \u0026#34;layer_3\u0026#34;: 128, \u0026#34;layer_4\u0026#34;: 64, \u0026#34;kernels\u0026#34;: 3 }, \u0026#34;output\u0026#34;: 3 } }  dataloader: Chứa code liên quan đến việc data loading và data preprocessing. evaluation: Chứa code thực hiện việc đánh giá hiệu năng và độ chính xác của model. executor: Chứa code (function, script) để huấn luyện model, hoặc sử dụng model để dự đoán. Trong Package này thường có file main.py. model: Chứa code định nghĩa kiến trúc của model. notebooks: Chứa tất cả các Jupyter/Colab Notebook của dự án (nếu có). ops: Chứa code liên quan đến các hoạt động kiểu như algebraic transformations, image manipulation techniques hay graph operations. Package này có thể có hoặc không. utils: Chứa các common functions, có thể sử dụng ở nhiều nơi trong các Packages khác. Những gì mà không nằm trong số các Packages kể trên thì cũng có thể đưa vào Package này.  2. Object Oriented Programming (OOP)\nLập trình hướng đối tượng (OOP) thường không được sử dụng nhiều trong Python giống như Java hay C#. Có lẽ là bởi vì Python là một ngôn ngữ lập trình kiểu Script. Tức là cứ viết là chạy, không cần phải có hàm, phải khai báo, bla bla.\nTuy nhiên, Python cũng hỗ trợ lập trình theo kiểu OOP. Và trong một dự án lớn thì sử dụng OOP cho Python mang lại hiệu quả rất tích cực, giống như Java hay C#.\n2.1 Tính đóng gói (encapsolution)\nVí dụ, ta viết một Class tên là Unet như sau:\nclass UNet(): def __init__(self, config): self.base_model = tf.keras.applications.MobileNetV2( input_shape=self.config.model.input, include_top=False) self.batch_size = self.config.train.batch_size . . . def load_data(self): \u0026#34;\u0026#34;\u0026#34;Loads and Preprocess data \u0026#34;\u0026#34;\u0026#34; self.dataset, self.info = DataLoader().load_data(self.config.data) self._preprocess_data() def _preprocess_data(self): . . . def _set_training_parameters(self): . . . def _normalize(self, input_image, input_mask): . . . def _load_image_train(self, datapoint): . . . def _load_image_test(self, datapoint): . . . def build(self): \u0026#34;\u0026#34;\u0026#34; Builds the Keras model based \u0026#34;\u0026#34;\u0026#34; layer_names = [ \u0026#39;block_1_expand_relu\u0026#39;, # 64x64 \u0026#39;block_3_expand_relu\u0026#39;, # 32x32 \u0026#39;block_6_expand_relu\u0026#39;, # 16x16 \u0026#39;block_13_expand_relu\u0026#39;, # 8x8 \u0026#39;block_16_project\u0026#39;, # 4x4 ] layers = [self.base_model.get_layer(name).output for name in layer_names] . . . self.model = tf.keras.Model(inputs=inputs, outputs=x) def train(self): . . . def evaluate(self): . . . Bạn có thể nhìn thấy rằng mỗi chức năng được đóng gói bên trong một phương thức riêng biệt, và các thuộc tính được khai báo như là các instance variables. Cách viết như thế này rõ ràng là giúp code trở nên sáng sủa, dễ mở rộng và bảo trì. Đây chính là tính đóng gói (encapsolution) của OOP. Có một điều mình nhận thấy là OOP trong Python không thiết lập phạm vi truy cập các thuộc tính và phương thức trong một Class. Tất cả chúng đều là public, tức có thể truy cập từ mọi nơi. TUy nhiên, để bắt chước cho giống với OOP chuẩn, khi code Python, chúng ta thường quy ước như sau:\n Để khai báo phạm vi truy cập thành phần trong Class là public thì tên của chúng phải bắt đầu bằng 1 chữ cái. Để khai báo phạm vi truy cập thành phần trong Class là protected thì tên của chúng phải bắt đầu bằng ký tự \u0026ldquo;_\u0026rdquo;. Để khai báo phạm vi truy cập thành phần trong Class là public thì tên của chúng phải bắt đầu bằng 2 ký tự \u0026ldquo;_\u0026rdquo; (tức là __).  2.2 Tính thừa kế, trừa tượng và đa hình\nNgoài tính Encapsolution, OOP có 3 tính chất quan trọng nưã là tính kế thừa(inheritance), tính trừu tượng(abstraction) và tính đa hình(polymorphism).\nGiả sử, khi bắt đầu viết code cho dự án, chúng ta mới chỉ hình dung sơ bộ là chắc chắn sẽ cần những phương thức này, nhưng cách thực hiện phương thức sẽ khác nhau tùy vào giải pháp lựa chọn. Khi đó, chúng ta sẽ sử dụng tính chất abstraction để viết ra một Class kiểu như sau:\nclass BaseModel(ABC): \u0026#34;\u0026#34;\u0026#34;Abstract Model class that is inherited to all models\u0026#34;\u0026#34;\u0026#34; def __init__(self, cfg): self.config = Config.from_json(cfg) @abstractmethod def load_data(self): pass @abstractmethod def build(self): pass @abstractmethod def train(self): pass @abstractmethod def evaluate(self): pass Tất cả các phương thức trong class này đều chưa được chi tiết các làm việc (không có body), chúng ta mới chỉ khai báo chúng.\nTiếp theo, với mỗi một giải pháp đưa ra để thử nghiệm, chúng ta sẽ viết chúng thành một Class riêng, kế thừa lại abstract class này. Các inheritance class (Child Class) sẽ chi tiết cách làm việc của các phương thức trong abstract class (Parent Class). Tất nhiên, Child Class cũng sẽ có thêm các phương thức của riêng nó.\nVí dụ dưới đây, Class Unet sẽ kế thừa Class BaseModel:\nclass UNet(BaseModel): def __init__(self, config): super().__init__(config) self.base_model = tf.keras.applications.MobileNetV2(input_shape=self.config.model.input, include_top=False) . . . def load_data(self): self.dataset, self.info = DataLoader().load_data(self.config.data ) self._preprocess_data() . . . def build(self): . . . self.model = tf.keras.Model(inputs=inputs, outputs=x) def train(self): self.model.compile(optimizer=self.config.train.optimizer.type, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=self.config.train.metrics) model_history = self.model.fit(self.train_dataset, epochs=self.epoches, steps_per_epoch=self.steps_per_epoch, validation_steps=self.validation_steps, validation_data=self.test_dataset) return model_history.history[\u0026#39;loss\u0026#39;], model_history.history[\u0026#39;val_loss\u0026#39;] def evaluate(self): predictions = [] for image, mask in self.dataset.take(1): predictions.append(self.model.predict(image)) return predictions Chú ý là ở trong Child Class, bên trong hàm tạo init, chúng ta phải gọi lệnh super().init() để thực hiện hàm tạo của Parent Class (super() là đại diện của Parent Class).\nViệc có nhiều Child Classes kế thừa cùng một Parent Class, mỗi Child Class lại thực hiện các phương thức của Parent Class theo các cách khác nhau, chính là tính đa hình của OOP.\n2.3 Instance Mothod, Class Method và Static Method\nInstancemethod, classmethod và staticmethod cũng là các đặc tính thú vị của OOP. Hãy tìm hiểu thêm về nó một chút. Hãy xem xét ví dụ sau để hiểu rõ hơn về các loại phương thức này:\n\u0026gt;\u0026gt;\u0026gt; class Dataset(): ... def im_load_data(self, x): ... print(\u0026#34;executing im_load_data(%s, %s)\u0026#34; % (self, x)) ... @classmethod ... def cm_load_data(cls, x): ... print(\u0026#34;executing cm_load_data(%s, %s)\u0026#34; % (cls, x)) ... @staticmethod ... def sm_load_data(x): ... print(\u0026#34;executing sm_load_data(%s)\u0026#34; % x) ... \u0026gt;\u0026gt;\u0026gt; dataset = Dataset() Dưới đây là cách đơn giản nhất để thực thi một phương thức:\n\u0026gt;\u0026gt;\u0026gt; dataset.im_load_data(\u0026#39;args\u0026#39;) executing im_load_data(\u0026lt;__main__.Dataset object at 0x7ff97a1bd280\u0026gt;, args) Đây là một Instance Method, là phương thức phổ biến nhất. Một đối tượng (instance của class) được ngầm truyền thành tham số thứ nhất (self) của phương thức này. Vì vậy, mặc dù im_load_data cần hai tham số, nhưng dataset.im_load_data chỉ cần một tham số thôi. Và dataset.im_load_data không còn là hàm nguyên gốc ban đầu mà là một phiên bản đã được \u0026ldquo;bind\u0026rdquo; cho dataset:\n\u0026gt;\u0026gt;\u0026gt; dataset.im_load_data \u0026lt;bound method Dataset.im_load_data of \u0026lt;__main__.Dataset object at 0x7ff97a1bd280\u0026gt;\u0026gt; Class Method là phương thức thuộc về cả Class. Khi thực thi, nó không dùng đến bất cứ một Instance nào của Class đó. Thay vào đó, cả Class sẽ được truyền thành tham số thứ nhất (cls) của phương thức này, tương tự như Instance Class.\n\u0026gt;\u0026gt;\u0026gt; Dataset.cm_load_data(\u0026#39;args\u0026#39;) executing cm_load_data(\u0026lt;class \u0026#39;__main__.Dataset\u0026#39;\u0026gt;, args) \u0026gt;\u0026gt;\u0026gt; Dataset.cm_load_data \u0026lt;bound method Dataset.cm_load_data of \u0026lt;class \u0026#39;__main__.Dataset\u0026#39;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; dataset.cm_load_data \u0026lt;bound method Dataset.cm_load_data of \u0026lt;class \u0026#39;__main__.Dataset\u0026#39;\u0026gt;\u0026gt; Một điều thú vị là Class Method cũng có thể gọi từ instance mà không gặp trở ngại gì (nhiều ngôn ngữ vẫn cho làm điều này kèm theo vài warning).\n\u0026gt;\u0026gt;\u0026gt; dataset.cm_load_data(\u0026#39;args\u0026#39;) executing cm_load_data(\u0026lt;class \u0026#39;__main__.Dataset\u0026#39;\u0026gt;, args) Static Method là một phương thức đặc biệt, nó không sử dụng bất cứ thứ gì liên quan đến Class hay Instance của Class đó. Cả Self hay Cls đều không xuất hiện trong tham số của loại phương thức này. Và Static Method hoạt động không khác gì một hàm thông thường.\n\u0026gt;\u0026gt;\u0026gt; dataset.sm_load_data(\u0026#39;args\u0026#39;) executing sm_load_data(args) \u0026gt;\u0026gt;\u0026gt; Dataset.sm_load_data(\u0026#39;args\u0026#39;) executing sm_load_data(args) Đối với Static Method, dù gọi dataset.sm_load_data hay Dataset.sm_load_data thì kết quả trả về vẫn là hàm ban đầu không hề được \u0026ldquo;bind\u0026rdquo; bất cứ một đối tượng nào:\n\u0026gt;\u0026gt;\u0026gt; dataset.sm_load_data \u0026lt;function Dataset.sm_load_data at 0x7ff97a093160\u0026gt; \u0026gt;\u0026gt;\u0026gt; Dataset.sm_load_data \u0026lt;function Dataset.sm_load_data at 0x7ff97a093160\u0026gt; Static method, với sự đặc biệt của nó, được dùng rất hạn chế. Bởi vì nó không giống như instance method hay class method, nó không có bất cứ sự liên quan tới đối tượng gọi nó. Static method không phải là phương thức được sử dụng thường xuyên.\nTuy nhiên, nó vẫn tồn tại là có lý do của nó. Static method thường dùng để nhóm các hàm tiện ích lại với nhau (trong cùng một class). Nó không yêu cầu gì từ class đó, ngoại trừ việc tham chiếu khi được gọi.\nNhưng hàm như vậy không cho vào class nào cũng không vấn đề gì. Nhưng nhóm chúng trong class và gọi chúng thông quan instance hoặc class sẽ giúp chúng ta hiểu hơn về bối cảnh cũng như chức năng của chúng.\n3. Documentation\nHay được hiểu là code phải có comments. Tác dụng của việc Comments code là không phải bàn cãi. Có 2 cách Comment code:\n Comment bằng những lời giải thích tường mình Comment bằng chính cách đặt tên hàm, tên biến, \u0026hellip;  Hãy xem một ví dụ về Code không có Comment:\ndef n(self, ii, im): ii = tf.cast(ii, tf.float32) / 255.0 im -= 1 return ii, im Đọc đoạn code trên, bạn có thấy khó hiểu không?\nBây giờ hãy thêm Comment vào cho nó:\ndef _normalize(self, input_image, input_mask): \u0026#34;\u0026#34;\u0026#34; Normalise input image Args: input_image (tf.image): The input image input_mask (int): The image mask Returns: input_image (tf.image): The normalized input image input_mask (int): The new image mask \u0026#34;\u0026#34;\u0026#34; input_image = tf.cast(input_image, tf.float32) / 255.0 input_mask -= 1 return input_image, input_mask Vẫn cùng 1 đoạn Code, nhưng sau khi thêm Comment, người đọc dễ dàng hiểu được mục đích của nó.\nCách Comment như trên trong Python gọi là docstrings. Chúng ta luôn luôn nên thêm Docstrings trước mỗi File, hàm, Module để chỉ ra mục đích của chúng. Có nhiều cách để định dạng Docstrings. Cách như trong ví dụ này là Style của Google, trong đó:\n Dòng đầu tiên chỉ ra mục đích của đoạn code Tiếp theo là giải thích các tham số truyền vào Cuối cùng là giải thích kết quả trả về là gì?  Hai phần sau có thể bỏ qua nếu chúng không cung cấp nhiều giá trị.\n4. Kết luận\nTrong bài này, chúng ta đã khám phá một vài Best Practices khi Code cho dự án về AI, từ việc tổ chức dự án, sử dụng OOP, đến việc Comment trong Code. Theo cách đó, Code của chúng ta sẽ trở nên ràng mạch, rõ ràng, dễ đọc hiểu, dễ bảo trì, mở rộng và hạn chế phát sinh các lỗi tiềm ẩn trong quá trình vận hành.\nCác bạn có thể tham khảo cấu trúc dự án mẫu tại đây.\nTrong bài tiếp theo, chúng ta sẽ cùng thảo luận về cách thức xây dựng và tổ chức Source Code trong các dự án về AI. Mời các bạn đón đọc.\n5. Tham khảo\n Theaisummer Learnpython Realpython Datacamp  ","permalink":"https://tiensu.github.io/blog/56_ai_code_structure/","tags":["MLOps"],"title":"Tổ chức code trong dự án AI"},{"categories":["Deep Learning","Siamese Network","One-shot Learning"],"contents":"Ngày nay, AI đã len lỏi vào mọi lĩnh vực của đời sống xã hội: y tế, giáo dục, giao thông, \u0026hellip; Có thể nói không ngoa rằng hầu như mọi bài toán AI đều có thể giải quyết được thông qua Neural Network (NN). Tuy nhiên, các NNs luôn đòi hỏi lượng lớn dữ liệu để huấn luyện chúng. Trong thực tế, có một số bài toán mà việc thu thập đủ dữ liệu là một nhiệm vụ bất khả thi, ví dụ như bài toán Face Recognition, Signature Verification, \u0026hellip; Siamese Network (SN) hay Siamese Neural Network (SNN) ra đời để giải quyết tốt hơn những bài toán dạng như thế này.\nSNN chỉ sử dụng một số lượng hình ảnh rất nhỏ (vài ảnh) để có được những dự đoán tốt hơn nhiều so với mạng NN truyền thống. Vì thế nó còn được gọi với 1 số cái tên như One-shot Learning, Few-shot Learning, \u0026hellip; Khả năng học hỏi từ rất ít dữ liệu đã khiến cho SNN trở nên phổ biến hơn trong những năm gần đây. Trong bài viết này, chúng ta sẽ tìm hiểu nó là gì và cách phát triển hệ thống Signature Verification với Pytorch bằng cách sử dụng SNN.\n1. Giới thiệu Siamese Neural Network\nSiamese Neural Network (SNN) là một kiến trúc mạng nơ-ron chứa hai hoặc nhiều mạng con giống hệt nhau. \u0026ldquo;Giống hệt nhau\u0026rdquo; ở đây có nghĩa là, chúng có cùng cấu hình với cùng thông số và trọng số. Việc cập nhật các thông số được phản ánh đồng thời trên cả hai mạng con của nó.\nSNN được sử dụng để tìm sự giống nhau của các dữ liệu đầu (Input Data) vào bằng cách so sánh các vectơ đặc trưng của chúng. Một số ứng dụng phổ biến của SNN có thể kể đến như là: Face Verification, Signature Verification, Image Seaching System, \u0026hellip;\nThông thường, một mạng nơ-ron học cách để dự đoán các lớp của một bài toán. Nếu muốn thêm hay bớt các lớp mới, chúng ta phải cập nhật (huấn luyện) lại mạng nơ-ron trên toàn bộ tập dữ liệu (cả dữ liệu mới và cũ). Ngoài ra, các mạng nơ-ron sâu cần một khối lượng lớn dữ liệu để có thể huấn luyện chúng. SNN, theo một cách khác, học cách tìm ra sự giống nhau giữa các Input Data. Vì vậy, nó cho phép chúng ta phân loại các lớp dữ liệu mới mà không cần huấn luyện lại mạng nơ-ron.  Luồng làm việc của SNN như sau:\n Chọn một cặp Input Data (trong phạm vi bài này là ảnh) được chọn từ dataset. Đưa mỗi ảnh qua mỗi Sub-network của SNN để xử lý. Output của các Sub-networks là một Embedding vector. Tính toán khoảng cách Euclidean giữa 2 Embedding vectors đó. Một Sigmoid Function có thể được áp trên khoảng cách để đưa ra giá trị Score trong đoạn [0,1], thể hiện mức độ giống nhau giữa 2 Embedding vectors. Score càng gần 1 thì 2 vectors càng giống nhau và ngược lại.   2. Ưu điểm của SNN\nSNN có một số ưu điểm nổi bật như sau:\n  Lượng dữ liệu cần thiết để huấn luyện SNN là rất ít. Chỉ cần vài Samples là đủ (1-5 samples) huấn luyện SNN. Phương pháp mà nó sử dụng ở đây là One-Shot Learning hoặc Few-Shot Learning. Chính vì cần ít dữ liệu huấn luyện như vậy nên chúng ta cũng không lo lắng việc dữ liệu bị mất cân bằng (Image Imbalance).\n  Khả năng kết hợp với các bộ phân loại khác cao. Do cơ chế học của SNN khác biệt với các bộ phân lớp thông thường khác, nên chúng ta hoàn toàn có thể kết hợp chúng lại với nhau. Việc làm này thường cho ra kết quả tốt hơn.\n  Học từ sự tương đồng về ngữ nghĩa: SNN tập trung vào việc học các Features ở các lớp sâu hơn, nơi mà các Features giống nhau được đặt gần nhau. Do đó, nó có thể hiểu được phần nào sự tương đồng về ngữ nghĩa của các Input Data.\n  3. Nhược điểm của SNN\nSNN cũng có những nhược điểm sau:\n  Thời gian huấn luyện lâu hơn. SNN học theo từng cặp đôi một với nhau nên khả năng học của nó chậm hơn các NN khác.\n  Không thể hiện xác suất mỗi lớp trong Output. SNN chỉ đưa đưa 1 giá trị Score trong đoạn [0,1], thể hiện sự giống nhau giữa 2 Input Data. Score càng gần 1 thì 2 Input Data càng giống nhau và ngược lại.\n  4. Loss Function của SNN\n Bởi vì, SNN học theo kiểu từng đôi một của Input Data nên Cross Entropy Loss Function thường không được sử dụng. Thay vào đó, 2 Loss Functions là Triple Loss và Contrastive Loss được sử dụng nhiều hơn.\n4.1 Triple Loss function\nÝ tưởng của Triple Loss là sử dụng bộ 3 Input Data bao gồm: Anchor (A), Positive (P) và Nagative (N) mà ở đó, khoảng cách từ A đến P được tối thiểu hóa, trong khi khoảng cách từ A đến N được tối đa hóa trong suốt quá trình huấn luyện model.\n$L(A,P,N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \\alpha,0)$\n Trong công thức trên,\n $\\alpha$ gọi là margin, được sử dụng để nhấn mạnh sự khác biệt hoặc sự tương đồng giữa các cặp Input Data. $f(A), f(P), f(N)$ là các vectors đặc trưng của các Input Data A, P, N, tương ứng.   4.2 Contrastive Loss\nÝ tưởng của Contrastive Loss cũng tương tự như Triplet Loss, sự khác nhau ở chỗ Contrastive Loss chỉ sử dụng 1 cặp Input Data, hoặc là cùng loại, hoặc là khác loại. Nếu cùng loại thì khoảng cách giữa các vectors đặc trưng của chúng sẽ được tối thiểu hóa, còn nếu khác loại thì khoảng cách giữa các vectors đặc trưng của chúng sẽ được tối đa hóa trong suốt quá trình huấn luyện.\nCông thức của Contrastive Loss: $(1 - Y)\\frac{1}{2}(D_w)^2 + (Y)\\frac{1}{2}{max(0,m - D_w)}^2$\n Trong đó, $D_w$ là khoảng cách Euclidean: $\\sqrt{{G_w(X_1) - G_w(X_2)}^2}$\n $G_w$ là Ouput của SNN đối với 1 Input Data.\n Việc lựa chọn sử dụng Loss Function nào còn tùy thuộc vào bài toán cụ thể. Chưa có công bố nào kết luận cái nào tốt hơn cái nào. Bạn nên thử cả 2 loại để tìm ra cái tốt hơn cho bài toán của bạn.\n5. Signature Verification với Siamese Networks\n Trong phần này, chúng ta sẽ xây dựng một SNN model bằng Pytorch để thực hiện nhiệm vụ Signature Verification.\n5.1 Signature dataset\nDataset sử dụng trong bài này là ICDAR 2011. Nó chứa các chữ ký của những người dân ở Hà Lan, cả chữ ký thật và chữ ký giả. Nhãn của dữ liệu nằm trong file CSV tương ứng.  Chúng ta sẽ đoc vào dataset và chuẩn bị cho việc huấn luyện model:\n#preprocessing and loading the dataset class SiameseDataset(): def __init__(self,training_csv=None,training_dir=None,transform=None): # used to prepare the labels and images path self.train_df=pd.read_csv(training_csv) self.train_df.columns =[\u0026#34;image1\u0026#34;,\u0026#34;image2\u0026#34;,\u0026#34;label\u0026#34;] self.train_dir = training_dir self.transform = transform def __getitem__(self,index): # getting the image path image1_path=os.path.join(self.train_dir,self.train_df.iat[index,0]) image2_path=os.path.join(self.train_dir,self.train_df.iat[index,1]) # Loading the image img1 = Image.open(image1_path) img2 = Image.open(image2_path) img1 = img0.convert(\u0026#34;L\u0026#34;) img2 = img1.convert(\u0026#34;L\u0026#34;) # Apply image transformations if self.transform is not None: img1 = self.transform(img1) img2 = self.transform(img2) return img1, img2 , th.from_numpy(np.array([int(self.train_df.iat[index,2])],dtype=np.float32)) def __len__(self): return len(self.train_df) # Load the the dataset from raw image folders siamese_dataset = SiameseDataset(training_csv,training_dir, transform=transforms.Compose([transforms.Resize((105,105)), transforms.ToTensor() ]) ) # Load the dataset as pytorch tensors using dataloader train_dataloader = DataLoader( siamese_dataset, shuffle=True, num_workers=8, batch_size=config.batch_size ) 5.2 SNN model\nChúng ta sẽ tạo SNN model như sau:\n#create a siamese network class SiameseNetwork(nn.Module): def __init__(self): super(SiameseNetwork, self).__init__() # Setting up the Sequential of CNN Layers self.cnn = nn.Sequential( nn.Conv2d(1, 96, kernel_size=11,stride=1), nn.ReLU(inplace=True), nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2), nn.MaxPool2d(3, stride=2), nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2), nn.ReLU(inplace=True), nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2), nn.MaxPool2d(3, stride=2), nn.Dropout2d(p=0.3), nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1), nn.ReLU(inplace=True), nn.Conv2d(384,256 , kernel_size=3,stride=1,padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(3, stride=2), nn.Dropout2d(p=0.3), ) # Defining the fully connected layers self.fc = nn.Sequential( nn.Linear(30976, 1024), nn.ReLU(inplace=True), nn.Dropout2d(p=0.5), nn.Linear(1024, 128), nn.ReLU(inplace=True), nn.Linear(128,2)) def forward_once(self, x): # Forward pass  output = self.cnn(x) output = output.view(output.size()[0], -1) output = self.fc(output) return output def forward(self, input1, input2): # forward pass of input 1 output1 = self.forward_once(input1) # forward pass of input 2 output2 = self.forward_once(input2) return output1, output2 5.3 Loss Function\nTrong bài này, mình sẽ sử dụng Contrastive Loss. Code của nó trong Pytorch như sau:\nclass ContrastiveLoss(torch.nn.Module): \u0026#34;\u0026#34;\u0026#34; Contrastive loss function. \u0026#34;\u0026#34;\u0026#34; def __init__(self, margin=1.0): super(ContrastiveLoss, self).__init__() self.margin = margin def forward(self, x0, x1, y): # euclidian distance diff = x0 - x1 dist_sq = torch.sum(torch.pow(diff, 2), 1) dist = torch.sqrt(dist_sq) mdist = self.margin - dist dist = torch.clamp(mdist, min=0.0) loss = y * dist_sq + (1 - y) * torch.pow(dist, 2) loss = torch.sum(loss) / 2.0 / x0.size()[0] return loss 5.4 Train SNN model\nCác bước tiến hành huấn luyện SNN model như sau:\n Khởi tạo model, Loss Funtion, và Optimizer (bài này sử dụng Adam) Đưa từng cặp Images vào SNN network. Tính toán Loss từ Ouput của mỗi ảnh. Tính toán Gradients của model theo phương pháp Back Propagate. Cập nhật các tham số của model, sử dụng Optimizer. Lưu lại model.  # Declare Siamese Network net = SiameseNetwork().cuda() # Decalre Loss Function criterion = ContrastiveLoss() # Declare Optimizer optimizer = th.optim.Adam(net.parameters(), lr=1e-3, weight_decay=0.0005) #train the model def train(): loss=[] counter=[] iteration_number = 0 for epoch in range(1,config.epochs): for i, data in enumerate(train_dataloader,0): img0, img1 , label = data img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda() optimizer.zero_grad() output1,output2 = net(img0,img1) loss_contrastive = criterion(output1,output2,label) loss_contrastive.backward() optimizer.step() print(\u0026#34;Epoch {}\\nCurrent loss {}\\n\u0026#34;.format(epoch,loss_contrastive.item())) iteration_number += 10 counter.append(iteration_number) loss.append(loss_contrastive.item()) show_plot(counter, loss) return net #set the device to cuda device = torch.device(\u0026#39;cuda\u0026#39; if th.cuda.is_available() else \u0026#39;cpu\u0026#39;) model = train() torch.save(model.state_dict(), \u0026#34;output/model.pt\u0026#34;) print(\u0026#34;Model Saved Successfully\u0026#34;) Kết quả huấn luyện sau 20 epochs:  Giá trị của Loss vẫn còn dao động, có lẽ chúng ta phải Tuning model nhiều hơn. Trong bài này, mình ko đi chi tiết phần đó.\n5.5 Test SNN model\nChúng ta sẽ thực hiện các bước sau để kiểm tra SNN model vừa mới huấn luyện:\n Đọc vào Test dataset, sử dụng lớp Dataloader của Pytorch Đưa cặp Image và nhãn tương ứng đi qua SNN model Tìm khoảng cách Euclidean giữa 2 Output Hiển thị kết quả  # Load the test dataset test_dataset = SiameseDataset(training_csv=testing_csv,training_dir=testing_dir, transform=transforms.Compose([transforms.Resize((105,105)), transforms.ToTensor() ]) ) test_dataloader = DataLoader(test_dataset,num_workers=6,batch_size=1,shuffle=True) #test the network count=0 for i, data in enumerate(test_dataloader,0): x0, x1 , label = data concat = torch.cat((x0,x1),0) output1,output2 = model(x0.to(device),x1.to(device)) eucledian_distance = F.pairwise_distance(output1, output2) if label==torch.FloatTensor([[0]]): label=\u0026#34;Original Pair Of Signature\u0026#34; else: label=\u0026#34;Forged Pair Of Signature\u0026#34; imshow(torchvision.utils.make_grid(concat)) print(\u0026#34;Predicted Eucledian Distance:-\u0026#34;,eucledian_distance.item()) print(\u0026#34;Actual Label:-\u0026#34;,label) count=count+1 if count ==10: break Kết quả:\nPredicted Eucledian Distance:- 1.2930774688720703 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.6725202798843384 Actual Label:- Original Pair Of Signature Predicted Eucledian Distance:- 0.8823959827423096 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.9346675276756287 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.25577670335769653 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.7937518358230591 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.7733522057533264 Actual Label:- Original Pair Of Signature Predicted Eucledian Distance:- 0.7810924649238586 Actual Label:- Original Pair Of Signature Predicted Eucledian Distance:- 1.2326889038085938 Actual Label:- Original Pair Of Signature Predicted Eucledian Distance:- 0.6290231347084045 Actual Label:- Original Pair Of Signature Một số hình ảnh:  6. Kết luận\nTrong bài này, chúng ta đã cùng tìm hiểu về Siamese Neural Network, đồng thời xây dựng một SNN đơn giản để giải quyết bài toán Signature Verification.\nToàn bộ source code của bài này, các bạn có thể tham khảo tại đây\nTrong bài tiếp theo, chúng ta sẽ cùng thảo luận về cách thức xây dựng và tổ chức Source Code trong các dự án về AI. Mời các bạn đón đọc.\n7. Tham khảo\n Towardsdatascience Hackernoon  ","permalink":"https://tiensu.github.io/blog/55_siamese_network/","tags":["Deep Learning","Siamese Network","One-shot Learning"],"title":"Tìm hiểu Siamese Neural Network"},{"categories":["Deep Learning","Face Recognition"],"contents":"1. Nhắc lại bài toán Face Recognition\nFace Recognition là bài toán nhận diện người dựa vào khuôn mặt của họ trong hình ảnh hoặc video. Hai trong số các bài toán của Face Recognition là:\n Face Verification: Ánh xạ 1-1, giữa khuôn mặt của một người đưa vào hệ thống nhận diện với một người đã biết trước. Face Verification trả lời câu hỏi: Đây có phải là anh/chị/ông/bà A không? Face Identification: Ánh xạ 1-nhiều, giữa giữa khuôn mặt của một người đưa vào hệ thống nhận diện với một tập những người đã biết trước trong CSDL. Face Identification trả lời câu hỏi: Đây là ai?  Trong bài này, chúng ta sẽ sử dụng FaceNet model để thực hiện bài toán Face Recognition.\n2. FaceNet model.\nFaceNet là một mô hình nhận dạng khuôn mặt được phát triển bởi Florian Schroff và đồng nghiệp tại Google trong bài báo năm 2015 của họ có tiêu đề FaceNet: A Unified Embedding for Face Recognition and Clustering. Trong mô hình này, hình ảnh của một khuôn mặt sẽ được trích xuất các đặc điểm chất lượng cao và biểu diễn thành vector 128 phần tử (Face Embedding vector 128 chiều).\nFaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity — FaceNet: A Unified Embedding for Face Recognition and Clustering, 2015.\nMô hình là một mạng CNN được đào tạo thông qua hàm Triplet Loss. Nó khuyến khích các Face Embedding vector của cùng một người trở nên giống nhau hơn (khoảng cách nhỏ hơn), trong khi các Face Embedding vectơ của những người khác nhau sẽ trở nên ít giống nhau hơn (khoảng cách lớn hơn). Việc tập trung vào đào tạo một mô hình để tạo ra các Face Embedding vector trực tiếp (thay vì trích xuất chúng từ một lớp trung gian của mô hình) là một đổi mới quan trọng của FaceNet so với VGGFace.\nOur method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. — FaceNet: A Unified Embedding for Face Recognition and Clustering, 2015.\nCác Face Embedding vectors này sau đó được sử dụng như là Input để đào tạo một mô hình phân loại trên bộ dữ liệu tiêu chuẩn về Face Recognition, đạt được kết quả state-of-the-art.\nBài báo cũng đề cập đến các ứng dụng khác của Face Embedding vector, chẳng hạn như phân cụm để nhóm các khuôn mặt giống nhau dựa trên các đặc điểm được trích xuất của chúng. Có thể nó FaceNet là một model rất mạnh mẽ và hiệu quả cho bài toán Face Recognition, và một loạt các ứng dụng khác.\n3. Load a FaceNet Model in Keras\nCó một số dự án cung cấp các công cụ để huấn luyện các mô hình dựa trên FaceNet (sử dụng Pre-trained FaceNet model). Có lẽ nổi bật nhất là dự án OpenFace. Trong dự án náy, các mô hình FaceNet được xây dựng và huấn luyện bằng PyTorch framework. Keras cũng có 1 dự án tương tự, gọi là Keras OpenFace, nhưng tại thời điểm viết bài, các mô hình đó yêu cầu Python 2, điều này hạn chế chúng ta rất nhiều trong việc tiếp cận các tính ưng ưu việt của Python 3. Vì thế mà Keras OpenFace ít được sử dụng.\nMột dự án nổi bật khác là facenet của David Sandberg. Nó cung cấp các mô hình FaceNet được xây dựng và huấn luyện bằng TensorFlow framework. Dự án có vẻ hoàn thiện, mặc dù tại thời điểm viết bài này không cung cấp cài đặt cũng như cách sử dụng API của nó. Một dự án có vẻ hữu ích hơn là Keras FaceNet của Hiroki Taniai. Trong dự án này, tác giả cung cấp một tập lệnh để chuyển đổi mô hình Inception ResNet v1 từ TensorFlow sang Keras. Một Pre-trained model bằng Keras cũng được cung cấp để sẵn sàng để sử dụng. Chúng ta sẽ sử dụng mô hình Keras FaceNet của Hiroki Taniai trong bài viết này. Nó đã được huấn luyện trên tập dữ liệu MS-Celeb-1M. Mô hình có thể được tải xuống từ đây.\n4. Detect Faces\nPhần này, chúng ta cũng sử dụng thư viện MTCNN tương tự như trong bài trước. Code như sau:\n# function for face detection with mtcnn from PIL import Image from numpy import asarray from mtcnn.mtcnn import MTCNN # extract a single face from a given photograph def extract_face(filename, required_size=(160, 160)): # load image from file image = Image.open(filename) # convert to RGB, if needed image = image.convert(\u0026#39;RGB\u0026#39;) # convert to array pixels = asarray(image) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] # bug fix x1, y1 = abs(x1), abs(y1) x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # load the photo and extract the face pixels = extract_face(\u0026#39;...\u0026#39;) Chúng ta sẽ sử dụng hàm này để trích xuất các Face Embedding vector để cung cấp làm đầu vào cho mô hình FaceNet trong phần tiếp theo.\n5. Face Recognition\nTrong phần này, chúng ta sẽ phát triển một hệ thống Face Recongtion để dự đoán danh tính của một khuôn mặt. Model sẽ được huấn luyện và kiểm tra bằng cách sử dụng bộ dữ liệu 5 Celebrity Faces Dataset, bao gồm rất nhiều bức ảnh của năm người nổi tiếng khác nhau. Mô hình MTCNN vẫn được dùng để thực hiện Face Detection, mô hình FaceNet được sử dụng để Face Embedding vector cho mỗi khuôn mặt được phát hiện, sau đó chúng ta sẽ phát triển một mô hình phân loại bằng thuật toán SVM để dự đoán danh tính của khuôn mặt đó.\n5.1 5 Celebrity Faces Dataset\n5 Celebrity Faces Dataset là một bộ dữ liệu nhỏ chứa các bức ảnh của những người nổi tiếng khác nhau. Nó bao gồm các bức ảnh của: Ben Affleck, Elton John, Jerry Seinfeld, Madonna và **. Bộ dữ liệu đã được cung cấp bởi Dan Becker, bạn có thể tải xuống miễn phí từ Kaggle. Lưu ý, cần phải có tài khoản Kaggle để tải xuống tập dữ liệu này.\nTải xuống tập dữ liệu, file data.zip 2.5MB và giải nén nó trong thư mục làm việc trên máy tính của bạn với tên thư mục 5-Celeb-face-dataset. Bây giờ bạn sẽ có một thư mục với cấu trúc sau:\n5-celebrity-faces-dataset ├── train │ ├── ben_afflek │ ├── elton_john │ ├── jerry_seinfeld │ ├── madonna │ └── mindy_kaling └── val ├── ben_afflek ├── elton_john ├── jerry_seinfeld ├── madonna └── mindy_kaling Chúng ta có thể thấy 2 thư mục: thư mục train chứa dữ liệu để huấn luyện model và thư mục val chứa dữ liệu để xác thực hoặc kiểm tra model. Nhìn vào một số bức ảnh trong mỗi thư mục, chúng ta có thể thấy rằng các khuôn mặt có nhiều hướng, ánh sáng và kích thước khác nhau. Điều quan trọng là mỗi bức ảnh chỉ có một khuôn mặt của người đó. Nếu bạn muốn sử dụng dữ liệu của riêng bạn, hãy thu thập và tổ chức dữ liệu tương tự như thế này.\n5.2 Detect Faces\nBước đầu tiên cần làm là phát hiện khuôn mặt trong mỗi bức ảnh. Ta sẽ sử dụng hàm extract_face() trong phần trước để làm việc này.\n# python extract_faces.py --dp \u0026#39;5-celebrity-faces-dataset/train/ben_afflek/\u0026#39; # demonstrate face detection on 5 Celebrity Faces Dataset from os import listdir from PIL import Image from numpy import asarray from matplotlib import pyplot from mtcnn.mtcnn import MTCNN import argparse import tensorflow as tf config_tf = tf.compat.v1.ConfigProto() config_tf.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config_tf) # extract a single face from a given photograph def extract_face(filename, required_size=(160, 160)): # load image from file image = Image.open(filename) # convert to RGB, if needed image = image.convert(\u0026#39;RGB\u0026#39;) # convert to array pixels = asarray(image) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] # bug fix x1, y1 = abs(x1), abs(y1) x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array def main(): ap = argparse.ArgumentParser() ap.add_argument(\u0026#39;-dp\u0026#39;, \u0026#39;--data_path\u0026#39;, required=True) args = vars(ap.parse_args()) i = 1 # enumerate files for filename in listdir(args[\u0026#39;data_path\u0026#39;]): # path path = args[\u0026#39;data_path\u0026#39;] + filename # get face face = extract_face(path) print(i, face.shape) # plot pyplot.subplot(2, 7, i) pyplot.axis(\u0026#39;off\u0026#39;) pyplot.imshow(face) i += 1 pyplot.show() if __name__ == \u0026#39;__main__\u0026#39;: main() Thử thực hiện với những ảnh của Ben Affleck và thể hiện các khuôn mặt được phát hiện lên đồ thị.\n$ python extract_faces.py --data_path \u0026#39;5-celebrity-faces-dataset/train/ben_afflek/\u0026#39; Kết quả:  Tiếp theo, chúng ta sẽ mở rộng ví dụ này để làm việc với toàn bộ ảnh trong các thư mục train và val. Ta viết hàm load_faces() như sau:\n# load images and extract faces for all images in a directory def load_faces(directory): faces = list() # enumerate files for filename in listdir(directory): # path path = directory + filename # get face face = extract_face(path) # store faces.append(face) return faces Tiếp theo là hàm load_dataset():\n# load a dataset that contains one subdir for each class that in turn contains images def load_dataset(directory): X, y = list(), list() # enumerate folders, on per class for subdir in listdir(directory): # path path = directory + subdir + \u0026#39;/\u0026#39; # skip any files that might be in the dir if not isdir(path): continue # load all faces in the subdirectory faces = load_faces(path) # create labels labels = [subdir for _ in range(len(faces))] # summarize progress print(\u0026#39;\u0026gt;loaded %dexamples for class: %s\u0026#39; % (len(faces), subdir)) # store X.extend(faces) y.extend(labels) return asarray(X), asarray(y) Việc còn lại là gọi hàm này với các thư mục train và val để tạo ra các Face Embedding của mỗi khuôn mặt và sử dụng chúng để tạo model phân loại.\nGộp tất cả lại, ta có code đầy đủ như sau:\n# USEAGE python extract_faces_dataset.py --train_data 5-celebrity-faces-dataset/train/ --val_data 5-celebrity-faces-dataset/val/ --save_data 5-celebrity-faces-dataset.npz # face detection for the 5 Celebrity Faces Dataset from os import listdir from os.path import isdir from PIL import Image from numpy import savez_compressed from numpy import asarray from mtcnn.mtcnn import MTCNN import argparse import tensorflow as tf config_tf = tf.compat.v1.ConfigProto() config_tf.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config_tf) # extract a single face from a given photograph def extract_face(filename, required_size=(160, 160)): # load image from file image = Image.open(filename) # convert to RGB, if needed image = image.convert(\u0026#39;RGB\u0026#39;) # convert to array pixels = asarray(image) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] # bug fix x1, y1 = abs(x1), abs(y1) x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # load images and extract faces for all images in a directory def load_faces(directory): faces = list() # enumerate files for filename in listdir(directory): # path path = directory + filename # get face face = extract_face(path) # store faces.append(face) return faces # load a dataset that contains one subdir for each class that in turn contains images def load_dataset(directory): X, y = list(), list() # enumerate folders, on per class for subdir in listdir(directory): # path path = directory + subdir + \u0026#39;/\u0026#39; # skip any files that might be in the dir if not isdir(path): continue # load all faces in the subdirectory faces = load_faces(path) # create labels labels = [subdir for _ in range(len(faces))] # summarize progress print(\u0026#39;\u0026gt;loaded %dexamples for class: %s\u0026#39; % (len(faces), subdir)) # store X.extend(faces) y.extend(labels) return asarray(X), asarray(y) def main(): ap = argparse.ArgumentParser() ap.add_argument(\u0026#39;-td\u0026#39;, \u0026#39;--train_data\u0026#39;, required=True) ap.add_argument(\u0026#39;-vd\u0026#39;, \u0026#39;--val_data\u0026#39;, required=True) ap.add_argument(\u0026#39;-sd\u0026#39;, \u0026#39;--save_data\u0026#39;, required=True) args = vars(ap.parse_args()) # load train dataset trainX, trainy = load_dataset(args[\u0026#39;train_data\u0026#39;]) print(trainX.shape, trainy.shape) # load test dataset testX, testy = load_dataset(args[\u0026#39;val_data\u0026#39;]) # save arrays to one file in compressed format savez_compressed(args[\u0026#39;save_data\u0026#39;], trainX, trainy, testX, testy) if __name__ == \u0026#39;__main__\u0026#39;: main() Chạy code trên:\n$ python extract_faces_dataset.py --train_data 5-celebrity-faces-dataset/train/ --val_data 5-celebrity-faces-dataset/val/ --save_data 5-celebrity-faces-dataset.npz Sẽ mất một chút thời gian để hoàn thành chương trình. Tất cả các khuôn mặt ở 2 tập train và test được lưu vào một tệp mảng NumPy nén có tên là 5-Celeb-face-dataset.npz có dung lượng khoảng 7.5MB và được lưu trữ trong thư mục làm việc hiện tại của chương trình.\n\u0026gt;loaded 17 examples for class: elton_john \u0026gt;loaded 19 examples for class: madonna \u0026gt;loaded 13 examples for class: ben_afflek \u0026gt;loaded 22 examples for class: mindy_kaling \u0026gt;loaded 21 examples for class: jerry_seinfeld (92, 160, 160, 3) (92,) \u0026gt;loaded 5 examples for class: elton_john \u0026gt;loaded 5 examples for class: madonna \u0026gt;loaded 5 examples for class: ben_afflek \u0026gt;loaded 5 examples for class: mindy_kaling \u0026gt;loaded 5 examples for class: jerry_seinfeld 5.3 Tạo Face Embedding vectors\nBước tiếp theo là tạo ra các Face Embedding vectors của mỗi khuôn mặt. Nhớ lại rằng Face Embedding là một vectơ đại diện cho các đặc điểm được trích xuất từ khuôn mặt. Các vectors này sau đó có thể được dùng để tính toán, so sánh với nhau, hoặc tạo ra một bộ phân loại để định danh cho từng khuôn mặt. Ở phần này, chúng ta sẽ dùng mô hình Pre-trained FaceNet để tạo ra các Face Embdding vectors đó.\nĐầu tiên, chúng ta sẽ đọc vào các khuôn mặt đã tạo ở bước trước.\n... # load the face dataset data = load(\u0026#39;5-celebrity-faces-dataset.npz\u0026#39;) trainX, trainy, testX, testy = data[\u0026#39;arr_0\u0026#39;], data[\u0026#39;arr_1\u0026#39;], data[\u0026#39;arr_2\u0026#39;], data[\u0026#39;arr_3\u0026#39;] print(\u0026#39;Loaded: \u0026#39;, trainX.shape, trainy.shape, testX.shape, testy.shape) Tiếp theo, đọc vào Pre-trained FaceNet model:\n... # load the facenet model model = load_model(\u0026#39;facenet_keras.h5\u0026#39;) print(\u0026#39;Loaded Model\u0026#39;) Chuẩn hóa dữ liệu đầu vào theo đúng kỳ vọng của Pre-trained FaceNet:\n... # scale pixel values face_pixels = face_pixels.astype(\u0026#39;float32\u0026#39;) # standardize pixel values across channels (global) mean, std = face_pixels.mean(), face_pixels.std() face_pixels = (face_pixels - mean) / std ... # transform face into one sample samples = expand_dims(face_pixels, axis=0) Tạo Face Embedding vectors:\n... # make prediction to get embedding yhat = model.predict(samples) # get embedding embedding = yhat[0] Viết chung lại thành hàm get_embedding() như bên dưới:\n# get the face embedding for one face def get_embedding(model, face_pixels): # scale pixel values face_pixels = face_pixels.astype(\u0026#39;float32\u0026#39;) # standardize pixel values across channels (global) mean, std = face_pixels.mean(), face_pixels.std() face_pixels = (face_pixels - mean) / std # transform face into one sample samples = expand_dims(face_pixels, axis=0) # make prediction to get embedding yhat = model.predict(samples) return yhat[0] Code đầy đủ từ lúc đọc vào ảnh khuôn mặt, đến khi lưu Face Embedding vector thành 1 file như sau:\n# USEAGE: # python predict_face_embeddings.py --face_dataset 5-celebrity-faces-dataset.npz --facenet_model facenet_keras.h5 --face_embedding 5-celebrity-faces-embeddings.npz # calculate a face embedding for each face in the dataset using facenet from numpy import load from numpy import expand_dims from numpy import asarray from numpy import savez_compressed from tensorflow.keras.models import load_model import argparse import tensorflow as tf tf.get_logger().setLevel(\u0026#39;ERROR\u0026#39;) config_tf = tf.compat.v1.ConfigProto() config_tf.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config_tf) # get the face embedding for one face def get_embedding(model, face_pixels): # scale pixel values face_pixels = face_pixels.astype(\u0026#39;float32\u0026#39;) # standardize pixel values across channels (global) mean, std = face_pixels.mean(), face_pixels.std() face_pixels = (face_pixels - mean) / std # transform face into one sample samples = expand_dims(face_pixels, axis=0) # make prediction to get embedding yhat = model.predict(samples) return yhat[0] def main(): # load the face dataset data = load(args[\u0026#39;face_dataset\u0026#39;]) trainX, trainy, testX, testy = data[\u0026#39;arr_0\u0026#39;], data[\u0026#39;arr_1\u0026#39;], data[\u0026#39;arr_2\u0026#39;], data[\u0026#39;arr_3\u0026#39;] print(\u0026#39;Loaded: \u0026#39;, trainX.shape, trainy.shape, testX.shape, testy.shape) # load the facenet model model = load_model(args[\u0026#39;facenet_model\u0026#39;]) print(\u0026#39;Loaded Model\u0026#39;) # convert each face in the train set to an embedding newTrainX = list() for face_pixels in trainX: embedding = get_embedding(model, face_pixels) newTrainX.append(embedding) newTrainX = asarray(newTrainX) print(newTrainX.shape) # convert each face in the test set to an embedding newTestX = list() for face_pixels in testX: embedding = get_embedding(model, face_pixels) newTestX.append(embedding) newTestX = asarray(newTestX) print(newTestX.shape) # save arrays to one file in compressed format savez_compressed(args[\u0026#39;face_embedding\u0026#39;], newTrainX, trainy, newTestX, testy) if __name__ == \u0026#39;__main__\u0026#39;: ap = argparse.ArgumentParser() ap.add_argument(\u0026#39;-fd\u0026#39;, \u0026#39;--face_dataset\u0026#39;, required=True) ap.add_argument(\u0026#39;-fnm\u0026#39;, \u0026#39;--facenet_model\u0026#39;, required=True) ap.add_argument(\u0026#39;-fe\u0026#39;, \u0026#39;--face_embedding\u0026#39;, required=True) args = vars(ap.parse_args()) main() Chạy code trên:\n$ python predict_face_embeddings.py --face_dataset 5-celebrity-faces-dataset.npz --facenet_model facenet_keras.h5 --face_embedding 5-celebrity-faces-embeddings.npz Kết quả, toàn bộ ảnh khuôn mặt đã được chuyển đổi thành các Face Embedding vectors 128 chiều. Tất cả được lưu lại thành file 5-celebrity-faces-embeddings.npz trong cùng thư mục làm việc.\nLoaded: (93, 160, 160, 3) (93,) (25, 160, 160, 3) (25,) Loaded Model (93, 128) (25, 128) 5.4 Thực hiện Face Classification\nTrong phần này, chúng ta sẽ phát triển một mô hình để phân loại các Face Embedding vectors thành 5 nhãn là tên của những người nổi tiếng trong bộ dữ liệu 5 Celebrity Faces Dataset. Đầu tiên, đọc vào Face Embedding vectors ở phần trước:\n... # load dataset data = load(\u0026#39;5-celebrity-faces-embeddings.npz\u0026#39;) trainX, trainy, testX, testy = data[\u0026#39;arr_0\u0026#39;], data[\u0026#39;arr_1\u0026#39;], data[\u0026#39;arr_2\u0026#39;], data[\u0026#39;arr_3\u0026#39;] print(\u0026#39;Dataset: train=%d, test=%d\u0026#39; % (trainX.shape[0], testX.shape[0])) Để train Classifier model, chúng ta cần chuẩn hóa dữ liệu:\n... # normalize input vectors in_encoder = Normalizer(norm=\u0026#39;l2\u0026#39;) trainX = in_encoder.transform(trainX) testX = in_encoder.transform(testX) .. # label encode targets out_encoder = LabelEncoder() out_encoder.fit(trainy) trainy = out_encoder.transform(trainy) testy = out_encoder.transform(testy) Tiếp theo, chúng ta sẽ tạo và huấn luyện một SVM model. Thuật toán này đã được chứng mình tính hiệu quả trong việc phân loại các Face Embedding vectors.\n... # fit model model = SVC(kernel=\u0026#39;linear\u0026#39;) model.fit(trainX, trainy) ... # predict yhat_train = model.predict(trainX) yhat_test = model.predict(testX) # score score_train = accuracy_score(trainy, yhat_train) score_test = accuracy_score(testy, yhat_test) # summarize print(\u0026#39;Accuracy: train=%.3f, test=%.3f\u0026#39; % (score_train*100, score_test*100)) Sau khi huấn luyện xong model, ta sẽ thử dự đoán một người ngẫu nhiên trong tập Test.\n... # test model on a random example from the test dataset selection = choice([i for i in range(testX.shape[0])]) random_face_pixels = testX_faces[selection] random_face_emb = testX[selection] random_face_class = testy[selection] random_face_name = out_encoder.inverse_transform([random_face_class]) ... # prediction for the face samples = expand_dims(random_face_emb, axis=0) yhat_class = model.predict(samples) yhat_prob = model.predict_proba(samples) ... # get name class_index = yhat_class[0] class_probability = yhat_prob[0,class_index] * 100 predict_names = out_encoder.inverse_transform(yhat_class) ... print(\u0026#39;Predicted: %s(%.3f)\u0026#39; % (predict_names[0], class_probability)) print(\u0026#39;Expected: %s\u0026#39; % random_face_name[0]) ... # plot for fun pyplot.imshow(random_face_pixels) title = \u0026#39;%s(%.3f)\u0026#39; % (predict_names[0], class_probability) pyplot.title(title) pyplot.show() Tổng hơp hết lại, ta có code đầy đủ như sau:\n# USEAGE: # python random_face_identity_classification.py --face_dataset 5-celebrity-faces-dataset.npz --face_embedding 5-celebrity-faces-dataset.npz # develop a classifier for the 5 Celebrity Faces Dataset from random import choice from numpy import load from numpy import expand_dims from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import Normalizer from sklearn.svm import SVC from matplotlib import pyplot import argparse def main(): ap = argparse.ArgumentParser() ap.add_argument(\u0026#39;-fd\u0026#39;, \u0026#39;--face_dataset\u0026#39;, required=True) ap.add_argument(\u0026#39;-fe\u0026#39;, \u0026#39;--face_embedding\u0026#39;, required=True) args = vars(ap.parse_args()) # load faces data = load(args[\u0026#39;face_dataset\u0026#39;]) testX_faces = data[\u0026#39;arr_2\u0026#39;] # load face embeddings data = load(args[\u0026#39;face_embedding\u0026#39;]) trainX, trainy, testX, testy = data[\u0026#39;arr_0\u0026#39;], data[\u0026#39;arr_1\u0026#39;], data[\u0026#39;arr_2\u0026#39;], data[\u0026#39;arr_3\u0026#39;] print(\u0026#39;Dataset: train=%d, test=%d\u0026#39; % (trainX.shape[0], testX.shape[0])) # normalize input vectors in_encoder = Normalizer(norm=\u0026#39;l2\u0026#39;) trainX = in_encoder.transform(trainX) testX = in_encoder.transform(testX) # label encode targets out_encoder = LabelEncoder() out_encoder.fit(trainy) trainy = out_encoder.transform(trainy) testy = out_encoder.transform(testy) # fit model model = SVC(kernel=\u0026#39;linear\u0026#39;, probability=True) model.fit(trainX, trainy) # predict yhat_train = model.predict(trainX) yhat_test = model.predict(testX) # score score_train = accuracy_score(trainy, yhat_train) score_test = accuracy_score(testy, yhat_test) # summarize print(\u0026#39;Accuracy: train=%.3f, test=%.3f\u0026#39; % (score_train*100, score_test*100)) # test model on a random example from the test dataset selection = choice([i for i in range(testX.shape[0])]) random_face_pixels = testX_faces[selection] random_face_emb = testX[selection] random_face_class = testy[selection] random_face_name = out_encoder.inverse_transform([random_face_class]) # prediction for the face samples = expand_dims(random_face_emb, axis=0) yhat_class = model.predict(samples) yhat_prob = model.predict_proba(samples) # get name class_index = yhat_class[0] class_probability = yhat_prob[0,class_index] * 100 predict_names = out_encoder.inverse_transform(yhat_class) print(\u0026#39;Predicted: %s(%.3f)\u0026#39; % (predict_names[0], class_probability)) print(\u0026#39;Expected: %s\u0026#39; % random_face_name[0]) # plot for fun pyplot.imshow(random_face_pixels) title = \u0026#39;%s(%.3f)\u0026#39; % (predict_names[0], class_probability) pyplot.title(title) pyplot.show() if __name__ == \u0026#39;__main__\u0026#39;: main() Kết quả chạy code:\nDataset: train=93, test=25 Accuracy: train=100.000, test=100.000 Predicted: jerry_seinfeld (88.476) Expected: jerry_seinfeld Kèm theo hình ảnh của người được lựa chọn để dự đoán:  6. Kết luận\nTrong bài này, chúng ta đã khám phá cách phát triển hệ thống nhận diện khuôn mặt bằng mô hình FaceNet và bộ phân loại SVM. Cụ thể:\n Giới thiệu về mô hình FaceNet. Cách chuẩn bị dữ liệu để huấn luyện hệ thống. Cách huấn luyện mô hình và sử dụng mô hình đã huấn luyện để tạo dự đoán.  Toàn bộ source code của bài này, các bạn có thể tham khảo tại đây\nTrong bài tiếp theo, chúng ta sẽ cùng nhau tìm hiểu về mô hình Siamese. Mời các bạn đón đọc.\n7. Tham khảo\n Machinelearningmastery  ","permalink":"https://tiensu.github.io/blog/54_face_recognition_facenet/","tags":["Deep Learning","Face Recognition"],"title":"Thực hiện Face Recognition với FaceNet"},{"categories":["Deep Learning","Face Recognition"],"contents":"1. Nhắc lại bài toán Face Recognition\nFace Recognition là bài toán nhận diện người dựa vào khuôn mặt của họ trong hình ảnh hoặc video. Hai trong số các bài toán của Face Recognition là:\n Face Verification: Ánh xạ 1-1, giữa khuôn mặt của một người đưa vào hệ thống nhận diện với một người đã biết trước. Face Verification trả lời câu hỏi: Đây có phải là anh/chị/ông/bà A không? Face Identification: Ánh xạ 1-nhiều, giữa giữa khuôn mặt của một người đưa vào hệ thống nhận diện với một tập những người đã biết trước trong CSDL. Face Identification trả lời câu hỏi: Đây là ai?  Trong bài này, chúng ta sẽ sử dụng VGGFace2 model (đã được Trained trên tập dữ liệu MS-Celeb-1M) để thực hiện bài toán Face Recognition.\n2. VGGFace model.\nVGG model được tạo ra bởi các nhà khoa học trong nhóm Visual Geometry Group (VGG) thuộc trường đại học Oxford. Đến thời điểm hiện tại, nó có 2 phiên bản: VGGFace và VGGFace2.\n2.1 VGGFace\nVGGFace model được công bố vào năm 2015 trong bài báo Deep Face Recognition bởi Omkar Parkhi và đồng nghiệp. Đóng góp lớn nhất của bài báo là miêu tả cách thức thu thập một số lượng lớn dữ liệu để huấn luyện một CNN model (2.6M images, 2.6K people). Tập dữ liệu này sau đó được sử dụng làm cơ sở để phát triển các model CNN cho các nhiệm vụ Face Recognition như Face Identification và Face Verification. Cụ thể, các models được đào tạo trên tập dữ liệu rất lớn, sau đó được định giá trên tập dữ liệu nhận dạng khuôn mặt điểm chuẩn, chứng tỏ rằng mô hình có hiệu quả trong việc tạo ra các đặc điểm tổng quát từ khuôn mặt.\nBài báo cũng mô tả quá trình đào tạo một bộ phân loại khuôn mặt. Trước tiên sử dụng hàm kích hoạt Softmax trong lớp đầu của CNN model ra để phân loại khuôn mặt theo từng người. Lớp này sau đó được loại bỏ để đầu ra của mạng CNN là một biểu diễn đặc trưng Vector của khuôn mặt, được gọi là Face Embedding. Sau đó, model được đào tạo thêm, thông qua tinh chỉnh, để khoảng cách Euclid giữa các Vectos của cùng một người nhỏ nhất có thể và các Vectos của 2 người khác nhau lớn nhất có thể. Điều này đạt được bằng cách sử dụng Triplet Loss.\nMạng CNN ở đây sử dụng theo kiểu kiến trúc của VGG, với các khối lớp có kích thước Kernel nhỏ, hàm kích hoạt ReLU. Tiếp theo là các lớp Max Pooling và cuối cùng là các lớp Fully Connected làm nhiệm vụ phân loại tại phần cuối của mạng. Chính vì sử dụng kiến trúc của họ VGG nên model này có tên là VGGFace.\n2.2 VGGFace 2\nQiong Cao và đồng nghiệp đã cho ra đời VGGFace2 trong bài báo có tiêu đề VGGFace2: A Dataset For Recognizing Faces Across Pose And Age. Vẫn là ý tưởng sử dụng lượng lớn dữ liệu như VGGFace, nhưng kích thước dữ liệu của VGGFace2 lớn hơn rất nhiều. Cụ thể, có tổng số 3.31 triệu ảnh của 9131 người khác nhau, tức trung bình mỗi người có 362.6 ảnh trong bộ dataset của VGGFace2. Phần lớn những ảnh này được thu thập từ Google Image Search với sự đa dạng về giới tính, tuổi tác, màu da, tư thế, sắc tộc, điều kiện môi trường, \u0026hellip; Có một điều khác so với VGGFace, đó là VGGFace2 không sử dụng kiến trúc của họ VGG, thay vào đó, nó sử dụng ResNet-50 hoặc SqueezeNet-ResNet-50. Những models này đều được đánh giá trên tập dữ liệu chuẩn, và đều đạt được state-of-the-art. Tuy nhiên, có lẽ vì chung ý tưởng là sử dụng lượng dữ liệu lớn như VGGFace nên nó vẫn lấy cái tên VGGFace2.\n3. Cài đặt thư viện keras-vggface\nCác tác giả của VGGFace/VGFFace2 cung cấp mã nguồn cho các models của họ, cũng như các Pre-trained models mà chúng ta có thể tải xuống và sử dụng được. Các Pre-trained models này được viết bằng Caffe và PyTorch, không có cho TensorFlow hoặc Keras. Mặc dù vậy, chúng ta hoàn toàn có thể chuyển đổi từ Caffe/Pytorch sang Tensorflow/Keras một cách dễ dàng. Và thực tế là có khá nhiều người đã làm sẵn việc này cho chúng ta. Nổi bật trong số đó là dự án keras-vggface của tác giả Refik Can Malli1. Thư viện này có thể được cài đặt thông qua pip:\n$ pip install git+https://github.com/rcmalli/keras-vggface.git Nếu cài đặt thành công, bạn sẽ nhận được thông báo:\nSuccessfully installed keras-2.4.3 keras-vggface-0.6 pyyaml-5.4.1 Kiểm tra lại bằng cách import vào trong code python:\n# check version of keras_vggface import keras_vggface # print version print(keras_vggface.__version__) Output:\n0.6 4. Detect Faces\nTrước khi có thể thực hiện nhận dạng khuôn mặt, chúng ta cần phát hiện khuôn mặt. Đó là quá trình tự động định vị các khuôn mặt trong một bức ảnh và khoanh vùng chúng bằng cách vẽ một hộp giới hạn xung quanh phạm vi của chúng. Trong bài viêt này, chúng ta sẽ sử dụng Multi-Task Cascaded Convolutional Neural Network, để phát hiện khuôn mặt. Đây là một mô hình học sâu hiện đại để phát hiện khuôn mặt, được mô tả trong bài báo năm 2016 có tiêu đề Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.\nCài đặt MTCNN từ dự án ipazc/mtcnn của Ivan de Paz:\n$ pip install mtcnn Kiểm tra cài đặt:\n# confirm mtcnn was installed correctly import mtcnn # print version print(mtcnn.__version__) Kết quả thực thi:\n0.1.0 Chúng ta sẽ sử dụng thư viện mtcnn để tạo phát hiện khuôn mặt, sau đó trích xuất khuôn mặt bằng VGGFace model trong các phần tiếp theo.\nTrước tiên, mở một hình ảnh dưới dạng một mảng NumPy, sử dụng hàm Matplotlib imread():\n... # load image from file pixels = pyplot.imread(filename) Tiếp theo, gọi thư viện MTCNN và sử dụng nó để phát hiện tất cả các khuôn mặt trong ảnh đã mở:\n... # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) Kết quả là danh sách các hộp giới hạn khuôn mặt, trong đó mỗi hộp xác định góc dưới bên trái của hộp giới hạn, cũng như chiều rộng và chiều cao. Giả sử chỉ có một khuôn mặt trong ảnh cho thí nghiệm lần này, chúng ta có thể xác định tọa độ pixel của hộp giới hạn như sau.\n... # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] x2, y2 = x1 + width, y1 + height Sử dụng kết quả bên trên để cắt ra hình ảnh khuôn mặt:\n... # extract the face face = pixels[y1:y2, x1:x2] VGGFace model yêu cầu mỗi khuôn mặt có kích thước 224x224. Vì thế, chúng ta sẽ resize lại hình ảnh khuôn mặt, sử dụng thư viện PIL:\n... # resize pixels to the model size image = Image.fromarray(face) image = image.resize((224, 224)) face_array = asarray(image) Code đầy đủ, bắt đầu từ việc mở hình ảnh, phát hiện khuôn mặt và hiển thị kết quả như bên dưới (file face_detection.py):\nimport argparse from matplotlib import pyplot from PIL import Image from numpy import asarray from mtcnn.mtcnn import MTCNN import tensorflow as tf from tensorflow.compat.v1.keras.backend import set_session config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True # dynamically grow the memory used on the GPU config.log_device_placement = True # to log device placement (on which device the operation ran) sess = tf.compat.v1.Session(config=config) set_session(sess) # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-i\u0026#34;, \u0026#34;--image\u0026#34;, required=True, help=\u0026#34;path to the image\u0026#34;) args = vars(ap.parse_args()) # extract a single face from a given photograph def extract_face(filename, required_size=(224, 224)): # load image from file pixels = pyplot.imread(filename) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # load the photo and extract the face pixels = extract_face(args[\u0026#39;image\u0026#39;]) # plot the extracted face pyplot.imshow(pixels) # show the plot pyplot.show() Chạy code trên:\n$ python face_detection.py --image sharon_stone1.jpg Kết quả:  5. Face Recognition\nTrong phần này, chúng ta sẽ sử dụng mô hình VGGFace2 để thực hiện Face Recognition với ảnh của những người nổi tiếng từ Wikipedia. Một mô hình VGGFace có thể được tạo ra bằng cách sử dụng hàm tạo VGGFace() và chỉ định loại mô hình cần tạo thông qua đối số mô hình.\n... model = VGGFace(model=✬...✬) Thư viện keras-vggface cung cấp ba Pre-trained VGGModel models, model VGGFace1 sử dụng kiến trúc vgg16 (mặc định) và model VGGFace2 sử dụng kiến trúc resnet50 hoặc senet50. Ví dụ dưới đây tạo VGGFace2 model với kiến trúc resnet50:\n# example of creating a face embedding from keras_vggface.vggface import VGGFace # create a vggface2 model model = VGGFace(model=\u0026#39;resnet50\u0026#39;) # summarize input and output shape print(\u0026#39;Inputs: %s\u0026#39; % model.inputs) print(\u0026#39;Outputs: %s\u0026#39; % model.outputs) Lần đầu tiên khi model được tạo, thư viện sẽ tải xuống các trọng số của mô hình và lưu chúng trong thư mục ./keras/models/vggface/ trong thư mục /home/. Kích thước của weights cho kiểu resnet50 là khoảng 158MB, vì vậy quá trình tải xuống có thể mất vài phút tùy thuộc vào tốc độ kết nối internet của bạn. Chạy ví dụ trên sẽ in ra kích thước của đầu vào và đầu ra của mô hình. Chúng ta có thể thấy rằng mô hình mong đợi đầu vào là hình ảnh của khuôn mặt có kích thước 244 × 244 và kết quả đầu ra sẽ là một dự đoán của lớp là 8.631 người. Điều này là bởi vì mô hình đã được huấn luyện với 8.631 người trong tập dữ liệu MS-Celeb-1M.\nInputs: [\u0026lt;tf.Tensor ✬input_1:0✬ shape=(?, 224, 224, 3) dtype=float32\u0026gt;] Outputs: [\u0026lt;tf.Tensor ✬classifier/Softmax:0✬ shape=(?, 8631) dtype=float32\u0026gt;] Mô hình Keras này có thể được sử dụng trực tiếp để dự đoán xác suất của một khuôn mặt nhất định thuộc về một hoặc nhiều hơn tám nghìn người nổi tiếng được biết đến; ví dụ:\n... # perform prediction yhat = model.predict(samples) Sau khi dự đoán được đưa ra, các chỉ số của các phần tử với xác suất lớn nhất có thể được ánh xạ với tên của những người nổi tiếng và có thể lấy ra năm tên hàng đầu có xác suất cao nhất. Hành vi này được cung cấp bởi hàm decode predictions() trong thư viện keras-vggface:\n... # convert prediction into names results = decode_predictions(yhat) # display most likely results for result in results[0]: print(\u0026#39;%s: %.3f%%\u0026#39; % (result[0], result[1]*100)) Trước khi chúng ta có thể đưa ra dự đoán với một khuôn mặt, các giá trị pixel phải được chia tỷ lệ giống như cách mà dữ liệu đã được chuẩn bị khi mô hình VGGFace được huấn luyện. Điều này có thể đạt được bằng cách sử dụng hàm prerocess_input() được cung cấp trong thư viện keras-vggface và chỉ định version=2 để phù hợp với VGGFace2 model. (version=1 dành cho VGGFace).\n... # convert one face into samples pixels = pixels.astype(✬float32✬) samples = expand_dims(pixels, axis=0) # prepare the face for the model, e.g. center pixels samples = preprocess_input(samples, version=2) Kết hợp tất cả những điều này lại với nhau ta được code đầy đủ:\nimport argparse from numpy import expand_dims from matplotlib import pyplot from PIL import Image from numpy import asarray from mtcnn.mtcnn import MTCNN from keras_vggface.vggface import VGGFace from keras_vggface.utils import preprocess_input from keras_vggface.utils import decode_predictions import tensorflow as tf from tensorflow.compat.v1.keras.backend import set_session config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True # dynamically grow the memory used on the GPU config.log_device_placement = True # to log device placement (on which device the operation ran) sess = tf.compat.v1.Session(config=config) set_session(sess) # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-i\u0026#34;, \u0026#34;--image\u0026#34;, required=True, help=\u0026#34;path to the image\u0026#34;) args = vars(ap.parse_args()) # extract a single face from a given photograph def extract_face(filename, required_size=(224, 224)): # load image from file pixels = pyplot.imread(filename) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # load the photo and extract the face pixels = extract_face(args[\u0026#39;image\u0026#39;]) # convert one face into samples pixels = pixels.astype(\u0026#39;float32\u0026#39;) samples = expand_dims(pixels, axis=0) # prepare the face for the model, e.g. center pixels samples = preprocess_input(samples, version=2) # create a vggface model model = VGGFace(model=\u0026#39;resnet50\u0026#39;) # perform prediction yhat = model.predict(samples) # convert prediction into names results = decode_predictions(yhat) # display most likely results for result in results[0]: print(\u0026#39;%s: %.3f%%\u0026#39; % (result[0][3:-1], result[1]*100)) Chạy code:\n$ python face_recognition.py --image sharon_stone1.jpg Một cách tuần tự, đầu tiên khuôn mặt được phát hiện và trích xuất, sau đó VGGFace2 sẽ dự đoán danh tính của khuôn mặt. Năm cái tên có xác suất cao nhất sẽ được hiển thị. Chúng ta có thể thấy rằng mô hình xác định chính xác khuôn mặt thuộc về Sharon Stone với khả năng là 99,618%:\nSharon_Stone: 99.618% Noelle_Reno: 0.096% Anita_Lipnicka: 0.021% Elisabeth_R\\xc3\\xb6hm: 0.017% Tina_Maze: 0.017% Thử kiểm tra với một người khác, lần này là Channing Tatum:\n$ python face_recognition.py --image Channing_Tatum_by_Gage_Skidmore_3.jpg Kết quả:\nChanning_Tatum: 90.526% Les_Miles: 0.238% Eoghan_Quigg: 0.212% Nico_Rosberg: 0.153% Venke_Knutson: 0.136% Chúng ta có thể thấy rằng mô hình VGGFace2 xác định chính xác khuôn mặt là của Channing Tatum với độ xác suất là 90,526%.\nBạn có thể thử nhận diện với các bức ảnh khác của những người nổi tiếng được lấy từ Wikipedia, bao gồm nhiều giới tính, chủng tộc và độ tuổi khác nhau. Bạn sẽ phát hiện ra rằng mô hình này không hoàn hảo, thi thoảng vẫn có sự nhầm lẫn hoặc xác suất không cao. Bạn cũng có thể thử các phiên bản khác của mô hình, chẳng hạn như vgg16 và senet50, sau đó so sánh kết quả. Ví dụ: mình thấy rằng với một bức ảnh của Oscar Isaac, vgg16 có hiệu quả, nhưng với các kiểu của VGGFace2 thì không. Mô hình còn có thể được sử dụng để xác định các khuôn mặt mới. Một cách tiếp cận sẽ là huấn luyện lại mô hình ở phần phân loại khuôn mặt, với một tập dữ liệu khuôn mặt mới. Chúng ta sẽ áp dụng cách tiếp cận này trong bài viết về FaceNet model.\n6. Face Verification\nMô hình VGGFace2 có thể được sử dụng để thực hiện Face Verification. Điều này liên quan đến việc tính toán và so sánh khoảng cách giữa Face Embedding vector của một khuôn mặt đưa vào với Face Embedding vector của một khuôn mặt đã biết trong hê thống. Nếu 2 vectors có khoảng cách gần nhau thì có thể kết luận 2 khuôn mặt là của cùng 1 người, và ngược lại.\nCác phép đo khoảng cách giữa 2 vectors thường dùng là khoảng cách Euclide và khoảng cách Cosine. Giá trị ngưỡng để xác định thế nào là gần hay xa cần được điều chỉnh cho mỗi tập dữ liệu hoặc ứng dụng cụ thể.\nĐể tạo ra Face Embedding vector, đầu tiên, chúng ta có thể gọi mô hình VGGFace mà không cần bộ phân loại bằng cách đặt đối số include_top=False, chỉ định kích thước của dữ liệu đầu vào, và gán đối số pooling=\u0026lsquo;avg\u0026rsquo; để bộ lọc ánh xạ đầu ra của mô hình được thành một vector, sử dụng global average pooling.\n... # create a vggface model model = VGGFace(model=\u0026#39;resnet50\u0026#39;, include_top=False, input_shape=(224, 224, 3), pooling=\u0026#39;avg\u0026#39;) Mô hình này, sau đó được sử dụng để đưa ra dự đoán, kết quả trả về là một Face Embedding vector của khuôn mặt.\n... # perform prediction yhat = model.predict(samples) Chúng ta sẽ tổng hợp lại những thứ trình bày ở trên thành 1 hàm, tham số truyền vào là danh sách các file ảnh có khuôn mặt. Hàm này sẽ tìm và trích xuất các khuôn mặt từ mỗi ảnh thông qua hàm extract_face() đã sử dụng trong phần trước. Mỗi khuôn mặt cần phải được tiền xử lý trước khi đưa vào mô hình VGGFace2 bằng cách hàm preprocess_input(). Kết quả cuối cùng trả về là một mảng chứa toàn bộ Face Embedding vectors của tất các các khuôn mặt có trong các ảnh truyền vào cho hàm số:\n# extract faces and calculate face embeddings for a list of photo files def get_embeddings(filenames): # extract faces faces = [extract_face(f) for f in filenames] # convert into an array of samples samples = asarray(faces, \u0026#39;float32\u0026#39;) # prepare the face for the model, e.g. center pixels samples = preprocess_input(samples, version=2) # create a vggface model model = VGGFace(model=\u0026#39;resnet50\u0026#39;, include_top=False, input_shape=(224, 224, 3), pooling=\u0026#39;avg\u0026#39;) # perform prediction yhat = model.predict(samples) return yhat Mình sẽ lấy ảnh của Sharon Stone - sharon stone1.jpg (đã được sử dụng trước đây) để làm tiêu chuẩn. Sau đó, mình lấy một ảnh khác cũng của Sharon Stone và một ảnh không phải là Sharon Stone để so sánh:\nFace Verification có thể được thực hiện bằng cách tính toán khoảng cách Cosine giữa Face Embedding vector của ảnh tiêu chuẩn và Face Embedding vector của ảnh cần Verrify. Để làm điều này, ta sẽ sử dụng hàm cosine() trong thư viện SciPy. Khoảng cách lớn nhất giữa 2 vectors là 1.0 (hai vectors trùng nhau hoàn toàn), và khoảng cách tối thiểu là 0.0 (hai vectors khác nhau hoàn toàn). Giá trị khoảng cách giới hạn phổ biến thường được sử dụng cho Face Recognition là từ 0.4 đến 0.6. Ban đầu, sử dụng giá trị 0.5 rồi sau đó dựa trên thực tế để điểu chỉnh dần. Hàm is_match() bên dưới sẽ thực hiện điều này:\n# determine if a candidate face is a match for a known face def is_match(known_embedding, candidate_embedding, thresh=0.5): # calculate distance between embeddings score = cosine(known_embedding, candidate_embedding) if score \u0026lt;= thresh: print(\u0026#39;\u0026gt;face is a Match (%.3f\u0026lt;= %.3f)\u0026#39; % (score, thresh)) else: print(\u0026#39;\u0026gt;face is NOT a Match (%.3f\u0026gt; %.3f)\u0026#39; % (score, thresh)) Code đầy đủ cho chức năng Face Verification như sau:\nfrom matplotlib import pyplot from PIL import Image from numpy import asarray from scipy.spatial.distance import cosine from mtcnn.mtcnn import MTCNN from keras_vggface.vggface import VGGFace from keras_vggface.utils import preprocess_input import tensorflow as tf from tensorflow.compat.v1.keras.backend import set_session config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True # dynamically grow the memory used on the GPU config.log_device_placement = True # to log device placement (on which device the operation ran) sess = tf.compat.v1.Session(config=config) set_session(sess) # extract a single face from a given photograph def extract_face(filename, required_size=(224, 224)): # load image from file pixels = pyplot.imread(filename) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # extract faces and calculate face embeddings for a list of photo files def get_embeddings(filenames): # extract faces faces = [extract_face(f) for f in filenames] # convert into an array of samples samples = asarray(faces, \u0026#39;float32\u0026#39;) # prepare the face for the model, e.g. center pixels samples = preprocess_input(samples, version=2) # create a vggface model model = VGGFace(model=\u0026#39;resnet50\u0026#39;, include_top=False, input_shape=(224, 224, 3), pooling=\u0026#39;avg\u0026#39;) # perform prediction yhat = model.predict(samples) return yhat # determine if a candidate face is a match for a known face def is_match(known_embedding, candidate_embedding, thresh=0.5): # calculate distance between embeddings score = cosine(known_embedding, candidate_embedding) if score \u0026lt;= thresh: print(\u0026#39;\u0026gt;face is a Match (%.3f\u0026lt;= %.3f)\u0026#39; % (score, thresh)) else: print(\u0026#39;\u0026gt;face is NOT a Match (%.3f\u0026gt; %.3f)\u0026#39; % (score, thresh)) # define filenames filenames = [\u0026#39;sharon_stone1.jpg\u0026#39;, \u0026#39;sharon_stone2.jpg\u0026#39;, \u0026#39;sharon_stone3.jpg\u0026#39;, \u0026#39;channing_tatum.jpg\u0026#39;] # get embeddings file filenames embeddings = get_embeddings(filenames) # define sharon stone sharon_id = embeddings[0] # verify known photos of sharon print(\u0026#39;Positive Tests\u0026#39;) is_match(embeddings[0], embeddings[1]) is_match(embeddings[0], embeddings[2]) # verify known photos of other people print(\u0026#39;Negative Tests\u0026#39;) is_match(embeddings[0], embeddings[3]) Chúng ta có thể kiểm tra một số hình ảnh ví dụ bằng cách tải thêm ảnh về Sharon Stone và Channing Tatum từ Wikipedia.\nBức ảnh đầu tiên được lấy làm tiêu bản cho Sharon Stone và những bức ảnh còn lại trong danh sách là để Verify. Chạy ví dụ:\n$ python face_verification.py Kết quả:\nPositive Tests \u0026gt;face is a Match (0.460 \u0026lt;= 0.500) \u0026gt;face is a Match (0.311 \u0026lt;= 0.500) Negative Tests \u0026gt;face is NOT a Match (0.701 \u0026gt; 0.500) Chúng ta có thể thấy rằng hệ thống đã xác minh chính xác hai bức ảnh về Sharon Stone, còn ảnh của Channing Tatum được xác minh chính xác không phải là Sharon Stone.\n7. Kết luận\nTrong bài viết này, chúng ta đã cùng nhau khám phá cách phát triển hệ thống Face Recognition để nhận dạng và xác minh khuôn mặt bằng mô hình VGGFace2. Cụ thể:\n Giới thiệu về VGGFace và VGGFace2 Cách cài đặt thư viện Keras VGGFace để sử dụng các mô hình này bằng Python với Keras. Cách phát triển hệ thống nhận dạng khuôn mặt để dự đoán tên của những người nổi tiếng trong các bức ảnh. Cách phát triển hệ thống xác minh khuôn mặt để xác nhận danh tính của một người được từ bức ảnh khuôn mặt của họ.  Toàn bộ source code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ khám phá cách thực hiện bài toán Face Recongition bằng mô hình FaceNet. Mời các bạn đón đọc.\n8. Tham khảo\n Machinelearningmastery  ","permalink":"https://tiensu.github.io/blog/53_face_recognition_vggface/","tags":["Deep Learning","Face Recognition"],"title":"Thực hiện Face Identification và Verification với VGGFace2"},{"categories":["Deep Learning","Face Recognition"],"contents":"1. Giới thiệu chung\nFace Recognition là bài toán nhận dạng và xác thực người dựa vào khuôn mặt của họ. Đối với con người thì đó là một nhiệm vụ rất đơn giản, thậm chí là ở trong những điều kiện môi trường khác nhau, tuổi tác thay đổi, đội mũ, đeo kính, \u0026hellip; Tuy nhiên, đối với máy tính thì nó vẫn còn là một thử thách khó khăn trong vài thập kỷ qua cho đến tận ngày nay. Trong thời đại bùng nổ của trí tuệ nhân tạo, tận dụng sức mạnh của các thuật toán DL và lượng dữ liệu vô cùng lớn, chúng ta có thể tạo ra các models hiện đại, cho phép biểu diễn khuôn mặt thành các vectors đặc trưng trong không gian nhiều chiều. Để từ đó, máy tính có thể thực hiện nhận diện ra từng người riêng biệt, mà thậm chí còn vượt qua khả năng của con người trong một số trường hợp.\n2. Phân loại\nFace Recognition có thể chia thành 3 bài toán nhỏ:\n Face Authentication: Hạn chế quyền truy cập của một người đến một nguồn tài nguyên nào đó. Face Verification: Xác nhận một người phù hợp với ID của họ. Face Identification: Gán chính xác tên của người.  Ba bài toán này thực ra chỉ khác nhau ở mục đích sử dụng kết quả nhận diện khuôn mặt vào việc gì, còn về bản chất vẫn là phân loại xem khuôn mặt cần nhận diện có thuộc vào nhóm nào trong bộ dữ liệu cho trước hay không?\nTất cả những bài toán này đều cần phải được giải quyết trong cả 3 trường hợp:\n Người trong ảnh Người trong file video Người thực (stream real-time từ camera)  Tuy nhiên, cũng lại xuất hiện thêm một bài toán con con nữa, đó là đôi khi chúng ta cần phân biệt đâu là người thật, đâu là người giả (người trong video hay ảnh). Vì nếu chúng ta đối xử với cả 3 trường hợp đều như nhau thì rất có thể kẻ gian sẽ lợi dụng để truy cập trái phép vào hệ thống thông qua một bức ảnh, cái mà rất dễ dàng có được.\n3. Luồng xử lý của bài toán Face Recognition\nBài toán Face Recognition bắt buộc phải bao gồm tối thiếu 3 bước sau:\n Bước 1: Face Detection - Xác định vị trí của khuôn mặt trong ảnh (hoặc video frame). Vùng này sẽ được đánh dấu bằng một hình chữ nhật bao quanh. Bước 2: Face Extraction (Face Embedding) - Trích xuất đặc trưng của khuôn mặt thành một vector đặc trưng trong không gian nhiều chiểu (thường là 128 chiều). Bước 3: Face Classification (Face Authentication - Face Verification - Face Identification).  Ngoài 3 bước trên, trong thực tế chúng ta thường bổ sung thêm một số bước để tăng độ chính xác nhận diện:\n Image Preprocessing: Xử lý giảm nhiễu, giảm mờ, giảm kích thước, chuyển sang ảnh xám, chuẩn hóa, \u0026hellip; Face Aligment: Nếu ảnh khuôn mặt bị nghiêng thì căn chỉnh lại sao cho ngay ngắn. Kết hợp nhiều phương pháp khác nhau tại bước 3.   3. Face Detection\nFace Detection là bước đầu tiên trong bài toán Face Recognition, có vai trò rất lớn trong việc nâng cao độ chính xác của toàn bộ hệ thống. Đầu vào của nó là một bức ảnh có chứa mặt người, đầu ra của nó sẽ là các tọa độ của vùng chứa khuôn mặt, thường thể hiện bằng một hình chữ nhật bao quanh khuôn mặt đó.\nCó 2 phương pháp tiếp cận để giải quyết vấn đề ở bước này:\n  Feature-based: Sử dụng các bộ lọc thủ công (hand-crafted filters) để tìm kiếm và định vị vị trí khuôn mặt trong ảnh. Phương pháp này rất nhanh và hiệu quả trong điều kiện gần lý tưởng, nhưng không hiệu quả trong điều kiện phức tạp hơn.\n  Điều kiện gần lý tưởng    Điều kiện phức tạp hơn      Image-based: Sử dụng các thuật toán DL để học và tự động định vị vị trí khuôn mặt dựa trên toàn bộ bức ảnh. Ưu điểm của phương pháp này là độ chính xác cao hơn so với phương pháp Feature-based, nhưng tốc độ thực hiện thì lại chậm hơn. Tùy theo điều kiện cụ thể của từng bài toán mà ta chọn phương pháp phù hợp. VD: chạy trên thiết bị nào (PC hay Embedded Device), có cần Real-time hay không, điều kiện môi trường xung quanh ra sao, \u0026hellip;\n  Dưới đây là bảng tổng hợp các thư viện và thuật toán cho mỗi phương pháp này:\n Nhìn chung, phương pháp Image-based có sử dụng các thuật toán DL nên độ chính xác cao hơn so với phương pháp Feature-based. Nhưng đổi lại, xét về tốc độ thực hiện thì Feature-based lại là kẻ chiến thắng. Tuy nhiên, điều này chỉ biểu hiện rõ rệt nếu chúng ta chạy trên các thiết bị có cấu hình thấp, kiểu như các thiết bị nhúng, còn nếu chạy trên PC hay server thì sự khác biệt về tốc độ thực thi giữa 2 phương pháp là không đáng kể.\n4. Face Embedding\nĐây là bước thứ 2 trong bài toán Face Recognition. Input của nó là bức ảnh khuôn mặt đã tìm ra ở bước 1, còn Output là một Vector nhiều chiều thể hiện đặc trưng của khuôn mặt đó.\nHai thuật toán phổ biến nhất hiện nay để thực hiện Face Embedding là FaceNet và VGGNet.\n FaceNet được tạo ra bởi Florian Schroff và đồng nghiệp tại Google. Họ đã miêu tả nó trong bài báo năm 2015 với tiêu đề FaceNet: A Unified Embedding for Face Recognition and Clustering. Ý tưởng của FaceNet được gọi là Triplet Loss, cho phép hình ảnh được mã hóa hiệu quả dưới dạng vectơ đặc trưng, để từ đó tính toán và đối sánh độ tương đồng nhanh chóng thông qua các phép tính khoảng cách trong không gian. Hệ thống của họ đã đạt được kết quả state-of-the-art.  FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. [\u0026hellip;] Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. — FaceNet: A Unified Embedding for Face Recognition and Clustering, 2015.\n VGGFace được phát triển bởi Omkar Parkhi và đồng nghiệp từ Visual Geometry Group (VGG) tại Oxford. Nó được mô tả trong bài báo năm 2015 của họ có tiêu đề Deep Face Recognition. Trọng tâm chính của họ là thu thập một tập dữ liệu đào tạo rất lớn và sử dụng tập dữ liệu này để đào tạo một mô hình CNN rất sâu về khả năng nhận diện khuôn mặt.  \u0026hellip; we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human in the loop — Deep Face Recognition, 2015.\nCả 2 thuật toán này đều có Pre-trained model. Chúng ta hoàn toàn có thể sử dụng chúng một cách miễn phí trong các dự án của mình. Mình sẽ đi chi tiết hơn về cách dùng mỗi thuật toán này trong các bài tiếp theo.\n5. Face Classification\nNhiệm vụ của bước này là phân loại khuôn mặt vào các nhóm xác định trước trong tập dữ liệu, dựa vào Vector đặc trưng của chúng. Chúng ta có 3 phương pháp:\n Dựa vào khoảng cách: Tính toán và so sánh khoảng cách giữa các Vectors. Khoảng cách càng nhỏ chứng tỏ các Vectors càng giống nhau. Thuật toán kNN là đại diện tiêu biểu cho việc sử dụng khoảng cách để phân loại, ta có thể áp dụng nó. Khoảng cách ở đây có thể sử dụng công thức Cosine hoặc Euclidean. Ưu điểm của phương pháp này là đơn giản, thực thi nhanh nếu số lượng khuôn mặt không nhiều. Nhược điểm là độ chính xác không cao, tốc độ thực thi giảm nếu số lượng khuôn mặt tăng lên. Sử dụng ML: Ta có thể dùng các Vectors đặc trưng của khuôn mặt để huấn luyện một ML model, với các thuật toán như SVM, Decision Tree, \u0026hellip; Thuật toán SVM thường được sử dụng nhiều hơn. Phương pháp này cân bằng giữa tốc độ thực hiện và độ chính xác. Sử dụng DL: Tương tự vậy, ta cũng có thể huấn luyện một DL model đơn giản (3-5 FC layers) từ các Vectors đặc trưng của khuôn mặt. Phương pháp này thường có độ chính xác cao nhất (nếu DL model đủ tốt), nhưng tốc độ thực hiện lại chậm nhất.  Ngoài ra, trong các bài toán thực tế, để tăng độ chính xác lên cao nhất có thể, chúng ta có thể kết hợp phương pháp 1 và 3, hoặc phương pháp 1 và 2.\n6. Kết luận\nTrong bài viết này, mình đã cùng các bạn khám phá bài toán Face Recognition, cụ thể:\n Face Recognition là một vấn đề chung của việc xác định hoặc xác minh người trong ảnh và video. Face Recognition là một quá trình bao gồm Face Detection, Face Embedding và Face Recongition. Các thuật tóan, các mô hình có thể sử dụng tại từng giai đoạn và ưu/nhược điểm của chúng.  Trong bài tiếp theo, mình sẽ hướng dẫn bạn cách thực hiện bài toán Face Identification và Face Verification bằng mô hình VGGFace2. Mời các bạn đón đọc!\n7. Tham khảo\n Machinelearningmastery  ","permalink":"https://tiensu.github.io/blog/52_face_recognition/","tags":["Deep Learning","Face Recognition"],"title":"Bài toán Face Recognition"},{"categories":["MLOps","Scalability"],"contents":"Nếu bạn là người thiết kế giải pháp cho hệ thống phần mềm, (ở công ty mình thường gọi là SA - Solution Architecture) (có sử dụng AI model hoặc không) thì bài viết này dành cho bạn!\nĐối với cá nhân mình, một SA ở công ty, công việc của mình bao gồm:\n Tiếp nhận bài toán của khách hàng (KH). KH có thể là các tổ chức, cá nhân ngoài công ty (outsoursing) hoặc chính là công ty mình nếu công ty phát tự triển sản phẩm (product). Cùng với đội ngũ BA, phân tích làm rõ yêu cầu của KH. Tạo tài liệu \u0026ldquo;đề xuất phát triển\u0026rdquo; (Development Proposal) để thống nhất với KH. Tài liệu này thường bao gồm các nội dung: Tóm tắt yêu cầu bài toán Đưa ra kiến trúc tổng thể giải pháp và các công nghệ sử dụng. Phần này tùy theo đối tượng KH là ai mà mình sẽ trình bày chi tiết hoặc mức tổng quát. Các chức năng mà sản phẩm (phần mềm) cung cấp. Kế hoạch phát triển dự án, từ thời gian nào để thời gian nào, các mốc phát triển chính. Tổ chức dự án: dự án có những ai liên quan (stakeholder), vai trò nhiệm vụ của mỗi người (không cần tên cụ thể, chỉ cần đưa ra các vị trí trong dự án. VD: PM, SM, Developers, QA, \u0026hellip;) Chi phí dự án: liệt kê chi phí cho từng chức năng lớn, bao gồm cả phần quản lý (chi phí Overhead) và phát triển. Cách thức làm việc giữa đội phát triển, quản lý dự án và KH: họp, báo cáo, demo, \u0026hellip; Các điều kiện cần, điều kiện ràng buộc và hạn chế của dự án. Trình bày tài liệu \u0026ldquo;đề xuất phát triển\u0026rdquo; với KH. Đây là bước rất quan trọng để thuyết phục KH. Nếu KH đồng ý với những đề xuất đưa ra thì sẽ chinh thức tiến hành thực hiện dự án. Đưa ra giải pháp chi tiết hơn cho đội phát triển dự án. Theo dõi và hỗ trợ đội phát triển dự án khi có vấn đề khó khăn về kỹ thuật xảy ra. (Viêc quản lý tiến độ phát triển, làm tài liệu dự án hay giao tiếp với KH là trách nhiệm của PM). Nghiên cứu, tìm hiểu những công nghệ mới để hướng dẫn lại cho mọi người hoặc áp dụng vào dự án. Trong trường hợp đội phát triển dự án không đủ người thì SA có thể tham gia làm cùng như một Developer hoặc nếu đội dự án không đủ năng lực kỹ thuật thì SA tổ chức các buổi đào tạo, huấn luyện cho các thành viên trong đội.  Ngoài ra, ở công ty, mình cũng đang quản lý team AI, đào tạo và phân bổ nguồn lực AI cho các dự án. Thi thoảng thì mình cũng có tham gia làm PM cho một vài dự án nếu mình cảm thấy thú vị.\nChi tiết hơn về công việc của một SA cũng như cách thức làm tài liệu \u0026ldquo;đề xuất phát triển\u0026rdquo;, mình sẽ có 1 bài viết chi tiết sau.\nQuay trở lại chủ đề chính của ngày hôm nay, mình sẽ hướng dẫn các bạn cách thiết kế một hệ thống phần mềm (có sử dụng công nghệ AI) để nó có thể hoạt động trơn tru từ lúc bắt đầu phát triển, một vài người dùng đến lúc phục vụ hàng triệu người dùng đồng thời.\nĐầu tiên, chúng ta phải xác định được phạm vi của dự án, của sản phẩm. Nếu sản phẩm của ta chỉ phục vụ một số ít người dùng hoặc làm ra với mục đích demo thì chúng ta không cần phải làm theo những gì được đề cập trong bài viết này. Vì như thế chẳng khác nào \u0026ldquo;mang dao mổ trâu giết gà\u0026rdquo;, vừa tốn công sức lại tốn cả chi phí. Ngược lại, nếu ứng dụng làm ra hướng đến phục vụ số lượng lớn người dùng thì chúng ta phải đặc biệt quan tâm đến thiết kế ngay từ đầu. Nếu không, nó có thể chạy tốt với một vài users, nhưng khi số lượng users tăng lên, nó sẽ không thể đáp ứng được. Lúc đó, có thể chúng ta phải đập đi làm lại từ đầu, rất tốn thời gian, công sức, tiền bạc.\nBài viết này dành cho các dự án đi theo hướng số 2.\n1. On-premise hay Cloud Services\nCái đầu tiên bạn phải nghĩ đến là hạ tầng phát triển và triển khai. Có 2 lựa chọn cho các bạn là: On-premise và Cloud Services.\nMỗi cái đều có ưu, nhược điểm riêng, bạn có thể tham khảo ở đây\nVề cơ bản thì Cloud Services lợi nhiều hơn hại. Nếu như ở local, chúng ta phải tự xây dựng từ đầu thông qua các thư viện/framework như Docker, Nginx, uWSGI, Kubernetes, \u0026hellip; thì trên Cloud hỗ trợ chúng ta rất nhiều trong việc \u0026ldquo;Scalability\u0026rdquo; sản phẩm thông qua các dịch vụ mà nó cung cấp. Chúng ta chỉ cần hiểu rõ chức năng, mục đích của từng service để sử dụng đúng mục đích, tránh lãng phí tiền bạc không cần thiết. Và mình cũng khuyến khích các bạn sử dụng Cloud cho mục đích này.\nMột số nhà cung cấp Cloud Services khá phổ biến hiện này là AWS, GCP, Azure. Mình chọn GCP để làm ví dụ trình bày trong bài này. Các bạn hoàn toàn có thể áp dụng cho các Cloud Services khác một cách tương tự, chỉ cần hiểu rõ các services của chúng là được.\n2. Bước đầu triển khai ứng dụng AI\nGiả sử, chúng ta đã huấn luyện thành công một AI model, độ chính xác như mong muốn. Chúng ta cũng tạo ra một API đơn giản (sử dụng Flask, uWSGI, Nginx, \u0026hellip;) để nhận các yêu cầu dự đoán (inference) gửi đến model.\nTrên GCP, chúng ta tạo ra một VM Instance (nên chon VM Instance dành riêng cho các tác vụ DL, bởi vì nó được tối ưu và bao gồm đầy đủ các thư viện cần thiết như TF, Cuda, \u0026hellip;) và copy toàn bộ dự án của chúng ta lên đó. Cấu hình cho phép HTTP traffic từ bên ngoài kết nối đến VM Instance đó.\n Thử gửi một request đến AI model, kết quả trả về không khác gì khi chạy trên local. Bước đầu, như vậy là thành công.\n3. Cấu hình CI/CD Pipeline\nSau một vài ngày (tuần) sử dụng, chúng ta muốn thay đổi model, thay đổi thư viện sử dụng. Chúng ta bắt đầu nhận ra một số bất cập:\n Việc triển khai yêu cầu rất nhiều thao tác thủ công, mất nhiều thời gian. Không có sự đồng bộ giữa các phiên bản của AI model cũng như các thư viện sử dụng. Khó khăn trong việc debug khi xảy ra lỗi trong quá trình sử dụng.  Để giải quyết 2 khó khăn đầu tiên, chúng ta bổ sung thêm CI/CD Pipeline (CD - Continuous integration, CI - Continuous deployment). Pipeline này sẽ tự động hóa việc tạo, kiểm thử và triển khai sản phẩm. Trên local, một số framework hỗ trợ việc này, bao gồm Jenkins, CircleCI. Trên GCP, Pipeline này được triển khai thành Cloud Build service.\nKhó khăn còn lại có thể vượt qua bằng cách thêm 2 services là Monitoring và Logging. Một khi có 2 services này, chúng ta có thể biết được toàn bộ quá trình hoạt động của hệ thống, và từ đó biết được chính xác nguyên nhân của lỗi (nếu có).\n Để thuận tiện hơn nữa, chúng ta nên đóng gói toàn bộ dự án trong một Docker Container, sau đó mới đặt Container đó trong VM Instance. Mỗi lần thêm mới hay cập nhật thư viện, AI model, chúng ta chỉ cần thay đổi trong file Dockerfile, Rebuild lại Container rồi Redeploy. Mọi thứ trở nên rất đơn giản và dễ dàng. Có rất nhiều Docker Container được xây dựng sẵn cho mục đích phục vụ các bài toán DL. Bạn hoàn toàn có thể tải về và sử dụng chúng miễn phí.\n4. Scaling\nSau một thời gian sử dụng, ứng dụng của chúng ta đã trở nên phổ biến, số lượng người dùng ngày càng tăng lên. Và VM Instance ban đầu bắt đầu bộc lộ những yếu điểm:\n Thời gian đáp ứng lâu hơn Chẳng may VM Instance gặp sự cố thì toàn bộ users không thể sử dụng được ứng dụng.  Lúc này, chúng ta cần đến Scaling. Có 3 loại Scaling:\n Vertical Scaling hay Scaling Up: Tăng cấu hình phần cứng bằng cách thêm CPU, Memory, GPU, Storage vào VM Instance đang sử dụng, làm cho nó có đủ sức mạnh để xử lý đồng thời nhiều yêu cầu đến từ số lượng lớn người dùng. Việc làm này tất nhiên là không thể diễn ra liên tục, nó phải có giới hạn nhất định. Hơn nữa, nếu VM Instance này bị chết thì toàn bộ hệ thống cũng chết theo. Horizontal Scaling hay Scaling Out: Tạo thêm các VM Instance mới bằng cách nhân bản VM Instance hiện tại. Các yêu cầu gửi đến sẽ được phân phối đồng đều trên tất cả các VM Instances. Loại Scaling này thường được sử dụng nhiều hơn. Scaling Down: Ngược lại với 2 loại Scaling trên, tức là giảm cấu hình phần cứng hoặc giảm số VM Instances sử dụng khi số lượng người dùng giảm.   Đối với Scaling Out, làm thế nào để phân phối các yêu cầu từ người dùng đến các VM Instances? câu trả lời là Load Balancing. Load Balancer chịu trách nhiệm điều tiết traffic ngang qua tất cả các VM Instances, làm tăng tính sẵn sàng phục vụ của toàn hệ thống. Nếu một VM Instance bị chết, Load Balancer sẽ chuyển Traffic đến các Instance khác và Scaling Out sẽ giúp tạo ra VM Instance mới để thay thế VM Instance bị chết. Nói chung, ứng dụng của chúng ta vẫn hoạt động bình thường.\nLoad Balancer cũng cung cấp các cơ chế mã hóa, bảo mật, xác thực, và kiểm tra sức khỏe các kết nối (health connections checks). Monitoring và Debugging cũng được tích hợp trong Load Balancer.\n Số lượng VM Instance được thêm mới là không có giới hạn, tùy thuộc vào nhu cầu bài toán. Ngoài ta, các VM Instances có thể được tạo ra ở các khu vực địa lý khác nhau để tối ưu hóa việc gửi nhận và xử lý Traffic.\n Đến lúc này, nhìn chung hệ thống của chúng ta đã có thể chạy ổn định nếu như không có gì bất thường xảy ra. Nhưng nếu như có gì bất thường xảy ra thì sao?\n5. Auto Scaling\nGiả sử tại một số thời điểm, ví dụ như các ngày lễ tết, số lượng Request đến hệ thống tăng lên đột biến. Đây là một common case mà chúng ta thường thấy, chúng được gọi chung với cái tên là sudden spikes in traffic.\nVậy làm thế nào để giải quyết tình huống này?\nBạn có thể nghĩ đến việc Scaling Out hệ thống để đáp ứng yêu cầu này. Nhưng khi nào thì thực hiện Scaling Out và Scaling Out bao nhiêu cho đủ. Chúng ta không thể (không nên) tạo ra các VM Instances một cách tùy ý, vì như vậy sẽ tốn rất nhiều tiền. Mọi thứ sử dụng trên hạ tầng Cloud đều phải trả tiền. Auto Scaling là giải pháp cho những tình huống như thế này. Nó là một phương pháp được sử dụng trong điện toán đám mây để thay đổi số lượng tài nguyên tính toán dựa trên tải. Thông thường, điều này có nghĩa là số lượng phiên bản VM Instances tăng lên hoặc giảm xuống, dựa trên một số chỉ số (metrics).\nCó 2 loại Auto Scaling:\n Schedule Scaling: Sử dụng khi chúng ta biết trước rằng Traffic sẽ thay đổi tại các thời điểm nào đó. Và để đáp ứng, chúng ta sẽ đặt lịch cho hệ thống tự động Scaling Out/Down tại những thời đỉểm đó. Dynamic Scaling: Sử dụng khi chúng ta không biết trước chính xác thời điểm nào lượng Traffic sẽ thay đổi. Vì thế, chúng ta sẽ cấu hình để hệ thống giám sát một số Metrics, khi các Metrics này vượt quá một ngưỡng nào đó thì các VM Instances sẽ tự động được tạo ra để đáp ứng đủ lượng Traffic. Các Metrics ở đây có thể là phần trăm sử dụng CPU, bộ nhớ, hay thời gian đáp ứng yêu cầu, \u0026hellip; Ví dụ, chúng ta cấu hình để hệ thống tạo ra số VM Instances gấp đôi khi phần trăm sử dụng của CPU vượt quá 90%, và khi phần trăm này giảm xuống nhỏ hơn 40% thì số lượng VM Instances cũng được giảm giống như trạng thái bình thường.  6. Caching\nCó một cách khác rất hiệu quả để giảm thời gian đáp ứng yêu cầu, đó là sử dụng cơ chế Caching. Cơ chế này có thể áp dụng được khi hệ thống của bạn có những yêu cầu đến giống hệt nhau. Khi đó, model của chúng ta chỉ phải xử lý yêu cầu lần đầu, kết quả từ model được lưu trong bộ nhớ Cache. Các yêu cầu đến sau mà giống yêu cầu này thì kết quả sẽ được lấy ra từ bộ nhớ Cache đó. Việc lấy kết quả ra từ từ bộ nhớ Cache thường nhanh hơn rất nhiều so với việc xử lý của model. Tuy nhiên, cũng cần phải lưu ý khi sử dụng cơ chế Caching, đó là phải thiết lập thời gian Time Out cho bộ nhớ Cache. Hết thời gian này mà kết quả nào ko được lấy ra để sử dụng thì sẽ tự động bị xóa khỏi bộ nhớ Cache.\n 7. Monitoring, Logging \u0026amp; Alerts\nMonitoring cũng là một phần không thể thiếu được của bất kỳ hệ thống phục vụ số đông người dùng nào. Mặc dù chúng ta có thể tự tin tạo ra một sản phẩm rất tốt, rất hoàn hảo, chạy trơn tru nhưng chúng ta không thể nào dự đoán hết được những lỗi có thể xảy ra trong suốt quá trình hoạt động của ứng dụng. Lỗi có thể đến từ nguyên nhân khách quan: mất mạng, mất điện, \u0026hellip; hoặc chủ quan: code sai, \u0026hellip; Nếu hệ thống chỉ phục vụ một vài người thì thi thoảng xảy ra lỗi cũng không phải vấn đề gì quá lớn lao. Nhưng nếu số lượng Users là hàng nghìn, hàng triệu thì mỗi phút không hoạt động sẽ làm tổn tất của chúng ta rất rất nhiều tiền. Đó là lý do chúng ta cần Monitor, Logging hệ thống và Alert khi xảy ra sự cố, để chúng ta có thể nhanh chóng nhận ra và khắc phục sự cố đó nhanh nhất có thể.\nHầu hết các nền tảng Cloud đều cung cấp cơ chế Monitor, Loging \u0026amp; Alert dưới dạng các Services của họ. Thông thường các bước để cấu hình Monitor, Logging \u0026amp; Alert sẽ là:\n Định nghĩa các Metrics và giám sát chúng liên tục (real-time) trong suốt quá hoạt động của hệ thống. Trực quan hóa các Metrics đó trên Dashboard dưới dạng các biểu đồ để theo dõi sự thay đổi của chúng theo thời gian (real-time). Thông báo đến người quản trị khi có hiện tượng bất thường xảy ra.   Có được cả 3 cơ chế này trong hệ thống, chúng ta có thể yên tâm ngủ ngon mỗi tối, vì mọi hoạt động của hệ thống đều đang được giám sát và điều khiển. Chúng ta chỉ phải quan tâm khi nhận được cảnh báo, báo cáo lỗi xảy ra.\nNhững thứ mà mình đã trình bày từ đầu cho đến bước này có thể áp dụng cho tất cả các loại phần mềm nói chung, trong đó có phần mềm sử dụng AI model. Tuy nhiên, riêng với dạng phần mềm có dính dáng đến AI model, chúng ta sẽ phải làm thêm một số bước khác, đặc thù hơn như dưới đây.\n8. Retraining AI Model\nỞ các bài trước mình đã trình bày về hiện tượng Data Driff và sự cần thiết phải Retraining model. Bạn có thể xem lại tại đây và ở đây.\nĐể có thể Retraining AI model thì chúng ta phải có dữ liệu mới. Dữ liệu này có thể chính là lịch sử hoạt động của model cũ, hoặc đến từ các phản hồi của KH, của người dùng. Do đó, chúng ta cần có một nơi để lưu trữ những dữ liệu như thế này, chúng ta cần một Database.\nXét về loại Database, chúng ta có thể hơi băn khoăn một chút là nên sử dụng loại Database nào? SQL hay NoSQL. Chi tiết so sánh giữa 2 loại này, các bạn có thể xem thêm tại đây. Còn riêng về khía cạnh Scability thì cả 2 lại Database này đều có thể đáp ứng được, mặc dù nói một cách thành thực thì NoSQL có nhiều điểm mạnh hơn SQL trong cá bài toán kiểu như này. Cá nhận mình thì khuyên các bạn sử dụng NoSQL.\nCó đủ data rồi thì chúng ta có thể tiến hành Retraining model theo cách đã trình bày trong bài này và bài [này]](https://tiensu.github.io/blog/40_ai_model_registry/).\nCó 2 điều cần chú ý ở đây:\n Các model hoàn toàn độc lập với nhau vì chúng được lưu trong các Docker Container riêng biệt. Bản thân model được lưu ở Storage service của Cloud Platform, như S3 của AWS hay Storage của GCP.  Một điều nữa cũng rất quan trọng, đó là không phải lúc nào model mới tạo ra cũng tốt hơn model cũ đang sử dụng. Vì thế, chúng ta phải sử dụng song song cả 2 model đồng thời 1 thời gian để đánh giá, so sánh tính hiệu quả của chúng. Đó chính là phương pháp A/B Testing. Để phân phối Traffic, chúng ta có thể cấu hình Load Balancer gửi đến mỗi model tương ứng. Ví dụ, ban đầu chỉ gửi 5% đến model mới, 95% đến model cũ, sau đó tăng dần lượng Traffic lên model mới cho đến khi đủ thông tin để kết luận rằng nó tốt hơn model cũ. Khi đó, chúng ta sẽ thay thế hoàn toàn model cũ bằng model mới.\n9. Offline Inference\nKhông phải bài toán AI nào cũng yêu cầu phải đáp ứng yêu cầu một cách tức thời. Trong chuyên môn, người ta gọi đó là Online Inference. Còn một loại khác mà model sẽ chỉ trả về kết quả tại một số thời điểm nhất định sau khi nhận được yêu cầu. Đó là kiểu Offline Inference hay Batch Inference, dựa trên cơ chế làm việc bất đồng bộ trong hệ thống. Một thằng thì cứ gửi yêu cầu, sau đó làm việc khác mà không phải chờ nhận kết quả. Một thằng thì cứ thong thả, gom đủ một số lượng yêu cầu nhất định mới trả lời một lần, rồi gửi thông báo đến cho thằng gửi yêu cầu đó.\n Để làm được việc này, chúng ta cần đến một kiểu dịch vụ, gọi là Message Queue. Nó là một dạng của dịch vụ không đồng bộ với giao tiếp dịch vụ. Message Queue lưu trữ các thông điệp đến từ một nhà sản xuất (Producer) và đảm bảo rằng mỗi thông báo sẽ chỉ được xử lý một lần bởi một người tiêu dùng (Consumer) duy nhất. Một Message Queue có thể có nhiều Producers và nhiều Consumers.\n 10. Kết luận\nNhư vậy là mình đã trình bày xong một cách tổng quát cách thức xây dựng kiến trúc hệ thống phần mềm có sử dụng AI model để phục vụ số lượng Users lớn. Thực tế thì tùy từng bài toán cụ thể mà các bạn có thể thay đổi, tối ưu hóa cho phù hợp với mục đích của các bạn. Hiện này, Kubernetes đang nổi là như là một xu hướng cho việc thiết kế hệ thống, bởi vì nó tập hơn gần như đầy đủ các thành phần được nhắc đến trong bài này. Mình cũng đã có 1 số bài viết về Kubernetes, các bạn có thể xem lại. Nếu có thời gian, mình sẽ viết thêm một bài về xây dựng 1 hệ thống đầy đủ như này trên GCP.\nHi vọng bài viết này mang lại những kiến thức bổ ích cho mọi người. Hẹn gặp lại các bạn trong những bài viết sau!\n11. Tham khảo\n Theaisummer  ","permalink":"https://tiensu.github.io/blog/51_scalability_in_ai_production/","tags":["MLOps","Scalability"],"title":"Thiết kế hệ thống cho AI model để phục vụ từ 1 đến 1 triệu người dùng"},{"categories":["Deep Learning","Autoencoder"],"contents":"Ở bài trước, chúng ta đã áp dụng Autoencoders model vào bài toán Data Denoising. Trong bài này, chúng ta sẽ tiếp tục cùng nhau tìm hiểu 1 ứng dụng nữa của Autoencoders model trong bài toán Abnormal/Outline Data Detection.\n1. Thế nào là Abnormal/Outline Data?\nĐầu tiên, chúng ta cần hiểu rõ thế nào là Abnormal/Outline Data? Hiểu một cách đơn giản thì Abnormal/Outline Data là những dữ liệu mà khác với đa số phần dữ liệu còn lại. Cái \u0026ldquo;khác\u0026rdquo; có thể được thể hiện một cách trực quan qua đồ thị phân phối dạng Histogram, Scatter, \u0026hellip;  Mặc dù thông thường tỷ lệ Abnormal/Outline Data rất nhỏ (cỡ \u0026lt; 1%) nhưng lại có tác hại rất lớn đến chất lương của model. Vì thế, nếu có thể phát hiện và loại bỏ được chúng thì có thể tạo ra được những model tốt.\nTrong bài toán triển khai AI model vào thực tế, Abnormal/Outline Data còn được gọi là hiện tượng Data Driff. Đó là hiện tượng khi mà model đã hoạt động được một thời gian, đến một thời điểm nào đó, độ chính xác của model giảm xuống mà nguyên nhân là do dữ liệu mới đưa vào model không giống như dữ liệu lúc đầu huấn luyện tạo ra model. Nếu chúng ta phát hiện được sớm hiện tượng này và cập nhật lại model theo dữ liệu mới (chính là Abnormal/Outline Data) thì sẽ tiếp tục duy trì được sự ổn định của model.\nHiện tại, có khá nhiều thuật toán có thể sử dụng để phát hiện ra Abnormal/Outline Data, ví dụ như Isolation Forests, One-class SVMs, Elliptic Envelopes, Local Outlier Factor, hay thậm chí là sử dụng một DL model(CNN).\n2. Sử dụng Autoencoders model cho bài toán Abnormal/Outline Data Detection\nĐể sử dụng Autoencoders model cho bài toàn Abnormal/Outline Data Detection, chúng ta căn cứ vào MSE giữa Output của Decoder và Input Data. Giả sử chúng ta đã huấn luyện được một Autoencoders model khá tốt, đạt được MSE nhỏ trên tập dữ liệu huấn luyện cũng như tập dữ liệu test. Bây giờ, nếu chúng ta đưa vào Encoder một dữ liệu mà khi tính MSE giữa dữ liệu đầu vào đó với Output của Decoder cho ra một giá trị tương đối lớn thì chúng ta có thể kết luận rằng, dữ liệu đầu vào đó là Abnormal/Outline Data.   Bây giờ chúng ta sẽ viết code để thực hiện việc này.\nCấu trúc thư mục làm việc như sau:\n$ tree --dirsfirst . ├── output │ ├── autoencoder.model │ └── images.pickle ├── sunt │ ├── __init__.py │ └── conv_autoencoder.py ├── find_anomalies.py ├── plot.png ├── recon_vis.png └── train_unsupervised_autoencoder.py File conv_autoencoder.py vẫn chứa định nghĩa Autoencoders model như những bài trước.\nFile train_unsupervised_autoencoder.py chứa code để huấn luyện model trên tập MNIST. Cụ thể là ở bài toán này, chúng ta chỉ sử dụng những ảnh chứa ký tự 1, còn những ảnh chứa ký tự khác sẽ được coi như là Abnormal/Outline Data. Kết quả huấn luyện sẽ là:\n autoencoder.model: Autoencoders model đã huấn luyện. images.pickle: Tập các images không có nhãn để tìm Abnormal/Outline Images trong đó. plot.png: Đồ thị Loss trong quá trình huấn luyện. recon_vis.png: So sánh Output của model với ảnh gốc ban đầu.  File find_anomalies.py chứa code để tìm ra Abnormal/Outline Images trong images.pickle.\nCode của file train_unsupervised_autoencoder.py như sau:\n# USAGE # python train_unsupervised_autoencoder.py --dataset output/images.pickle --model output/autoencoder.model # set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use(\u0026#34;Agg\u0026#34;) from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) # import the necessary packages from sunt.conv_autoencoder import ConvAutoencoder from tensorflow.keras.optimizers import Adam from tensorflow.keras.datasets import mnist from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import numpy as np import argparse import random import pickle import cv2 def build_unsupervised_dataset(data, labels, validLabel=1, anomalyLabel=3, contam=0.01, seed=42): # grab all indexes of the supplied class label that are *truly* that particular label, then grab the indexes of the image labels that will serve as our \u0026#34;anomalies\u0026#34; validIdxs = np.where(labels == validLabel)[0] anomalyIdxs = np.where(labels == anomalyLabel)[0] # randomly shuffle both sets of indexes random.shuffle(validIdxs) random.shuffle(anomalyIdxs) # compute the total number of anomaly data points to select\ti = int(len(validIdxs) * contam) anomalyIdxs = anomalyIdxs[:i] # use NumPy array indexing to extract both the valid images and \u0026#34;anomlay\u0026#34; images validImages = data[validIdxs] anomalyImages = data[anomalyIdxs] # stack the valid images and anomaly images together to form a single data matrix and then shuffle the rows images = np.vstack([validImages, anomalyImages]) np.random.seed(seed) np.random.shuffle(images) # return the set of images return images def visualize_predictions(decoded, gt, samples=10): # initialize our list of output images outputs = None # loop over our number of output samples for i in range(0, samples): # grab the original image and reconstructed image original = (gt[i] * 255).astype(\u0026#34;uint8\u0026#34;) recon = (decoded[i] * 255).astype(\u0026#34;uint8\u0026#34;) # stack the original and reconstructed image side-by-side output = np.hstack([original, recon]) # if the outputs array is empty, initialize it as the current side-by-side image display if outputs is None: outputs = output # otherwise, vertically stack the outputs else: outputs = np.vstack([outputs, output]) # return the output images return outputs # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-d\u0026#34;, \u0026#34;--dataset\u0026#34;, type=str, required=True, help=\u0026#34;path to output dataset file\u0026#34;) ap.add_argument(\u0026#34;-m\u0026#34;, \u0026#34;--model\u0026#34;, type=str, required=True, help=\u0026#34;path to output trained autoencoder\u0026#34;) ap.add_argument(\u0026#34;-v\u0026#34;, \u0026#34;--vis\u0026#34;, type=str, default=\u0026#34;recon_vis.png\u0026#34;, help=\u0026#34;path to output reconstruction visualization file\u0026#34;) ap.add_argument(\u0026#34;-p\u0026#34;, \u0026#34;--plot\u0026#34;, type=str, default=\u0026#34;plot.png\u0026#34;, help=\u0026#34;path to output plot file\u0026#34;) args = vars(ap.parse_args()) # initialize the number of epochs to train for, initial learning rate, and batch size EPOCHS = 20 INIT_LR = 1e-3 BS = 32 # load the MNIST dataset print(\u0026#34;[INFO] loading MNIST dataset...\u0026#34;) ((trainX, trainY), (testX, testY)) = mnist.load_data() # build our unsupervised dataset of images with a small amount of contamination (i.e., anomalies) added into it print(\u0026#34;[INFO] creating unsupervised dataset...\u0026#34;) images = build_unsupervised_dataset(trainX, trainY, validLabel=1, anomalyLabel=3, contam=0.01) # add a channel dimension to every image in the dataset, then scale the pixel intensities to the range [0, 1] images = np.expand_dims(images, axis=-1) images = images.astype(\u0026#34;float32\u0026#34;) / 255.0 # construct the training and testing split (trainX, testX) = train_test_split(images, test_size=0.2, random_state=42) # construct our convolutional autoencoder print(\u0026#34;[INFO] building autoencoder...\u0026#34;) (encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1) opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS) autoencoder.compile(loss=\u0026#34;mse\u0026#34;, optimizer=opt) # train the convolutional autoencoder H = autoencoder.fit( trainX, trainX, validation_data=(testX, testX), epochs=EPOCHS, batch_size=BS) # use the convolutional autoencoder to make predictions on the testing images, construct the visualization, and then save it to disk print(\u0026#34;[INFO] making predictions...\u0026#34;) decoded = autoencoder.predict(testX) vis = visualize_predictions(decoded, testX) cv2.imwrite(args[\u0026#34;vis\u0026#34;], vis) # construct a plot that plots and saves the training history N = np.arange(0, EPOCHS) plt.style.use(\u0026#34;ggplot\u0026#34;) plt.figure() plt.plot(N, H.history[\u0026#34;loss\u0026#34;], label=\u0026#34;train_loss\u0026#34;) plt.plot(N, H.history[\u0026#34;val_loss\u0026#34;], label=\u0026#34;val_loss\u0026#34;) plt.title(\u0026#34;Training Loss\u0026#34;) plt.xlabel(\u0026#34;Epoch #\u0026#34;) plt.ylabel(\u0026#34;Loss\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.savefig(args[\u0026#34;plot\u0026#34;]) # serialize the image data to disk print(\u0026#34;[INFO] saving image data...\u0026#34;) f = open(args[\u0026#34;dataset\u0026#34;], \u0026#34;wb\u0026#34;) f.write(pickle.dumps(images)) f.close() # serialize the autoencoder model to disk print(\u0026#34;[INFO] saving autoencoder...\u0026#34;) autoencoder.save(args[\u0026#34;model\u0026#34;], save_format=\u0026#34;h5\u0026#34;) Để cho giống với bài toán thực tế, mình chọn 99% hình ảnh chứa ký tự 1 và 1% hình ảnh chứa ký tự 3. Ký tự 3 coi như là Abnormal/Outline Data.\nTrên Terminal, thực hiện lệnh sau:\n$ python train_unsupervised_autoencoder.py --dataset output/images.pickle --model output/autoencoder.model [INFO] loading MNIST dataset... [INFO] creating unsupervised dataset... [INFO] building autoencoder... Epoch 1/20 2021-03-11 00:42:17.826156: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10 2021-03-11 00:42:17.981520: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7 171/171 [==============================] - 1s 7ms/step - loss: 0.0428 - val_loss: 0.0468 Epoch 2/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0076 - val_loss: 0.0347 Epoch 3/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0039 - val_loss: 0.0129 Epoch 4/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0033 - val_loss: 0.0035 Epoch 5/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0029 - val_loss: 0.0029 Epoch 6/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0027 - val_loss: 0.0028 Epoch 7/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0024 - val_loss: 0.0028 Epoch 8/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0023 - val_loss: 0.0028 Epoch 9/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0025 Epoch 10/20 171/171 [==============================] - 1s 4ms/step - loss: 0.0021 - val_loss: 0.0024 Epoch 11/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0020 - val_loss: 0.0024 Epoch 12/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0023 Epoch 13/20 171/171 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0024 Epoch 14/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0018 - val_loss: 0.0022 Epoch 15/20 171/171 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0023 Epoch 16/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0022 Epoch 17/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0017 - val_loss: 0.0022 Epoch 18/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0017 - val_loss: 0.0022 Epoch 19/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0021 Epoch 20/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0016 - val_loss: 0.0022 [INFO] making predictions... [INFO] saving image data... [INFO] saving autoencoder... Đồ thị Loss trong quá trình huấn luyện:  Loss trên 2 tập Training và Validation khá tương đồng, hầu như không có hiện tượng Overfitting.\nSo sánh Input Data và Output của model:  Sau khi có được Autoencoders model đã huấn luyện, ta thử đi tìm Abnormal/Outline Images. Code cho file find_anomalies.py như sau:\n# USAGE # python find_anomalies.py --dataset output/images.pickle --model output/autoencoder.model # import the necessary packages from tensorflow.keras.models import load_model import numpy as np import argparse import pickle import cv2 from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-d\u0026#34;, \u0026#34;--dataset\u0026#34;, type=str, required=True, help=\u0026#34;path to input image dataset file\u0026#34;) ap.add_argument(\u0026#34;-m\u0026#34;, \u0026#34;--model\u0026#34;, type=str, required=True, help=\u0026#34;path to trained autoencoder\u0026#34;) ap.add_argument(\u0026#34;-q\u0026#34;, \u0026#34;--quantile\u0026#34;, type=float, default=0.999, help=\u0026#34;q-th quantile used to identify outliers\u0026#34;) args = vars(ap.parse_args()) # load the model and image data from disk print(\u0026#34;[INFO] loading autoencoder and image data...\u0026#34;) autoencoder = load_model(args[\u0026#34;model\u0026#34;]) images = pickle.loads(open(args[\u0026#34;dataset\u0026#34;], \u0026#34;rb\u0026#34;).read()) # make predictions on our image data and initialize our list of reconstruction errors decoded = autoencoder.predict(images) errors = [] # loop over all original images and their corresponding reconstructions for (image, recon) in zip(images, decoded): # compute the mean squared error between the ground-truth image and the reconstructed image, then add it to our list of errors mse = np.mean((image - recon) ** 2) errors.append(mse) # compute the q-th quantile of the errors which serves as our threshold to identify anomalies -- any data point that our model reconstructed with \u0026gt; threshold error will be marked as an outlier thresh = np.quantile(errors, args[\u0026#34;quantile\u0026#34;]) idxs = np.where(np.array(errors) \u0026gt;= thresh)[0] print(\u0026#34;[INFO] mse threshold: {}\u0026#34;.format(thresh)) print(\u0026#34;[INFO] {} outliers found\u0026#34;.format(len(idxs))) # initialize the outputs array outputs = None # loop over the indexes of images with a high mean squared error term for i in idxs: # grab the original image and reconstructed image original = (images[i] * 255).astype(\u0026#34;uint8\u0026#34;) recon = (decoded[i] * 255).astype(\u0026#34;uint8\u0026#34;) # stack the original and reconstructed image side-by-side output = np.hstack([original, recon]) # if the outputs array is empty, initialize it as the current side-by-side image display if outputs is None: outputs = output # otherwise, vertically stack the outputs else: outputs = np.vstack([outputs, output]) # show the output visualization cv2.imshow(\u0026#34;Output\u0026#34;, outputs) cv2.waitKey(0) Để tính toán ngưỡng MSE cho việc phân loại dữ liệu là Abnormal/Outline Data hay không, chúng ta sử dụng Quantile với p = 0.999.\nThực hiện lệnh sau trên Terminal để tìm Abnorlmal/Ouline Images:\n$ python find_anomalies.py --dataset output/images.pickle --model output/autoencoder.model [INFO] loading autoencoder and image data... [INFO] mse threshold: 0.03480463531613351 [INFO] 7 outliers found   2 nhận xét:\n Mặc dù Autoencoders model chỉ được huấn luyện với 1% dữ liệu chứa ký tự 3 (*trên tống số *) nhưng nó đã tái hiện lại khá tốt khi đưa hình ảnh chứa số 3 vào. Những hình ảnh chứa ký tự 3 có giá trị MSE cao hơn ngưỡng và được xác định là Abnormal/Outline Data.  3. Kết luận\nTrong bài này, chúng ta đã tìm hiểu về Abnormal/Outline Data và cách xây dựng một Autoencoders model để giải quyết nó.\nTrong thực tế thì không có một phương pháp nào có thể phát hiện hoàn toàn Abnormal/Outline Data. Các bạn có thể tìm hiểu thêm một số phương pháp khác tại đây.\nToàn bộ Source Code của bài này, các bạn có thể xem tại github cá nhân của mình.\n4. Tham khảo\n Pyimagesearch  ","permalink":"https://tiensu.github.io/blog/50_autoencoders_detect_abnormal/","tags":["Deep Learning","Autoencoder"],"title":"Sử dụng Autoencoders model cho bài toán Abnormal/Outline Data Detection"},{"categories":["Deep Learning","Autoencoder"],"contents":"Ở bài trước, chúng ta đã tìm hiểu về Autoencoders model và một số ứng dụng của nó. Trong bài này, mình sẽ cũng các bạn sử dụng Autoencoders model để thực hiện việc giảm nhiễu cho dữ liệu hình ảnh. Việc này có thể áp dụng để tăng độ chính xác của hệ thống OCR bằng cách nâng cao chất lượng của ảnh đầu vào hệ thống.\n1. Denoising Autoencoders\n Về bản chất, Autoencoders model cho bài toán Denoising là sự mở rộng của Autoencoders model ở bài trước. Điểm khác biệt ở chỗ, Output của Decoder không phải là Input Data (có nhiễu) mà là Input Data (không có nhiễu).\nĐể chuẩn bị dữ liệu cho việc huấn luyện Autoencoders model phục vụ mục đích Denoising, ta có thể làm như sau:\n Chọn ra những dữ liệu (gọi là tập B) không có nhiễu từ tập dữ liệu đầy đủ ban đầu (tập A). Thêm nhiễu ngẫu nhiên vào tập B, tạo thành tập C. Huấn luyện Autoencoders model với Input Data là tập C, Output từ Decoder là tập B. Áp dụng Autoencoders model đã trên toàn bộ tập A.  2. Denoising Autoencoders với Keras và TensorFlow\nCấu trúc thư mục làm việc:\n$ tree --dirsfirst . ├── sunt │ ├── __init__.py │ └── conv_autoencoder.py ├── output.png ├── plot.png └── train_denoising_autoencoder.py Module sunt chứa lớp conv_autoencoder.py mà chúng ta đã sử dụng ở bài trước.\nTrọng tâm của bài này nằm ở file train_denoising_autoencoder.py. Sử dụng tập dữ liệu MNIST, chúng ta thêm nhiễu vào rồi huấn luyện Autoencoders model. Ở đây, nhiễu được tạo ra bằng cách sinh ngẫu nhiên dữ liệu theo phân phối chuẩn có điểm trung tâm là 0.5, độ lệch chuẩn là 0.5. Toàn bộ code của file này như sau:\n# USAGE # python train_denoising_autoencoder.py # set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use(\u0026#34;Agg\u0026#34;) from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) # import the necessary packages from sunt.conv_autoencoder import ConvAutoencoder from tensorflow.keras.optimizers import Adam from tensorflow.keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np import argparse import cv2 # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-s\u0026#34;, \u0026#34;--samples\u0026#34;, type=int, default=8, help=\u0026#34;# number of samples to visualize when decoding\u0026#34;) ap.add_argument(\u0026#34;-o\u0026#34;, \u0026#34;--output\u0026#34;, type=str, default=\u0026#34;output.png\u0026#34;, help=\u0026#34;path to output visualization file\u0026#34;) ap.add_argument(\u0026#34;-p\u0026#34;, \u0026#34;--plot\u0026#34;, type=str, default=\u0026#34;plot.png\u0026#34;, help=\u0026#34;path to output plot file\u0026#34;) args = vars(ap.parse_args()) # initialize the number of epochs to train for and batch size EPOCHS = 25 BS = 32 # load the MNIST dataset print(\u0026#34;[INFO] loading MNIST dataset...\u0026#34;) ((trainX, _), (testX, _)) = mnist.load_data() # add a channel dimension to every image in the dataset, then scale the pixel intensities to the range [0, 1] trainX = np.expand_dims(trainX, axis=-1) testX = np.expand_dims(testX, axis=-1) trainX = trainX.astype(\u0026#34;float32\u0026#34;) / 255.0 testX = testX.astype(\u0026#34;float32\u0026#34;) / 255.0 # sample noise from a random normal distribution centered at 0.5 (since our images lie in the range [0, 1]) and a standard deviation of 0.5 trainNoise = np.random.normal(loc=0.5, scale=0.5, size=trainX.shape) testNoise = np.random.normal(loc=0.5, scale=0.5, size=testX.shape) trainXNoisy = np.clip(trainX + trainNoise, 0, 1) testXNoisy = np.clip(testX + testNoise, 0, 1) # construct our convolutional autoencoder print(\u0026#34;[INFO] building autoencoder...\u0026#34;) (encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1) opt = Adam(lr=1e-3) autoencoder.compile(loss=\u0026#34;mse\u0026#34;, optimizer=opt) # train the convolutional autoencoder H = autoencoder.fit( trainXNoisy, trainX, validation_data=(testXNoisy, testX), epochs=EPOCHS, batch_size=BS) # construct a plot that plots and saves the training history N = np.arange(0, EPOCHS) plt.style.use(\u0026#34;ggplot\u0026#34;) plt.figure() plt.plot(N, H.history[\u0026#34;loss\u0026#34;], label=\u0026#34;train_loss\u0026#34;) plt.plot(N, H.history[\u0026#34;val_loss\u0026#34;], label=\u0026#34;val_loss\u0026#34;) plt.title(\u0026#34;Training Loss and Accuracy\u0026#34;) plt.xlabel(\u0026#34;Epoch #\u0026#34;) plt.ylabel(\u0026#34;Loss/Accuracy\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.savefig(args[\u0026#34;plot\u0026#34;]) # use the convolutional autoencoder to make predictions on the testing images, then initialize our list of output images print(\u0026#34;[INFO] making predictions...\u0026#34;) decoded = autoencoder.predict(testXNoisy) outputs = None # loop over our number of output samples for i in range(0, args[\u0026#34;samples\u0026#34;]): # grab the original image and reconstructed image original = (testXNoisy[i] * 255).astype(\u0026#34;uint8\u0026#34;) recon = (decoded[i] * 255).astype(\u0026#34;uint8\u0026#34;) # stack the original and reconstructed image side-by-side output = np.hstack([original, recon]) # if the outputs array is empty, initialize it as the current side-by-side image display if outputs is None: outputs = output # otherwise, vertically stack the outputs else: outputs = np.vstack([outputs, output]) # save the outputs image to disk cv2.imwrite(args[\u0026#34;output\u0026#34;], outputs) Thực hiện lệnh sau trên Terminer:\n$ python train_denoising_autoencoder.py 2021-03-10 23:09:12.350152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4932 MB memory) -\u0026gt; physical GPU (device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5) [INFO] loading MNIST dataset... [INFO] building autoencoder... Epoch 1/25 2021-03-10 23:09:14.814333: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10 2021-03-10 23:09:14.963101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0276 - val_loss: 0.0195 Epoch 2/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0187 - val_loss: 0.0213 Epoch 3/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0177 - val_loss: 0.0182 Epoch 4/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0172 - val_loss: 0.0181 Epoch 5/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0167 - val_loss: 0.0185 Epoch 6/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0164 - val_loss: 0.0167 Epoch 7/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0162 - val_loss: 0.0161 Epoch 8/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0160 - val_loss: 0.0162 Epoch 9/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0158 - val_loss: 0.0159 Epoch 10/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0156 - val_loss: 0.0166 Epoch 11/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0155 - val_loss: 0.0164 Epoch 12/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0154 - val_loss: 0.0157 Epoch 13/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0153 - val_loss: 0.0158 Epoch 14/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0152 - val_loss: 0.0167 Epoch 15/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0151 - val_loss: 0.0157 Epoch 16/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0150 - val_loss: 0.0155 Epoch 17/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0149 - val_loss: 0.0160 Epoch 18/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0149 - val_loss: 0.0174 Epoch 19/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0148 - val_loss: 0.0155 Epoch 20/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0147 - val_loss: 0.0155 Epoch 21/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0147 - val_loss: 0.0157 Epoch 22/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0146 - val_loss: 0.0154 Epoch 23/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0146 - val_loss: 0.0156 Epoch 24/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0145 - val_loss: 0.0156 Epoch 25/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0145 - val_loss: 0.0154 [INFO] making predictions... Đồ thị thể hiện quá trình huấn luyện:  Ta thấy Train Loss và Validation Loss đều giảm dần khi số lượng epochs tăng và không xảy ra hiện tượng Overfitting.\nẢnh mới sinh ra từ Autoencoders model so với ảnh đưa vào:  Bên trái là Input Data, còn bên phải là Output của Autoencoder model. Ta thấy rất rõ, nhiễu đã được khử gần như hoàn toàn.\n3. Kết luận\nTrong bài này, chúng ta đã cùng xây dựng một Autoencoders model để phục vụ mục đích giảm nhiễu của dữ liệu. Đây là một trong những ứng dụng rất hay của Autoencoders model, giúp nâng cao chất lượng dữ liệu trước khi đưa vào huấn luyện các mô hình DL khác, đặc biệt hữu ích trong bài toán OCR.\nToàn bộ Source Code của bài này, các bạn có thể tham khảo tại github cá nhân của mình tại đây.\nHẹn các bạn trong các bài viết tiếp theo.\n4. Tham khảo\n Pyimagesearch  ","permalink":"https://tiensu.github.io/blog/49_autoencoders_denoise/","tags":["Deep Learning","Autoencoder"],"title":"Sử dụng Autoencoders model cho bài toán Denoising Data"},{"categories":["Deep Learning","Autoencoder"],"contents":"Loạt bài tiếp theo mình sẽ viết về kiến trúc Autoencoders và một số ứng dụng của chúng.\nBài đầu tiên, chúng ta sẽ thảo luận Autoencoders là gì, nó có gì khác so với các Generative models khác (ví dụ: GAN), những ứng dụng của chúng, \u0026hellip; Mình cũng sẽ xây dựng một Autoencoders model đơn giản sử dụng Keras và Tensorflow.\n1. Autoencoders là gì?\nAutoencoders là một dạng của Unsupervised Neural Network. Hoạt động của nó được mô tả như sau:\n Nhận một tập dữ liệu đầu vào (input data). Chuyển đổi Input Data sang một dạng biểu diễn khác trong không gian tiềm ẩn (Latent Space). Tái hiện lại Input Data từ Latent Space Representation.  Xét về mặt cấu tạo, một Autoencoders model gồm 2 thành phần (subnetworks):\n Encoder: Nhận Input Data rồi chuyển nó sang dạng khác trong Latent Space. $s = E(x)$\n   Trong đó $x$ là Input Data, $E$ là Encoder, và $s$ là Output trong Latent Space.\n Decoder: Nhận Output của Encoder trong Latent Space, $s$, và tái hiện lại Input Data. $o = D(s)$\n   Trong đó, $o$ là Output của Decoder $D$.\nCông thức chung cho toàn bộ quá trình Autoencoder sẽ là: $o = D(E(x))$\n Kiến trúc tổng quát của Autoencoders được minh họa như sau:  Để huấn luyện Autoencoders model, chúng ta đưa cho nó Input Data, nó sẽ cố gắng tái hiện lại Input Data bằng cách tối thiểu hóa Mean Squared Error giữa Ouput của model và Input Data. Hay nói cách khác Input Data là nhãn của chính nó.\nMột Autoencoders model lý tưởng khi Output của nó giống hệt với Input Data.\nLiệu bạn có thắc mắc là tại sao chúng ta phải tạo ra Autoencoders model, đi một vòng chỉ để tái hiện lại Input Data? Sao ta không copy luôn Input Data ra là xong chuyện???\nMình cũng từng thắc mắc như bạn khi mới bắt đầu tìm hiểu về Autoencoders.\nNhưng bạn nên biết rằng, giá trị thực sự của Autoencoders model nằm ở Output của Encoder trong Latent Space. Hay nói cách khác, chúng ta (thường) chỉ quan tâm đến Encoder và Latent Space mà không quá quan tâm đến Decoder.\nMột ví dụ để bạn dễ hình dung hơn. Giả sử chúng ta có một tập ảnh, mỗi ảnh có kích thước 28x28x1, tức là chúng ta phải sử dụng 28x28x1 = 784 bytes để lưu mỗi ảnh. Sử dụng Autoencoders model, chúng ta chuyển đổi ảnh đó sang một vector nhỏ hơn, chỉ còn 16 bytes trong Latent Space. Sử dụng 16 bytes của vector này, chúng ta sau đó có thể tái hiện lại ảnh ban đầu giống đến 98%. Điều này giúp ta tiết kiệm được rất nhiều không gian lưu trữ, đặc biệt khi phải truyền dữ liệu đó qua môi trường mạng Internet. Đó chính là một trong những ứng dụng của Autoencoders.\n2. Ứng dụng của Autoencoders model\nKiến trúc tổng quát của Autoencoders được minh họa như sau:  Một số ứng dụng của Autoencoders trong lĩnh vực Computer Vision có thể kể đến như:\n Giảm chiểu dữ liệu (Dimensionality Reduction): Ứng dụng này giống như thuật toán PCA nhưng hiệu quả hơn. Giảm nhiễu dữ liệu (Denoising): Giảm nhiễu dữ liệu, một bước trong quá trình tiền xử lý ảnh, giúp nâng cao độ chính xác của hệ thống OCR. Phát hiện bất thường (Anomaly/outlier Detection): Phát hiện những điểm dữ liệu bất thường trong tập dataset, ví dụ như thiết nhãn, lệch ra khỏi phân phối chung của toàn dữ liệu. Một khi đã phát hiện được dữ liệu bất thường, ta có thể loại bỏ chúng hoặc huấn luyện lại model để tăng độ chính xác.  Trong lĩnh vực Natual Language Processing (NLP), Autoencoders model giúp giải quyết các bài toán:\n Tạo ra đoạn text miêu tả nội dung bức ảnh (Image Caption Generation) Tóm tắt nội dung đoạn văn (Text Summarization) Trích xuất đăc trưng (Word Embedding)  3. So sánh Autoencoders và Generative Adversarial Networks (GAN)\nNếu bạn biết về GAN, bạn có thể nhận thấy Autoencoders và GAN có sự tương đồng về cách làm việc. Cả 2 models đề thuộc dạng Generative.\n Autoencoders: Nhận Input Data, chuyển thành một vector có số chiều nhỏ hơn trong Latent Space. Sau đó, cố gắng tái hiện lại Input Data từ các vector trong Latent Space. GAN: Nhận Input Data có số chiều nhỏ, chuyển thành một vector có số chiều lớn hơn. Sau đó, sinh ra một dữ liệu mới từ vector này, đáp ứng một tiêu chí nào đó. Mình sẽ viết một series bài về GAN trong tương lai.  Chi tiết hơn về so sánh giữa Autoencoders và GAN, bạn có thể đọc tại đây.\n4. Xây dựng một Autoencoders model đơn giản với Keras và Tensorflow\nChúng ta sẽ thử cùng nhau huấn luyện một Autoencoders model trên tập dữ liệu MNIST.\nCấu trúc thư mục làm việc như sau:\n$ tree --dirsfirst . ├── sunt │ ├── __init__.py │ └── conv_autoencoder.py ├── output.png ├── plot.png └── train_conv_autoencoder.py  File conv_autoencoder.py: Chứa lớp ConvAutoencoder và phương thúc build để xây dựng kiến trúc mạng của Autoencoders model. File train_conv_autoencoder.py: Huấn luyện Autoencoders model trên tập MINIST.  Mở file conv_autoencoder.py và viết code như sau:\n# import the necessary packages from tensorflow.keras.layers import BatchNormalization from tensorflow.keras.layers import Conv2D from tensorflow.keras.layers import Conv2DTranspose from tensorflow.keras.layers import LeakyReLU from tensorflow.keras.layers import Activation from tensorflow.keras.layers import Flatten from tensorflow.keras.layers import Dense from tensorflow.keras.layers import Reshape from tensorflow.keras.layers import Input from tensorflow.keras.models import Model from tensorflow.keras import backend as K import numpy as np class ConvAutoencoder: @staticmethod def build(width, height, depth, filters=(32, 64), latentDim=16): # initialize the input shape to be \u0026#34;channels last\u0026#34; along with the channels dimension itself channels dimension itself inputShape = (height, width, depth) chanDim = -1 # define the input to the encoder inputs = Input(shape=inputShape) x = inputs # loop over the number of filters for f in filters: # apply a CONV =\u0026gt; RELU =\u0026gt; BN operation x = Conv2D(f, (3, 3), strides=2, padding=\u0026#34;same\u0026#34;)(x) x = LeakyReLU(alpha=0.2)(x) x = BatchNormalization(axis=chanDim)(x) # flatten the network and then construct our latent vector volumeSize = K.int_shape(x) x = Flatten()(x) latent = Dense(latentDim)(x) # build the encoder model encoder = Model(inputs, latent, name=\u0026#34;encoder\u0026#34;) print(encoder.summary()) # start building the decoder model which will accept the output of the encoder as its inputs latentInputs = Input(shape=(latentDim,)) x = Dense(np.prod(volumeSize[1:]))(latentInputs) x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x) # loop over our number of filters again, but this time in reverse order for f in filters[::-1]: # apply a CONV_TRANSPOSE =\u0026gt; RELU =\u0026gt; BN operation x = Conv2DTranspose(f, (3, 3), strides=2, padding=\u0026#34;same\u0026#34;)(x) x = LeakyReLU(alpha=0.2)(x) x = BatchNormalization(axis=chanDim)(x) # apply a single CONV_TRANSPOSE layer used to recover the original depth of the image x = Conv2DTranspose(depth, (3, 3), padding=\u0026#34;same\u0026#34;)(x) outputs = Activation(\u0026#34;sigmoid\u0026#34;)(x) # build the decoder model decoder = Model(latentInputs, outputs, name=\u0026#34;decoder\u0026#34;) print(decoder.summary()) # our autoencoder is the encoder + decoder autoencoder = Model(inputs, decoder(encoder(inputs)), name=\u0026#34;autoencoder\u0026#34;) print(autoencoder.summay()) # return a 3-tuple of the encoder, decoder, and autoencoder return (encoder, decoder, autoencoder) Mở file train_conv_autoencoder.py và viết code như sau:\n# USAGE # python train_conv_autoencoder.py # set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use(\u0026#34;Agg\u0026#34;) from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) # import the necessary packages from sunt.convautoencoder import ConvAutoencoder from tensorflow.keras.optimizers import Adam from tensorflow.keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np import argparse import cv2 # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-s\u0026#34;, \u0026#34;--samples\u0026#34;, type=int, default=8, help=\u0026#34;# number of samples to visualize when decoding\u0026#34;) ap.add_argument(\u0026#34;-o\u0026#34;, \u0026#34;--output\u0026#34;, type=str, default=\u0026#34;output.png\u0026#34;, help=\u0026#34;path to output visualization file\u0026#34;) ap.add_argument(\u0026#34;-p\u0026#34;, \u0026#34;--plot\u0026#34;, type=str, default=\u0026#34;plot.png\u0026#34;, help=\u0026#34;path to output plot file\u0026#34;) args = vars(ap.parse_args()) # initialize the number of epochs to train for and batch size EPOCHS = 25 BS = 32 # load the MNIST dataset print(\u0026#34;[INFO] loading MNIST dataset...\u0026#34;) ((trainX, _), (testX, _)) = mnist.load_data() # add a channel dimension to every image in the dataset, then scale # the pixel intensities to the range [0, 1] trainX = np.expand_dims(trainX, axis=-1) testX = np.expand_dims(testX, axis=-1) trainX = trainX.astype(\u0026#34;float32\u0026#34;) / 255.0 testX = testX.astype(\u0026#34;float32\u0026#34;) / 255.0 # construct our convolutional autoencoder print(\u0026#34;[INFO] building autoencoder...\u0026#34;) (encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1) opt = Adam(lr=1e-3) autoencoder.compile(loss=\u0026#34;mse\u0026#34;, optimizer=opt) # train the convolutional autoencoder H = autoencoder.fit( trainX, trainX, validation_data=(testX, testX), epochs=EPOCHS, batch_size=BS) # construct a plot that plots and saves the training history N = np.arange(0, EPOCHS) plt.style.use(\u0026#34;ggplot\u0026#34;) plt.figure() plt.plot(N, H.history[\u0026#34;loss\u0026#34;], label=\u0026#34;train_loss\u0026#34;) plt.plot(N, H.history[\u0026#34;val_loss\u0026#34;], label=\u0026#34;val_loss\u0026#34;) plt.title(\u0026#34;Training Loss and Accuracy\u0026#34;) plt.xlabel(\u0026#34;Epoch #\u0026#34;) plt.ylabel(\u0026#34;Loss/Accuracy\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.savefig(args[\u0026#34;plot\u0026#34;]) # use the convolutional autoencoder to make predictions on the # testing images, then initialize our list of output images print(\u0026#34;[INFO] making predictions...\u0026#34;) decoded = autoencoder.predict(testX) outputs = None # loop over our number of output samples for i in range(0, args[\u0026#34;samples\u0026#34;]): # grab the original image and reconstructed image original = (testX[i] * 255).astype(\u0026#34;uint8\u0026#34;) recon = (decoded[i] * 255).astype(\u0026#34;uint8\u0026#34;) # stack the original and reconstructed image side-by-side output = np.hstack([original, recon]) # if the outputs array is empty, initialize it as the current # side-by-side image display if outputs is None: outputs = output # otherwise, vertically stack the outputs else: outputs = np.vstack([outputs, output]) # save the outputs image to disk cv2.imwrite(args[\u0026#34;output\u0026#34;], outputs) Trong code đã có đầy đủ comments, hi vọng các bạn có thể hiểu được.\nTiến hành chạy code (mình dùng python 3.8, tensorflow 2.3.0 trong môi trường ảo anaconda):\n$ python train_conv_autoencoder.py Output:\n Kiến trúc của Encoder:  Model: \u0026#34;encoder\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param #  ================================================================= input_1 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ conv2d (Conv2D) (None, 14, 14, 32) 320 _________________________________________________________________ leaky_re_lu (LeakyReLU) (None, 14, 14, 32) 0 _________________________________________________________________ batch_normalization (BatchNo (None, 14, 14, 32) 128 _________________________________________________________________ conv2d_1 (Conv2D) (None, 7, 7, 64) 18496 _________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 7, 7, 64) 0 _________________________________________________________________ batch_normalization_1 (Batch (None, 7, 7, 64) 256 _________________________________________________________________ flatten (Flatten) (None, 3136) 0 _________________________________________________________________ dense (Dense) (None, 16) 50192 ================================================================= Total params: 69,392 Trainable params: 69,200 Non-trainable params: 192 Ta thấy từ kiến trúc trên, Input Data ban đầu là 28x28x1 = 784 bytes, sau khi chuyển sang Latent Space, dữ liệu chỉ còn là vector 16 bytes.\n Kiến trúc của Decoder:  Model: \u0026#34;decoder\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param #  ================================================================= input_2 (InputLayer) [(None, 16)] 0 _________________________________________________________________ dense_1 (Dense) (None, 3136) 53312 _________________________________________________________________ reshape (Reshape) (None, 7, 7, 64) 0 _________________________________________________________________ conv2d_transpose (Conv2DTran (None, 14, 14, 64) 36928 _________________________________________________________________ leaky_re_lu_2 (LeakyReLU) (None, 14, 14, 64) 0 _________________________________________________________________ batch_normalization_2 (Batch (None, 14, 14, 64) 256 _________________________________________________________________ conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32) 18464 _________________________________________________________________ leaky_re_lu_3 (LeakyReLU) (None, 28, 28, 32) 0 _________________________________________________________________ batch_normalization_3 (Batch (None, 28, 28, 32) 128 _________________________________________________________________ conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1) 289 _________________________________________________________________ activation (Activation) (None, 28, 28, 1) 0 ================================================================= Total params: 109,377 Trainable params: 109,185 Non-trainable params: 192 Ngược lại với Encoder, từ vector 16 bytes trong Latent Space, Decoder tái hiện lại Input Data với 28x28x1 = 784 bytes.\n Kiến trúc của Autoencoder:  Model: \u0026#34;autoencoder\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param #  ================================================================= input_1 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ encoder (Functional) (None, 16) 69392 _________________________________________________________________ decoder (Functional) (None, 28, 28, 1) 109377 ================================================================= Total params: 178,769 Trainable params: 178,385 Non-trainable params: 384  Log train:  Epoch 1/25 2021-03-10 23:17:48.593945: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10 2021-03-10 23:17:48.741732: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0191 - val_loss: 0.0113 Epoch 2/25 1875/1875 [==============================] - 10s 5ms/step - loss: 0.0104 - val_loss: 0.0097 Epoch 3/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0094 - val_loss: 0.0087 Epoch 4/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0088 - val_loss: 0.0083 Epoch 5/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0084 - val_loss: 0.0081 Epoch 6/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0081 - val_loss: 0.0081 Epoch 7/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0079 - val_loss: 0.0077 Epoch 8/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0077 - val_loss: 0.0076 Epoch 9/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0076 - val_loss: 0.0078 Epoch 10/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0074 - val_loss: 0.0074 Epoch 11/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0073 - val_loss: 0.0073 Epoch 12/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0072 - val_loss: 0.0072 Epoch 13/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0071 - val_loss: 0.0073 Epoch 14/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0070 - val_loss: 0.0071 Epoch 15/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0070 - val_loss: 0.0071 Epoch 16/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0069 - val_loss: 0.0070 Epoch 17/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0068 - val_loss: 0.0069 Epoch 18/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0068 - val_loss: 0.0069 Epoch 19/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0067 - val_loss: 0.0069 Epoch 20/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0067 - val_loss: 0.0070 Epoch 21/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0066 - val_loss: 0.0069 Epoch 22/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0066 - val_loss: 0.0069 Epoch 23/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0066 - val_loss: 0.0068 Epoch 24/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0065 - val_loss: 0.0068 Epoch 25/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0065 - val_loss: 0.0067 [INFO] making predictions...  Đồ thị quá trình huấn luyện:    Ta thấy Train Loss và Validation Loss đều giảm dần khi số lượng epochs tăng và không xảy ra hiện tượng Overfitting.\n Output code Autoencoder model, so với Input Data:    Bên trái là Input Data, còn bên phải là Output của Autoencoder model. Gần như không có sự khác biệt dữ 2 bên.\n5. Kết luận\nTrong bài này, chúng ta đã cùng tìm hiểu về Autoencoders, cấu tạo, cách hoạt động, ứng dụng, cũng như sự khác nhau của nó so với GAN. Chúng ta cũng đã huấn luyện một Autoencoders đơn giản trên tập MNIST.\nToàn bộ source code của bài này, các bạn có thể tham khảo tại github cá nhân của mình tại đây.\nHẹn các bạn trong các bài viết tiếp theo.\n6. Tham khảo\n Pyimagesearch  ","permalink":"https://tiensu.github.io/blog/48_autoencoders_introduction/","tags":["Deep Learning","Autoencoder"],"title":"Autoencoder với Keras, Tensorflow và Deep Learning"},{"categories":["MLOps","Data Driff"],"contents":"Sau một thời gian nghỉ tết thì hôm nay mình đã trở lại. Trong bài viết mình sẽ cùng các bạn làm một ví dụ nhỏ về Data Driff để các bạn hiểu rõ hơn về nó. Cá nhận mình đánh giá, đây là một trong những vấn đề quan trọng nhất để giữ cho AI model chạy ổn định trong thực tế. Hãy xem lại bài này nếu bạn chưa biết về Data Driff.\n1. Ví dụ Giả sử chúng ta muốn dự đoán chất lượng của rượu tại một cửa hàng chuyên bán rượu, để quyết định xem có nên mua chai rượu đó hay không?\nChúng ta sẽ sử dụng UCI Wine Quality dataset để xây dựng một ML model dự đoán. Mỗi loại rượu có tất cả 12 features: type, fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, và alcohol rate. Nhãn là quality score có giá trị từ 0 đến 10.\nImport các thư viện sử dụng: Output:  Đọc và kiểm tra dữ liệu:  Thống kê dữ liệu:  Kiểm tra xem data có chứa giá trị NULL hay không?  Kiểm tra xem dữ liệu có bị trùng lặp hay không? Nếu có thì xóa bỏ những dữ liệu bị trùng đó.  Kiểm tra sự tương quan (liên hệ) giữa các features đôi một.  Từ đây, ta có thể loại bợt những features mà không có sự liên hệ nhiều đến nhãn (hệ số corr giữa feature đó và nhãn nhỏ).\nĐể đơn giản hóa model, chúng ta sẽ model hóa bài toán thành dạng binary classification. Cụ thể, rượu được coi là ngon khi quality score có giá trị lớn hơn 6 và ngược lại, rượu có quality score nhỏ hơn hoặc bằng 6 được coi là không ngon.  Kiểm tra sự phân phối dữ liệu giữa 2 nhãn.  Ta có thể thấy số lượng dữ liệu phân phối khá đồng đều giữa 2 nhãn. Điều này là cần thiết để tránh việc bias dữ liệu.\nĐể minh họa hiện tượng Data Driff, chúng ta chia tập dữ liệu thành 2 phần:\n Phần 1, chứa tất cả rượu có giá trị của alcohol rate lớn hơn 11%. Phần 2, chứa tất cả rượu có giá trị của alcohol rate nhỏ hơn hoặc bằng 11%.    Với việc phân chia như thế này, rõ ràng là dữ liệu ở phần 2 đã xảy ra hiện tượng Data Driff so với dữ liệu ở phần 1, cụ thể là ở feature alcohol.\nToàn bộ dữ liệu ở phần 1 sẽ được sử dụng để train và test model. Ở đây, mình không thực hiện việc tuning model mà chỉ xây dựng model đơn giản để minh họa ảnh hưởng của Data Driff.\nTách phần 1 thành 2 phần: features và labels. Sau đó lại chia mỗi phần đó thành 2 phần train và test theo tỉ lệ 80:20.  Thử kiểm tra sự phân bố dữ liệu giữa:\n Tập train và test của phần 1:   Tập train và phần 2:    Từ đồ thị phân bố có thể quan sát rõ ràng hiện tượng Data Driff khi mà feature alcohol của tập train và phần 2 nằm về 2 phía của giá trị 11. Tập train và test của phần 1 không có hiện tượng này.\nTiến hành tạo model và huấn luyện trên tập train:  Đánh giá model trên tập test:  Đánh giá model trên phần 2. Chúng ta dự đoán rằng, kết quả test trên 20% của phần 1 sẽ lớn hơn trên toàn bộ phần 2, vì hiện tượng Data Driff xảy ra ở phần 2 so với phần 1.  Ở đây, chúng ta sử dụng 3 metrics để đánh giá: accuracy score, f1 score và confusion matrix. Kết quả đánh giá chỉ ra, giá trị của các metrics trên phần 2 nhỏ hơn rất nhiều so với trên tập test, đúng như dự đoán ban đầu của chúng ta.\n2. Kết luận\nNhư vậy là chúng ta đã cùng nhau làm 1 ví dụ về hiện tượng Data Driff, một trong những vấn đề rất quan trọng của quá trình triển khai AI/ML model trong thực tế. Hi vọng là các bạn có cái nhiều sâu sắc hơn về nó thông qua bài này.\nToàn bộ source code của bài này, các bạn có thể tham khảo tại github cá nhân của mình tại đây.\nHẹn các bạn trong các bài viết tiếp theo.\n","permalink":"https://tiensu.github.io/blog/47_a_example_data_driff/","tags":["MLOps","Data Driff"],"title":"Một ví dụ về hiện tượng Data Driff trong Machine Learning"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Đây là bài viết cuối cùng về Kubernetes trên local. Bài sau (nếu có) thì sẽ là hướng dẫn cấu hình Kubernetes trên cloud.\nTrong bài này, chúng ta sẽ cùng tìm hiểu về Kubernetes Serice và áp dụng chúng vào bài toán AI.\n1. Kubernetes Service là gì?\nỞ bài trước, chúng ta đã biết rằng mặc dù Development rất hiệu quả trong việc giải quyết tác vụ Online Inference, nhưng nó có một nhược điểm là REST API chỉ có tác dụng trong phạm vi Cluster, không thể kết nối ra ngoài. Service chính là giải pháp để giải quyết cho vấn đề đó.\nService cung cấp một Stable Virtual IP (VIP) với mục đích forward dữ liệu đến tới các Pods. Một tiến trình kube-poluxy chịu trách nhiệm ánh xạ giữa VIP và các Pods (vì địa chỉ của các Pods luôn thay đổi).\n2. Làm việc với Kubernetes Service\nChúng ta sẽ sử dụng lại cấu hình của Deployment trong bài trước để tạo REST API cho tác vụ Online Inference trong bài toán AI. Sau đó, sử dụng Service để mở các REST API đó ra bên ngoài.\n2.1 Tạo Kubernetes Deployment\nChạy các lệnh sau để tạo và kiểm tra trạng thái của Deployment, Pods:\n$ kubectl create -f development-online-inference.yaml deployment.apps/online-inference-development created $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-world 5/5 5 5 37m online-inference-development 2/2 2 2 2m11s $ kubectl get pods NAME READY STATUS RESTARTS AGE online-inference-development-5d46c5c7dc-hrkrn 1/1 Running 0 12m online-inference-development-5d46c5c7dc-r4qfz 1/1 Running 0 12m 2.2 Tạo Kubernetes Service\nSử dụng lệnh sau để tạo Service:\n$ kubectl expose deployment online-inference-development --type NodePort --name online-inference-service service/online-inference-service exposed Một số thông tin:\n type: Loại Service. Ở đây sử dụng NodePort để mở REST API thông qua Port của các Worker Node. Chi tiết về các loại Type, tham khảo tại đây. name: Tên của Service đuọc tạo ra.  Kiểm tra Service vừa tạo:\n$ NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 72m online-inference-service NodePort 10.100.53.16 \u0026lt;none\u0026gt; 5000:31412/TCP 3m26s Chú ý đến 2 thông tin: EXTERNAL-IP và PORT(S). Đây là 2 thông tin để cho các yêu cầu đến truy cập vào trong các Pods.\n Giá trị \u0026lt;none\u0026gt; của EXTERNAL-IP được ngầm hiểu là IP của Worker Node, vì chúng ta đã chọn TYPE của Service là NodePort. Giá trị 5000:31412/TCP của PORT(S) có nghĩa là yêu cầu từ bên ngoài Cluster gửi đến Port 31412 của Worker Node sẽ được chuyển tiếp đến Port 5000 của các Pods. TCP là giao thức trao đổi dữ liệu.  Xem đầy đủ thông tin của Service:\n$ kubectl describe services online-inference-service Name: online-inference-service Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=model-api Type: NodePort IP Families: \u0026lt;none\u0026gt; IP: 10.100.53.16 IPs: 10.100.53.16 Port: \u0026lt;unset\u0026gt; 5000/TCP TargetPort: 5000/TCP NodePort: \u0026lt;unset\u0026gt; 31412/TCP Endpoints: 192.168.24.227:5000,192.168.24.228:5000 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; Ta có thể nhìn thấy 2 IP:PORT của 2 Pods là: 192.168.24.227:5000 và 192.168.24.228:5000\n2.3 Thực hiện Online Inference\nĐã cấu hình xong Service, chúng ta thử gửi một yêu cầu dự đoán từ bên ngoài Cluster xem sao:\n$ curl -i -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;CRIM\u0026#34;: 15.02, \u0026#34;ZN\u0026#34;: 0.0, \u0026#34;INDUS\u0026#34;: 18.1, \u0026#34;CHAS\u0026#34;: 0.0, \u0026#34;NOX\u0026#34;: 0.614, \u0026#34;RM\u0026#34;: 5.3, \u0026#34;AGE\u0026#34;: 97.3, \u0026#34;DIS\u0026#34;: 2.1, \u0026#34;RAD\u0026#34;: 24.0, \u0026#34;TAX\u0026#34;: 666.0, \u0026#34;PTRATIO\u0026#34;: 20.2, \u0026#34;B\u0026#34;: 349.48, \u0026#34;LSTAT\u0026#34;: 24.9}\u0026#39; 10.1.30.130:31412/predict HTTP/1.0 200 OK Content-Type: application/json Content-Length: 41 Server: Werkzeug/1.0.1 Python/3.8.6 Date: Tue, 02 Feb 2021 11:06:49 GMT { \u0026#34;prediction\u0026#34;: 12.273424794987877 } Có kết quả trả về, tức là chúng ta đã thành công, :)).\n3. Kết luận\nĐây là bài viết cuối cùng trong năm Canh Tý của mình. Xong bài này mình sẽ về quê đón tết cùng gia đình.\nBài viết đầu tiên trong năm Nhâm Sửu mình sẽ hướng dẫn các bạn cách nhận biết hiện tượng Data Driff, một vấn đề mà theo mình rất quan trong việc giải quyết các bài toán AI thực tế. Mời các bạn đón đọc!\nKính chúc mọi người năm mới AN KHANG THỊNH VƯỢNG!!!\n8. Tham khảo\n Mlinproduction Kubernetes Service  ","permalink":"https://tiensu.github.io/blog/46_kubernetes_services/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 5: Kubernetes Service"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Trong bài toán AI, nếu như Job và CronJob phù hợp nhất cho các tác vụ thực hiện không liên tục, không realtime (VD: Batch Inference, Training, \u0026hellip;) thì Deployment lại là lựa chọn tốt nhất cho các tác vụ cần chạy liên tục, realtime (VD: Online Inference, \u0026hellip;). Trong bài này, hãy cùng tìm hiểu về Deployment và cách sử dụng nó.\n1. Kubernetes Deployment là gì?\nDeployment có thể hiểu là một tập các Pods giống nhau chạy trên một Kubernetes Cluster. Giống như Job, nó cũng quản lý các Pods trong việc thực hiện một nhiệm vụ nào đó. Sự khác nhau giữa Job và Deployment ở tính chất nhiệm vụ mà chúng thực hiện. Đối với Job, các tasks của nó chỉ chạy một lần, sau đó kết thúc luôn. Ngược lại, các tasks của Deployment chạy liên tục từ lúc được khởi tạo và chỉ kết thúc khi có sự can thiệp của người quản trị hoặc một ngoại lệ bất thường.\nMột số đặc điểm trong cách quản lý Pod của Deployment:\n Trong quá trình làm việc, nếu một Pod bị chết, Deployment sẽ tạo ra một Pod khác thay thế. Deployment cũng có khả năng tự động scale up/down số lượng các Pods tùy thuộc vào mức độ nặng/nhẹ của công viêc mà nó thực hiện. Có thể thay đổi cấu hình của Deployment trực tiếp trong file cấu hình mà không phải downtime. Có thể quay lại những thay đổi trước đó trong trường hợp sự thay đổi mới gây ra lỗi.  Chính vì vậy mà Deployment rất phù hợp với nhiệm vụ Online Inference trong bài toán AI. Chúng ta train một model, tạo một REST API để lắng nghe các yêu cầu dự đoán. Sau đó, tạo ra một Deployment để chấp nhận và thực hiện các yêu cầu đó một các realtime. Nếu số lượng các yêu cầu tăng lên cao, Deployment sẽ tự động tạo thêm các Pod để xử lý và ngược lại. Nếu có một phiên bản mới của model, ta có thể dễ dàng đưa luôn vào sử dụng mà không phải downtime. Và nếu model mới đó không hiệu quả bằng model cũ, ta hoàn toàn có thể quay về sử dụng model cũ đó.\n2. Làm việc với Kubernetes Deployment\nChúng ta sẽ thực hiện tạo một Deployment để phục vụ nhiệm vụ Online Inference trong bài toán AI.\nHãy xem cấu trúc thư mục làm việc:\nkubernetes_deployment │ ├── deployment │ │ └── deployment-online-inference.yaml │ └── docker │ ├── api.py │ ├── Dockerfile │ └── train.py 2.1 Train model AI và tạo REST API\nTạo thư mục docker và hai file code python bên trong nó:\n File train.py: Train model AI và lưu file model. File api.py: Tạo API để cho phép yêu cầu dự đoán gửi đến và trả về kết quả.  Nội dung của file train.py như sau:\nimport json import os from joblib import dump import matplotlib.pyplot as plt import numpy as np from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Load data print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() print(\u0026#34;Splitting data...\u0026#34;) X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset]\tX_test, y_test = X[offset:], y[offset:] # ############################################################################# # Fit regression model print(\u0026#34;Fitting model...\u0026#34;) params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) train_mse = mean_squared_error(y_train, clf.predict(X_train)) test_mse = mean_squared_error(y_test, clf.predict(X_test)) metadata = { \u0026#34;train_mean_square_error\u0026#34;: train_mse, \u0026#34;test_mean_square_error\u0026#34;: test_mse } print(\u0026#34;Serializing model to: {}\u0026#34;.format(MODEL_PATH)) dump(clf, MODEL_PATH) print(\u0026#34;Serializing metadata to: {}\u0026#34;.format(METADATA_PATH)) with open(METADATA_PATH, \u0026#39;w\u0026#39;) as outfile: json.dump(metadata, outfile) Nội dung của file api.py như sau:\nimport os from flask import Flask from flask_restful import Resource, Api, reqparse from joblib import load import numpy as np MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) print(\u0026#34;Loading model from: {}\u0026#34;.format(MODEL_PATH)) clf = load(MODEL_PATH) app = Flask(__name__) api = Api(app) class Prediction(Resource): def __init__(self): self._required_features = [\u0026#39;CRIM\u0026#39;, \u0026#39;ZN\u0026#39;, \u0026#39;INDUS\u0026#39;, \u0026#39;CHAS\u0026#39;, \u0026#39;NOX\u0026#39;, \u0026#39;RM\u0026#39;, \u0026#39;AGE\u0026#39;, \u0026#39;DIS\u0026#39;, \u0026#39;RAD\u0026#39;, \u0026#39;TAX\u0026#39;, \u0026#39;PTRATIO\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;LSTAT\u0026#39;] self.reqparse = reqparse.RequestParser() for feature in self._required_features: self.reqparse.add_argument( feature, type = float, required = True, location = \u0026#39;json\u0026#39;, help = \u0026#39;No {} provided\u0026#39;.format(feature)) super(Prediction, self).__init__() def post(self): args = self.reqparse.parse_args() X = np.array([args[f] for f in self._required_features]).reshape(1, -1) y_pred = clf.predict(X) return {\u0026#39;prediction\u0026#39;: y_pred.tolist()[0]} api.add_resource(Prediction, \u0026#39;/predict\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True, host=\u0026#39;0.0.0.0\u0026#39;) Code của 2 files này khá đơn giản nên mình không giải thích gì thêm, hi vọng các bạn có thể tự hiểu được.\n2.2 Chuẩn bị Docker Image\nCũng trong thư mục docker, ta file Dockerfile như sau:\nFROM jupyter/scipy-notebook USER root WORKDIR /docker ADD . /docker RUN pip install flask flask-restful joblib RUN mkdir /docker/model ENV MODEL_DIR=/docker/model ENV MODEL_FILE=clf.joblib ENV METADATA_FILE=metadata.json RUN python3 train.py Sau đó tiến hành build Docker Image:\n$ docker build -t docker-ml-online . Sending build context to Docker daemon 6.656kB Step 1/10 : FROM jupyter/scipy-notebook ---\u0026gt; c1a7c7ef5e27 Step 2/10 : USER root ---\u0026gt; Using cache ---\u0026gt; 0d9f55e9c7e0 Step 3/10 : WORKDIR /docker ---\u0026gt; Using cache ---\u0026gt; 4ed21d81d110 Step 4/10 : ADD . /docker ---\u0026gt; a266bfc5ca35 Step 5/10 : RUN pip install flask flask-restful joblib ---\u0026gt; Running in 97888ed0b989 Collecting flask Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB) Collecting flask-restful Downloading Flask_RESTful-0.3.8-py2.py3-none-any.whl (25 kB) Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (1.0.0) Requirement already satisfied: click\u0026gt;=5.1 in /opt/conda/lib/python3.8/site-packages (from flask) (7.1.2) Collecting Werkzeug\u0026gt;=0.15 Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB) Requirement already satisfied: Jinja2\u0026gt;=2.10.1 in /opt/conda/lib/python3.8/site-packages (from flask) (2.11.2) Collecting itsdangerous\u0026gt;=0.24 Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB) Requirement already satisfied: MarkupSafe\u0026gt;=0.23 in /opt/conda/lib/python3.8/site-packages (from Jinja2\u0026gt;=2.10.1-\u0026gt;flask) (1.1.1) Requirement already satisfied: six\u0026gt;=1.3.0 in /opt/conda/lib/python3.8/site-packages (from flask-restful) (1.15.0) Requirement already satisfied: pytz in /opt/conda/lib/python3.8/site-packages (from flask-restful) (2020.5) Collecting aniso8601\u0026gt;=0.82 Downloading aniso8601-8.1.1-py2.py3-none-any.whl (44 kB) Installing collected packages: Werkzeug, itsdangerous, flask, aniso8601, flask-restful Successfully installed Werkzeug-1.0.1 aniso8601-8.1.1 flask-1.1.2 flask-restful-0.3.8 itsdangerous-1.1.0 Removing intermediate container 97888ed0b989 ---\u0026gt; d9f31d7e7c83 Step 6/10 : RUN mkdir /docker/model ---\u0026gt; Running in 89b237f6427c Removing intermediate container 89b237f6427c ---\u0026gt; b2778ed90f4a Step 7/10 : ENV MODEL_DIR=/docker/model ---\u0026gt; Running in d7a52c9249f9 Removing intermediate container d7a52c9249f9 ---\u0026gt; 5157d919abd5 Step 8/10 : ENV MODEL_FILE=clf.joblib ---\u0026gt; Running in a7f75c6f79e5 Removing intermediate container a7f75c6f79e5 ---\u0026gt; 790a21e54588 Step 9/10 : ENV METADATA_FILE=metadata.json ---\u0026gt; Running in b0b94567182c Removing intermediate container b0b94567182c ---\u0026gt; 92a98ce95a8d Step 10/10 : RUN python3 train.py ---\u0026gt; Running in d8055e4ef00d Loading data... Splitting data... Fitting model... Serializing model to: /docker/model/clf.joblib Serializing metadata to: /docker/model/metadata.json Removing intermediate container d8055e4ef00d ---\u0026gt; b1fb95b775ec Successfully built b1fb95b775ec Successfully tagged docker-ml-online:latest Có Docker Image rồi, tiến hành push nó lên Docker Hub:\n$ docker push tiensu/ml-model-online-infer:latest The push refers to repository [docker.io/tiensu/ml-model-online-infer] f0e40a44cb9c: Pushed a079ef4fd38e: Pushed 76cba4a3a958: Pushed 3451a539eae2: Pushed 66f4cc63b50c: Mounted from tiensu/docker-ml 5f70bf18a086: Mounted from tiensu/docker-ml 6f5a41ae77fd: Mounted from tiensu/docker-ml 5a1b9a3f9355: Mounted from tiensu/docker-ml b1d7816bac14: Mounted from tiensu/docker-ml c91fed2d1998: Mounted from tiensu/docker-ml cc70098d00e3: Mounted from tiensu/docker-ml 88727e93cbac: Mounted from tiensu/docker-ml cadaf24035f3: Mounted from tiensu/docker-ml 8f170f4774e3: Mounted from tiensu/docker-ml 33bd52db887f: Mounted from tiensu/docker-ml 21e5dd010f50: Mounted from tiensu/docker-ml ea370ab22368: Mounted from tiensu/docker-ml 421d1408f872: Mounted from tiensu/docker-ml 18fd1ca0de51: Mounted from tiensu/docker-ml 8f01aab6d756: Mounted from tiensu/docker-ml e18a1c4e1d31: Mounted from tiensu/docker-ml 8552f27c3cd8: Mounted from tiensu/docker-ml 1a4c57efcc23: Mounted from tiensu/docker-ml 94b8fe888eac: Mounted from tiensu/docker-ml 02473afd360b: Mounted from tiensu/docker-ml dbf2c0f42a39: Mounted from tiensu/docker-ml 9f32931c9d28: Mounted from tiensu/docker-ml latest: digest: sha256:67c219ed32f9748c0c3ce64e8c4274932a8dadaf05510402f5d64a038bca2165 size: 6790 2.3 Tạo Kubernetes Deployment\nTrong thư mục deployment, tạo file cấu hình (deployment-online-inference.yaml) của Deployment với nội dung như sau:\napiVersion: apps/v1 kind: Deployment metadata: name: online-inference-deployment spec: replicas: 2 selector: matchLabels: app: model-api template: metadata: labels: app: model-api spec: containers: - name: model-api imagePullPolicy: Always image: tiensu/ml-model-online-infer:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;api.py\u0026#34;] ports: - containerPort: 5000 Một số thông tin cần lưu ý ở đây:\n replicas: Số lượng Pods được tạo ra lúc ban đầu. selector: Định nghĩa tên của Pods/Containers mà nó quản lý.  Chạy các lệnh sau để tạo và kiểm tra Deployment:\n$ kubectl create -f deployment-online-inference.yaml deployment.apps/online-inference-deployment created $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE online-inference-deployment 2/2 2 2 41s Thực ra, Deployment không trực tiếp quản lý các Pods. Thay vào đó, nó sẽ tạo ra các ReplicaSet với mục đích duy trì sự ổn định của các Pods tại bất kì thời điểm nào trong suốt quá trình hoạt động.\nKiểm tra ReplicaSet được Deployment tạo ra:\n$ kubectl get rs NAME DESIRED CURRENT READY AGE online-inference-deployment-59c8579f48 2 2 2 68s Chú ý: Tên của ReplicaSet = Tên của Deployment + chuỗi ngẫu nhiên.\nKiểm tra thử các Pods được quản lý bởi online-inference-deployment-59c8579f48 ReplicaSet:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE online-inference-deployment-59c8579f48-bg2vj 1/1 Running 0 97s online-inference-deployment-59c8579f48-j9fl2 1/1 Running 0 97s Chú ý: Tên của Pod = Tên của ReplicaSet + chuỗi ngẫu nhiên.\nThử debug một Pod xem có gì bất thường không?\n$ kubectl describe pod online-inference-deployment-59c8579f48-bg2vj Name: online-inference-deployment-59c8579f48-bg2vj Namespace: default Priority: 0 Node: duynm-vostro-3670/10.1.30.130 Start Time: Mon, 01 Feb 2021 18:15:27 +0700 Labels: app=model-api pod-template-hash=59c8579f48 Annotations: cni.projectcalico.org/podIP: 192.168.24.197/32 cni.projectcalico.org/podIPs: 192.168.24.197/32 Status: Running IP: 192.168.24.197 IPs: IP: 192.168.24.197 Controlled By: ReplicaSet/online-inference-deployment-59c8579f48 Containers: model-api: Container ID: docker://4cde562c962b48ff4c6bc3c812b140d2555e1984f064108bd8bf607b122cef9a Image: tiensu/ml-model-online-infer Image ID: docker-pullable://tiensu/ml-model-online-infer@sha256:67c219ed32f9748c0c3ce64e8c4274932a8dadaf05510402f5d64a038bca2165 Port: 5000/TCP Host Port: 0/TCP Command: python3 api.py State: Running Started: Mon, 01 Feb 2021 18:15:45 +0700 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-wp4xr (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-wp4xr: Type: Secret (a volume populated by a Secret) SecretName: default-token-wp4xr Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m25s default-scheduler Successfully assigned default/online-inference-deployment-59c8579f48-bg2vj to duynm-vostro-3670 Normal Pulling 2m22s kubelet Pulling image \u0026#34;tiensu/ml-model-online-infer\u0026#34; Normal Pulled 2m8s kubelet Successfully pulled image \u0026#34;tiensu/ml-model-online-infer\u0026#34; in 14.038005704s Normal Created 2m7s kubelet Created container model-api Normal Started 2m7s kubelet Started container model-api OK, mọi thứ đều đang hoạt động đúng như mong muốn.\nTa cũng có thể xem logs của Pod khi chạy:\n$ kubectl logs -f online-inference-development-5d46c5c7dc-bg2vj Loading model from: /docker/model/clf.joblib * Serving Flask app \u0026#34;api\u0026#34; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 263-920-719 10.1.30.130 - - [02/Feb/2021 11:06:49] \u0026#34;POST /predict HTTP/1.1\u0026#34; 200 - Tham số -f dùng để xem log một các realtime.\n2.4 Chạy Online Inference\nBây giờ ta sẽ thử gửi một yêu cầu dự đoán thông qua REST API để xem kết quả trả về. Tuy nhiên, có một chú ý quan trọng là REST API này chỉ mới hoạt động được bên trong phạm vi của Kubernetes Cluster. Để mở rộng nó ra ngoài Internet, chúng ta cần phải sử dụng thêm Service. Service sẽ được trình bày trong bài viết tiếp theo.\nChúng ta sẽ thực hiện Online Inference từ một Pod trong cùng Cluster với Deployment. Sử dụng lệnh sau để chạy và truy cập vào Pod python3:\n$ kubectl run python3 -ti --image=python:3.6 --command=true bash If you don\u0026#39;t see a command prompt, try pressing enter. root@python3:/#  Phần xử lý Inference bây giờ đang nằm trên 2 Pods mà Deployment tạo ra. Ta sẽ gửi yêu cầu dự đoán đến chúng. Xem lại phần debug bên trên của Pod online-inference-deployment-59c8579f48-bg2vj ta thấy Internal IP của nó là 192.168.24.197\nTừ trong Pod python3, thực hiện lệnh sau để gửi yêu cầu dự đoán:\n$ curl -i -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;CRIM\u0026#34;: 15.02, \u0026#34;ZN\u0026#34;: 0.0, \u0026#34;INDUS\u0026#34;: 18.1, \u0026#34;CHAS\u0026#34;: 0.0, \u0026#34;NOX\u0026#34;: 0.614, \u0026#34;RM\u0026#34;: 5.3, \u0026#34;AGE\u0026#34;: 97.3, \u0026#34;DIS\u0026#34;: 2.1, \u0026#34;RAD\u0026#34;: 24.0, \u0026#34;TAX\u0026#34;: 666.0, \u0026#34;PTRATIO\u0026#34;: 20.2, \u0026#34;B\u0026#34;: 349.48, \u0026#34;LSTAT\u0026#34;: 24.9}\u0026#39; 192.168.24.197:5000/predict HTTP/1.0 200 OK Content-Type: application/json Content-Length: 41 Server: Werkzeug/1.0.1 Python/3.8.6 Date: Mon, 01 Feb 2021 11:22:25 GMT { \u0026#34;prediction\u0026#34;: 12.273424794987877 } Như vậy là ta đã nhận được kết quả dự đoán trả về, chứng tỏ Deployment của chúng ta đã hoạt động đúng như ta dự tính.\n2.5 Xóa Deployment khi không sử dụng\nNếu không sử dụng nữa, ta thực hiện lệnh sau để xóa Deployment và các tài nguyên của nó:\n$ kubectl delete deployment online-inference-development deployment.apps \u0026#34;online-inference-development\u0026#34; deleted Kiểm tra lại:\n$ kubectl get rs No resources found. $ kubectl get pods No resources found. 3. Kết luận\nXong, chúng ta đã thực hành thành công với Deployment, và ta cũng biết một thiếu sót của Deployment phải cần đến Service để giải quyết. Đó chính là nội dung của bài tiếp theo. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n8. Tham khảo\n Mlinproduction CronJob  ","permalink":"https://tiensu.github.io/blog/45_kubernetes_deployment/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 4: Kubernetes Deployment"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Ở bài trước, chúng ta đã tìm hiểu và thực hành với Kubernetes Job và thấy được sự phù hợp và hiệu quả của nó đối với các tác vụ trong bài toán AI. Tuy nhiên, có thể dễ dàng nhận thấy một nhược điểm của Job, đó là Job phải được tạo một cách thủ công. Điều này khá là bất tiện, vì chúng ta sẽ phải mất công giám sát hoạt động của hệ thống để can thiệp vào (tạo Job) khi cần. Liệu có cách nào làm cho Job có thể tự động được tạo ra và thực hiện nhiệm vụ của nó tại những thời điểm nhất định, theo chu kỳ? Kubernetes CronJob chính là câu trả lời. Trong bài này, hãy cùng nhau làm việc với CronJob nhé!\n1. Kubernetes CronJob là gì?\nCronJob là một bộ lập lịch, tương tự như Cron Task trong nhân Linux. Nó giúp chúng ta tạo ra một kế hoạch thực hiện một công việc nào đó (bằng cách tạo ra các Jobs), tại những thời điểm trong tương lai theo một chu kỳ mà ta định nghĩa.\nĐối với bài toán AI, CronJob phù hợp với các tác vụ cần chạy định kỳ, ví dụ như là Batch Inference, Feature Extraction, \u0026hellip;\n2. Làm việc với CronJob\nChúng ta sẽ sử dụng lại Docker Image ở bài trước.\nMục đích của mình ở đây là tạo ra một CronJob để thực hiện Batch Inference mỗi phút. File model được lưu trên AWS S3.\nHãy xem cấu trúc thư mục làm việc:\nkubernetes_cronjob │ ├── cronjob │ │ └── cronjob-inference.yaml │ └── docker │ ├── batch_inference.py │ ├── Dockerfile │ └── train.py Giống như Pod và Job, CronJob cũng được tạo thông qua file cấu hình (cronjob-inference.yaml):\napiVersion: batch/v1beta1 kind: CronJob metadata: name: inference-cronjob spec: schedule: \u0026#34;* * * * *\u0026#34; jobTemplate: spec: template: spec: containers: - name: inference-container imagePullPolicy: Always image: tiensu/docker-ml:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;inference.py\u0026#34;] env: - name: AWS_ACCESS_KEY_ID value: \u0026#34;\u0026#34; - name: AWS_SECRET_ACCESS_KEY value: \u0026#34;\u0026#34; restartPolicy: Never backoffLimit: 0 Các thông tin cấu hình khá giống với của Pod, Job. Chỉ có 1 thông tin mới cần lưu ý ở đây:\n schedule: Đây là giá trị chỉ ra chu kỳ chạy của Job, tuân theo các quy tắc định trước. Tham khảo các quy tắc tại đây. Ngoài ra, nếu bạn cảm thấy bối rối khi sử dụng những quy tắc để tạo schedule, bạn có thể sử dụng công cụ này để giúp đỡ bạn. Ở đây, mình đang cấu hình Schedule là 1 phút, tức cứ mỗi phút, CronJob sẽ tạo ra 1 Job để thực hiện lệnh ``python3 inference.py`.  Để tạo CronJob, chạy lệnh sau:\n$ kubectl create -f cronjob-inference.yaml cronjob.batch/inference-cronjob created Xem thông tin của CronJob vừa tạo:\nkubectl get cronjobs NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE inference-cronjob * * * * * False 0 \u0026lt;none\u0026gt; 19s Bởi vì bản chất của CronJob là tạo ra các Jobs, nên ta hãy xem thử các Jobs được tạo ra sau mỗi phút:\n$ kubectl get jobs --watch NAME COMPLETIONS DURATION AGE inference-cronjob-1611919440 1/1 15s 82s inference-cronjob-1611919500 1/1 14s 21s inference-cronjob-1611919560 0/1 0s inference-cronjob-1611919560 0/1 0s 0s inference-cronjob-1611919560 1/1 15s 15s inference-cronjob-1611919380 1/1 16s 3m21s inference-cronjob-1611919620 0/1 0s inference-cronjob-1611919620 0/1 0s 0s Cờ --watch sẽ cho phép lắng nghe sự kiện có bất kỳ sự thay đổi nào trong việc sử dụng tài nguyên của Job, chẳng hạn như tạo, hủy Job.\nChú ý: Tên của Job = Tên của CronJob + chuỗi số ngẫu nhiên.\nĐể xem các Pods tạo ra bởi Job sau mỗi phút, cần kết hợp với Job tại thời điểm đó. Ví dụ xem logs của Job inference-cronjob-1611919380:\nkubectl get pods --selector=job-name=inference-cronjob-1611919380 NAME READY STATUS RESTARTS AGE inference-cronjob-1611919380-cqdtd 0/1 Completed 0 78s Có Pod rồi, ta có thể xem logs tạo ra bởi Pod đó:\n$ kubectl logs inference-cronjob-1611919380-cqdtd Running inference... Loading data... Loading model from: /docker/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.21374322 31.94786177 10.40175849 34.31050209 22.05210667 11.58265489 13.19650094 42.84036647 33.03218733 15.77635169 23.93521876 19.85532224 25.43466604 20.55132127 13.67707622 47.44313586 17.6460682 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.77877263 30.23411048 19.67713185 23.19380271 24.96712102 18.65459129 30.35476911 8.9560549 13.8130382 14.18848318 17.3840622 19.83840166 24.09904134 20.52649052 15.32433651 25.8157052 16.47533793 19.2214524 19.86928427 21.47113681 21.56443118 24.64517965 22.43665872 22.1020877 ] Như ta thấy, Batch Inference đã được thực hiện thành công thông qua CronJob.\nCuối cùng, xóa CronJob khi không sử dụng nữa:\n$ kubectl delete inference-cronjob cronjob.batch/inference-cronjob deleted 3. Kết luận\nXong, mình đã cũng nhau tìm hiểu và sử dụng CronJob để thực hiện nhiệm vụ Batch Inference trong bài toán AI. Mình đặt lịch chạy Job mỗi phút mục đích là để nhanh chóng nhìn thấy kết quả cho lần demo này. Tùy theo yêu cầu thực tế bài toán, các bạn có thể set giá trị phù hợp cho mình.\nBài viết tiếp theo, chúng ta sẽ tìm hiểu và thực hành với Development. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n4. Tham khảo\n Mlinproduction CronJob Crontab  ","permalink":"https://tiensu.github.io/blog/44_kubernetes_cronjob/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 3: Kubernetes CronJob"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Trong bài trước, chúng ta đã tìm hiểu về Pod, cách tương tác với Pod và hạn chế của nó. Bài này, chúng ta sẽ làm việc với một thành phần ở mức high level hơn của Kubernetes, đó là Job. Cụ thể, mình sẽ cùng nhau tạo ra các Job để train model và thực hiện Batch Inference.\n1. Kubernetes Job là gì?\nTheo định nghĩa từ trang chủ của Kubernetes thì:\nA Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.\nHiểu một cách đơn giản thì Jobs chịu trách nhiệm quản lý một hoặc nhiều Pods để thực hiện một công việc nào đó. Trong quá trình làm việc, các Pods có thể chạy song song với nhau, và nếu một Pod bị chết thì Job sẽ tạo ra một Pod khác để thay thể. Job chỉ được coi là hoàn thành thì tất cả các Pod của nó hoàn thành. Khi xóa Job, các Pods được quản lý bởi nó cũng bị xóa theo.\nJob rất phù hợp để chạy các tác vụ kiểu Batch, tức là các tác vụ mà chạy trong một khoảng thời gian nào đó rồi kết thúc. Trong AI, có khá nhiều tác vụ kiểu như vậy, có thể kể ra như Feature Engineering, Cross-Validation, Model Training, Batch Inference. Ví dụ, chúng ta tạo ra một Job để train một model, sau đó lưu model đó vào Storage. Một Job khác sẽ sử dụng model đó để thực hiện Batch Inference.\n2. Sử dụng Job cho các tác vụ AI\nChúng ta sẽ thử tạo 2 Jobs:\n Job thứ nhất để train ML model, lưu model ra file trên AWS S3. Job thứ hai sử dụng model đã trained để thực hiện Batch Inference.  Hãy xem cấu trúc thư mục làm việc:\nkubernetes_job │ ├── docker │ │ ├── batch_inference.py │ │ ├── Dockerfile │ │ └── train.py │ └── job │ ├── job-inference.yaml │ └── job-train.yaml 2.1 Code train \u0026amp; inference model\nTạo thư mục docker và copy 2 file train.py và batch_inference.py đã sử dụng trong các bài trước vào thư mục vừa tạo. Sử a lại nội dung của file train.py như sau:\nimport json import os import boto3 from joblib import dump import matplotlib.pyplot as plt import numpy as np from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] BUCKET_NAME = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Load data print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() print(\u0026#34;Splitting data...\u0026#34;) X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] # ############################################################################# # Fit regression model print(\u0026#34;Fitting model...\u0026#34;) params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) train_mse = mean_squared_error(y_train, clf.predict(X_train)) test_mse = mean_squared_error(y_test, clf.predict(X_test)) metadata = { \u0026#34;train_mean_square_error\u0026#34;: train_mse, \u0026#34;test_mean_square_error\u0026#34;: test_mse } print(\u0026#34;Serializing model to: {}\u0026#34;.format(MODEL_PATH)) dump(clf, MODEL_PATH) print(\u0026#34;Serializing metadata to: {}\u0026#34;.format(METADATA_PATH)) with open(METADATA_PATH, \u0026#39;w\u0026#39;) as outfile: json.dump(metadata, outfile) print(\u0026#34;Moving to S3\u0026#34;) s3 = boto3.client(\u0026#39;s3\u0026#39;) s3.upload_file(MODEL_PATH, BUCKET_NAME, MODEL_FILE) Sửa lại code của file batch_inference.py như sau:\nimport os import boto3 from joblib import load import numpy as np from sklearn import datasets from sklearn.utils import shuffle MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] BUCKET_NAME = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) def load_model(): s3 = boto3.resource(\u0026#39;s3\u0026#39;) try: s3.Bucket(BUCKET_NAME).download_file(MODEL_FILE, MODEL_PATH) except Exception as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] == \u0026#34;404\u0026#34;: print(\u0026#34;The object does not exist.\u0026#34;) else: raise return load(MODEL_PATH) def get_data(): \u0026#34;\u0026#34;\u0026#34; Return data for inference. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] return X_test, y_test print(\u0026#34;Running inference...\u0026#34;) X, y = get_data() # ############################################################################# # Load model print(\u0026#34;Loading model from: {}\u0026#34;.format(MODEL_PATH)) clf = load_model() # ############################################################################# # Run inference print(\u0026#34;Scoring observations...\u0026#34;) y_pred = clf.predict(X) print(y_pred) 2.2 Tạo Docker Images\n Tạo file Dokerfile  Cũng trong cùng thư mục docker, tạo file Dockerfile với nội dung như sau:\nFROM jupyter/scipy-notebook USER root WORKDIR /docker ADD . /docker RUN pip install awscli joblib boto3 RUN mkdir /docker/model # Env variables ENV MODEL_DIR=/docker/model ENV MODEL_FILE=clf.joblib ENV METADATA_FILE=metadata.json ENV BUCKET_NAME=kubernetes-job  Build Docker Image này:  $ docker build -t docker-ml . Sending build context to Docker daemon 6.656kB Step 1/10 : FROM jupyter/scipy-notebook ---\u0026gt; c1a7c7ef5e27 Step 2/10 : USER root ---\u0026gt; Using cache ---\u0026gt; 0c1dbc43bef8 Step 3/10 : WORKDIR /docker ---\u0026gt; Running in fdae735976d0 Removing intermediate container fdae735976d0 ---\u0026gt; b795fe3bbd80 Step 4/10 : ADD . /docker ---\u0026gt; 16082b6c9bda Step 5/10 : RUN pip install awscli joblib boto3 ---\u0026gt; Running in e4f9036ae9fc Collecting awscli Downloading awscli-1.18.221-py2.py3-none-any.whl (3.5 MB) Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (1.0.0) Collecting boto3 Downloading boto3-1.16.61-py2.py3-none-any.whl (130 kB) Collecting s3transfer\u0026lt;0.4.0,\u0026gt;=0.3.0 Downloading s3transfer-0.3.4-py2.py3-none-any.whl (69 kB) Collecting botocore==1.19.61 Downloading botocore-1.19.61-py2.py3-none-any.whl (7.2 MB) Collecting PyYAML\u0026lt;5.4,\u0026gt;=3.10 Downloading PyYAML-5.3.1.tar.gz (269 kB) Collecting colorama\u0026lt;0.4.4,\u0026gt;=0.2.5 Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB) Collecting rsa\u0026lt;=4.5.0,\u0026gt;=3.1.2 Downloading rsa-4.5-py2.py3-none-any.whl (36 kB) Collecting docutils\u0026lt;0.16,\u0026gt;=0.10 Downloading docutils-0.15.2-py3-none-any.whl (547 kB) Collecting jmespath\u0026lt;1.0.0,\u0026gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Requirement already satisfied: urllib3\u0026lt;1.27,\u0026gt;=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore==1.19.61-\u0026gt;awscli) (1.26.3) Requirement already satisfied: python-dateutil\u0026lt;3.0.0,\u0026gt;=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore==1.19.61-\u0026gt;awscli) (2.8.1) Requirement already satisfied: six\u0026gt;=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil\u0026lt;3.0.0,\u0026gt;=2.1-\u0026gt;botocore==1.19.61-\u0026gt;awscli) (1.15.0) Collecting pyasn1\u0026gt;=0.1.3 Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB) Building wheels for collected packages: PyYAML Building wheel for PyYAML (setup.py): started Building wheel for PyYAML (setup.py): finished with status \u0026#39;done\u0026#39; Created wheel for PyYAML: filename=PyYAML-5.3.1-cp38-cp38-linux_x86_64.whl size=44618 sha256=421030371a2f82fdfd722d0b032ce5b0c8d01e02a5ca379c9a0e2eea3a03fd78 Stored in directory: /tmp/pip-ephem-wheel-cache-cs65titp/wheels/13/90/db/290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c Successfully built PyYAML Installing collected packages: jmespath, pyasn1, botocore, s3transfer, rsa, PyYAML, docutils, colorama, boto3, awscli Attempting uninstall: PyYAML Found existing installation: PyYAML 5.4.1 Uninstalling PyYAML-5.4.1: Successfully uninstalled PyYAML-5.4.1 Successfully installed PyYAML-5.3.1 awscli-1.18.221 boto3-1.16.61 botocore-1.19.61 colorama-0.4.3 docutils-0.15.2 jmespath-0.10.0 pyasn1-0.4.8 rsa-4.5 s3transfer-0.3.4 Removing intermediate container e4f9036ae9fc ---\u0026gt; 0f7e7ec1c6a0 Step 6/10 : RUN mkdir /docker/model ---\u0026gt; Running in 7f4c3bd5253a Removing intermediate container 7f4c3bd5253a ---\u0026gt; 0b2c845fbb40 Step 7/10 : ENV MODEL_DIR=/docker/model ---\u0026gt; Running in 643ef25a50eb Removing intermediate container 643ef25a50eb ---\u0026gt; 339c22e0a4f9 Step 8/10 : ENV MODEL_FILE=clf.joblib ---\u0026gt; Running in d81f810dc092 Removing intermediate container d81f810dc092 ---\u0026gt; cd7ecdc2f380 Step 9/10 : ENV METADATA_FILE=metadata.json ---\u0026gt; Running in 701460e9b463 Removing intermediate container 701460e9b463 ---\u0026gt; 7646e477d5a9 Step 10/10 : ENV BUCKET_NAME=kubernetes-job ---\u0026gt; Running in ce92e3cdbc3b Removing intermediate container ce92e3cdbc3b ---\u0026gt; 31d25c8be720 Successfully built 31d25c8be720 Successfully tagged docker-ml:latest  Push Docker Image lên Docker Hub:  Sử dụng các lệnh sau để push Docker Image vừa build lên Docker Hub\n$ docker tag docker-ml:latest tiensu/docker-ml:latest $ docker push tiensu/docker-ml:latest The push refers to repository [docker.io/tiensu/docker-ml] 76fba3826ca9: Pushed 59928edb97b5: Pushed b5e012598fbb: Pushed c4e3257e6eb5: Pushed 5f70bf18a086: Mounted from tiensu/ml-model-batch-infer 6f5a41ae77fd: Mounted from tiensu/ml-model-batch-infer 5a1b9a3f9355: Mounted from tiensu/ml-model-batch-infer b1d7816bac14: Mounted from tiensu/ml-model-batch-infer c91fed2d1998: Mounted from tiensu/ml-model-batch-infer cc70098d00e3: Mounted from tiensu/ml-model-batch-infer 88727e93cbac: Mounted from tiensu/ml-model-batch-infer cadaf24035f3: Mounted from tiensu/ml-model-batch-infer 8f170f4774e3: Mounted from tiensu/ml-model-batch-infer 33bd52db887f: Mounted from tiensu/ml-model-batch-infer 21e5dd010f50: Mounted from tiensu/ml-model-batch-infer ea370ab22368: Mounted from tiensu/ml-model-batch-infer 421d1408f872: Mounted from tiensu/ml-model-batch-infer 18fd1ca0de51: Mounted from tiensu/ml-model-batch-infer 8f01aab6d756: Mounted from tiensu/ml-model-batch-infer e18a1c4e1d31: Mounted from tiensu/ml-model-batch-infer 8552f27c3cd8: Mounted from tiensu/ml-model-batch-infer 1a4c57efcc23: Mounted from tiensu/ml-model-batch-infer 94b8fe888eac: Mounted from tiensu/ml-model-batch-infer 02473afd360b: Mounted from tiensu/ml-model-batch-infer dbf2c0f42a39: Mounted from tiensu/ml-model-batch-infer 9f32931c9d28: Mounted from tiensu/ml-model-batch-infer latest: digest: sha256:40678bdd8d763129322db38be9f83bc70d1278b7836c7c7f4f4ac3ef6af20e5e size: 6582 2.3 Tạo Kubernetes Job để train ML model\nTương tự như tạo Pod, để tạo Job ta cũng cần khai báo các thông tin cần thiết trong file cấu hình job-train.yaml:\napiVersion: batch/v1 kind: Job metadata: name: job-train-ml-model spec: template: spec: containers: - name: train-container imagePullPolicy: Always image: tiensu/docker-ml:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;train.py\u0026#34;] env: - name: AWS_ACCESS_KEY_ID value: \u0026#34;\u0026#34; - name: AWS_SECRET_ACCESS_KEY value: \u0026#34;\u0026#34; restartPolicy: Never backoffLimit: 0 Một số thông tin như sau:\n apiVersion: Phiên bản của Kubernetes API. kind: Loại tài nguyên của Kubernetes cần tạo, ở đây là Job. metadata: Danh sách các nhãn, các thuộc tính tùy ý mà người phát triển có thể gắn cho Job. Thường các thông tin về Metadata của ML model được gắn ở đây. Kubernetes cũng khuyến nghị một số nhãn ở đây. spec.template: Chính là phần cấu hình của Pod mà ta cần khai báo, tương tự như cấu hình của Pod mà ta đã tạo ở bài trước.  imagePullPolicy: Cho phép Kubernetes luôn luôn sử dụng Docker Image từ Docker Hub thay vì Cache Image. env: Danh sách các biến môi trường để Pod sử dụng. Ở đây, chúng ta khai bào 2 biến liên quan đến AWS để làm việc với AWS S3. Mình đã xóa các key mà mình sử dụng. Nếu bạn muốn chạy thử thì hãy thêm key của bạn vào nhé!   restartPolicy: Có khởi động lại Container khi nó bị chết hay không? backoffLimit: Số lần cố gắng thực hiện lại Job khi nó bị thất bị.  Chạy lệnh sau để tạo và kiểm tra trạng thái của Job:\n$ kubectl create -f job-train.yaml job.batch/job-train-ml-model created $ kubectl get jobs NAME COMPLETIONS DURATION AGE job-train-ml-model 1/1 58s 2m19s Kiểm tra xem các pods của Job là gì và trạng thái của chúng:\n$ kubectl get pods --selector=job-name=job-train-ml-model NAME READY STATUS RESTARTS AGE job-train-ml-model-6fkcd 0/1 Completed 0 2m19s Xem logs Job/Pod:\n$ kubectl logs job-train-ml-model-6fkcd Loading data... Splitting data... Fitting model... Serializing model to: /docker/model/clf.joblib Serializing metadata to: /docker/model/metadata.json Moving to S3 Như vậy, có thể thấy là Job đã chạy xong, file model đã được lưu trên S3.\nCuối cùng, ta có thể xóa Job sau khi chúng đã hoàn thành nhiệm vụ của mình:\n$ kubectl delete job job-train-ml-model job.batch \u0026#34;job-train-ml-model\u0026#34; deleted 2.4 Tạo Kubernetes Job để thực hiện Batch Inference\nChúng ta sẽ sử dụng lại Docker Image đã tạo ở trên cho Job này.\nFile cấu hình của Job (job-inference.yaml) như sau:\napiVersion: batch/v1 kind: Job metadata: name: job-inference-ml-model spec: template: spec: containers: - name: inference-container imagePullPolicy: Always image: tiensu/docker-ml:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;batch_inference.py\u0026#34;] env: - name: AWS_ACCESS_KEY_ID value: \u0026#34;\u0026#34; - name: AWS_SECRET_ACCESS_KEY value: \u0026#34;\u0026#34; restartPolicy: Never backoffLimit: 0 So với cấu hình của Job phía trên, chỉ có các thông tin sau thay đổi: Job name, container name, container command.\nĐể tạo và liểm tra trạng thái của Job, chạy lệnh sau:\n$ kubectl create -f job-inference.yaml job.batch/job-inference-ml-model created $ kubectl get jobs NAME COMPLETIONS DURATION AGE job-inference-ml-model 1/1 13s 66s Kiểm tra xem các Pods của Job và trạng thái tương ứng:\n$ kubectl get pods --selector=job-name=job-inference-ml-model NAME READY STATUS RESTARTS AGE job-inference-ml-model-sk2m4 0/1 Completed 0 2m11s Chú ý: Tên của Pod = Tên của Job + chuỗi ngẫu nhiên.\nXem logs của Job/Pod:\n$ kubectl logs job-inference-ml-model-sk2m4 Running inference... Loading data... Loading model from: /docker/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.21374322 31.94786177 10.40175849 34.31050209 22.05210667 11.58265489 13.19650094 42.84036647 33.03218733 15.77635169 23.93521876 19.85532224 25.43466604 20.55132127 13.67707622 47.44313586 17.6460682 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.77877263 30.23411048 19.67713185 23.19380271 24.96712102 18.65459129 30.35476911 8.9560549 13.8130382 14.18848318 17.3840622 19.83840166 24.09904134 20.52649052 15.32433651 25.8157052 16.47533793 19.2214524 19.86928427 21.47113681 21.56443118 24.64517965 22.43665872 22.1020877 ] Như vậy là Job đã thực hiện Batch Inference thành công bằng model nhận được từ S3.\nCuối cùng, xóa Job sau khi nó đã hoàn thành nhiệm vụ để tiết kiệm tài nguyên server:\n$ kubectl delete job job-inference-ml-model job.batch \u0026#34;job-inference-ml-model\u0026#34; deleted 3. Kết luận\nNhư vậy là mình đã cùng các bạn tìm hiểu và sử dụng Kubernetes Job để thực hiện các tác vụ của một bài toán AI. Có một lưu ý dành cho các bạn đó là trong trường hợp việc thực hiện tạo Job thất bại, hãy nhớ sử dụng lệnh kubectl describe pod \u0026lt;pod_name\u0026gt;, trong đó pod_name là tên Pod của Job để xem đầy đủ logs. Dựa vào logs này, các bạn có thể dễ dàng phát hiện ra nguyên nhân lỗi và cách khắc phục chúng.\nbài viết tiếp theo, chúng ta sẽ tìm hiểu và thực hành với CronJob. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n4. Tham khảo\n Mlinproduction Kubernetes Jobs  ","permalink":"https://tiensu.github.io/blog/43_kubernetes_job/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 2: Kubernetes Job"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Trong các bài viết trước, mình đã giới thiệu về Docker, sử dụng kết hợp với Nginx, uWSGI, Flask để deploy model trong môi trường production. Nhìn chung mà nói, cách kết hợp 4 dịch vụ này đủ để áp ứng cho hầu hết các bài toán AI, ngoại trừ vấn đề cấu hình tương đối phức tạp và khó triển khai trên cloud (thực tế là AWS và GCP đề không hỗ trợ cách này, nếu muốn chúng ta vẫn phải cấu hình bằng tay như dưới local).\nGần đây, Kubernetes nổi lên như là một xu hướng mới, đáp ứng đầy đủ các yêu cầu của việc triển khai model trong môi trường production. Hơn thế nữa, việc cấu hình rất đơn giản và được hỗ trợ bởi các ông lớn cloud (AWS và GCP đều có dịch vụ Kubernetes). Trong loạt bài tiếp theo, mình sẽ cùng mọi người tìm hiểu về hot trend này và cách thức sử dụng nó để deploy các AI model của chúng ta nhé!\n1. Kubernetes là gì?\nTheo định nghĩa từ trang chủ của Kubernetes thì:\nKubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.\nPhần tử hạt nhân của Kubernetes chính là các Containers (Docker Container). Nói theo một cách khác, Kubernetes giúp chúng ta:\n Manage containers: Quản lý đồng thời nhiều docker containers của cùng một ứng dụng hoặc thậm chí là nhiều ứng dụng khác nhau. Manage lifecycle: Quản lý toàn bộ lifecycle của các containers, từ lúc được tạo ra đến khi bị xóa bỏ. Hardware optimization: Tối ưu hóa, tối đa hóa khả năng của phần cứng thiết bị. Schedule: Lập lịch khi nào cần bật/tắt containers. Load balancer: Phân tải xử lý đều cho các containers. Backup: Khi một container chết, sẽ có một container khác đươc tạo để thay thế. Scalling: Dễ dàng scale các containers up/down theo một trong 2 chế độ: tự động hoặc bằng tay. Monitor system: Dễ dàng giám sát hoạt động của toàn bộ hệ thống.  Việc cấu hình cho Kubernetes khá đơn giản, tất cả chỉ thông qua một file cấu hình duy nhất.\nTrong tiếng Hy Lạp, Kubernetes có nghĩa là người chỉ huy hay thuyền trưởng.\nTất cả những đặc điểm trên đều phù hợp với giải pháp mà chúng ta tìm kiếm để đưa AI model vào môi trường production. Tất nhiên là phạm vi ứng dụng của Kubernetes còn rộng lớn hơn rất nhiều, nhưng trong lĩnh vực làm việc và nghiên cứu của mình, mình chỉ tập trung tìm hiểu và sử dụng Kubernetes cho các bài toán về AI.\n2. Kiến trúc và thành phần của Kubernetes\n Kubernetes bao gòm các Nodes, được chia thành 2 loại: Master Node và Worker Nodes. Master Nodes chịu trách nhiệm quản lý các Worker Nodes, trong khi các Worker Nodes làm nhiệm vụ thực hiện các công việc tính toán, \u0026hellip; Mỗi Worker Node lại được chia nhỏ thành các Pods, và trong mỗi Pod chính là các Containers.\nPhần còn lại của bài hôm nay, mình sẽ cùng các bạn tìm hiểu về Pod. Các bài tiếp theo, chúng ta sẽ làm việc với Job, CronJob, Deployment, Service.\n3. Kubernetes Pod\n3.1 Kubernetes Pod là gì?\nTheo định nghĩa, Pod là đối tượng nhỏ nhất có khả năng triển khai trong kiến trúc của Kubernetes, tức là bạn có thể tạo, sử dụng, hay xóa Pod. Có thể coi Pod chính là đại diện của một ứng dụng (instance application) chạy trong Kubernetes.\nNhư đã nói ở phần 2, mỗi Pod chứa một hoặc nhiều Containers để thực hiện một công việc (Job) nào đó. Các Containers trong cùng Pod nằm trong cùng một mạng local và chia sẻ tài nguyên sử dụng với nhau. Chính vì thế mà chúng dễ dàng giao tiếp và làm việc với nhau.\nVì Pod là Single Instance của ứng dụng chạy trong Kebernetes, số lượng Pod được tạo ra hay xóa đi một cách tự động (Load Balancing \u0026amp; Failure Recovery), tùy theo tải mà ứng dụng phải phục vụ.\n3.2 Tạo Pod từ Docker Image có sẵn\nĐể làm việc được với Pod, trước tiên cần phải cài đặt kubectl theo hướng dẫn trên trang chủ của Kubernetes tại đây hoặc tại đây\nCách dễ nhất để tạo và triển khai Kubernetes là sử dụng config file. File này sẽ chỉ định đối tượng được tạo là gì, các metadata gắn với đối tượng đó, tài nguyên cần thiết là bao nhiêu, \u0026hellip;\nDưới đây là template của config file (pod_public.yaml) để tạo một Pod:\napiVersion: v1 kind: Pod metadata: name: python3-pod labels: app: python3 spec: containers: - name: python3-container image: python:3.6 command: [\u0026#39;python3\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;print(\u0026#34;Hello, World!\u0026#34;)\u0026#39;] restartPolicy: Never File config này bao gồm những thông tin sau:\n apiVersion: Phiên bản của Kubernetes API đang sử dụng. kind: Loại tài nguyên (đối tượng) của Kubernetes được tạo ra: Pod, Job, Development, \u0026hellip; Ở đây là Pod object. metadata: Là một tập hợp các labels và các thuộc tính của model mà người phát triển có thể thêm vào tùy ý giống như phiên bản, độ chính xác, thuật toán, \u0026hellip; spec: Bao gồm thông tin của các Containers chạy bên trong Pod: tên, docker image, command. Như trong cấu hình hiện tại thì chỉ có 1 Container. restartPolicy: Cho phép Container có restart hay không khi nó bị lỗi. Giá trị never ở đây tức là không cho phép restart.  Thực hiên lệnh sau để tạo Pod:\n$ kubectl create -f pod_public.yaml pod \u0026#34;python3-pod\u0026#34; created Kiểm tra trạng thái của pod vừa tạo:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE python3-pod 0/1 Completed 0 3s Xem log của pod vừa tạo:\n$ kubectl logs python3-pod Hello, World! Xóa pod vừa tạo:\n$ kubectl delete -f pod_public.yaml pod \u0026#34;python3-pod\u0026#34; deleted 3.3 Tạo Pod từ Docker Image tự tạo\nMình sẽ sử dụng Docker Image đã tạo từ bài này để đưa vào Pod.\nTrước tiên, bạn hãy login vào Docker Hub để tạo một Repository. Giả sử mình tạo Repository tên là ml-model-batch-infer.\nở máy local, thực hiện các bước sau để đưa Docker Image đã tạo lên Repository:\n Login vào Docker Hub  $ docker login -u tiensu Trong đó, tiensu là tên đăng nhập của mình, bạn hãy thay bằng tên đăng nhập của bạn. Nhập mật khẩu khi được hỏi.\n Gán Tag cho Docker Image theo tên mới trên Repository  docker tag docker-model-batch-infer:latest tiensu/ml-model-batch-refer:latest  Push Docker Image đã gắn Tag lên Repository  docker push tiensu/ml-model-batch-infer:latest Output:\nThe push refers to repository [docker.io/tiensu/ml-model-batch-infer] 2a0a8f09fca2: Pushed ea3e588d9e9f: Pushed 2b8e8179f02d: Pushed 254c54a05297: Pushed bdeb303132f3: Pushed 5f70bf18a086: Mounted from jupyter/scipy-notebook 6f5a41ae77fd: Mounted from jupyter/scipy-notebook 5a1b9a3f9355: Mounted from jupyter/scipy-notebook b1d7816bac14: Mounted from jupyter/scipy-notebook c91fed2d1998: Mounted from jupyter/scipy-notebook cc70098d00e3: Mounted from jupyter/scipy-notebook 88727e93cbac: Mounted from jupyter/scipy-notebook cadaf24035f3: Mounted from jupyter/scipy-notebook 8f170f4774e3: Mounted from jupyter/scipy-notebook 33bd52db887f: Mounted from jupyter/scipy-notebook 21e5dd010f50: Mounted from jupyter/scipy-notebook ea370ab22368: Mounted from jupyter/scipy-notebook 421d1408f872: Mounted from jupyter/scipy-notebook 18fd1ca0de51: Mounted from jupyter/scipy-notebook 8f01aab6d756: Mounted from jupyter/scipy-notebook e18a1c4e1d31: Mounted from jupyter/scipy-notebook 8552f27c3cd8: Mounted from jupyter/scipy-notebook 1a4c57efcc23: Mounted from jupyter/scipy-notebook 94b8fe888eac: Mounted from jupyter/scipy-notebook 02473afd360b: Mounted from jupyter/scipy-notebook dbf2c0f42a39: Mounted from jupyter/scipy-notebook 9f32931c9d28: Mounted from jupyter/scipy-notebook latest: digest: sha256:2552cb24c104d9b4fe3a43cc952371a7a1b0cce84e1c95821622b4fe508a6877 size: 6786 Để tạo Pod với Docker Image này, cập nhật lại file config (đổi tên thành pod_custom.yaml) của Pod như sau:\napiVersion: v1 kind: Pod metadata: name: pod-ml-model-batch-infer labels: app: python3 spec: containers: - name: container-ml-model-batch-infer image: tiensu/ml-model-batch-infer:latest command: [\u0026#39;python3\u0026#39;, \u0026#39;batch_inference.py\u0026#39;] restartPolicy: Never Chạy lệnh sau để tạo Pod:\n$ kubectl create -f pod_custom.yaml pod/pod-ml-model-batch-infer created Kiểm tra trạng thái của Pod vừa tạo:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE command-demo 0/1 Completed 0 113m pod-ml-model-batch-infer 0/1 Completed 0 5m28s python3-pod 0/1 Completed 0 90m Chú ý là Docker Image của chúng ta được tải về trên Worker Node. Bạn có thể kiểm tra trên đó bằng lệnh $ docker ps.\nChúng ta có thể xem miêu tả chi tiết quá trình tạo Pod như sau:\n$ kubectl describe pod pod-ml-model-batch-infer Name: pod-ml-model-batch-infer Namespace: default Priority: 0 Node: duynm-vostro-3670/10.1.34.169 Start Time: Wed, 27 Jan 2021 16:44:15 +0700 Labels: app=python3 Annotations: cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs: Status: Succeeded IP: 192.168.24.198 IPs: IP: 192.168.24.198 Containers: container-ml-model-batch-infer: Container ID: docker://535749cae10e6dd605030b6d84ba978cc245bfd44bb6981d3307a3ffa8a5bf94 Image: tiensu/ml-model-batch-infer:latest Image ID: docker-pullable://tiensu/ml-model-batch-infer@sha256:2552cb24c104d9b4fe3a43cc952371a7a1b0cce84e1c95821622b4fe508a6877 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: python3 batch_inference.py State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 27 Jan 2021 16:44:24 +0700 Finished: Wed, 27 Jan 2021 16:44:24 +0700 Ready: False Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-gmswp (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: default-token-gmswp: Type: Secret (a volume populated by a Secret) SecretName: default-token-gmswp Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m18s default-scheduler Successfully assigned default/pod-ml-model-batch-infer to duynm-vostro-3670 Normal Pulling 4m15s kubelet Pulling image \u0026#34;tiensu/ml-model-batch-infer:latest\u0026#34; Normal Pulled 4m11s kubelet Successfully pulled image \u0026#34;tiensu/ml-model-batch-infer:latest\u0026#34; in 4.353859959s Normal Created 4m9s kubelet Created container container-ml-model-batch-infer Normal Started 4m9s kubelet Started container container-ml-model-batch-infer Cuối cùng, hãy xem log tạo ra khi thực hiên Inference:\n$ kubectl logs pod-ml-model-batch-infer Running inference... Loading data... Loading model from: /code/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.20025598 31.94786177 10.42732759 34.12058193 22.05210667 11.58265489 13.1649368 42.84036647 33.03218733 15.77635169 23.93521876 19.91587166 25.43466604 20.55132127 13.65254047 47.47279364 17.58214889 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.65978361 30.18436261 19.67895051 23.22841646 24.94197905 18.65459129 30.19731636 8.9560549 13.8130382 14.23277857 17.3840622 19.83840166 24.91315811 20.44991809 15.32433651 25.8157052 16.47533793 19.2214524 19.87110293 21.47113681 21.56443118 24.64517965 22.43665872 22.18289286] 7. Kết luận\nMặc dù Pod là đối tượng quan trọng, không thể thiếu trong bất kỳ kiến trúc Kubernetes nào nhưng các Best Practice đều không khuyến khích việc sử dung nó một cách trực tiếp, mà nên được triển khai cùng với các đối tượng khác ở mức cao hơn của Kubernetes để quản lý nó, như Job chẳng hạn. Job sẽ tạo ra một hoặc nhiều Pods, và khi một Pod bị chết thì Pod khác sẽ được bật lên để sẵn sàng thay thế cho nó.\nChúng ta sẽ tìm hiểu vấn đề này trong bài viết tiếp theo. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n8. Tham khảo\n Mlinproduction Docker Hub  ","permalink":"https://tiensu.github.io/blog/42_kubernetes_in_ai/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 1: Kubernetes Pod"},{"categories":["MLOps"],"contents":"Hẳn các bạn đã biết, trong hầu hết các bài toán AI, chúng ta không chỉ train model 1 lần rồi thôi (mình không nói đến việc thử-sai trong quá trình tuning model). Tại thời điểm này, model hoạt động tốt đúng như những gì ta mong đợi, nhưng sau một thời gian, hiệu năng của model có thể giảm xuống. Đó là một trong những dấu hiệu chỉ ra rằng ta phải retrain lại model. Trong bài hôm nay, mình sẽ cùng các bạn tìm hiểu chi tiết hơn về vấn đề này.\n1. Model Drift\nModel Drift là khái niệm mô tả hiện tượng hiệu năng dự đoán của model suy giảm theo thời gian do có sự thay đổi của môi trường làm sai lệch các giả thiết ban đầu của model. Thuật ngữ Model Drift (model chuyển dịch) có thể khiến chúng ta hơi bối rối 1 chút, vì bản chất là model không thay đổi, chỉ có các yếu tố môi trường bên ngoài thay đổi, input data thay đổi.\n2. Làm sao để nhận biệt hiện tượng Model Drift\n2.1 Kiểm tra độ chính xác của model\nBiểu hiên trực tiếp và rõ ràng nhất của Model Drift là độ chính xác dự đoán (độ chính xác ở đây dùng chung cho tất cả các metrics đánh giá model) giảm dần theo thời gian. Nhưng việc giám sát việc này không phải lúc nào cũng đơn giản bởi vì ta phải có cả kết quả dự đoán của model và ground truth, đặc biệt khi model đang chạy trong sản phầm thực tế (môi trường production hay online).\nCó một cách đơn giản hơn để kiểm tra độ chính xác của model có bị suy giảm hay không, đó là offline monitor. Cách này được thực hiện trước khi model triển khai model vào môi trường production. Giả sử ra có dữ liệu từ 01/2019 đến 01/2021. Ta sẽ sử dụng dữ liệu từ 01/2019 đến 06/2020 đê train và đánh giá model, sau đó sử dụng model này để dự đoán trên dữ liệu tháng 07/2020 đến 01/2021. Kết quả dự đoán được lưu lại để đánh giá xem độ chính xác của model có suy giảm hay không, nếu có thì mức độ suy giảm như thế nào? \u0026hellip; Sử dụng cách này cho phép chúng ta ước lượng được tốc độ suy giảm độ chính xác, từ đó lên kế hoạch retrain lại model.\n2.2 Kiểm tra phân bố của dữ liệu\nNếu phân bố của dữ liệu mới có sự sai khác so với dữ liệu huấn luyện model từ ban đầu thì độ chính xác của model cũng sẽ giảm. Vì thế, đây cũng là một dấu hiệu nhận biết sớm của hiện tượng Model Drift.\nĐể đánh giá sự phân bố của dữ liệu, có thể dựa vào các yếu tố sau:\n Phạm vi giá trị của các features Đồ thị histogram của các features Các features có được cho phép nhận giá trị NULL hay không? \u0026hellip;  Facets là một công cụ cho phép chúng ta nhanh chóng nhận ra sự thay đổi trong phân bố dữ liệu dựa trên sự quan sát các đồ thị phân bố trên dashboards. Việc theo dõi này có thể được thực hiện một cách tự động và nó sẽ gửi thống báo cho chúng ta khi sự phân bố dữ liệu thay đổi vượt quá một ngưỡng nào đó.\n2.3 Kiểm tra sự tương quan giữa các features trong dữ liệu\nMối qua hệ giữa các features cũng ảnh hướng đến độ chính xác của model. Vì vậy, kiểm tra sự tương quan giữa các features từng đôi một xem chúng thay đổi ra sao cũng là một cách để nhận biết Model Drift.\n3. Hiểu đúng về Model Retraining\nChúng ta đều hiểu rằng Model Retraining tức là training lại model, tạo ra model mới tốt hơn model cũ. Nhưng nếu chỉ chung chung như thế thì có rất nhiều cách để retraining model:\n Thay đổi hyper-parameters Thay đổi thuật toán ML/DL (model algorithm) Thêm/bớt các features \u0026hellip;  Giữa những cách retraining model kể trên, đâu là cách đúng nhất để loại bỏ hiện tượng Model Drift?\nQuay lại khái niệm của Model Drift, đó là hiện tượng độ chính xác của model suy giảm do có sự thay đổi trong phân phối dữ liệu. Vậy ta chỉ cần train lại model trên tập dữ liệu mới và giữ nguyên tất cả những cái khác: hyper-parameters, thuật toán, features, \u0026hellip; Hiểu một cách đơn giản hơn thì tức là ta sẽ không thay đổi dòng code nào cả, chỉ thay đổi nội dung của file chứa dữ liệu mới để train model.\nNói vậy, không có nghĩa là chúng ta bỏ qua hoàn toàn các cách retraing model khác. Nếu bạn có đủ thời gian, công sức, bạn hay các thành viên trong dự án của bạn hoàn toàn có thể thử nghiệm cách retraining model kể trên. Sau đó sử dụng chiến lược A/B Test để đánh giá các models dựa trên các tiêu chí của bài toán. Model nào cho cho kết quả tốt hơn thì sẽ được sử dụng trong môi trường production.\n4. Tần suất Retrain Model\nMột vấn đề tiếp theo cần quan tâm là tần suất retrain model như thế nào là hợp lý?\nCâu trả lời là không có một quy định, quy tắc cụ thể nào cả. Tùy từng bài toán mà ta có cách xử lý khác nhau.\n Retrain model tại một thời điểm cố định nếu ta biết trước chính xác thời điểm dữ liệu có sự thay đổi lớn. VD: tại các trường đại học, đầu mỗi năm học đều có số lượng lớn sinh viên nhập học và ra trường thì ta nên retrain lại model tại thời điểm đó. Retrain model khi thu thập được đủ một lượng dữ liệu nhất định Retrain model khi các metrics mà ta theo dõi (như đề cập trong mục 2) thay đổi vượt quá một ngưỡng nào đó.  Đối với cách thứ 2\u0026amp;3, cần phải có một hạ tầng độc để giám sát và đưa ra cảnh báo khi sự thay đổi đạt đến mức quy định. Việc chọn ngưỡng cho các metrics cũng cần phải xem xét cẩn thận. Ngưỡng quá thấp sẽ làm cho tần suất retrain model thường xuyên hơn, dẫn đến tốn kém chi phí tính toán (đặc biệt quan trong trường trường hợp sử dụng tài nguyên trên cloud). Ngưỡng quá cao làm cho model không thay đổi kịp với sự thay đổi của môi trường, dẫn đến không tối ưu hóa lợi nhuận, \u0026hellip;\nĐặc biệt trong trường hợp model cần thay đổi realtime mỗi khi có bất cứ dữ liệu mới (VD model dự đoán giao dịch ngân hàng an toàn hay không an toàn) thì nên sử dụng phương pháp học tăng dần, Incremental Learning / Online Learning. Phương pháp này khác các cách Retrain Model đã đề cập ở chỗ model được retrain (cập nhật) chỉ sử dụng dữ liệu mới, không phải retrain trên toàn bộ dữ liệu.\n5. Chạy Retrain Model tự động\nCách cấu hình để Retrain Model tự động liên quan đến tần suất retrain model của bạn.\n  Nếu model được retrained định kỳ, chúng ta có thể sử dụng Kubernetes CronJobs hoặc Jenkins để lập lịch cho model chạy retrain.\n  Nếu model đươc retrained dựa vào trigger khi các metrics thay đổi đến ngưỡng được phát hiện, chúng ta có thể sử dụng Kubernetes Jobs hoặc Jenkins để làm việc này.\n  Cuối cùng, nếu model cần retrain realtime, sử dụng phương pháp Online Learning. River là thư viện lý tưởng cho việc này. Tên cũ của nó là Creme.\n  6. Implement code prototype\n6.1 Query Data by Date Range function\nBởi vì quá trình retraining dựa trên dữ liệu mới, nên chúng ta cần 1 hàm lấy ra những dữ liệu đó, theo 1 khoảng thời gian quy đinh. Dữ liệu mới có thể được lưu ở SQL database, S3, local storage, \u0026hellip;\ndef get_raw_data(end_date, date_window=365): \u0026#39;\u0026#39;\u0026#39; Retrieve all data in date range (end_date - date_window, end_date) \u0026#39;\u0026#39;\u0026#39; Trong đó:\n end_date: ngày cuối cùng trong khoảng thời gian của dữ liệu mới. date_window: số lượng ngày trong khoảng thời gian của dữ liệu mới, tính từ end_date trở lại.  Để nhận dữ liệu mới cho việc retraining model, chúng ta sẽ gọi:\nfrom datetime import date training_data = get_new_data(date.today()) 6.2 Generate a Machine Learning Model function\nHàm này chịu trách nhiệm train AI model: chia dataset thành tập train và tập test, trích xuất vector đặc trừng từ dữ liệu, thực hiện tuning hyper-parameters, huấn luyện model, đánh giá model, \u0026hellip;\nfind_optimal_model(data, ...): \u0026#39;\u0026#39;\u0026#39; Split data, generate features, tune hyper-parameters, train model, ... \u0026#39;\u0026#39;\u0026#39; Tham số data là dữ liệu để huấn luyện mode. Kết quả thực thi của hàm sẽ trả về model đã được trained và các training metrics.\n6.3 Store Trained Model\nMột khi model được trained xong, ta cần lưu nó lại để sử dụng về sau. Cách đơn giản nhất là sử dụng thư viện pickle có sẵn của python. Ngoài ta, bạn cũng có thể sử dụng ONNX hoặc PMML.\n# Serialize and store model on local storage def serialize_model(training_arfifacts): \u0026#39;\u0026#39;\u0026#39; Return a local path to serialized model \u0026#39;\u0026#39;\u0026#39; 6.4 Registry Model\nTham khảo bài Model Registry\n6.5 Model Retraining Enpoint\nTập hợp tất cả các hàm lại trong một script để đơn giản hóa quy trình, retrain.py. Script sẽ chấp nhận một tham số từ command line là end_date, nhận về dữ liệu mới, train mode, store model và registry model.\nfrom datetime import date import sys def retrain(end_date): \u0026#39;\u0026#39;\u0026#39;Model retraining loop.\u0026#39;\u0026#39;\u0026#39; data = get_raw_data(end_date) training_artifacts = find_optimal_model(data, ...) local_path = serialize_model(training_artifacts) model_registry(local_path, training_artifacts) if __name__ == \u0026#39;__main__\u0026#39;: retrain(sys.argv[1]) 6.6 Scheduling the Retraining Procedure\nScript retrain.py lại tiếp tục được đóng gói trong bash script, retrain.sh:\ntoday_date=\u0026#39;date +”%m/%d/%Y”\u0026#39; python retrain.py $today_date Để trigger event gọi đến bash script này, chúng ta có thể lập lịch, sử dụng một trong các công cụ sau:\n Jenkins Airflow Kubernetes Cronjobs Crontab  Các công cụ này đều hỗ trợ đầy đủ việc xử lý ngoại lệ, cơ chế retry, \u0026hellip; Tuy nhiên, viêc thiết lập và cài đặt sẽ tương đối mất thời gian nếu bạn chưa quen thuộc với chúng.\n6.7 Retrieve the Model at Inference Time\nTham khảo bài Model Registry\n7. Kết luận\nBài này, chúng ta đã bàn rất nhiều về Model Retraining. Hi vọng là bạn đã hiểu được phần nào tất cả các khía cạnh của nó để xem xét áp dụng vào dự án của bạn.\nBài viết tiếp theo, chúng ta sẽ cùng tìm hiểu về Kubernetes và áp dụng nó cho các bài toàn AI. Mời các bạn đón đọc!\n8. Tham khảo\n Mlinproduction  ","permalink":"https://tiensu.github.io/blog/41_ai_model_data_driff_and_retraining/","tags":["MLOps","Data Driff","Model Retraining"],"title":"Tìm hiểu về hiện tượng Data Driff và cách cấu hình AI Model Retraining"},{"categories":["MLOps"],"contents":"Khi giải quyết một bài toán AI, rất hiếm khi số lượng model được huấn luyện và đưa vào sử dụng dừng lại ở con số 1. Bởi vì theo thời gian, dữ liệu thay đổi, yêu cầu thay đổi, \u0026hellip; dẫn đến việc chúng ta cần cập nhật lên model mới hơn. Trong trường hợp đó, làm thế nào để quản lý được tất cả các models đó một các hợp lý, đảm bảo sử dụng đúng model mong muốn để thực hiện inference và không làm gián đoạn quá trình inference đang chạy? Đó chính là câu chuyện của Model Registry.\nTrong bài hôm nay, chúng ta sẽ cùng nhau implement một Model Registry đơn giản, sử dụng SQLite. Bạn cũng sử dụng bất kỳ database nào bạn muốn.\n1. Model Registry Database\nĐầu tiên, hãy tạo một database, registry.db, như sau:\nimport sqlite3 conn = sqlite3.connect(\u0026#39;registry.db\u0026#39;) Đối tượng conn tạo ra một kết nối đến registry.db. Chúng ta sẽ sử dụng nó để thực thi các câu lệnh truy vấn sql.\nTiếp theo, tạo bảng model_registry bao gồm các trường thông tin của model.\ncur = conn.cursor() cur.execute(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE model_registry ( id INTEGER PRIMARY KEY ASC, name TEXT UNIQUE NOT NULL, version TEXT NOT NULL, registered_date TEXT DEFAULT CURRENT_TIMESTAMP, metrics TEXT NOT NULL, remote_path TEXT NOT NULL, stage TEXT DEFAULT \u0026#39;DEVELOPMENT\u0026#39; NOT NULL ); \u0026#34;\u0026#34;\u0026#34;) cur.close() 2. Xây dựng Model Registry API\nMục đích của việc xây dựng các API là làm đơn giản hóa quá trình thao tác với database. Tất cả các công việc chung một hành động sẽ được gom vào thành một API.\nChúng ta sẽ xây dựng các API sau:\n Thêm một model mới train vào database. Cập nhật trạng thái của model. Có 2 trạng thái là Development và Production. Lấy tất cả thông tin của model.  Dưới đây là implement các API:\nimport panda as pd class ModelRegistry: def __init__(self, conn, table_name=\u0026#39;model_registry\u0026#39;): self.conn = conn self.table_name = table_name def _insert(self, values): query = \u0026#34;\u0026#34;\u0026#34; INSERT INTO {} (name, version, metrics, remote_path) VALUES (?, ?, ?, ?)\u0026#34;\u0026#34;\u0026#34;.format(self.table_name) self._query(query, values) def _query(self, query, values=None): cur = self.conn.cursor() cur.execute(query, values) cur.close() def publish_model(self, model_name, model_metrics): model_version_query = \u0026#34;\u0026#34;\u0026#34; SELECT version FROM {} WHERE name = \u0026#39;{}\u0026#39; ORDER BY registered_date DESC LIMIT 1 ;\u0026#34;\u0026#34;\u0026#34;.format(self.table_name, model_name) model_version = pd.read_sql_query(model_version_query, conn) if model_version is not None: model_version = int(version.iloc[0][\u0026#39;version\u0026#39;]) model_version = model_version + 1 # Assume that trained models are stored on S3 model_path = \u0026#39;s3://models/{}::v{}\u0026#39;.format(model_name, model_version) self._insert((model_name, model_version, model_metrics, model_path)) def update_stage(self, model_name, model_version, model_stage): query = \u0026#34;\u0026#34;\u0026#34; UPDATE {} SET stage = ? WHERE name = ? AND version = ? ;\u0026#34;\u0026#34;\u0026#34;.format(self.table_name) self._query(query, (model_stage, model_name, model_version)) def get_production_model(self, model_name): query = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM {} WHERE name = \u0026#39;{}\u0026#39; AND stage = \u0026#39;PRODUCTION\u0026#39; ;\u0026#34;\u0026#34;\u0026#34;.format(self.table_name, model_name) return pd.read_sql_query(query, self.conn) Code implement API khá đơn giản, hi vọng bạn có thể hiểu được dễ dàng, :)\n3. Sử dụng Model Registry API\nTrên thực tế , Training và Inference là 2 quá trình cùng chạy đồng thời và Model Registry cung cấp cơ chế trao đổi thông tin giữa 2 quá trình này thông qua database.\nGiả sử rằng chúng ta đã trained xong model thoả mãn yêu cầu đề bài, giờ là lúc ta sử dụng Model Registry API.\nChú ý: Sau mỗi đoạn code ví dụ, ta sẽ sử dụng câu truy vấn sau đây để kiểm tra kết quả:\npd.read_sql_query(\u0026#34;SELECT * FROM model_registry;\u0026#34;, conn) 3.1 Model Training\n Training lần đầu  conn = sqlite3.connect(\u0026#39;registry.db\u0026#39;) model_registry = ModelRegistry(conn=conn) model = None # This would be replaced by the trained model. name = \u0026#39;house_price_prediction\u0026#39; metrics = {\u0026#39;accuracy\u0026#39;: 0.87} model_registry.publish_model(model=model, name=name, metrics=metrics)    id name version registered_data remote_path stage     1 house_price_prediction 1 2021-01-10 12:42:25 s3://models/house_price_prediction::v1 DEVELOPMENT     Training lần thứ 2  model = None # This would be replaced by the trained model. name = \u0026#39;house_price_prediction\u0026#39; metrics = {\u0026#39;accuracy\u0026#39;: 0.89} model_registry.publish_model(model=model, name=name, metrics=metrics)    id name version registered_data remote_path stage     1 house_price_prediction 1 2020-07-12 12:45:27 s3://models/house_price_prediction::v1 DEVELOPMENT   2 house_price_prediction 2 2021-01-10 12:42:25 s3://models/house_price_prediction::v2 DEVELOPMENT    3.2 Chuyển model sang trạng thái sẵn sàng sử dụng cho sản phẩm thực tế\nmodel_registry.update_stage(name=name, version=\u0026#39;2\u0026#39;, stage=\u0026#34;PRODUCTION\u0026#34;)    id name version registered_data remote_path stage     1 house_price_prediction 1 2020-07-12 12:45:27 s3://models/house_price_prediction::v1 DEVELOPMENT   2 house_price_prediction 2 2021-01-10 12:42:25 s3://models/house_price_prediction::v2 PRODUCTION    3.3 Lấy thông tin model\nmodel_registry.get_production_model(name=name)    id name version registered_data remote_path stage     2 house_price_prediction 2 2021-01-10 12:42:25 s3://models/house_price_prediction::v2 PRODUCTION    4. Kết luận\nNhư vậy là chúng ta đã implemented xong Model Register, sử dụng SQLite database. Bạn hoàn toàn có thể áp dụng những gì được trình bày trong bài viết này vào trong dự án của bạn.\nHiện nay cũng có một số open-source giúp bạn thực hiện việc này một cách trực quan hơn. Nổi bật trong số đó là MLflow. Mình sẽ có một bài viết hướng dẫn sử dụng MLflow cho Model Registry trong tương lai.\nBài viết tiếp theo, mình sẽ thảo luận về vấn đề Retraining model. Mời các bạn đón đọc!\n5. Tham khảo\n Mlinproduction MLflow  ","permalink":"https://tiensu.github.io/blog/40_ai_model_registry/","tags":["MLOps"],"title":"Cấu hình AI Model Registry"},{"categories":["MLOps","Docker"],"contents":"Trong bài trước, chúng ta đã tìm hiểu và sử dụng Docker để triển khai AI model theo kiểu online inference. Trong bài này, ta sẽ train một model khác để inference theo kiểu thứ 2, batch inference, sử dụng docker. Mình cũng sẽ thực hiện việc train model bên trong docker luôn (các bài trước đó là train model bên ngoài docker, sau đó chỉ copy model đã train vào trong docker để thực hiện online inference).\n1. Tạo cấu trúc thư mục\nTrước tiên, chúng ta sẽ tạo ra một thư mục để lát nữa khi build docker image, nó sẽ được copy vào trong docker image đó.\ncode ├── Dockerfile ├── batch_inference.py └── train.py Trong đó:\n code: thư mục làm việc chính. Dockerfile: File cấu hình để build docker image. train.py: File code python để thực hiện train model. batch_inference.py: File code python để thực hiện batch inference.  2. Tạo code python để train model và inference data\nLần này, chúng ta sẽ thực hiện train model để dự đoán giá nhà tại Boson, sử dụng tập dữ liệu boson trong thư viện scikit-learn. Đây là kiểu model Leaner Regression.\nTạo file train.py với nội dung như sau:\nimport json import os from joblib import dump import matplotlib.pyplot as plt import numpy as np from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error # ############################################################################# # Load directory paths for persisting model and metadata MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Load and split data print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() print(\u0026#34;Splitting data...\u0026#34;) X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] # ############################################################################# # Fit regression model print(\u0026#34;Fitting model...\u0026#34;) params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) train_mse = mean_squared_error(y_train, clf.predict(X_train)) test_mse = mean_squared_error(y_test, clf.predict(X_test)) metadata = { \u0026#34;train_mean_square_error\u0026#34;: train_mse, \u0026#34;test_mean_square_error\u0026#34;: test_mse } # ############################################################################# # Serialize model and metadata print(\u0026#34;Serializing model to: {}\u0026#34;.format(MODEL_PATH)) dump(clf, MODEL_PATH) print(\u0026#34;Serializing metadata to: {}\u0026#34;.format(METADATA_PATH)) with open(METADATA_PATH, \u0026#39;w\u0026#39;) as outfile: json.dump(metadata, outfile) Tạo file batch_inference.py với nội dung như sau:\nimport os from joblib import load import numpy as np from sklearn import datasets from sklearn.utils import shuffle MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Get a batch of data to inference def get_data(): \u0026#34;\u0026#34;\u0026#34; Return data for inference. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] return X_test, y_test print(\u0026#34;Running inference...\u0026#34;) X, y = get_data() # ############################################################################# # Load model print(\u0026#34;Loading model from: {}\u0026#34;.format(MODEL_PATH)) clf = load(MODEL_PATH) # ############################################################################# # Run inference print(\u0026#34;Scoring observations...\u0026#34;) y_pred = clf.predict(X) print(y_pred) Mình tin chắc các bạn có thể dễ dàng hiểu được đoạn code trên. Ở đây, có một chú ý là mình sử dụng một số biến môi trường MODEL_DIR, MODEL_FILE, \u0026hellip;. Mình không muốn hard-code những biến này vì chúng được sử dụng ở 2 nơi, train và inference model. Thay vào đó, giá trị của chúng sẽ được truyền vào lúc build docker.\n3. Build Docker image để train model\nTạo file Dockerfile với nội dung như sau:\nFROM jupyter/scipy-notebook USER root WORKDIR /code ADD . /code RUN pip install joblib RUN mkdir model # Env variables ENV MODEL_DIR=/code/model ENV MODEL_FILE=clf.joblib ENV METADATA_FILE=metadata.json # COPY train.py ./train.py # COPY batch_inference.py ./batch_inference.py RUN python3 train.py Để build docker image, chạy lệnh sau:\n$ docker build -t docker-model-batch-infer . Output:\nSending build context to Docker daemon 4.608kB Step 1/10 : FROM jupyter/scipy-notebook ---\u0026gt; 069532086d63 Step 2/10 : USER root ---\u0026gt; Running in c580ea3bbd7f Removing intermediate container c580ea3bbd7f ---\u0026gt; efcad69c0b79 Step 3/10 : WORKDIR /code ---\u0026gt; Running in 08ee819c1e52 Removing intermediate container 08ee819c1e52 ---\u0026gt; d05432266229 Step 4/10 : ADD . /code ---\u0026gt; 6b1f81f3a9f6 Step 5/10 : RUN pip install joblib ---\u0026gt; Running in 9a2c41424c59 Removing intermediate container 9a2c41424c59 ---\u0026gt; 11e642103f1f Step 6/10 : RUN mkdir /code/model ---\u0026gt; Running in 5a4068194bfa Removing intermediate container 5a4068194bfa ---\u0026gt; 6ae6727bf9aa Step 7/10 : ENV MODEL_DIR=/code/model ---\u0026gt; Running in f8992466e635 Removing intermediate container f8992466e635 ---\u0026gt; c3491c25966e Step 8/10 : ENV MODEL_FILE=clf.joblib ---\u0026gt; Running in 15f321f925e4 Removing intermediate container 15f321f925e4 ---\u0026gt; a643969fdfd1 Step 9/10 : ENV METADATA_FILE=metadata.json ---\u0026gt; Running in 72c0b8ef67db Removing intermediate container 72c0b8ef67db ---\u0026gt; efce67f3494c Step 10/10 : RUN python3 train.py ---\u0026gt; Running in 10bec25fc25e Loading data... Splitting data... Fitting model... Serializing model to: /code/model/clf.joblib Serializing metadata to: /code/model/metadata.json Removing intermediate container 10bec25fc25e ---\u0026gt; 05d1c3a12437 Successfully built 05d1c3a12437 Successfully tagged docker-model-batch-infer:latest Kiểm tra nội dung file metadata:\n$ docker run docker-model-batch-infer cat /code/model/metadata.json Kết quả:\n{\u0026#34;train_mean_square_error\u0026#34;: 1.7677391462344387, \u0026#34;test_mean_square_error\u0026#34;: 6.588673999729974} Chú ý: Model ở đây chưa được tuning để tối ưu hóa hiệu năng, các thông tin metadata khác như thuật toán, phân phối dữ liệu, phiên bản model, \u0026hellip; cũng không được lưu lại. Nếu bạn sử dụng hướng dẫn này trong thực tế thì cần lưu ý những điểm trên.\n4. Thực hiện Batch Inference\nChúng ta đã có một model đã train, được lưu thành file clf.joblib trong docker image docker-model-batch-infer. Giờ là lúc sử dụng nó để thực hiện inference.\nĐể thực hiện batch inference, ta sẽ khởi động docker image và chạy lệnh thực thi code kèm theo đó:\n$ docker run docker-model-batch-infer python3 batch_inference.py Kết quả:\nRunning inference... Loading data... Loading model from: /code/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.23789723 31.94786177 10.43966955 34.25663827 22.05210667 11.58265489 13.36407623 42.87157933 33.03218733 15.77635169 23.93521876 19.88239305 25.43466604 20.55132127 13.65254047 47.45491473 17.5734174 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.51637481 30.34664466 19.67895051 23.22841646 25.02203256 18.65459129 30.09762517 8.96667041 13.8130382 14.18734797 17.3840622 19.83840166 24.23822033 20.52076144 15.32433651 25.8157052 16.47533793 19.2214524 19.87110293 21.47113681 21.56443118 24.64517965 22.43665872 22.22261406] 5. Kết luận\nNhư vậy là chúng ta đã thực hiên xong việc train một ML model và sử dụng nó để inference dữ liệu bằng docker. Tuy nhiên, để mang nó vào sử dụng trong môi trường production thì chúng ta cần thực hiện thêm một số công việc nữa như là lập lịch đê thực hiện batch inference định kỳ, tuning hyper-parameter, lưu trữ metadata của model, \u0026hellip;\nToàn bộ source code sử dụng trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong các bài viết tiếp theo, mình sẽ tổng hợp lại các thuật toán Deep Learning, sau đó sẽ đi chi tiết vào một số thụât toán với các ứng dụng cụ thể. Mời các bạn đón đọc!\n6. Tham khảo\n Docker Mlinproduction  ","permalink":"https://tiensu.github.io/blog/39_package_ai_model_using_docker_batch_inference/","tags":["MLOps","Docker"],"title":"Đóng gói quá trình Train AI model và Batch Inference sử dụng Docker"},{"categories":["MLOps","Docker"],"contents":"Đã bao giờ bạn gặp tình huống:\nTại sao code trên máy tính của tôi chạy mà mang sang máy tính của bạn lại không chạy?\n99% câu trả lời cho câu hỏi này là do sự khác biệt về môi trường giữa 2 máy tính, 1% còn lại là do các nguyên nhân khác như copy thiếu file, sai đường dẫn, sử dụng câu lệnh không đúng, \u0026hellip;\nTrong bài toán AI, tình trạng này lại càng phổ biến hơn, bởi vì một model AI yêu cầu cơ man nào là thư viện đi kèm, thư viện này liên kết, ràng buộc với thư viện kia. Không những thế, với tốc độ phát triển như vũ bão hiện nay của AI, các thư viện cũng liên tục cập nhật phiên bản mới, và có khi code sử dụng phiên bản cũ lại không chạy được trên phiên bản mới. Rồi thì thư viện A phiên bản 1.x lại chỉ tương thích với thư viện B phiên bản 1.x.x, nếu ta cứ nhắm mắt gõ pip install abc thì mặc định sẽ là phiên bản mới nhất. Rất rất nhiều vấn đề xung đột thư viện xảy ra trong phát triển một bài toán AI trên nhiều máy tính khác nhau hoặc nhiều người cùng làm việc.\nVấn đề đặt ra lúc này là phải làm sao cô lập được môi trường phát triển dành riêng cho 1 bài toán AI cụ thể. Môi trường đó phải tách biệt hoàn toàn với môi trường trên máy tính, và phải dễ dàng di chuyển giữa nhiều máy tính với nhau.\nMột số công cụ đã ra đời để hỗ trợ giải quyết vấn đề này bằng cách tạo ra các môi trường ảo. Có thể kể đến như anaconda, venv, \u0026hellip; Mỗi loại đều có những ưu nhược điểm riêng, và phụ thuộc vào thói quen sử dụng của mỗi người. Cá nhân mình cũng đã từng sử dụng qua các loại kể trên nhưng thấy chúng vẫn chưa thể giải quyết được triệt để vấn đề về xung đột môi trường \u0026hellip;\nCho đến khi mình biết đến Docker, một công cụ rất powerfull, rất tuyệt vời. Có thể nói docker đã giải quyết được tận gốc vấn đề làm đau đầu những nhà phát triể n AI bấy lâu nay.\nTrong bài này, chúng ta sẽ cùng nhau tìm hiểu về docker và cách sử dụng nó trong viêc đóng gói một AI model để thực hiện Inference theo kiểu Online Inference.\n1. Docker là gì?\nTheo định nghĩa chính thức tại trang chủ của docker thì:\nDocker is an open platform for developing, shipping, and running applications.   Hiểu một cách đơn giản thì docker là một nền tảng mã nguồn mở cho việc phát triển, chạy và phân phối các ứng dụng. Nó cho phép chúng ta tách biệt ứng dụng ra khỏi kiến trúc hạ tầng chung của toàn hệ thống và dễ dang mang toàn bộ ứng dụng đó (bao gồm cả môi trường thực thi) sang một máy tính hoàn toàn mới. Điều này giúp các nhà phát triển ứng dụng giảm được thời gian đáng kể ở công đoạn đưa sản phẩm vào sử dụng trong thực tế.\nMột số khái niệm cần biết khi làm việc với docker:\n Docker image: Là một file không thể thay đổi (read-only), chứa toàn bộ source code, thư viện, công cụ, \u0026hellip; cần thiết để một ứng dụng có thể chạy được. Docker container: Là một \u0026ldquo;bản sao\u0026rdquo;, hay một \u0026ldquo;instance\u0026rdquo; của docker image tại thời điểm khởi chạy docker image. Và thực tế là chúng ta chỉ làm việc trên các containers chứ không làm việc với các images. Sau khi kết thúc phiên làm việc thì container sẽ biến mất, và các thay đổi của container đó sẽ không được lưu lại vào docker image sinh ra container đó. Nếu bạn muốn lưu lại các thay đổi bạn đã thực hiện thì có thể sử dụng lệnh \u0026ldquo;docker commit\u0026rdquo;, nhưng nó sẽ tạo ra một docker image mới bao gồm docker image cũ và phần thay đổi. Đối với sự thay đổi trên các file, thư mục, ta có thể lưu lại sự thay đổi để sử dụng ở nơi khác mà không cần tạo docker image mới bằng cách đặt các file cần thay đổi ở thư mục chung, chia sẻ với máy tính bên ngoài (host). Docker hub: Là một kho (repository) chứa các docker images, cho phép bạn chia sẻ các docker images của bạn cho người khác bằng cách upload nó lên docker hub. Khi người nào muốn sử dụng docker image của bạn, họ chỉ cần tải về để sử dụng. Host: Là máy tính cài đặt docker và chạy các docker containers.  2. Cài đặt Docker\nĐể sử dụng docker thì trước tiên cần phải cài đặt docker engine. Các cài đặt khá đơn giản, hãy làm theo hướng dẫn trên trang chủ của docker.\nSau khi cài xong docker, hãy thử chạy lệnh sau:\n$ docker run tensorflow/tensorflow:2.3.0-gpu Nếu thấy output như sau tức là ta đã cài đặt thành công:  Ở đây, tensorflow/tensorflow:2.3.0-gpu là docker image trên docker hub, được cài đặt sẵn tensorflow 2.3.0 và cuda.\nKiểm tra image vừa tải về trong danh sách:\n$ docker images Kết quả: ```python REPOSITORY TAG IMAGE ID CREATED SIZE tensorflow/tensorflow 2.3.0-gpu 3b8d4cbd6723 3 weeks ago 3.18GB Nếu bạn muốn sử dụng GPU (giả sử là NVIDIA) trong docker thì bạn cần thêm 2 điều kiện:\n Máy tính của bạn phải có GPU và đã cài đặt đầy đủ driver, cuda, cudnn (có thể sử dụng GPU bình thường trong các task DL mà không sử dụng docker). Cài thêm NVIDIA Container Tookit theo hướng dẫn ở đây  Như trên máy tính của mình đã có đủ 2 điều kiện trên, mình kiểm tra GPU bên trong docker như sau:\n$ docker run --gpus all --rm tensorflow/tensorflow:2.3.0-gpu nvidia-smi Kết quả:  Hoặc chi tiết hơn:\n$ docker run --gpus all --rm tensorflow/tensorflow:2.3.0-gpu python -c \u0026#34;import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\u0026#34; Kết quả:  Trong đó:\n \u0026ndash;gpus all: Cho phép sử dụng GPU trong docker. \u0026ndash;rm: Xóa docker container sau khi chạy lệnh xong.  Để cho phép mặc định sử dụng GPU trong docker (không cần sử dụng \u0026ndash;gpus all), bạn có thể làm như sau:\n Thêm default-runtime\u0026quot;: \u0026quot;nvidia\u0026quot; vào trong file /etc/docker/daemon.json  # filename: /etc/docker/daemon.json { \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } }  Khởi động lại docker:  $ sudo pkill -SIGHUP dockerd Mình đã tổng hợp lại các lệnh hay sử dụng của docker ở phần phụ lục. Các bạn có thể tham khảo thêm.\n3. Xây dựng uWSGI docker image\nBên cạnh những images được xây dựng sẵn và chia sẻ trên docker hub, docker cũng hỗ trợ bạn tự build các images cho riêng bạn, phù hợp với từng nhu cầu của bạn. Tất cả được thực hiện thông qua 1 file cấu hình, gọi là Dockerfile.\nTrong phần này, ta sẽ cùng nhau xây dựng một DL docker image, bao gồm toàn bộ source code ở bài trước. Ta cũng cấu hình Flask, uWSGI trong docker image để chạy được source code đó.\n 3.1 Bước 1\nTạo một thư mục tên là uwsgi, và copy toàn bộ source code của bài trước (bao gồm cả model, bỏ đi file client.py vì ta sẽ chạy client ở bên ngoài docker) vào thư mục vừa tạo.\n3.2 Bước 2\nTạo file requirements.txt chứa toàn bộ thư viện cần dùng để chạy code (cũng đặt trong thư mục uWSGI). Bạn có thể sinh ra file này tự động bằng lệnh pip freeze \u0026gt; requirements.txt. Tuy nhiên, nếu làm theo cách này thì file requirements.txt sẽ chứa rất nhiều thư viện không cần thiết, bởi vì hầu hết chúng phụ thuộc vào các thư viện khác. Nếu bạn theo dõi từ đầu, bạn chắc chắc biết rằng, ta sẽ chỉ cần những những thư viện sau là đủ: tensorflow, uwsgi, flask, opencv. Trong đó, vì mình dự định không build docker image từ đầu mà kế thừa từ 1 image đã build sẵn (cụ thể là tensorflow/tensorflow:2.3.0-gpu đã tải về ở phần trước) nên tensorflow đã được tích hợp sẵn, không cần cài lại nữa. Cuối cùng, file requirements.txt của chúng ta chỉ như sau:\nFlask==1.1.2 uWSGI==2.0.18 opencv-python==4.4.0.46 3.3 Bước 3\nTạo file cấu hình cho uWSGI. Vì mình muốn kiểm tra riêng sự hoạt động của uWSGI nên file cấu hình sẽ như sau:\n[uwsgi] http = 0.0.0.0:8080 wsgi-file = server.py callable = app die-on-term = true processes = 4 threads = 2 chdir = /uwsgi master = false vacuum = truemodule Trong phần sau, chúng ta sẽ kết hợp thêm Nginx. Khi đó sẽ cần thay đổi lại cấu hình của uWSGI lại một chút.\n3.3 Bước 4\nTạo Dockerfile (trong thư mục uwsgi) chứa các thông tin cấu hình cần thiết để tạo docker image. Nội dung của file này như sau:\nFROM tensorflow/tensorflow:2.3.0-gpu # kế thừa từ image tensorflow/tensorflow:2.3.0-gpu WORKDIR /uwsgi # thư mục làm viêc mặc định bên trong docker ADD . /uwsgi # local folder để copy vào thư mục làm việc của docker, chính là thư mục chúng ta tạo ở bước 1 RUN pip install -r requirements.txt # cài đặt các thư viện cần thiết trong file requirements.txt RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y # sửa lỗi opencv, thử bỏ đi để xem điều gì xảy ra? CMD [\u0026#34;uwsgi\u0026#34;, \u0026#34;app.ini\u0026#34;] # chạy lệnh \u0026#34;uwsgi app.ini\u0026#34; khi khởi chạy docker image Có rất rất nhiều tùy chọn khi viết Dockerfile, tham khảo ở đây nếu bạn cần thêm thông tin.\nThự mục uwsgi lúc này sẽ như sau:\n├── animal_model_classification.h5 ├── app.ini ├── cat.1.jpg ├── Dockerfile ├── dog.1.jpg ├── requirements.txt └── server.py Trong đó, 2 files ảnh là để chúng ta thực hiện việc test về sau.\n4 Build docker image\nOK, mọi thứ cần thiết đã chuẩn bị xong, ta sẽ chạy lệnh sau để buidl docker image:\n$ docker build -t image-classification-production:1.0 . Docker image được sinh ra sẽ có tên là image-classification-production, kèm theo tag 1.0 để phân biệt nó với các phiên bản khác trong tương lai.\nNếu build, thành công, output sẽ như sau:\n---\u0026gt; a2a60f32471b Step 7/7 : CMD [\u0026#34;uwsgi\u0026#34;, \u0026#34;app.ini\u0026#34;] ---\u0026gt; Running in 3776d496ca68 Removing intermediate container 3776d496ca68 ---\u0026gt; 53150d1373a3 Successfully built 53150d1373a3 Successfully tagged image-classification-production:1.0 Kiểm tra docker image trong danh sách:\ndocker images Kết quả:\nREPOSITORY TAG IMAGE ID CREATED SIZE image-classification-production 1.0 53150d1373a3 About a minute ago 5.37GB tensorflow/tensorflow 2.3.0-gpu 3b8d4cbd6723 3 weeks ago 3.18GB 5 Chạy docker image\nĐể chạy docker image vừa tạo, ta sử dụng lệnh sau:\n$ docker run --rm --publish 80:8080 --name dlp image-classification-production:1.0 Có 2 cái mà ta phải chú ý ở đây:\n Tham số --public 80:8080 sẽ \u0026ldquo;expose\u0026rdquo; port 8080 của container tới port 80 của host. Nói cách khác, tất cả các requests đến địa chỉ localhost:80 sẽ được chuyển tiếp đến địa chỉ 0.0.0.0:8080 bên trong container. 8080 được gọi là listening port của uWSGI. Tham số --name dlp sẽ đặt tên cho container là dlp. Ta nên đặt tên cho container để dễ làm việc với nó hơn. Ngược lại, docker sẽ tạo cho nó một ID ngẫu nhiên.  Kết quả:\n2021-01-06 02:52:45.347188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2021-01-06 02:52:45.347580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2021-01-06 02:52:45.347925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4676 MB memory) -\u0026gt; physical GPU (device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5) 2021-01-06 02:52:47.271932: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:47.621364: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:47.918398: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:48.759371: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:49.637126: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. WSGI app 0 (mountpoint=\u0026#39;\u0026#39;) ready in 8 seconds on interpreter 0x55cf948bc720 pid: 1 (default app) uWSGI running as root, you can use --uid/--gid/--chroot options *** WARNING: you are running uWSGI as root !!! (use the --uid flag) *** *** uWSGI is running in multiple interpreter mode *** spawned uWSGI worker 1 (and the only) (pid: 1, cores: 1) Ta quan sá thấy container đã chạy thành công và uWSGI cũng đã được khởi động.\nLưu ý: Trong trường hợp port 80 đã được sử dụng bới ứng dụng khác (nginx trong bài trước chẳng hạn), bạn phải close ứng dụng đó hoặc sử dụng một port khác.\nĐể kiểm tra xem uWSGI có làm viêc đúng hay không, ta có thể sử dụng lại client đã chuẩn bị từ bài trước (nhớ đổi port của ENDPOINT_URL từ 8080 thành 80).\n$ python client.py Kết quả:\nb\u0026#39;cat\u0026#39; Như vậy là uWSGI đã được cài đặt thành công vào docker.\n Lưu ý: Trong quá trình viết bài này, mình gặp lỗi liên quan đến cuda khi chạy test uWSGI. Mình xóa hết các docker images đi và chạy lại từ đầu thì không bị lỗi nữa.\n6. Xây dựng Nginx docker image\nTương tự như việc xây dựng uWSGI docker image, chúng ta sẽ đi build một Nginx docker image, đặt trước uWSGI server thực hiện vai trò như một reverse proxy.\n6.1 Bước 1\nTạo thư mục nginx, cùng cấp với thư mục uwsgi.\n6.2 Bước 2\nTạo file cấu hình Nginx, tên là nginx.conf, đặt trong thư mục nginx, với nội dung như sau:\nserver { listen 80; location / { include uwsgi_params; uwsgi_pass uwsgi:660 ; } Với cấu hình này thì Nginx sẽ lắng nghe trên port 80, chuyển tiếp các requests đến port 660 của uWSGI server thông qua socket (sử dụng giao thức uwsgi).\n6.3 Bước 3\nCập nhật lại cấu hình của uWSGI (file app.ini) để làm việc được với Nginx, như sau:\nmodule = server socket= :660 callable = app die-on-term = true processes = 1 master = false vacuum = true 6.4 Bước 4\nTạo file Dockerfile cho Nginix docker trong thư mục nginx. Nginix docker image được kế thừa từ nginx image trên docker hub, ta chỉ việc thay thế cấu hình mặc định của nó bằng cấu hình mà ta vừa tạo ở bước 3.\nFROM nginx RUN rm /etc/nginx/conf.d/default.conf COPY nginx.conf /etc/nginx/conf.d/ Đến đây, nếu chạy lệnh docker build ... thì ta sẽ có được nginx docker image. Nhưng nếu chỉ chạy một mình image này thì không có tác dụng gì cả. Ta cần phải kết hợp cả 2 docker images uwsgi và nginx. Đó chính là công viêc của docker-compose.\n7. Chạy đồng thời nhiều docker containers với Docker Compose\nLiệu bạn có thắc mắc rằng tại sao ta không build cả Nginx và uWSGI vào chung 1 docker image? Chẳng phải như thế sẽ tiện hơn hay sao?\nCâu trả lời là không nên làm vậy. Theo kiến trúc làm việc kết hợp giữa Nginx và uWSGI thì một Nginx instance có thể kết hợp với nhiề u uWSGI instances. Nếu ta kết hợp chung lại, sẽ không tận dụng được khả năng này. Thêm nữa, dung lượng của docker image sẽ rất lớn nếu ta kết hợp lại.\n Mở rộng ra, nếu một hệ thống của chúng ta bao gồm cả database, backend, front-end, messaging systems, task queue, \u0026hellip; ta không thể chạy tất tần tật mọi thứ trong một docker container được.\nTừ góc độ của nhà phát triển phần mềm, docker-compose chỉ là một file cấu hình, định nghĩa tất cả containers và cách thức mà các containers đó tương tác với nhau.\n7.1 Cài đặt docker-compose\nĐể cài đặt docker-compose. Bạn hãy làm theo hướng dẫn sau trên trang chủ của docker.\n7.2 Định nghĩa cấu hình của docker-compose\nTạo file docker-compose.yml (bên ngoài 2 thư mục uwsgi và nginx), với nội dung như sau:\nversion : \u0026#34;3.7\u0026#34; services: uwsgi: build: ./uwsgi container_name: uwsgi_img_classification restart: always expose: - 660 nginx: build: ./nginx container_name: nginx restart: always ports: - \u0026#34;80:80\u0026#34; Phần chính của cấu hình này là khai báo 2 containers, gọi là 2 services. Hai tham số quan trọng của mỗi services là:\n build: thư mục chứa Dockerfile và các files cần thiết của mỗi container. restart: tự động khởi động lại service nếu xay ra lỗi. expose: uwsgi lắng nghe request đến trên port 660 (chỉ trong phạm vi docker). port: nginx mở port 80 ra bên ngoài (có thể chọn tùy ý) để lắng nghe requests đến, ánh xạ đến port 80 (theo như cấu hình trong file nginx.conf) của container.  Như vậy, có thể tóm tắt lại flow như sau:\n Các requests từ clients đến port 80 của host. Các requests được ánh xạ sang port 80 của nginx container. Các requests tiếp tục được chuyển tiếp đến port 660 của uwsgi container. uwsgi gọi Flask endpoint và thực hiện quá trình nhận diện. uwsgi gửi lại kết quả nhận diện theo hướng ngược lại.  7.3 Build docker-compose\nChạy lệnh sau để build docker-compose với cả 2 containers.\n$ docker-compose build Nếu build thành công, output sẽ như sau:\nStep 7/7 : CMD [\u0026#34;uwsgi\u0026#34;, \u0026#34;app.ini\u0026#34;] ---\u0026gt; Running in 9608e1187e82 Removing intermediate container 9608e1187e82 ---\u0026gt; 357fe8e41768 Successfully built 357fe8e41768 Successfully tagged docker_uwsgi:latest Building nginx Step 1/3 : FROM nginx latest: Pulling from library/nginx 6ec7b7d162b2: Pull complete cb420a90068e: Pull complete 2766c0bf2b07: Pull complete e05167b6a99d: Pull complete 70ac9d795e79: Pull complete Digest: sha256:4cf620a5c81390ee209398ecc18e5fb9dd0f5155cd82adcbae532fec94006fb9 Status: Downloaded newer image for nginx:latest ---\u0026gt; ae2feff98a0c Step 2/3 : RUN rm /etc/nginx/conf.d/default.conf ---\u0026gt; Running in 11140e051282 Removing intermediate container 11140e051282 ---\u0026gt; 1fcc92cfdfc4 Step 3/3 : COPY nginx.conf /etc/nginx/conf.d/ ---\u0026gt; 21bda0089cca Successfully built 21bda0089cca Successfully tagged docker_nginx:latest Kiểm tra thử danh sách images bằng lệnh docker images:\nREPOSITORY TAG IMAGE ID CREATED SIZE docker_nginx latest 21bda0089cca 4 minutes ago 133MB docker_uwsgi latest 357fe8e41768 5 minutes ago 5.37GB image-classification-production 1.0 bd9928abee21 2 hours ago 5.37GB nginx latest ae2feff98a0c 3 weeks ago 133MB tensorflow/tensorflow 2.3.0-gpu 3b8d4cbd6723 3 weeks ago 3.18GB Ta thấy hai containers docker_nginx và docker_uwsgi đã xuất hiện.\n7.4 Kiểm tra hoạt động của hệ thống\nTa sẽ khởi động các containers lên để kiểm tra thử xem hê thống có làm việc chính xác không.\n$ docker-compose up Khởi động thành công:\nStarting nginx ... done Starting uwsgi_img_classification ... done Attaching to nginx, uwsgi_img_classification nginx | /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration nginx | /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ uwsgi_img_classification | [uWSGI] getting INI configuration from app.ini ..... uwsgi_img_classification | 2021-01-06 09:18:12.292414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4662 MB memory) -\u0026gt; physical GPU (device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5) uwsgi_img_classification | 2021-01-06 09:18:14.386546: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:14.737805: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:15.045424: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:16.238951: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:17.246651: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | WSGI app 0 (mountpoint=\u0026#39;\u0026#39;) ready in 7 seconds on interpreter 0x56494b98c680 pid: 1 (default app) uwsgi_img_classification | uWSGI running as root, you can use --uid/--gid/--chroot options uwsgi_img_classification | *** WARNING: you are running uWSGI as root !!! (use the --uid flag) *** uwsgi_img_classification | *** uWSGI is running in multiple interpreter mode *** uwsgi_img_classification | spawned uWSGI worker 1 (and the only) (pid: 1, cores: 1) Chạy client để nhận diện: python client.py.\nKết quả:\nb\u0026#39;cat\u0026#39; 8. Kết luận\nPhù, thật tuyệt vời, mọi thứ đã chạy đúng như mong muốn.\nBài hôm nay khá là dài và khó. Mình đã phải thực hiện cài cắm rất nhiều lần để có thể hoàn thành bài viết này. Hi vọng sẽ có ích cho các bạn trong việc tìm kiếm giải pháp triể n khải AI model vào trong các sản phẩm để đưa đến tay người dùng!\nToàn bộ source code sử dụng trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây. Giống như bài trước, vì model animal_model_classification.h5 có dung lượng khá lớn (\u0026gt; 1.5GB) nên mình không upload lên github được. Các bạn hãy sử dụng model của chính mình để thực hành nhé!\nTrong các bài viết tiếp theo, mình sẽ sử dụng docker để train một model khác và thực hiện batch inference bằng model đó. Mời các bạn đón đọc!\n9. Phụ lục một số lệnh cơ bản của Docker\n1. List docker image $ docker images 2. List container $ docker ps \u0026lt;-a\u0026gt; 3. Run a docker $ docker run -it [image_name] bash 4. Access to running container $ docker exec -it [container_id or container_name] bash 5. Commit change of container to docker image $ docker commit [container_name or container_id] [new_image_name] 6. Stop running container $ docker stop [container_id or container_name] $ docker stop $(docker ps -aq) # Stop all container 7. Start stoped container $ docker start [container_id or container_name] 8. Remove container $ docker rm [container_id or container_name] $ docker rm $(docker ps -aq) # Remove all 9. Export container $ docker export [container_id or container_name] | gzip \u0026gt; file_export.tar.gz 10. Import docker =\u0026gt; images $ zcat file_export.tar.gz | docker [new_name_image] $ docker images # check 11. Remove docker image $ docker rmi [image_name] Loi: docker: Error response from daemon: Unknown runtime specified nvidia. Solution: 1. $ sudo systemctl daemon-reload $ sudo systemctl restart docker 2. $ sudo mkdir -p /etc/systemd/system/docker.service.d $ sudo tee /etc/systemd/system/docker.service.d/override.conf \u0026lt;\u0026lt;EOF [Service] ExecStart= ExecStart=/usr/bin/dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime EOF $ sudo systemctl daemon-reload $ sudo systemctl restart docker ** Move docker image to other computer 1. Save images $ docker save \u0026lt;REPOSITORY\u0026gt; \u0026gt; \u0026lt;images_name\u0026gt;.tar 2. Load images $ docker load \u0026lt; \u0026lt;images_name\u0026gt;.tar 3. Run images $ docker run -it --runtime=nvidia --rm --net=host --privileged \u0026lt;Image ID\u0026gt; 10. Tham khảo\n Docker Nginx uWSGI Flask AI Summer  ","permalink":"https://tiensu.github.io/blog/38_package_ai_model_using_docker_online_inference/","tags":["MLOps","Docker"],"title":"Đóng gói AI model theo kiểu Online Inference sử dụng Docker"},{"categories":["MLOps"],"contents":"Như đã giới thiệu trong bài trước, mặc dù Flask rất dễ để sử dụng nhưng nó không có đầy đủ chức năng để có thể áp dụng vào các sản phẩm trong thực tế. Đó là tính bảo mật, khả năng xử lý đồng thời nhiều kết nối, khả năng mở rộng và nâng cấp model, \u0026hellip; Sử dụng kết hợp bộ ba Flask, uWSGI và Nginx chính là giải pháp hữu hiệu khắc phục những thiếu sót này. Trong bài viết này, chúng ta sẽ cùng tìm hiểu cách cài đặt, cấu hình và sử dụng bộ 3 kể trên để triển khai Animal Classification model dưới dạng server phục vụ các yêu cầu nhận dạng từ các clients.\n1. WSGI, uWSGI, và uwsgi là gì?\nTrước tiên cần hiểu rõ một số thuật ngữ mà ta sử dụng trong bài này.\n WSGI: Viết tắt của Web Server Gateway Interface, là một Interface giữa server và client, được viết bằng python. Hiểu một cách đơn giản, nó quy định các thức để client có thể kết nối và gửi nhận dữ liệu với server. uWSGI: Là một server, sử dụng WSGI để giao tiếp với client (hoặc sử dụng giao thức HTTP trong trường hợp client là web application). uwsgi: Là một giao thức ở tầng thấp hơn, cho phép các servers giao tiếp với nhau.  Bạn có thể xem sơ đồ kiến trúc triển khai sử dụng uWSGI như hình bên dưới đây:\n Phần xử lý nhận diện của ta vẫn được gọi thông qua Flask. Phía trước Flask ta đặt uWSGI rồi đến web application (client).\nViệc đặt uWSGI như vậy mang lại cho ta các lợi ích như sau:\n Quản lý tiến trình: Quản lý việc tạo và duy trì các tiến trình trong quá trình làm việc. Các tiến trình được đồng bộ với nhau trong cùng 1 môi trường và có khả năng scale-up để phục vụ cho nhiều users. Cân bằng tải: Phân phối tải (các requests) đến các tiến trình khác nhau. Giám sát: Giám sát hiệu năng và tài nguyên sử dụng. Hạn chế tài nguyên: Cho phép chỉ định mức tối đa tài nguyên có thể sử dụng.  2. Nginx là gì và tại sao phải sử dụng nó?\nNginx là một webserver với các đặc tính:\n High performance: Hiệu năng cao Highly scalable: Khả năng mở rộng cao Highly available: Tính sẵn sàng cao  Nó hoạt động giống như một bộ cân bằng tải, một reverse proxy cùng với cơ chế caching, cơ chế mã hóa và bảo mật trên các bản tin giao tiếp giữa client và server. Nginx được cho là có thể phục vụ hơn 10,000 kết nối đồng thời.\nNginx được sử dụng khá phổ biến trong các công ty công nghệ lớn, trong nhiều sản phẩm, ứng dụng cần phục vụ số lượng lớn người dùng đồng thời.\nXét về kiến trúc tổng thể, nó thường được sử dụng cùng với uWSGI, đứng trước uWSGI như trong hình sau:\n Mục đích của việc sử dụng đồng thời cả uWSGI và Nginx là để tận dụng những ưu điểm của cả 2. Tất nhiên điều này là không bắt buộc nếu ứng dụng của chúng ta ở mức đơn giản, không cần phải phục vụ số lượng users đồng thời quá lớn. Nhưng dù sao vẫn nên sử dụng kiến trúc này để có thể dễ dàng mở rộng ứng dụng về sau.\n3. Chuẩn bị Flask server\nTa vẫn cần có Flask làm server trực tiếp xử lý request từ client. Có thể hiểu Flask server ở đây là endpoint cũng được. Mình sẽ sử dụng lại file server.py ở bài trước, nhưng đã bỏ đi phần phục vụ web client.\nimport cv2 import os import numpy as np import tensorflow as tf from flask_cors import CORS from tensorflow.keras.models import load_model from flask import Flask, request, render_template, make_response, jsonify config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) app = Flask(__name__) CORS(app) image_width = 300 image_height = 300 classes = [\u0026#39;cat\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;pandas\u0026#39;] APP_ROOT_2 = os.getenv(\u0026#39;APP_ROOT\u0026#39;, \u0026#39;/infer\u0026#39;) model = load_model(\u0026#39;animal_model_classification.h5\u0026#39;) @app.route(APP_ROOT_2, methods=[\u0026#34;POST\u0026#34;]) def infer(): data = request.json img_path = data[\u0026#39;img_path\u0026#39;] return classify_animal(img_path) def classify_animal(img_path): # read image image = cv2.imread(img_path) image = image/255 image = cv2.resize(image, (image_width,image_height)) image = np.reshape(image, [1,image_width,image_height,3]) # pass the image through the network to obtain our predictions preds = model.predict(image) label = classes[np.argmax(preds)] return label if __name__ == \u0026#39;__main__\u0026#39;: app.run() 4. Cài đặt và cấu hình uWSGI\nĐể cài đặt uWSGI, sử dụng lệnh sau:\n$ pip install uwsgi Kiểm tra cài đặt bằng cách chạy câu lệnh sau:\n$ uwsgi --http 0.0.0.0:8080 --wsgi-file server.py --callable app Nếu output như sau thì tức là viêc cài đặt thành công:\n Câu lệnh trên có ý nghĩa là chạy một server tại địa chỉ 0.0.0.0, port 8080, sử dụng ứng dụng đặt trong file server.py.\nuWSGI có rất nhiều tùy chọn cấu hình. Do đó, để thuận tiện, ta thường tạo một file cấu hình, tên là app.ini như sau:\n[uwsgi] http = 0.0.0.0:8080 # địa chỉ server socket = service.sock # socket giao tiếp với Nginx chmod-socket = 660 # cấp quyền truy câp socket wsgi-file = server.py # file chứa code xử lý yêu cầu từ client callable = app # function được gọi khi tạo uWSGI die-on-term = true # cho phép kill server từ terminal processes = 4 # số lượng process threads = 2 # số lượng thread chdir = /media/sunt/DATA/GITHUB/Model_Deployment/uWSGI/ # thư mục dự án virtualenv = /home/sunt/anaconda3/envs/tf2/ # môi trường ảo (nếu có) master = false vacuum = truemodule # định kỳ xóa những file ko cần thiết được sinh ra Để chạy uWSGI, dùng lệnh:\n$ uwsgi app.ini Nếu thành công, output trên terminal sẽ như sau:\n 5. Cài đặt và cấu hình Nginx\nĐể cài đặt Nginx, sử dụng lệnh sau:\n$ sudo apt-get install nginx Tiếp theo, tạo một file cấu hình đặt trong thư mục /etc/nginx/sites-available, tên là service.conf, với nội dụng như sau:\nserver { listen 80; server_name 0.0.0.0; location / { include uwsgi_params; uwsgi_pass unix:/media/sunt/DATA/GITHUB/Model_Deployment/uWSGI/service.sock; } } Theo cấu hình này, Nginx sẽ lắng nghe trên cổng 80 (mặc định) cho tất cả các yêu cầu đến server đặt tại địa chỉ 0.0.0.0. Các yêu cầu sau đó được chuyển đến uWSGI server thông qua socket service.sock (sử dụng giao thức uwsgi).\nTiếp theo, để áp dụng các cấu hình trên cho Nginx, ta cần trỏ liên kết của chúng tới thư mục sites-enabled:\n$ sudo ln -s /etc/nginx/sites-available/service.config /etc/nginx/sites-enabled Kiểm tra lại xem các cấu hình đã đúng hay chưa?:\n$ sudo nginx -t Nếu mọi thứ OK, sẽ có output như sau:\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful Cuối cùng, restart lại Nginx server:\n$ sudo systemctl status nginx   6. Tạo client và kiểm tra kết quả\nSử dụng lại client viết bằng python như ở bài trước:\nimport requests from PIL import Image import numpy as np ENDPOINT_URL = \u0026#39;http://0.0.0.0:8080/infer2\u0026#39; def infer(): data = { \u0026#39;img_path\u0026#39;: \u0026#39;dog.1.jpg\u0026#39; } response = requests.post(ENDPOINT_URL, json = data) response.raise_for_status() print(response.content) if __name__ ==\u0026#34;__main__\u0026#34;: infer() Để kiểm tra hoạt động của client và server, đầu tiên khởi chạy uWSGI (như phần 4.), sau đó chạy client:\n$ python client.py Nếu nhận được kết quả trả vê từ server tức là hệ thống đã hoạt động chính xác:\nb`dog` 7. Kết luận\nNhư vậy là chúng ta đã cùng nhau triển khai thành công DL model sử dụng Nginx, uWSGI và Flask. Nginx thì mặc định được chạy dưới dạng service sau khi cài đặt xong, còn uWSGI thì không. Ta nên cấu hình uWSGI để nó cũng chạy dưới dạng service cho thuận tiện sử dụng. Hi vọng qua bài này, các bạn đã hiểu rõ hơn về cách thức triển khai một AI model trong các sản phẩm thực tế, đáp ứng số lượng lớn user sử dụng đồng thời.\nToàn bộ source code của backend và front-end trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây. Vì model animal_model_classification.h5 có dung lượng khá lớn (\u0026gt; 1.5GB) nên mình không upload lên github được. Các bạn hãy sử dụng model của chính mình để thực hành nhé!\nBài viết tiếp theo, chúng ta sẽ đi nâng cao hơn 1 chút nữa, đó là đóng gói tất cả những phần đã làm hôm nay vào một cái gọi là docker. Docker là gì, và tại sao lại nên dùng nó? Tất cả sẽ được giải đáp trong bài viết đó. Mời các bạn đón đọc!\n8. Tham khảo\n Nginx uWSGI Flask AI Summer  ","permalink":"https://tiensu.github.io/blog/37_deploy_ai_model_with_uwsgi_online_inference/","tags":["MLOps"],"title":"Triển khai AI model sử dụng uWSGI và Nginx"},{"categories":["Text Classification"],"contents":"Bài này mình xin phép đổi chủ đề một chút. Chúng ta sẽ thử làm bài toán phân loại text theo các chủ đề khác nhau. Đây là một trong những bài toán thuộc phạm vi của chủ đề xử lý ngôn ngữ tự nhiên (NLP).\nMình sẽ sử dụng bộ dữ liệu BBC news để thực hành. Bạn hãy download của 2 file Train.csv và Test.csv, sau đó gộp chung chúng lại thành 1 file để làm dữ liệu huấn luyện. Tổng số records là 2225, chia thành 6 chủ đề.\nĐầu tiên, import các thư viện sẽ sử dụng:\nimport csv import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences Khai báo một số tham số:\nvocab_size = 1000 embedding_dim = 16 max_length = 120 trunc_type = \u0026#39;post\u0026#39; padding_type = \u0026#39;post\u0026#39; oov_tok = \u0026#39;\u0026lt;OOV\u0026gt;\u0026#39; training_portion = 0.8 sentences = [] labels = [] stopwords = [ \u0026#34;a\u0026#34;, \u0026#34;about\u0026#34;, \u0026#34;above\u0026#34;, \u0026#34;after\u0026#34;, \u0026#34;again\u0026#34;, \u0026#34;against\u0026#34;, \u0026#34;all\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;and\u0026#34;, \u0026#34;any\u0026#34;, \u0026#34;are\u0026#34;, \u0026#34;as\u0026#34;, \u0026#34;at\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;because\u0026#34;, \u0026#34;been\u0026#34;, \u0026#34;before\u0026#34;, \u0026#34;being\u0026#34;, \u0026#34;below\u0026#34;, \u0026#34;between\u0026#34;, \u0026#34;both\u0026#34;, \u0026#34;but\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;could\u0026#34;, \u0026#34;did\u0026#34;, \u0026#34;do\u0026#34;, \u0026#34;does\u0026#34;, \u0026#34;doing\u0026#34;, \u0026#34;down\u0026#34;, \u0026#34;during\u0026#34;, \u0026#34;each\u0026#34;, \u0026#34;few\u0026#34;, \u0026#34;for\u0026#34;, \u0026#34;from\u0026#34;, \u0026#34;further\u0026#34;, \u0026#34;had\u0026#34;, \u0026#34;has\u0026#34;, \u0026#34;have\u0026#34;, \u0026#34;having\u0026#34;, \u0026#34;he\u0026#34;, \u0026#34;he\u0026#39;d\u0026#34;, \u0026#34;he\u0026#39;ll\u0026#34;, \u0026#34;he\u0026#39;s\u0026#34;, \u0026#34;her\u0026#34;, \u0026#34;here\u0026#34;, \u0026#34;here\u0026#39;s\u0026#34;, \u0026#34;hers\u0026#34;, \u0026#34;herself\u0026#34;, \u0026#34;him\u0026#34;, \u0026#34;himself\u0026#34;, \u0026#34;his\u0026#34;, \u0026#34;how\u0026#34;, \u0026#34;how\u0026#39;s\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;i\u0026#39;d\u0026#34;, \u0026#34;i\u0026#39;ll\u0026#34;, \u0026#34;i\u0026#39;m\u0026#34;, \u0026#34;i\u0026#39;ve\u0026#34;, \u0026#34;if\u0026#34;, \u0026#34;in\u0026#34;, \u0026#34;into\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;it\u0026#34;, \u0026#34;it\u0026#39;s\u0026#34;, \u0026#34;its\u0026#34;, \u0026#34;itself\u0026#34;, \u0026#34;let\u0026#39;s\u0026#34;, \u0026#34;me\u0026#34;, \u0026#34;more\u0026#34;, \u0026#34;most\u0026#34;, \u0026#34;my\u0026#34;, \u0026#34;myself\u0026#34;, \u0026#34;nor\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;on\u0026#34;, \u0026#34;once\u0026#34;, \u0026#34;only\u0026#34;, \u0026#34;or\u0026#34;, \u0026#34;other\u0026#34;, \u0026#34;ought\u0026#34;, \u0026#34;our\u0026#34;, \u0026#34;ours\u0026#34;, \u0026#34;ourselves\u0026#34;, \u0026#34;out\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;own\u0026#34;, \u0026#34;same\u0026#34;, \u0026#34;she\u0026#34;, \u0026#34;she\u0026#39;d\u0026#34;, \u0026#34;she\u0026#39;ll\u0026#34;, \u0026#34;she\u0026#39;s\u0026#34;, \u0026#34;should\u0026#34;, \u0026#34;so\u0026#34;, \u0026#34;some\u0026#34;, \u0026#34;such\u0026#34;, \u0026#34;than\u0026#34;, \u0026#34;that\u0026#34;, \u0026#34;that\u0026#39;s\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;their\u0026#34;, \u0026#34;theirs\u0026#34;, \u0026#34;them\u0026#34;, \u0026#34;themselves\u0026#34;, \u0026#34;then\u0026#34;, \u0026#34;there\u0026#34;, \u0026#34;there\u0026#39;s\u0026#34;, \u0026#34;these\u0026#34;, \u0026#34;they\u0026#34;, \u0026#34;they\u0026#39;d\u0026#34;, \u0026#34;they\u0026#39;ll\u0026#34;, \u0026#34;they\u0026#39;re\u0026#34;, \u0026#34;they\u0026#39;ve\u0026#34;, \u0026#34;this\u0026#34;, \u0026#34;those\u0026#34;, \u0026#34;through\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;too\u0026#34;, \u0026#34;under\u0026#34;, \u0026#34;until\u0026#34;, \u0026#34;up\u0026#34;, \u0026#34;very\u0026#34;, \u0026#34;was\u0026#34;, \u0026#34;we\u0026#34;, \u0026#34;we\u0026#39;d\u0026#34;, \u0026#34;we\u0026#39;ll\u0026#34;, \u0026#34;we\u0026#39;re\u0026#34;, \u0026#34;we\u0026#39;ve\u0026#34;, \u0026#34;were\u0026#34;, \u0026#34;what\u0026#34;, \u0026#34;what\u0026#39;s\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;when\u0026#39;s\u0026#34;, \u0026#34;where\u0026#34;, \u0026#34;where\u0026#39;s\u0026#34;, \u0026#34;which\u0026#34;, \u0026#34;while\u0026#34;, \u0026#34;who\u0026#34;, \u0026#34;who\u0026#39;s\u0026#34;, \u0026#34;whom\u0026#34;, \u0026#34;why\u0026#34;, \u0026#34;why\u0026#39;s\u0026#34;, \u0026#34;with\u0026#34;, \u0026#34;would\u0026#34;, \u0026#34;you\u0026#34;, \u0026#34;you\u0026#39;d\u0026#34;, \u0026#34;you\u0026#39;ll\u0026#34;, \u0026#34;you\u0026#39;re\u0026#34;, \u0026#34;you\u0026#39;ve\u0026#34;, \u0026#34;your\u0026#34;, \u0026#34;yours\u0026#34;, \u0026#34;yourself\u0026#34;, \u0026#34;yourselves\u0026#34; ] Chúng ta có một mảng chứa các stop words, tức là các từ thường hay xuất hiện trong câu nhưng lại không mang nhiều ý nghĩa. Chúng ta sẽ loại bỏ chúng đi trước khi huấn luyện model phân loại.\nBây giờ, ta sẽ đọc dataset và chuẩn bị dữ liệu training:\nwith open(\u0026#39;bbc-text.csv\u0026#39;, \u0026#39;r\u0026#39;) as csvfile: reader = csv.reader(csvfile, delimiter=\u0026#39;,\u0026#39;) next(reader) for row in reader: labels.append(row[0]) sentence = row[1] # remove stop words for word in stopwords: token = \u0026#39; \u0026#39; + word + \u0026#39; \u0026#39; sentence = sentence.replace(token, \u0026#39; \u0026#39;) sentence = sentence.replace(\u0026#39; \u0026#39;, \u0026#39; \u0026#39;) sentences.append(sentence) print(len(sentences)) Chia dataset thành 2 phần: train và validation:\ntrain_size = int(len(sentences) * training_portion) train_sentences = sentences[:train_size] train_labels = labels[:train_size] validation_sentences = sentences[train_size:] validation_labels = labels[train_size:] Để model có thể hiểu được dataset, cần phải chuyển các câu dạng text sang dạng vector:\n# chuyển text sang vector tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok) tokenizer.fit_on_texts(train_sentences) word_index = tokenizer.word_index label_tokenizer = Tokenizer() label_tokenizer.fit_on_texts(labels) training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels)) validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels)) # padding để các câu có cùng chiều dài train_sequences = tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length) validation_sequences = tokenizer.texts_to_sequences(validation_sentences) validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length) Mình sẽ đi chi tiết phần này trong 1 bài viết khác. Hôm nay các bạn chỉ cần hiểu ý tưởng của nó như vậy là được rồi.\nSau khi đã có dữ liệu huấn luyện, giờ là lúc chúng ta định nghĩa kiến trúc model.\nmodel = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), tf.keras.layers.GlobalAveragePooling1D(), tf.keras.layers.Dense(24, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(6, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) model.summary() Ở bài này, mình chỉ sử dụng một model đơn giản gồm các lớp Embedding, GlobalAveragePooling1D, và Dense.\nHàm plot để vẽ đồ thị quá trình training:\ndef plot_graph(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10,6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training \u0026amp; Validation, Accuracy \u0026amp; Loss\u0026#39;) plt.legend(loc=0) plt.show() Bước cuối cùng là chạy train model, ta sẽ train model với 30 epochs:\nnum_epochs = 30 history = model.fit( train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=1 ) Training output:\nEpoch 1/30 56/56 [==============================] - 0s 4ms/step - loss: 1.7737 - acc: 0.2180 - val_loss: 1.7481 - val_acc: 0.2270 Epoch 2/30 56/56 [==============================] - 0s 2ms/step - loss: 1.7163 - acc: 0.2303 - val_loss: 1.6755 - val_acc: 0.2270 Epoch 3/30 56/56 [==============================] - 0s 2ms/step - loss: 1.6299 - acc: 0.2371 - val_loss: 1.5792 - val_acc: 0.2539 .................... Epoch 27/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0580 - acc: 0.9949 - val_loss: 0.2115 - val_acc: 0.9506 Epoch 28/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0521 - acc: 0.9961 - val_loss: 0.2080 - val_acc: 0.9506 Epoch 29/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0467 - acc: 0.9961 - val_loss: 0.2051 - val_acc: 0.9506 Epoch 30/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0420 - acc: 0.9989 - val_loss: 0.2014 - val_acc: 0.9506 Quá trình train diễn ra khá nhanh, mất khoảng 2 phút trên máy tính của mình. Tại epochs cuối cùng, độ chính xác trên tập validation là 95,06%.\nToàn bộ quá trình này được thể hiện trên đồ thị như sau:\nplot_graph(history)   Model hội tụ khá nhanh và cho kết quả tốt, không có hiện tượng overfitting. Có lẽ train thêm một số epochs nữa sẽ cho kết quả tốt hơn. Bạn có thể thử.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong bài viết tiếp theo, mình sẽ vẫn thực hành bài toán phân loại văn bản nhưng sử dụng kỹ thuật Transfer Learning giống như bên Computer Vision. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/ddd_bbc_text_topic_classification-copy/","tags":["Text Classification"],"title":"Phân loại text theo chủ đề  - Transfer Learning"},{"categories":["Text Classification"],"contents":"Bài này mình xin phép đổi chủ đề một chút. Chúng ta sẽ thử làm bài toán phân loại text theo các chủ đề khác nhau. Đây là một trong những bài toán thuộc phạm vi của chủ đề xử lý ngôn ngữ tự nhiên (NLP).\nMình sẽ sử dụng bộ dữ liệu BBC news để thực hành. Bạn hãy download của 2 file Train.csv và Test.csv, sau đó gộp chung chúng lại thành 1 file để làm dữ liệu huấn luyện. Tổng số records là 2225, chia thành 6 chủ đề.\nĐầu tiên, import các thư viện sẽ sử dụng:\nimport csv import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences Khai báo một số tham số:\nvocab_size = 1000 embedding_dim = 16 max_length = 120 trunc_type = \u0026#39;post\u0026#39; padding_type = \u0026#39;post\u0026#39; oov_tok = \u0026#39;\u0026lt;OOV\u0026gt;\u0026#39; training_portion = 0.8 sentences = [] labels = [] stopwords = [ \u0026#34;a\u0026#34;, \u0026#34;about\u0026#34;, \u0026#34;above\u0026#34;, \u0026#34;after\u0026#34;, \u0026#34;again\u0026#34;, \u0026#34;against\u0026#34;, \u0026#34;all\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;and\u0026#34;, \u0026#34;any\u0026#34;, \u0026#34;are\u0026#34;, \u0026#34;as\u0026#34;, \u0026#34;at\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;because\u0026#34;, \u0026#34;been\u0026#34;, \u0026#34;before\u0026#34;, \u0026#34;being\u0026#34;, \u0026#34;below\u0026#34;, \u0026#34;between\u0026#34;, \u0026#34;both\u0026#34;, \u0026#34;but\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;could\u0026#34;, \u0026#34;did\u0026#34;, \u0026#34;do\u0026#34;, \u0026#34;does\u0026#34;, \u0026#34;doing\u0026#34;, \u0026#34;down\u0026#34;, \u0026#34;during\u0026#34;, \u0026#34;each\u0026#34;, \u0026#34;few\u0026#34;, \u0026#34;for\u0026#34;, \u0026#34;from\u0026#34;, \u0026#34;further\u0026#34;, \u0026#34;had\u0026#34;, \u0026#34;has\u0026#34;, \u0026#34;have\u0026#34;, \u0026#34;having\u0026#34;, \u0026#34;he\u0026#34;, \u0026#34;he\u0026#39;d\u0026#34;, \u0026#34;he\u0026#39;ll\u0026#34;, \u0026#34;he\u0026#39;s\u0026#34;, \u0026#34;her\u0026#34;, \u0026#34;here\u0026#34;, \u0026#34;here\u0026#39;s\u0026#34;, \u0026#34;hers\u0026#34;, \u0026#34;herself\u0026#34;, \u0026#34;him\u0026#34;, \u0026#34;himself\u0026#34;, \u0026#34;his\u0026#34;, \u0026#34;how\u0026#34;, \u0026#34;how\u0026#39;s\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;i\u0026#39;d\u0026#34;, \u0026#34;i\u0026#39;ll\u0026#34;, \u0026#34;i\u0026#39;m\u0026#34;, \u0026#34;i\u0026#39;ve\u0026#34;, \u0026#34;if\u0026#34;, \u0026#34;in\u0026#34;, \u0026#34;into\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;it\u0026#34;, \u0026#34;it\u0026#39;s\u0026#34;, \u0026#34;its\u0026#34;, \u0026#34;itself\u0026#34;, \u0026#34;let\u0026#39;s\u0026#34;, \u0026#34;me\u0026#34;, \u0026#34;more\u0026#34;, \u0026#34;most\u0026#34;, \u0026#34;my\u0026#34;, \u0026#34;myself\u0026#34;, \u0026#34;nor\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;on\u0026#34;, \u0026#34;once\u0026#34;, \u0026#34;only\u0026#34;, \u0026#34;or\u0026#34;, \u0026#34;other\u0026#34;, \u0026#34;ought\u0026#34;, \u0026#34;our\u0026#34;, \u0026#34;ours\u0026#34;, \u0026#34;ourselves\u0026#34;, \u0026#34;out\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;own\u0026#34;, \u0026#34;same\u0026#34;, \u0026#34;she\u0026#34;, \u0026#34;she\u0026#39;d\u0026#34;, \u0026#34;she\u0026#39;ll\u0026#34;, \u0026#34;she\u0026#39;s\u0026#34;, \u0026#34;should\u0026#34;, \u0026#34;so\u0026#34;, \u0026#34;some\u0026#34;, \u0026#34;such\u0026#34;, \u0026#34;than\u0026#34;, \u0026#34;that\u0026#34;, \u0026#34;that\u0026#39;s\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;their\u0026#34;, \u0026#34;theirs\u0026#34;, \u0026#34;them\u0026#34;, \u0026#34;themselves\u0026#34;, \u0026#34;then\u0026#34;, \u0026#34;there\u0026#34;, \u0026#34;there\u0026#39;s\u0026#34;, \u0026#34;these\u0026#34;, \u0026#34;they\u0026#34;, \u0026#34;they\u0026#39;d\u0026#34;, \u0026#34;they\u0026#39;ll\u0026#34;, \u0026#34;they\u0026#39;re\u0026#34;, \u0026#34;they\u0026#39;ve\u0026#34;, \u0026#34;this\u0026#34;, \u0026#34;those\u0026#34;, \u0026#34;through\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;too\u0026#34;, \u0026#34;under\u0026#34;, \u0026#34;until\u0026#34;, \u0026#34;up\u0026#34;, \u0026#34;very\u0026#34;, \u0026#34;was\u0026#34;, \u0026#34;we\u0026#34;, \u0026#34;we\u0026#39;d\u0026#34;, \u0026#34;we\u0026#39;ll\u0026#34;, \u0026#34;we\u0026#39;re\u0026#34;, \u0026#34;we\u0026#39;ve\u0026#34;, \u0026#34;were\u0026#34;, \u0026#34;what\u0026#34;, \u0026#34;what\u0026#39;s\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;when\u0026#39;s\u0026#34;, \u0026#34;where\u0026#34;, \u0026#34;where\u0026#39;s\u0026#34;, \u0026#34;which\u0026#34;, \u0026#34;while\u0026#34;, \u0026#34;who\u0026#34;, \u0026#34;who\u0026#39;s\u0026#34;, \u0026#34;whom\u0026#34;, \u0026#34;why\u0026#34;, \u0026#34;why\u0026#39;s\u0026#34;, \u0026#34;with\u0026#34;, \u0026#34;would\u0026#34;, \u0026#34;you\u0026#34;, \u0026#34;you\u0026#39;d\u0026#34;, \u0026#34;you\u0026#39;ll\u0026#34;, \u0026#34;you\u0026#39;re\u0026#34;, \u0026#34;you\u0026#39;ve\u0026#34;, \u0026#34;your\u0026#34;, \u0026#34;yours\u0026#34;, \u0026#34;yourself\u0026#34;, \u0026#34;yourselves\u0026#34; ] Chúng ta có một mảng chứa các stop words, tức là các từ thường hay xuất hiện trong câu nhưng lại không mang nhiều ý nghĩa. Chúng ta sẽ loại bỏ chúng đi trước khi huấn luyện model phân loại.\nBây giờ, ta sẽ đọc dataset và chuẩn bị dữ liệu training:\nwith open(\u0026#39;bbc-text.csv\u0026#39;, \u0026#39;r\u0026#39;) as csvfile: reader = csv.reader(csvfile, delimiter=\u0026#39;,\u0026#39;) next(reader) for row in reader: labels.append(row[0]) sentence = row[1] # remove stop words for word in stopwords: token = \u0026#39; \u0026#39; + word + \u0026#39; \u0026#39; sentence = sentence.replace(token, \u0026#39; \u0026#39;) sentence = sentence.replace(\u0026#39; \u0026#39;, \u0026#39; \u0026#39;) sentences.append(sentence) print(len(sentences)) Chia dataset thành 2 phần: train và validation:\ntrain_size = int(len(sentences) * training_portion) train_sentences = sentences[:train_size] train_labels = labels[:train_size] validation_sentences = sentences[train_size:] validation_labels = labels[train_size:] Để model có thể hiểu được dataset, cần phải chuyển các câu dạng text sang dạng vector:\n# chuyển text sang vector tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok) tokenizer.fit_on_texts(train_sentences) word_index = tokenizer.word_index label_tokenizer = Tokenizer() label_tokenizer.fit_on_texts(labels) training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels)) validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels)) # padding để các câu có cùng chiều dài train_sequences = tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length) validation_sequences = tokenizer.texts_to_sequences(validation_sentences) validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length) Mình sẽ đi chi tiết phần này trong 1 bài viết khác. Hôm nay các bạn chỉ cần hiểu ý tưởng của nó như vậy là được rồi.\nSau khi đã có dữ liệu huấn luyện, giờ là lúc chúng ta định nghĩa kiến trúc model.\nmodel = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), tf.keras.layers.GlobalAveragePooling1D(), tf.keras.layers.Dense(24, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(6, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) model.summary() Ở bài này, mình chỉ sử dụng một model đơn giản gồm các lớp Embedding, GlobalAveragePooling1D, và Dense.\nHàm plot để vẽ đồ thị quá trình training:\ndef plot_graph(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10,6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training \u0026amp; Validation, Accuracy \u0026amp; Loss\u0026#39;) plt.legend(loc=0) plt.show() Bước cuối cùng là chạy train model, ta sẽ train model với 30 epochs:\nnum_epochs = 30 history = model.fit( train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=1 ) Training output:\nEpoch 1/30 56/56 [==============================] - 0s 4ms/step - loss: 1.7737 - acc: 0.2180 - val_loss: 1.7481 - val_acc: 0.2270 Epoch 2/30 56/56 [==============================] - 0s 2ms/step - loss: 1.7163 - acc: 0.2303 - val_loss: 1.6755 - val_acc: 0.2270 Epoch 3/30 56/56 [==============================] - 0s 2ms/step - loss: 1.6299 - acc: 0.2371 - val_loss: 1.5792 - val_acc: 0.2539 .................... Epoch 27/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0580 - acc: 0.9949 - val_loss: 0.2115 - val_acc: 0.9506 Epoch 28/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0521 - acc: 0.9961 - val_loss: 0.2080 - val_acc: 0.9506 Epoch 29/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0467 - acc: 0.9961 - val_loss: 0.2051 - val_acc: 0.9506 Epoch 30/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0420 - acc: 0.9989 - val_loss: 0.2014 - val_acc: 0.9506 Quá trình train diễn ra khá nhanh, mất khoảng 2 phút trên máy tính của mình. Tại epochs cuối cùng, độ chính xác trên tập validation là 95,06%.\nToàn bộ quá trình này được thể hiện trên đồ thị như sau:\nplot_graph(history)   Model hội tụ khá nhanh và cho kết quả tốt, không có hiện tượng overfitting. Có lẽ train thêm một số epochs nữa sẽ cho kết quả tốt hơn. Bạn có thể thử.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong bài viết tiếp theo, mình sẽ vẫn thực hành bài toán phân loại văn bản nhưng sử dụng kỹ thuật Transfer Learning giống như bên Computer Vision. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/ddd_bbc_text_topic_classification/","tags":["Text Classification"],"title":"Phân loại text theo chủ đề "},{"categories":["MLOps"],"contents":"Bạn đã xây dựng thành công một DL model với độ chính xác rất cao, 99%. Xin chúc mừng bạn!\nVấn đề tiếp theo bạn cần nghĩ đến là làm sao đưa model đó vào trong sản phẩm thực tế, để mọi người có thể sử dụng model của bạn một cách đơn giản và dễ dàng. Xây dựng và triển khai model luôn là 2 công đoạn bắt buộc trong một bài toán về AI. Trong các bài tiếp theo, mình sẽ chia sẻ các cách thức mà chúng ta có thể sử dụng để triển khai một DL model vào trong ứng dụng để sử dụng trong thưc tế.\nCó 2 kiểu Inference mà một AI model có thể được sử dụng:\n  Online Inference: Model phải liên tục xử lý và trả về kết quả dự đoán gần như real-time khi nó nhận được yêu cầu. Số lượng yêu cầu đến thường rất lớn, thậm chí là nhiều yêu cầu đến tại cùng 1 thời điểm. Chính vì vậy mà việc triển khai model theo kiểu này thường phức tạp hơn rất nhiều so với kiểu thứ 2.\n  Batch Inference: Model chỉ chạy Inference tại một số thời điểm cố định trong ngày, và mỗi lần Inference sẽ xử lý một tập hợp (batch) các input data nhất định.\n  Mỗi kiểu Inference phù hợp với các bài toán khác nhau, có lẽ mình sẽ viết một bài so sánh chi tiết hơn về 2 kiểu Inference này.\nTrong bài đầu tiên này chúng ta sẽ sử dụng Flask, một web server framework nhỏ nhẹ, dễ dàng trong việc cài đạt và sử dụng, để triển khai model phân loại Cat\u0026amp;Dog\u0026amp;Panda trong bài trước theo kiểu Online Inference.\n1. Giới thiệu về kiến trúc Client-Server và Flask\nClient-server là kiểu kiến trúc xử lý phân tán, gồm 2 thành phần chính là client và server:\n Client gửi các yêu cầu (requests) đến server. Server xử lý các yêu cầu đó và trả lại kết quả cho client. Yêu cầu có thể là truy vấn database, tính toán, so sánh, dự đoán, \u0026hellip;  Dữ liệu trao đổi giữa client và server gọi là các messages.\nGiao thức trao đổi giữa client-server thường là HTTP/HTTPS trong trường hợp client là website, và server khi đó gọi là webserver. Nếu client không phải là website thì giao thức có thể là TCP/UDP hoặc uwsgi. Giao thức sẽ định nghĩa định dạng dữ liệu, cơ chế truyền, truyền lại, cơ chế xác thực dữ liệu, \u0026hellip;. của các bản tin trao đổi giữa 2 bên. Ví dụ, một HTTP request/Response bao gồm 4 thành phần cơ bản:\n URL đích: đường dẫn (path) đến một dịch vụ cụa thể của server mà client cần giao tiếp. Phương pháp giao tiếp (method): có 4 phương pháp là GET, POST, UPDATE, DELETE. Tùy từng yêu cầu cụ thể của bài toán mà ta sử dụng phương pháp phù hợp. Header: là các metadata kiểu như ngày tháng năm (date), tình trạng(status), kiểu dữ liệu (content-type), \u0026hellip; giúp server xử lý và đồng bộ dữ liệu với client. Body: chứa dữ liệu thực tế mà ta cần gửi từ client đến server hoặc ngược lại.  Flask là một framework để tạo ra thành phần server, bao gồm cả web server. Một số ưu điểm của Flask có thể kể đến như sau:\n Nó giúp triển khai DL model dưới dạng web application nếu bạn muốn cung cấp giao diện web cho người dùng. Đơn giản, dễ dàng cài đặt và sử dụng. Hỗ trợ nhiều chức năng thông qua các end-point URL khác nhau.  Tuy nhiên, Flask không hỗ trợ đầy đủ các chức năng cần thiết của 1 server để có thể sử dụng trong môi trường sản phẩm thực tế giống như là tính bảo mật và sự hỗ trợ cùng lúc nhiều client truy cập. Nó chỉ phù hợp cho các ứng dụng mang tính demo, kiểm thử tính năng model. Nếu cần triển khai thực tế, uWSGI là một sự lựa chọn phù hợp (sẽ được đề cập chi tiết trong bài sau).\n2. Xây dựng Server với Flask\nTa sẽ sử dụng Flask để xây dựng một server phục vụ cả 2 loại client: dạng web và dạng code python (dạng thông thường).\nĐầu tiên, tạo file server.py và import Flask và các thư viện cần thiết:\nimport cv2 import os import numpy as np import tensorflow as tf from flask_cors import CORS from tensorflow.keras.models import load_model from flask import Flask, request, render_template, make_response, jsonify Tạo một instance của Flask:\napp = Flask(__name__) Định nghĩa 1 vài hằng số sử dụng:\nimage_width = 300 image_height = 300 classes = [\u0026#39;cat\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;pandas\u0026#39;] APP_ROOT_1 = os.getenv(\u0026#39;APP_ROOT\u0026#39;, \u0026#39;/infer1\u0026#39;) APP_ROOT_1 = os.getenv(\u0026#39;APP_ROOT\u0026#39;, \u0026#39;/infer2\u0026#39;) Hàm render UI mặc định khi truy cập vào địa chỉ server:\n# render default webpage @app.route(\u0026#39;/\u0026#39;) def home(): return render_template(\u0026#39;home.html\u0026#39;) Hàm này chỉ phục vụ client dạng web. File home.html chính là phần front-end mà chúng ta sẽ viết code để tạo giao diện tương tác với người dùng trên web.\nHàm nhận và xử lý request từ client dạng web:\n@app.route(APP_ROOT_1, methods = [\u0026#39;POST\u0026#39;, \u0026#39;GET\u0026#39;]) def classify_image(): if request.method == \u0026#39;POST\u0026#39;: # geting data from html form img_path = request.form[\u0026#34;img_path\u0026#34;] # call funtion to classify image and receive result result = classify_animal(img_path) # return result to client response = {\u0026#39;result\u0026#39;: result, \u0026#39;image\u0026#39;: img_path} return make_response(jsonify(response), 200) Hàm nhận và xử lý request từ client dạng thông thường:\n@app.route(APP_ROOT_2, methods=[\u0026#34;POST\u0026#34;]) def infer(): data = request.json image = data[\u0026#39;image_path\u0026#39;] return classify_animal(img_path) Hãy nhớ lại ở bài trước, sau khi huấn luyện xong model, ta đã lưu nó thành file animal_model_classification.h5 vào ổ cứng. File này có kích thước khá nặng, khoảng 1.7GB. Bây giờ, ta sẽ sử dụng model đó đã nhận diện.\nLoad DL model:\nmodel = load_model(\u0026#39;animal_model_classification.h5\u0026#39;) Hàm dưới đây nhận vào tham số là đường dẫn đến ảnh cần nhận diện và trả về kết quả:\ndef classify_animal(img_path): # read image image = cv2.imread(img_path) image = image/255 image = cv2.resize(image, (image_width,image_height) image = np.reshape(image, [1,image_width,image_height,3]) # pass the image through the network to obtain our predictions preds = model.predict(image) label = classes[np.argmax(preds)] return label Cuối cùng, sử dụng hàm run() để khởi tạo server:\nif __name__ == \u0026#39;__main__\u0026#39;: app.run() Để chạy server, mở cửa sổ terminal và gõ lệnh:\n$ python server.py Nếu mọi thứ Ok thì cửa sổ terminal sẽ xuất hiện như sau:\n Như vậy là đã xong phần backend, tiếp theo ta sẽ viết code cho front-end.\n3. Web client\nWeb client có giao diện đơn giản gồm 1 button cho phép user chọn ảnh cần nhận diện và 1 khu vực để hiển thị ảnh kèm kết quả.\n Vì web không phải là lĩnh vực chuyên sâu của mình nên mình sẽ không đi chi tiết code ở đây. Các bạn có thể tham khảo code web trên github của mình.\nĐể kiểm tra hoạt động, truy cập vào địa chỉ http://localhost:5000, upload một bức ảnh, click Detect button. Kết quả phân loại sẽ được hiển thị.\n 4. Python client\nTạo file client.py và code như sau:\nimport requests import numpy as np ENDPOINT_URL = \u0026#39;http://0.0.0.0:5000/infer2\u0026#39; def infer(): img_path = \u0026#39;dog1.jpg\u0026#39; data = { \u0026#39;image\u0026#39;: image_path } response = requests.post(ENDPOINT_URL, json = data) response.raise_for_status() print(response.content) if __name__ ==\u0026#34;__main__\u0026#34;: infer() Khởi chạy client:\n$ python client.py Kết quả trả về từ server:\nb\u0026#39;dog\u0026#39; Thử lại với một ảnh ảnh con mèo, cat.1.jpg. Kết qủa trả về:\nb\u0026#39;cat\u0026#39; 5. Kết luận\nNhư vậy là chúng ta đã cùng nhau triển khai xong DL model sử dụng Flask. Mặc dù tồn tại nhiều nhược điểm nhưng khi cần nhanh chóng đạt được kết quả để thử nghiệm thì Flask vẫn được tin dùng.\nBài viết tiếp theo, mình sẽ giới thiệu một cách nâng cao hơn để triển khai DL model, thường được áp dụng trong các bài toán thực tế. Mời các bạn đón đọc!\nToàn bộ source code của backend và front-end trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây. Vì model animal_model_classification.h5 có dung lượng khá lớn (\u0026gt; 1.5GB) nên mình không upload lên github được. Các bạn hãy sử dụng model của chính mình để thực hành nhé!\n6. Tham khảo\n Flask AI Summer  ","permalink":"https://tiensu.github.io/blog/36_deploy_ai_model_with_flask_online_inference/","tags":["MLOps"],"title":"Triển khai AI model sử dụng Flask"},{"categories":["CNN","Image Classification"],"contents":"Những bài toán mà chỉ có 2 lớp cần phân biệt gọi là binary classification, còn những bài toán có nhiều hơn 2 lớp được gọi là multiple classification.\nCó một vài sự khác biệt trong cách cài đặt CNN model giữa 2 loại bài toán này. Trong hôm nay chúng ta sẽ cùng tìm hiểu điều đó thông qua thực hiện phân loại 3 classes: Cat, Dog và Panda của bộ dữ liệu animals. Bộ dataset này gồm 3000 ảnh chia đều cho mỗi class.\nTa sẽ bắt tay vào thực hiện code luôn, những điểm khác biệt sẽ được đề cập trong quá trình viết code.\nSử dụng kiến thức đã học từ bài trước, ta sẽ thực hiện bài này theo 2 cách và so sánh kết quả:\n Không sử dụng Transfer Learning Sử dụng Transfer Learning  Đầu tiên, download dataset về thư mục làm việc và dùng thư viện split-folers để chia dữ liệu thành 2 tập train và validation.\nImport các thư viện sẽ sử dụng:\nimport os import random import shutil import tensorflow as tf import matplotlib.pyplot as plt from tensorflow import keras from tensorflow.keras import layers, Model from tensorflow.keras.applications.inception_v3 import InceptionV3 from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping Chuẩn bị dữ liệu training:\ndef data_gen(): train_gen = ImageDataGenerator( rescale=1/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\u0026#39;nearest\u0026#39; ) validation_gen = ImageDataGenerator( rescale=1/255 ) train_datagen = train_gen.flow_from_directory( \u0026#39;Animals/training\u0026#39;, target_size=(300,300), batch_size=32, class_mode=\u0026#39;categorical\u0026#39; ) validation_datagen = validation_gen.flow_from_directory( \u0026#39;Animals/validation\u0026#39;, target_size=(300,300), batch_size=32, class_mode=\u0026#39;categorical\u0026#39; ) return train_datagen, validation_datagen Model tự định nghĩa:\ndef create_own_model(): model = keras.Sequential([ # CONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; POOL =\u0026gt; DO layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, input_shape=(300, 300, 3)), layers.BatchNormalization(), layers.MaxPooling2D(3,3), layers.Dropout(0.25), # (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.MaxPooling2D(2,2), layers.Dropout(0.25), # (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.MaxPooling2D(2,2), layers.Dropout(0.25), # (FC =\u0026gt; RELU =\u0026gt; BN =\u0026gt; DO)*2 =\u0026gt; FC =\u0026gt; SOFTMAX layers.Flatten(), layers.Dense(1024, activation=\u0026#39;relu\u0026#39;), layers.BatchNormalization(), layers.Dropout(0.25), layers.Dense(512, activation=\u0026#39;relu\u0026#39;), layers.BatchNormalization(), layers.Dropout(0.25), layers.Dense(3, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Kiến trúc model:\nCONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; POOL =\u0026gt; DO =\u0026gt; (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO =\u0026gt; (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO =\u0026gt; (FC =\u0026gt; RELU =\u0026gt; BN =\u0026gt; DO)*2 =\u0026gt; FC =\u0026gt; SOFTMAX\nModel sử dụng Transfer Learning:\ndef create_transfer_learning_model(): base_model = InceptionV3( input_shape=(300,300,3), include_top=False, weights=\u0026#39;imagenet\u0026#39; ) for layer in base_model.layers: layer.trainable = False head_model = base_model.output head_model = layers.Flatten()(head_model) head_model = layers.Dense(1024, activation=\u0026#39;relu\u0026#39;)(head_model) head_model = layers.BatchNormalization()(head_model) head_model = layers.Dropout(0.25)(head_model) head_model = layers.Dense(512, activation=\u0026#39;relu\u0026#39;)(head_model) head_model = layers.BatchNormalization()(head_model) head_model = layers.Dropout(0.5)(head_model) head_model = layers.Dense(3, activation=\u0026#39;softmax\u0026#39;)(head_model) model = Model(inputs=base_model.input, outputs=head_model) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Trong phương pháp Transfer Learning, ta sử dụng pre-trained InceptionV3 làm base_model.\nNhư chúng ta thấy, có 2 điểm khác biệt ở đây:\n Hàm activation ở layer cuối Nếu như bài toán binary classification sử dụng hàm sigmoid chỉ có 1 output thì bài toán multiple classification sử dụng hàm softmax, số lượng output bằng số classes cần phân loại. Mình sẽ có bài phân tích chi tiết về các loại hàm này sau. Hàm loss Binary classification sử dụng hàm binary_crossentropy, còn multiple classification sử dụng categorical_crossentropy hoặc sparse_categorical_crossentropy. Mình cũng sẽ viết một bài về các dạng hàm loss trong tương lai.  Định nghĩa callback functions:\ndef create_callbacks(): callback_1 = EarlyStopping(monitor=\u0026#39;val_acc\u0026#39;, patience=4) callback_2 = ModelCheckpoint( \u0026#39;Animals_ModelCheckpoints/model-{epoch:02d}-{val_acc:.2f}.hdf5\u0026#39;, save_best_only=True, save_weights_only=True, monitor=\u0026#39;val_acc\u0026#39;, save_freq=\u0026#39;epoch\u0026#39;, mode=\u0026#39;auto\u0026#39;, verbose=1 ) return [callback_1, callback_2] Hàm vẽ đồ thị:\ndef plot_graph(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epoch = range(len(acc)) plt.plot(epoch, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epoch, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epoch, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epoch, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training \u0026amp; Validation, Accuracy \u0026amp; Loss\u0026#39;) plt.legend(loc=0) plt.show() Train model với kiến trúc tự định nghĩa:\ntraining_datagen, validation_datagen = data_gen() model = create_own_model() history = model.fit( training_datagen, epochs=30, validation_data=validation_datagen, callbacks=create_callbacks(), verbose=1 ) Training output:\nFound 2400 images belonging to 3 classes. Found 600 images belonging to 3 classes. Epoch 1/30 2/75 [..............................] - ETA: 4s - loss: 1.8446 - acc: 0.3906WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0442s vs `on_train_batch_end` time: 0.0907s). Check your callbacks. 75/75 [==============================] - ETA: 0s - loss: 1.2381 - acc: 0.5292 Epoch 00001: val_acc improved from -inf to 0.33333, saving model to Animals_ModelCheckpoints/model-01-0.33.hdf5 75/75 [==============================] - 40s 533ms/step - loss: 1.2381 - acc: 0.5292 - val_loss: 4.0135 - val_acc: 0.3333 Epoch 2/30 75/75 [==============================] - ETA: 0s - loss: 0.8709 - acc: 0.6042 Epoch 00002: val_acc did not improve from 0.33333 75/75 [==============================] - 40s 533ms/step - loss: 0.8709 - acc: 0.6042 - val_loss: 3.9076 - val_acc: 0.3333 Epoch 3/30 75/75 [==============================] - ETA: 0s - loss: 0.8323 - acc: 0.6008 Epoch 00003: val_acc improved from 0.33333 to 0.35000, saving model to Animals_ModelCheckpoints/model-03-0.35.hdf5 75/75 [==============================] - 40s 535ms/step - loss: 0.8323 - acc: 0.6008 - val_loss: 3.0383 - val_acc: 0.3500 ........ Epoch 11/30 75/75 [==============================] - ETA: 0s - loss: 0.6230 - acc: 0.6958 Epoch 00011: val_acc did not improve from 0.72667 75/75 [==============================] - 40s 536ms/step - loss: 0.6230 - acc: 0.6958 - val_loss: 0.9326 - val_acc: 0.5933 Epoch 12/30 75/75 [==============================] - ETA: 0s - loss: 0.5867 - acc: 0.7133 Epoch 00012: val_acc did not improve from 0.72667 75/75 [==============================] - 40s 539ms/step - loss: 0.5867 - acc: 0.7133 - val_loss: 0.6075 - val_acc: 0.7067 Epoch 13/30 75/75 [==============================] - ETA: 0s - loss: 0.5752 - acc: 0.7262 Epoch 00013: val_acc did not improve from 0.72667 75/75 [==============================] - 41s 540ms/step - loss: 0.5752 - acc: 0.7262 - val_loss: 0.5461 - val_acc: 0.7250 Model dừng train sau 13 epochs do giá trị của val_acc không tăng sau 5 epochs liên tiếp từ epoch 9 đến epoch 13. Độ chính xác cao nhất đạt được trên tập validation 72.67% tại epoch thứ 9.\nĐồ thị quá trình training:  Bây giờ, thử sử dụng pre-trained model:\nmodel = create_transfer_learning_model() training_datagen, validation_datagen = data_gen() history = model.fit( training_datagen, validation_data=validation_datagen, epochs=30, callbacks=create_callbacks(), verbose=1 ) Traning output:\nFound 2400 images belonging to 3 classes. Found 600 images belonging to 3 classes. Epoch 1/30 75/75 [==============================] - ETA: 0s - loss: 0.1724 - acc: 0.9525 Epoch 00001: val_acc improved from -inf to 0.95667, saving model to Animals_ModelCheckpoints/model-01-0.96.hdf5 75/75 [==============================] - 41s 548ms/step - loss: 0.1724 - acc: 0.9525 - val_loss: 0.2077 - val_acc: 0.9567 Epoch 2/30 75/75 [==============================] - ETA: 0s - loss: 0.1153 - acc: 0.9679 Epoch 00002: val_acc improved from 0.95667 to 0.99167, saving model to Animals_ModelCheckpoints/model-02-0.99.hdf5 75/75 [==============================] - 42s 559ms/step - loss: 0.1153 - acc: 0.9679 - val_loss: 0.0331 - val_acc: 0.9917 Epoch 3/30 75/75 [==============================] - ETA: 0s - loss: 0.0719 - acc: 0.9771 Epoch 00003: val_acc improved from 0.99167 to 0.99333, saving model to Animals_ModelCheckpoints/model-03-0.99.hdf5 75/75 [==============================] - 42s 557ms/step - loss: 0.0719 - acc: 0.9771 - val_loss: 0.0082 - val_acc: 0.9933 ........ Epoch 8/30 75/75 [==============================] - ETA: 0s - loss: 0.0410 - acc: 0.9833 Epoch 00008: val_acc did not improve from 0.99500 75/75 [==============================] - 43s 569ms/step - loss: 0.0410 - acc: 0.9833 - val_loss: 0.0288 - val_acc: 0.9933 Epoch 9/30 75/75 [==============================] - ETA: 0s - loss: 0.0554 - acc: 0.9842 Epoch 00009: val_acc did not improve from 0.99500 75/75 [==============================] - 44s 588ms/step - loss: 0.0554 - acc: 0.9842 - val_loss: 0.0140 - val_acc: 0.9917 Epoch 10/30 75/75 [==============================] - ETA: 0s - loss: 0.0632 - acc: 0.9804 Epoch 00010: val_acc did not improve from 0.99500 75/75 [==============================] - 43s 572ms/step - loss: 0.0632 - acc: 0.9804 - val_loss: 0.0219 - val_acc: 0.9933 Model dừng train sớm hơn, tại epoch thứ 10 sau 5 epochs liên tiếp không cải thiện về giá trị của val_acc (từ epoch 6 đến epoch 10). Độ chính xác trên tập validation cao nhất đạt được là 99,5% tại epoch thứ 5. Kết quả tốt hơn so với sử dụng model tự định nghĩa rất nhiều.\nĐồ thị quá trình training:  Như vậy, qua bài này ta đã biết được cách thức xây dựng kiến trúc CNN model cho bài toán multiple classification. Đồng thời ta cũng nhận thấy rõ ràng ưu điểm của kỹ thuật Transfer Learning so với cách tự xây dựng CNN model. Có thể nói rằng, sử dụng pre-trained model luôn cho kết quả tốt hơn, trừ khi chúng ta có lý do cụ thể để không sử dụng chúng.\nCuối cùng, ta lưu lại model để sử dụng cho việc dự đoán về sau khi triển khai model vào sản phẩm thực tế:\nmodel.save(\u0026#39;animal_classification_model.h5\u0026#39;) Source code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong các bài viết tiếp theo, mình sẽ viết về một số bài toán NLP và các kỹ thuật cần dùng để giải quyết chúng. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/35_cnn_model_multiple_classification/","tags":["CNN","Image Classification"],"title":"Xây dựng CNN model cho bài toán đa lớp"},{"categories":["CNN","Image Classification"],"contents":"Trong các bài toán thực tế, khi làm việc với bộ dataset lớn và kiến trúc model phức tạp, việc huấn luyện model sẽ mất rất nhiều thời gian. Vài ngày hoặc thậm chí cả tuần mới ra được kết quả. Nếu ta chỉ train một lần thì không có gì đáng nói, nhưng thường thì ta sẽ train đi train lại rất nhiều lần, mỗi lần điều chỉnh hyper-parameter lại phải chạy train lại. Việc này quả thực rât rất mất thời gian và chán nản.\nHoặc khi chỉ có một lượng nhỏ dữ liệu để train model, chắc chắn sẽ cho ra một model không tốt, vì nó không học được kết các khía cạnh của dữ liệu. Khi triển khai thực tế chắc chắn sẽ thất bại.\nKỹ thuật Transfer Learning ra đời để giải quyết khó khăn này. Ý tưởng chính của nó là tận dụng lại những model kinh diển (VGG, Resnet, InceptionNet, \u0026hellip;), đã được train trên những tập dữ liệu lớn (pre-trained models), loại bỏ các layers classification ở gần cuối (thường là các lớp FC), chỉ giữ lại các layers ở đầu làm nhiệm vụ trích xuất đặc trưng của dữ liệu.\nCó 2 loại transfer learning:\n Feature extractor: Sử dụng pre-trained model như là bộ trích xuất đặc trưng của dữ liệu. Các đặc trưng này sau đó sẽ được phân loại sử dụng các thuật toán ML như kNN, SVM, Decision Tree, \u0026hellip; Fine tuning: Loại bỏ các layers cuối làm nhiệm vụ phân loại trong pre-trained model, thêm vào các layers mới dựa theo bộ dữ liệu mà chúng ta có. Sau đó, train lại model tại những layers mà ta mới thêm vào.  Vậy thì khi nào ta nên sử dụng Transfer Learning?\nDựa trên kích thước và độ tương quan giữa CSDL mới và CSDL gốc (chủ yếu là ImageNet) để train các mô hình có sẵn, CS231n đưa ra một vài lời khuyên:\n  CSDL mới là nhỏ và tương tự như CSDL gốc. Vì CSDL mới nhỏ, việc tiếp tục train model dễ dẫn đến hiện tượng overfitting. Cũng vì hai CSDL là tương tự nhau, ta dự đoán rằng các high-level features là tương tự nhau. Vậy nên ta không cần train lại model mà chỉ cần train một classifer dựa trên feature vectors ở đầu ra ở layer gần cuối.\n  CSDL mới là lớn và tương tự như CSDL gốc. Vì CSDL này lớn, overfitting ít có khả năng xảy ra hơn, ta có thể train mô hình thêm một chút nữa (toàn bộ hoặc chỉ một vài layers cuối).\n  CSDL mới là nhỏ và rất khác với CSDL gốc. Vì CSDL này nhỏ, tốt hơn hết là dùng các classifier đơn giản (các linear classifiers) để tránh overfitting). Nếu muốn train thêm, ta cũng chỉ nên train các layer cuối. Hoặc có một kỹ thuật khác là coi đầu ra của một layer xa layer cuối hơn làm các feature vectors.\n  CSDL mới là lớn và rất khác CSDL gốc. Trong trường hợp này, ta vẫn có thể sử dụng mô hình đã train như là điểm khởi tạo cho mô hình mới, không nên train lại từ đầu.\n  Có một điểm đáng chú ý nữa là khi tiếp tục train các mô hình này, ta chỉ nên chọn learning rate nhỏ để các weights mới không đi quá xa so với các weights đã được trained ở các mô hình trước.\nOK, ta sẽ bắt tay vào thực hành kỹ thuật này (theo cách thứ 2) ngay bây giờ!\nYêu cầu bài toán là huấn luyện một CNN model để phân loại ảnh chứa ngựa và người trong bộ dataset horse-or-humand.\nDownload bộ dataset horse-or-humand về máy tính, giải nén và sử dụng thư viện split-folders để chia thành 2 phần train set và validation set.\nĐầu tiên, import các thư viện cần thiết:\nimport os import tensorflow as tf import matplotlib.pyplot as plt from tensorflow.keras import layers from tensorflow.keras import Model from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.applications.inception_v3 import InceptionV3 config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Các pre-trained model phổ biến đã được tích hợp sẵn trong tensorflow. Ở đây ta khai báo lớp InceptionV3 để sử dụng InceptionNet pre-trained model.\nTiếp theo là chuẩn bị dữ liệu huấn luyện:\ndef data_gen(): training_datagen = ImageDataGenerator( rescale=1/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, horizontal_flip=True, fill_mode=\u0026#39;nearest\u0026#39; ) validation_datagen = ImageDataGenerator(rescale=1/255) training_generator = training_datagen.flow_from_directory( \u0026#39;horse-and-humand/train\u0026#39;, target_size=(224,224), batch_size=16, class_mode=\u0026#39;binary\u0026#39; ) validation_generator = validation_datagen.flow_from_directory( \u0026#39;horse-and-humand/validation\u0026#39;, target_size=(224,224), batch_size=16, class_mode=\u0026#39;binary\u0026#39; ) return training_generator, validation_generator Khai báo 2 hàm callback: EarlyStopping và ModelCheckpoint:\ndef create_callbacks(): callback_1 = ModelCheckpoint( \u0026#39;horse-humand_model_checkpoint/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\u0026#39;, monitor=\u0026#39;val_acc\u0026#39;, save_best_only=True, save_weights_only=True, save_freq=\u0026#39;epoch\u0026#39;, mode=\u0026#39;auto\u0026#39;, verbose=1 ) callback_2 = EarlyStopping(monitor=\u0026#39;val_acc\u0026#39;, patience=5) return [callback_1, callback_2] Phần quan trọng nhất trong bài này là định nghĩa model, sử dụng kỹ thuật Transfer Learning:\ndef create_model(): # Load pre-trained model base_model = InceptionV3( input_shape=(224,224,3), # Kích thước ảnh đầu vào include_top=False, # Loại bỏ các FC layers ở cuối weights=\u0026#39;imagenet\u0026#39; # Sử dụng các weights được train trên tập imagenet ) # Đóng băng các layers của pre-trained model, không cho chúng update for layer in base_model.layers: layer.trainable = False # Tạo head_model head_model = base_model.output head_model = layers.Flatten()(head_model) head_model = layers.Dense(1024, activation=\u0026#39;relu\u0026#39;)(head_model) head_model = layers.Dropout(0.2)(head_model) head_model = layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;)(head_model) model = Model(inputs=base_model.input, outputs=head_model) model.compile(optimizer=RMSprop(lr=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Model được tạo thành gồm 2 phần:\n base_model: chính là pre-trained model (đã loại bỏ các FC layers ở cuối). head_model: là các FC layers được thêm vào làm nhiệm vụ phân loại dựa theo tập dữ liệu mới.  Trong bài toán này, ta sử dụng pre-trained model của mạng InceptionNetV3 trên tập dữ liệu imagenet. Head_model bao gồm 2 FC layers xen kẽ DO layer ở giữa.\nHàm vẽ đồ thị:\ndef plot_chart(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10, 6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Traing and Validation, Accuracy and Loss\u0026#39;) plt.legend(loc=0) plt.show() Gộp tất cả lại và tiến hành train model:\nmodel = create_model() training_generator, validation_generator = data_gen() callbacks = create_callbacks() history = model.fit( training_generator, epochs=30, callbacks=[callback_1, callback_2], validation_data=validation_generator, verbose=1 ) Training output:\nEpoch 1/30 65/65 [==============================] - ETA: 0s - loss: 0.1342 - acc: 0.9786 Epoch 00001: val_acc improved from -inf to 0.91797, saving model to horse-humand_model_checkpoint/weights-improvement-01-0.92.hdf5 65/65 [==============================] - 12s 180ms/step - loss: 0.1342 - acc: 0.9786 - val_loss: 0.4456 - val_acc: 0.9180 Epoch 2/30 65/65 [==============================] - ETA: 0s - loss: 0.0992 - acc: 0.9708 Epoch 00002: val_acc did not improve from 0.91797 65/65 [==============================] - 12s 189ms/step - loss: 0.0992 - acc: 0.9708 - val_loss: 0.9824 - val_acc: 0.8594 Epoch 3/30 65/65 [==============================] - ETA: 0s - loss: 0.0501 - acc: 0.9903 Epoch 00003: val_acc improved from 0.91797 to 0.95312, saving model to horse-humand_model_checkpoint/weights-improvement-03-0.95.hdf5 ............ Epoch 8/30 65/65 [==============================] - ETA: 0s - loss: 0.1031 - acc: 0.9834 Epoch 00008: val_acc did not improve from 0.99219 65/65 [==============================] - 13s 201ms/step - loss: 0.1031 - acc: 0.9834 - val_loss: 0.5211 - val_acc: 0.9219 Epoch 9/30 65/65 [==============================] - ETA: 0s - loss: 0.0893 - acc: 0.9786 Epoch 00009: val_acc did not improve from 0.99219 65/65 [==============================] - 13s 203ms/step - loss: 0.0893 - acc: 0.9786 - val_loss: 0.5921 - val_acc: 0.9180 Epoch 10/30 65/65 [==============================] - ETA: 0s - loss: 0.1015 - acc: 0.9815 Epoch 00010: val_acc did not improve from 0.99219 65/65 [==============================] - 13s 204ms/step - loss: 0.1015 - acc: 0.9815 - val_loss: 0.4264 - val_acc: 0.9453 Model dừng train sau 10 epochs, độ chính xác cao nhất đạt được trên tập validation là 94.53%, một kết quả khá cao với số lượng epochs \u0026ldquo;khiêm tốn\u0026rdquo; như vậy.\nQuan sát thư mục horse-humand_model_checkpoint ta cũng thấy model được lưu tại một số điểm checkpoint. Model có độ chính xác cao nhất tại epoch thứ 3.\n├── weights-improvement-01-0.92.hdf5 ├── weights-improvement-03-0.95.hdf5 Kiểm tra quá trình huấn luyện bằng cách thể hiện giá trị loss và accuracy lên đồ thị:\nplot_chart(history)   Mặc dù giá trị của val_loss dao động 1 chút nhưng acc, val_acc và loss đều khá lý tưởng, chứng tỏ sự hiệu quả của kỹ thuật Transfer Learning trong bài toàn này.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nCác bài viết từ trước đến giờ đều chỉ phân loại 2 lớp. Đối với bài toán phân loại nhiều lớp thì sẽ như thế nào? Câu trả lời sẽ có ở bài tiếp theo . Mời các bạn đón đọc!\nTham khảo\n Coursera MachineLearningCoban  ","permalink":"https://tiensu.github.io/blog/34_transfer_learning/","tags":["CNN","Image Classification"],"title":"Sử dụng kỹ thuật Transfer Learning khi huấn luyện CNN model"},{"categories":["CNN","Image Classification"],"contents":"Tiếp theo bài trước, trong bài này chúng ta sẽ áp dụng thêm 2 kỹ thuật mới vào CNN model Cat\u0026amp;Dog classification:\n  Data Augmentation: Đây là kỹ thuật tăng cường dữ liệu huấn luyện cho model. Nó đặc biệt hữu ích khi chúng ta có ít dữ liệu vì từ một ảnh gốc ban đầu, thông qua các phép biến đổi hình thái học (xoay, lật, phóng to, thu nhỏ, thay đổi độ sáng, độ tương phải, \u0026hellip;) ta có thêm được nhiều ảnh mới. Kỹ thuật này không chỉ giới hạn trong các bài toán liên quan đến ảnh, mà các bài toán Data Science và NLP cũng có thể sử dụng được.    Model Checkpoint: Đây thực chất là một hàm callback, được gọi sau mỗi epoch trong quá trình huấn luyện model. Nó sẽ lưu lại model nếu giá trị loss hoặc accuracy được cải thiện sau mỗi epoch.\n  Cùng với EarlyStopping thì 2 kỹ thuật này được cũng được sử dụng rất thường xuyên để hạn chế hiện tượng overfitting của model.\nBây giờ, ta sẽ sử dụng chúng trong bài toán xây dựng CNN model phân loại Cat\u0026amp;Dog.\nImport thư viện:\nimport os import random import tensorflow as tf import shutil import matplotlib.pyplot as plt from tensorflow import keras from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint from tensorflow.keras.preprocessing.image import ImageDataGenerator config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Hai kỹ thuật Data augmentation và Model checkpoint sẽ được sử dụng thông qua lớp ImageDataGenerator và ModelCheckpoint, tương ứng.\nLớp ImageDataGenerator cho phép ta biến đổi ảnh gốc thành nhiều ảnh khác nhau thông quá các tham số truyền vào. Như trong hàm gen_data() dưới đây, ta sinh ra được 6 ảnh mới thông qua các phép biến đổi:\n Quay 40 độ Dịch theo chiều rộng 0.2 pixcel Dịch theo chiều cao 0.2 pixcel Cắt (xén) 0.2 pixcel Phóng to 0.2 pixcel Lật ngang  Chú ý rằng, các ảnh mới sinh ra không được lưu vào ổ cứng máy tính, mà chỉ được sinh ra tại thời điểm huấn luyện model và lưu tạm thời trong RAM. Khi kết thức quá trình training thì các ảnh đó cũng sẽ mất.\nTham số fill_mode='nearest' chỉ ra phương pháp bù lại giá trị cho những pixcel tại các vị trí bị mất mát do quá trình biến đổi. Nearest tức là dựa vào giá trị của các pixcel xung quanh, gần nó nhất (theo một tiêu chuẩn nào đó).\ndef gen_data(): training_datagen = ImageDataGenerator( rescale=1/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\u0026#39;nearest\u0026#39; ) validation_datagen = ImageDataGenerator( rescale=1/255 ) training_generator = training_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/train\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) validation_generator = validation_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/val\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) return training_generator, validation_generator Tiếp theo, ta khai báo một instance của ModelCheckpoint:\ncallback_1 = ModelCheckpoint( \u0026#39;cat_dog_model_checkpoint/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\u0026#39;, # Tên model tại mỗi điểm checkpoint monitor=\u0026#39;val_acc\u0026#39;, # Giá trị cần theo dõi save_best_only=True, # Chỉ lưu những model tốt nhất đến thời điểm checkpoint save_weights_only=True, # Chỉ lưu weights của model (để giảm kích thước) save_freq=\u0026#39;epoch\u0026#39;, # Checkpoint sau mỗi epoch mode=\u0026#39;auto\u0026#39;, # Val_acc phải tăng mới tính là model được cải thiện. Nếu monitor=\u0026#39;loss/val_loss\u0026#39; thì nó phải giảm mới tính là model được cải thiện verbose=1 # Hiển thị thông tin model lúc checkpoint ) Chi tiết từng tham số của ModelCheckpoint instance được giải thích chi tiết theo các comments trong code khai báo.\nTa vẫn sử dụng thêm EarlyStopping để tiết kiệm thời gian training:\ncallback_2 = EarlyStopping(monitor=\u0026#39;val_acc\u0026#39;, patience=5) Kiến trúc CNN model vẫn giữ nguyên như bài trước:\ndef create_model(): model = keras.models.Sequential([ keras.layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150, 150, 3)), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Flatten(), keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=RMSprop(lr=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Hàm plot_chart để thể hiện kết quả training lên đồ thị:\ndef plot_chart(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10, 6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Traing and Validation, Accuracy and Loss\u0026#39;) plt.legend(loc=0) plt.show() Cuối cùng, gộp tất cả lại và train model:\ntraining_generator, validation_generator = gen_data() model = create_model() history = model.fit( training_generator, epochs=30, validation_data=validation_generator, callbacks=[callback_1, callback_2], verbose=1 ) Output:\nFound 20000 images belonging to 2 classes. Found 5000 images belonging to 2 classes. Epoch 1/30 2/625 [..............................] - ETA: 17s - loss: 1.6500 - acc: 0.5469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0223s vs `on_train_batch_end` time: 0.0339s). Check your callbacks. 625/625 [==============================] - ETA: 0s - loss: 0.6804 - acc: 0.5882 Epoch 00001: val_acc improved from -inf to 0.69340, saving model to horse-humand_model_checkpoint/weights-improvement-01-0.69.hdf5 625/625 [==============================] - 115s 184ms/step - loss: 0.6804 - acc: 0.5882 - val_loss: 0.6017 - val_acc: 0.6934 Epoch 2/30 625/625 [==============================] - ETA: 0s - loss: 0.6239 - acc: 0.6591 Epoch 00002: val_acc improved from 0.69340 to 0.73060, saving model to horse-humand_model_checkpoint/weights-improvement-02-0.73.hdf5 625/625 [==============================] - 112s 179ms/step - loss: 0.6239 - acc: 0.6591 - val_loss: 0.5446 - val_acc: 0.7306 Epoch 3/30 625/625 [==============================] - ETA: 0s - loss: 0.5922 - acc: 0.6922 Epoch 00003: val_acc improved from 0.73060 to 0.76340, saving model to horse-humand_model_checkpoint/weights-improvement-03-0.76.hdf5 625/625 [==============================] - 113s 181ms/step - loss: 0.5922 - acc: 0.6922 - val_loss: 0.5106 - val_acc: 0.7634 ............ Epoch 30/30 625/625 [==============================] - ETA: 0s - loss: 0.4392 - acc: 0.8173 Epoch 00030: val_acc did not improve from 0.86600 625/625 [==============================] - 112s 179ms/step - loss: 0.4392 - acc: 0.8173 - val_loss: 0.4723 - val_acc: 0.7768 Model được train đầy đủ 30 epochs, không bị dừng giữa chừng do không thỏa mãn điều kiện của EarlyStopping.\nQuan sát thư mục cat_dog_model_checkpoint ta cũng thấy model được lưu tại một số điểm checkpoint. Model có độ chính xác cao nhất tại epoch thứ 29.\n├── weights-improvement-01-0.69.hdf5 ├── weights-improvement-01-0.98.hdf5 ├── weights-improvement-02-0.73.hdf5 ├── weights-improvement-02-1.00.hdf5 ├── weights-improvement-03-0.76.hdf5 ├── weights-improvement-05-0.80.hdf5 ├── weights-improvement-07-0.81.hdf5 ├── weights-improvement-10-0.83.hdf5 ├── weights-improvement-12-0.83.hdf5 ├── weights-improvement-17-0.83.hdf5 ├── weights-improvement-20-0.85.hdf5 ├── weights-improvement-24-0.86.hdf5 └── weights-improvement-29-0.87.hdf5 Kiểm tra quá trình huấn luyện bằng cách thể hiện giá trị loss và accuracy lên đồ thị:\nplot_chart(history)   Các giá trị loss và accuracy tuy có sự dao động nhưng kết quả cuối cùng vẫn khá tốt. Model không bị overfit quá nhiều, có thể chấp nhận được.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, ta sẽ học thêm một kỹ thuật rất thú vị nữa, giúp chúng ta giảm rất nhiều công sức trong việc huấn luyện model, đó là Tranfer Learning. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/33_dataaugmentation_modelcheckpoint_cnn/","tags":["CNN","Image Classification"],"title":"Sử dụng kỹ thuật Data Augmentation và Model Checkpoint khi huấn luyện CNN model"},{"categories":["CNN","Image Classification"],"contents":"Nếu như bài trước, toàn bộ dữ liệu được đưa vào training, thì bài này, ta sẽ chia tập dữ liệu thành 2 phần:\n Train set: Dùng để huấn luyện model. Validation set: Dùng đễ đánh giá model trong suốt quá trình training.  OK, hãy bắt đầu!\nTrước tiên, download bộ dataset cat-and-dog và giải nén về máy tính của bạn tại thư mục làm việc. Ta chỉ cần download file train.zip.\nBộ dataset này bao gồm 25.000 bức ảnh, chia thành 2 lớp chó và mèo. Mỗi lớp có 12.500 ảnh, kích thước 150x150.\nTa tiếp tục sử dụng lớp ImageDataGenerator để chuẩn bị dữ liệu cho training model. Lớp ImageDataGenerator yêu cầu cấu trúc thư mục dataset có dạng như sau:\n- training - class 1 - image 1 - image 2 - ... - class 2 - image 3 - image 4 - ... - ... - validation - class 1 - image 5 - image 6 - ... - class 2 - image 7 - image 8 Để chuẩn hóa cấu trúc thư mục như yêu cầu, ta có thể tự viết code để copy các ảnh vào đúng thư mục mong muốn. Hoặc có 1 cách đơn giản hơn là sử dụng thư viện split-folders.\nCài đặt thư viện:\npip install split-folders Sử dụng lệnh sau để tạo dữ liệu theo cấu trúc mong muốn:\nsplitfolders cats-and-dogs --ratio .8 .2 --output cat-dog-dataset Ta được thư mục output cat-dog-dataset:\ncat-dog-dataset ├── train │ ├── cats │ └── dogs └── val ├── cats └── dogs Có dữ liệu chuẩn chỉ rồi, giờ ta sẽ bắt tay vào viết code để train model.\nĐầu tiên, như thường lệ vẫn là import các thư viện sử dụng:\nimport os import random import tensorflow as tf import shutil import matplotlib.pyplot as plt from tensorflow import keras from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.preprocessing.image import ImageDataGenerator config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Ở đây, ngoài các thư viện, các lớp quen thuộc, ta có sử dụng thêm hàm EarlyStopping của lớp callback. Hàm này cho phép model dừng training khi nó thoả mãn một tiêu chí về độ chính xác mà người dùng có thể định nghĩa được. So với hàm callback tự viết như trong các bài trước thì việc sử dụng EarlyStopping linh động hơn rất nhiều. Ta sẽ đi chi tiết ở phần sau.\nTiếp theo, ta sử dụng lớp ImageDataGenerator để chuẩn bị dữ liệu cho việc huấn luyện model.\ndef gen_data(): training_datagen = ImageDataGenerator( rescale=1/255 ) validation_datagen = ImageDataGenerator( rescale=1/255 ) training_generator = training_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/train\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) validation_generator = validation_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/val\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) return training_generator, validation_generator Vì dataset đã được chia thành 2 phần, train set và validation set, nên ở đây ta cũng có 2 instances của lớp ImageDataGenerator tương ứng. Một cái dành cho train model, 1 cái dành cho validate model. Hai instances này chỉ khác nhau đường dẫn đến nơi chứa data, còn lại các thông số khác đều giống nhau. Cần lưu ý đến giá trị của tham số batch_size, vì lần này chúng ta sử dụng dữ liệu thật nên nếu bạn set giá trị của nó cao quá có thể dẫn đến hiện tượng out of memory. Thực tế, ban đầu mình set batch_size=64 nhưng bị lỗi nên phải giảm xuống còn 32.\nCNN model được tạo ra giống như bài trước:\ndef create_model(): model = keras.models.Sequential([ keras.layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150, 150, 3)), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Flatten(), keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=RMSprop(lr=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Optimizer vẫn là RMSprop nhưng được learning_rate được khởi tạo với giá trị 0.001 thay vì sử dụng giá trị mặc đinh. Loss Function là binary_crossentroy vì có 2 classes cần phân biệt.\nTa định nghĩa thêm hàm plot_chart để thể hiện kết quả training lên đồ thị:\ndef plot_chart(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10, 6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Traing and Validation, Accuracy and Loss\u0026#39;) plt.legend(loc=0) plt.show() Bên trên ta đã nói về hàm callback sử dụng EarlyStopping. Ta định nghĩa nó như sau:\ncallback = EarlyStopping(monitor=\u0026#39;loss\u0026#39;, patience=5) Mục đích của hàm này là buộc model dừng quá trình training nếu sau 5 epochs liên tiếp mà giá trị của loss không giảm. Bạn cũng có thể thay loss bằng acc, khi đó, model sẽ dừng training nếu giá trị accuracy không tăng sau 5 epochs liên tiếp.\nGộp tất cả lại và train model:\ntraining_generator, validation_generator = gen_data() model = create_model() history = model.fit( training_generator, epochs=30, validation_data=validation_generator, callbacks=[callback], verbose=1 ) Dòng `validation_data=validation_generator` trong hàm fit chỉ ra dữ liệu được dùng để validate model trong suốt quá trình training. Output: ```python Found 20000 images belonging to 2 classes. Found 5000 images belonging to 2 classes. Epoch 1/30 1/625 [..............................] - ETA: 0s - loss: 0.6942 - acc: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0141s vs `on_train_batch_end` time: 0.0326s). Check your callbacks. 625/625 [==============================] - 116s 186ms/step - loss: 0.6851 - acc: 0.5663 - val_loss: 0.6292 - val_acc: 0.6240 Epoch 2/30 625/625 [==============================] - 121s 194ms/step - loss: 0.6250 - acc: 0.6571 - val_loss: 0.5594 - val_acc: 0.7356 Epoch 3/30 625/625 [==============================] - 117s 188ms/step - loss: 0.5928 - acc: 0.6921 - val_loss: 0.5248 - val_acc: 0.7652 ... Epoch 14/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4828 - acc: 0.7799 - val_loss: 0.4186 - val_acc: 0.8050 Epoch 15/30 625/625 [==============================] - 123s 197ms/step - loss: 0.4877 - acc: 0.7753 - val_loss: 0.5091 - val_acc: 0.7568 Epoch 16/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4787 - acc: 0.7796 - val_loss: 0.5068 - val_acc: 0.8126 Epoch 17/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4844 - acc: 0.7826 - val_loss: 0.3928 - val_acc: 0.8374 Epoch 18/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4876 - acc: 0.7832 - val_loss: 0.4588 - val_acc: 0.8388 Ta thấy rằng quá trình training model dừng lại tại epoch thứ 18, vì từ epoch 13 đến epoch 18, giá trị của loss không giảm đi chút nào, thậm chí còn tăng lên.\nThử vẽ đồ thị loss và accurcy:\nplot_chart(history)   Từ đồ thị ta có thể nhận xét rằng model được training khá tốt, có một chút overfitting nhưng không đáng kể.\nCuối cùng, model nên được lưu lại để sử dụng cho việc dự đoán về sau:\nmodel.save(\u0026#39;cats-dogs-model.h5\u0026#39;) Source code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, ta sẽ áp dụng thêm 2 kỹ thuật quan trọng nữa cho bài toán phân loại cat-dog, đó là data augmentation và ModelCheckpoint. Cũng giống như EarlyStopping, mục đích của 2 kỹ thuật này không gì hơn là ngăn chặn hiện tượng Overfitting của model trong quá trình training. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/32_earlystopping_cnn/","tags":["CNN","Image Classification"],"title":"Sử dụng kỹ thuật EarlyStopping khi huấn luyện CNN model"},{"categories":["CNN","Image Classification"],"contents":"Như đã hứa ở bài trước, ở bài này chúng ta sẽ sử dụng một bộ dữ liệu \u0026ldquo;thực tế\u0026rdquo; hơn để huấn luyện một DL model phân loại hình ành, đó là happy-or-sad dataset. Tập dữ liệu này gồm 80 ảnh, được chia thành 2 lớp happy và sad.\nNhư thường lệ, đầu tiên ta sẽ import các thư viện sử dụng:\nimport PIL import pathlib import tensorflow as tf import numpy as np from tensorflow import keras from tensorflow.keras.preprocessing.image import ImageDataGenerator Cũng giống như đã đề cập trong bài trước, ta cần thêm 3 dòng sau để tránh sự xung đột giữa 2 phiên bản của Tensorflow.\nconfig = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Tiếp theo, download và giải nén tập dữ liệu này về máy tính theo đường dẫn bên trên. Ở đây, mình giả sử bạn đặt thư mục happy-or-sad trong cùng thư mục dự án.\nDataset gồm 80 ảnh, chia thành 2 lớp: happy và sad, mỗi lớp có 40 ảnh.\nĐịnh nghĩa hàm callback như các bài trước:\nclass MyCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if logs.get(\u0026#39;acc\u0026#39;) \u0026gt; 0.99: print(\u0026#39;Reach to 99%, stop training!\u0026#39;) self.model.stop_training = True Định nghĩa hàm load dữ liệu huấn luyện:\ndef load_data(): train_datagen = ImageDataGenerator( rescale=1/255 ) train_generator = train_datagen.flow_from_directory( \u0026#39;happy-or-sad\u0026#39;, target_size=(150,150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) return train_generator Trong bài này, vì dataset là ảnh thực tế được lưu trong ổ cứng của máy tính nên ta sử dụng lớp ImageDataGenerator để chuẩn bị dữ liệu cho model training. Bản chất của lớp này là không load toàn bộ dataset một lần (*điều này có thể sẽ không khả thi nếu *) Ngoài chức năng cơ bản đó, nó còn giúp tăng cường dữ liệu (data augmentation), một trong những kỹ thuật hạn chế hiện tượng Overfitting khi huấn luyện model. Chúng ta sẽ sử dụng lớp này thường xuyên trong các bài tiếp theo.\nỞ đây, kích thước ảnh đầu vào là (150,150), các giá trị pixels của ảnh được rescale xuống 255 lần, và sẽ có 32 ảnh được load mỗi lần.\nCNN model được tạo bởi hàm sau:\ndef create_model(): model = keras.models.Sequential([ tf.keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150,150,3)), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(16, (3,3), activation=\u0026#39;relu\u0026#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Model được tạo theo kiến trúc: [CONV =\u0026gt; POOL]x3 =\u0026gt; FCx3. Compiler là rmsprop, Loss Function là binary_crossentropy vì chúng ta chỉ có 2 lớp cần phân biệt.\nCuối cùng, gọi các hàm định nghĩa bên trên để huấn luyện model:\nx_train, y_train = load_data() model = create_model() history = model.fit(x_train, y_train, epochs=100, callbacks=[MyCallback()], verbose=1) Output:\nEpoch 1/100 3/3 [==============================] - 0s 58ms/step - loss: 1.2190 - acc: 0.5500 Epoch 2/100 3/3 [==============================] - 0s 88ms/step - loss: 0.6919 - acc: 0.5000 Epoch 3/100 3/3 [==============================] - 0s 64ms/step - loss: 0.6957 - acc: 0.5375 ................... Epoch 98/100 3/3 [==============================] - 0s 65ms/step - loss: 0.1934 - acc: 0.9000 Epoch 99/100 3/3 [==============================] - 0s 89ms/step - loss: 0.1710 - acc: 0.9250 Epoch 100/100 3/3 [==============================] - 0s 68ms/step - loss: 0.2187 - acc: 0.8875 Như ta quan sát thấy, model không thể đạt đến được độ chính xác 99% như điều kiện dừng ở hàm callback. Độ chính xác cuối cùng là 88,75%.\nQua bài này, ta đã biết cách sử dụng dữ liệu thực tế, lưu trong ổ đĩa cứng để huấn luyện một CNN model, bằng cách sử dụng lớp ImageDataGenerator của thư viện Tensorflow.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, ta sẽ huấn luyện CNN model để phân loại 2 đối tượng trong ảnh: chó và mèo. Khác biệt so với bài này là ta sẽ sử dụng dữ liệu validation trong quá trình huấn luyện model, giúp cho model học tốt hơn.\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/31_happy_and_sad_classification_cnn/","tags":["CNN","Image Classification"],"title":"Xây dựng CNN model với tập dữ liệu Happy\u0026Sad"},{"categories":["CNN","Image Classification"],"contents":"Trong bài trước, chúng ta đã sử dụng các lớp FC để xây dựng model phân loại các sản phẩm trong tập MNIST thành 10 nhóm khác nhau.\nViệc phân loại hình ảnh (trong thực tế)là một nhìệm vụ tương đối phức tạp, nó yêu cầu phải xây dựng các NN model với nhiều lớp. Nếu chỉ sử dụng hoàn toàn FC layer thì sẽ không hiệu quả về cả độ chính xác cũng như hiệu năng của model. Thay vào đó, các lớp CONV, POOL, \u0026hellip; được sử dụng thường xuyên hơn.\nYêu cầu cần giải quyết ở bài này vẫn giống như bài trước, chỉ có điều ta sẽ sử dụng các lớp CONV, POOL, \u0026hellip; trong thư viện tensorflow xây dựng model phân loại các sản phẩm trong tập MNIST. Môi trường thực hành vẫn giống như các bài trước.\nĐầu tiên, như thường lệ ta sẽ import thư viện tensorflow:\nimport tensorflow as tf Tiếp đến là hàm callback:\nclass MyCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if logs.get(\u0026#39;acc\u0026#39;) \u0026gt; 0.99: print(\u0026#39;Reached to 99%, stop training!\u0026#39;) self.model.stop_training = True Nhắc lại là hàm này sẽ được gọi tại thời điểm kết thúc mỗi epoch trong quá trình train model. Nó làm nhiệm vụ kiểm tra độ chính xác của model tại thời điểm đó. Nếu độ chính xác đạt đến 99% thì kết thúc quá trình train.\nHàm load dữ liệu MNIST từ trong tensorflow:\ndef load_data(): (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = x_train.reshape(-1, 28, 28, 1) x_train = x_train/255 return x_train, y_train Giờ đến lúc quan trọng nhất của bài này, đó là tạo CNN model:\ndef create_model(): model = keras.models.Sequential([ keras.layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, input_shape=(28,28,1)), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Flatten(), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model CNN model được tạo thành từ các layers: CONV2D, MaxPooling2D, Flatten, Dense, \u0026hellip; Tùy thuộc mức độ phức tạp của dữ liệu và yêu cầu của bài toán mà ta sử dụng số lượng và cách thức kết hợp các layers này theo những cách khác nhau. Chi tiết về các common pattern, các rules và các tham số sử dụng khi xây dựng mạng CNN, các bạn có thể đọc thêm tại bài viết trước của mình tại đây.\nỞ bài này, chúng tạo một mạng CNN đơn giản theo kiến trúc: [CONV2D =\u0026gt; MaxPooling2D]x2 =\u0026gt; FLATTEN =\u0026gt; DENSEx2.\nCNN model sau đó được compile sử dụng thuật toán RMSprop 9cùng với SGD, Adam và RMSprop là 2 thuật toán tối ưu cũng thường hay được sử dụng khi train DL model. Mình dự định sẽ có các bài viết chi tiết về 2 thuật toán này. Mời các bạn đón đọc), hàm loss là sparse_categorical_crossentropy, và metric là accuracy trên tập train.\nCó model rồi, đã đến lúc tiến hành train model:\nx_train, y_train = load_data() model = create_model() history = model.fit(x_train, y_train, epochs=100, callbacks=[MyCallback()], verbose=1) Model sẽ được train tối đa 100 epochs, hàm callback được truyền vào như 1 tham số để kểm tra điều kiện dừng train của model sau mỗi epoch.\nOutput:\nEpoch 1/100 1875/1875 [==============================] - 14s 7ms/step - loss: 0.1141 - acc: 0.9652 Epoch 2/100 1875/1875 [==============================] - 14s 7ms/step - loss: 0.0406 - acc: 0.9877 Epoch 3/100 1875/1875 [==============================] - ETA: 0s - loss: 0.0318 - acc: 0.9913Reached to 99%, stop training! 1875/1875 [==============================] - 14s 7ms/step - loss: 0.0318 - acc: 0.9913 Rất nhanh, model đạt đến độ chính xác 99% chỉ sau 3 epochs, so với 7 epochs nếu sử dụng hoàn toàn FC layer như bài trước. Với bộ dữ liêu đơn giản như MNIST, 3 epochs hay 7 epochs không có sự khác biệt nhiều về thời gian cũng như độ chính xác. Nhưng nếu dữ liệu rất lớn thì sự khác nhau đó sẽ trở nên rất rõ rệt.\nNếu khi gọi hàm fit() để train model mà gặp lỗi:\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [[node sequential/conv2d/Conv2D (defined at \u0026lt;ipython-input-6-04cbaae553c1\u0026gt;:1) ]] [Op:__inference_train_function_856] thì bạn hãy thêm 3 dòng bên dưới ngay sau khi import tensorflow.\nconfig = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Nguyên nhân lỗi ở đây là do xung đột giữa 2 phiên bản 1.x và 2.x của tensorflow.\nOk, như vậy là chúng ta đã hoàn thành việc xây dựng CNN model để phân loại hình ảnh trong tập dữ liệu MNIST. Tuy đơn giản nhưng đây sẽ là bước đầu để dần dần bạn có thể tự tin tạo ra các CNN model phức tạp hơn, với các tập dữ liệu lớn hơn.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong bài sau, chúng ta sẽ xây dựng một CNN model khác với dữ liệu thực tế hơn. Hãy đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/30_fashion_mnist_classification_advanced/","tags":["CNN","Image Classification"],"title":"Xây dựng CNN model phân loại với tập dữ liệu MNIST"},{"categories":["Neural Network","Image Classification"],"contents":"Bài này, ta sẽ nâng độ khó hơn 1 chút so với bài trước. Yêu cầu đề bài như sau:\n Xây dựng một NN model phân loại các hình ảnh trong tập dữ liệu MNIST Trong quá trình train, khi độ chính xác của model đạt đến 99% thì dừng train.  Mục đích của bài này là giúp ta làm quen với dữ liệu \u0026ldquo;thực tế\u0026rdquo; hơn 1 chút so với bài trước và cách sử dụng hàm callback để điều khiển quá trình train model.\nMôi trường thực hành của bài này giống hệt bài dự đoán giá nhà trước đó.\nOk, hãy cùng bắt đầu!\nĐầu tiên, import tensorflow:\n1 import tensorflow as tf Tiếp theo, ta định nghĩa hàm callback. Hàm này sẽ được gọi mỗi khi model train xong một epoch.\n2 class CustomCallback(keras.callbacks.Callback): 3 def on_epoch_end(self, epoch, logs={}): 4 if logs.get(\u0026#39;acc\u0026#39;) \u0026gt; 0.99: 5 print(\u0026#39;Reached to 99%, stop training!\u0026#39;) 6 self.model.stop_training = True Ở đây, ta sẽ cho model dừng train khi độ chính xác đạt đến 99% như yêu cầu đề bài.\nDataset được load như code của hàm sau:\n7 def load_data(): 8 (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() 9 x_train = x_train/255 10 x_test = x_test/255 11 return x_train, y_train, x_test, y_test MNIST có thể coi là bộ dataset kinh điển mà hầu như bất kỳ ai cũng sử dụng khi mới học AI. Có lẽ vì thế mà nó được tích hợp sẵn trong thư viện tensorflow.\nCó một chú ý ở hàm load_data() là ta cũng scale down giá trị của x_train, x_test bằng cách chia cho 255. Mục đích của việc làm này cũng giống như mình đã trình bày trong bài trước.\nPhần chính của chúng ta là định nghĩa model:\n12 def create_model(): 13 model = tf.keras.models.Sequential([ 14 tf.keras.layers.Flatten(), 15 tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;, input_shape=(28,28)), 16 tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) 17 ]) 18 model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) 19 return model Model của chúng ta hôm nay gồm 2 lớp: 1 lớp input (128 nodes) và 1 lớp output (10 nodes), không có lớp ẩn (hidden layer).\nKích thước của dữ liệu đầu vào là (28,28), bằng với kích thước của mỗi bức ảnh trong tập MNIST.\nSố node của lớp output là 10, bằng với số lớp của tập MNIST mà ta cần phân loại. Hàm kích hoạt Softmax sử dụng ở lớp này sẽ cho ta biết chính xác xác suất của hình ảnh thuộc về mỗi lớp. Lớp nào có xác suất lớn nhất sẽ được lấy làm kết quả cuối cùng.\nModel được compile với thuật toán tối ưu SGD, hàm loss là sparse_categorical_crossentropy, và metric là accuracy trên tập train.\nBây giờ ta sẽ tiến hành train model:\n20 x_train, y_train, x_test, y_test = load_data() 21 model = create_model() 22 23 history = model.fit(x_train, y_train, epochs=100, verbose=1, callbacks=[CustomCallback()]) Model được train tối đa 100 epochs, hàm callback mà ta định nghĩa bên trên được truyền vào như 1 tham số.\nOutput:\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 1s 0us/step Epoch 1/100 1875/1875 - 1s - loss: 0.2573 - acc: 0.9266 Epoch 2/100 1875/1875 - 1s - loss: 0.1128 - acc: 0.9666 Epoch 3/100 1875/1875 - 1s - loss: 0.0771 - acc: 0.9765 Epoch 4/100 1875/1875 - 1s - loss: 0.0573 - acc: 0.9825 Epoch 5/100 1875/1875 - 1s - loss: 0.0445 - acc: 0.9856 Epoch 6/100 1875/1875 - 1s - loss: 0.0345 - acc: 0.9895 Epoch 7/100 Reached to 99%, stop training! 1875/1875 - 1s - loss: 0.0283 - acc: 0.9913 Đầu tiên, tập MNIST sẽ được download về local, sau đó model sẽ được train. Quá trình train dừng lại sau 7 epochs vì độ chính xác đã đạt đến 99% như định nghĩa ở hàm callback.\nNhư vậy là chúng ta đã giải quyết xong yêu cầu đặt ra lúc đầu. Qua bài này ta đã biết:\n Cách load dataset được tích hợp trong tensorflow. Cách xây dựng và sử dụng hàm callback khi train model. Các tạo và train model với 2 lớp NN sử dụng tensorflow.  Source code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, chúng ta sẽ xây dựng model sử dụng lớp CONV trong tensorflow để nâng cao độ chính xác cũng như hiệu năng của model. Mời các bạn đón đọc.\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/29_fashion_mnist_classification/","tags":["Neural Network","Image Classification"],"title":"Xây dựng NN model phân loại với tập dữ liệu MNIST"},{"categories":["Neural Network"],"contents":"Sau một số bài về lý thuyết thì hôm nay chúng ta sẽ bắt tay vào thực hành code một bài toán mẫu giáo, áp dụng những lý thuyết mà ta đã tìm hiểu xem sao nhé!\nThông tin về bài toán và yêu cầu đặt ra như sau:\n Quy tắc tính giá nhà như sau: 50k + 50k/bedroom. Ví dụ: nhà có 1 bedroom thì giá sẽ là 100k, nhà có 2 bedrooms thì giá sẽ là 150k, \u0026hellip; Xây dựng một NN model để dự đoán giá tiền của một căn nhà có 7 phòng ngủ. So sánh giá của căn nhà đó theo 2 cách: tính theo các thông thường và theo kết quả dự đoán của NN model.  Mình giả sử là các bạn đã cài sẵn môi trường trên máy tính của các bạn (hoặc các bạn có thể sử dụng colab cũng được.). Chúng ta sẽ sử dụng tensorflow 2.3.0 và numpy 1.18.5\nĐầu tiên, import và kiểm tra thư viện:\n1 import tensorflow as tf 2 import numpy as np 3 import os 4 from tensorflow import keras 5 6 print(tf.__version__) 7 print(np.__version__) 8 print(tf.config.list_physical_devices(\u0026#39;GPU\u0026#39;)) Kết quả:\n2.3.0 1.18.5 [PhysicalDevice(name=\u0026#39;/physical_device:GPU:0\u0026#39;, device_type=\u0026#39;GPU\u0026#39;)] Máy của các bạn có thể không có GPU cũng không sao, vì bài này rất đơn giản nên cũng không cần đến GPU. Các bài sau thì nên có vì như thế thời gian train model sẽ nhanh hơn rất nhiều.\nTiếp theo, chúng ta sẽ tạo ra dữ liệu huấn luyện dựa theo quy tắc tính giá nhà như trong đề bài:\n9 x_train = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], dtype=float) 10 y_train = np.array([100.0, 150.0, 200.0, 250.0, 300.0, 350.0], dtype=float) Mỗi phần tử của x_train là số phòng ngủ của căn nhà, còn mỗi phần tử của y_train là giá nhà tương ứng với số phòng ngủ đó.\nBây giờ là lúc chúng ta xây dựng NN model:\n11 model = keras.Sequential([ 12 keras.layers.Dense(units=1, input_shape=[1]) 13 ]) 14 model.compile(optimizer=\u0026#39;sgd\u0026#39;, lost=\u0026#39;mse\u0026#39;) Dòng 11-13 định nghĩ một NN model với chỉ một layer, một input có kích thước là 1. Dòng 14, model được compiled, sử thuật toán tối ưu SGD và Lost Function là MSE (Mean Square Error).\nĐến đây, model đã sẵn sàng để train:\n15 model.fit(x_train, y_train/255, epochs=1000) Model được train với dữ liệu train đã tạo ở bên trên, số lượng epoch là 1000. Chú ý rằng ở đây, y_train được chia cho 255. Mục đích của việc scale down này là để model không bị quá bias vào giá trị của y_train khi mà giá trị của y_train lớn hơn giá trị của x_train rất nhiều.\nOutput:\nEpoch 1/1000 1/1 [==============================] - 0s 1ms/step - loss: 37.2759 Epoch 2/1000 1/1 [==============================] - 0s 920us/step - loss: 17.2548 Epoch 3/1000 1/1 [==============================] - 0s 525us/step - loss: 7.9884 .................. Epoch 998/1000 1/1 [==============================] - 0s 535us/step - loss: 3.0594e-06 Epoch 999/1000 1/1 [==============================] - 0s 938us/step - loss: 3.0372e-06 Epoch 1000/1000 1/1 [==============================] - 0s 1ms/step - loss: 3.0151e-06 Ta thấy giá trị loss giảm từ 37.2759 đến 3.0151e-06. Một con số khá thấp.\nCuối cùng, ta sẽ dùng model vừa trained để dự đoán giá của ngôi nhà có 7 phòng ngủ.\n16 prediction = model.predict([7.0]) 17 print(prdiction * 100) Kết quả:\n[[400.25043]] Giá của ngôi nhà nếu tính theo cách thông thường sẽ là 400$. So sánh 2 kết quả ta thấy chúng khá gần nhau. Như vậy là model đã làm việc khá tốt. Bạn có thể tăng số epochs lên và train lại model, sau đó kiểm trả lại kết quả dự đoán xem nó có được cải thiện hay không!\nOK, như vậy là chúng ta đã giải quyết xong yêu cầu của bài toán. Đây chỉ là 1 bài tập trình độ mẫu giáo để chúng ta làm quen với việc sử dụng tensorflow để xây dựng NN model. Trong các bài sau, chúng ta sẽ giải quyết các bài toán phức tạp hơn.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/28_hourse_prices_prediction/","tags":["Neural Network"],"title":"Xây dựng NN model đơn giản dự đoán giá nhà"},{"categories":["Algorithm Optimization"],"contents":"\u0026ldquo;Nearly all of deep learning is powered by one very important algorithm: Stochastic Gradient Descent (SGD)\u0026rdquo; – Goodfellow et al.\nTừ bài trước chúng ta đã biết rằng để model có thể dự đoán đúng thì phải tìm được giá trị phù hợp cho $W$ và $b$. Nếu chúng ta chỉ dựa hoàn toàn vào việc chọn ngẫu nhiên thì gẫn như không bao giờ có thể tìm được giá trị mong muốn. Thay vì thế, chúng ta cần định nghĩa một thuật toán tối ưu (optimization) và sử dụng nó để cải thiện $W$ và $b$. Trong bài này, chúng ta sẽ tìm hiểu một thuật toán tối ưu được sử dụng rất rất phổ biến trong NN and DL model - Gradient Descent (GD) và các biến thể của nó. Ý tưởng chung của họ các thuật toán GD là đánh giá các tham số, tính toán loss, sau đó thực hiện một bước nhỏ theo hướng giảm loss. Cả 3 bước này được thực hiện trong các vòng lặp cho đến khi gặp một điều kiện dừng nào đó.\n1. The Loss Landscape và Optimization Surface\nGradient descent là thuật toán hoạt động theo kiểu tối ưu qua từng vòng lặp thông qua một mặt tối ưu(optimization surface / loss landscape), như minh họa ở hình bên dưới.\n Phía bên trái biểu diễn trong không gian 2 chiều để chúng ta dễ hình dùng, còn bên phải biểu diễn một cách thực tế hơn trong không gian nhiều chiều. Mục đích sử dụng gradient descent là tìm ra điểm global minumum (đáy của cái bát ở bên phải).\nChúng ta có thể thấy, optimization surface có rất nhiều đỉnh (peaks) và thung lũng (valleys*). Mỗi valley có một điểm đáy mà tại đó giá trị loss đạt giá trị cực tiểu, gọi là local minimum. Trong số các điểm local minimum, có 1 điểm mà giá trị loss đạt giá trị nhỏ nhất được gọi là gloabal minimum. Đây chính là điểm mà chính ta muốn tìm trong quá trình training AI model thông qua việc cập nhật các tham số.\nHãy tưởng tượng, việc dò tìm điểm global minimum trên optimization surface giống như việc đặt 1 viên bi (*chính là * $W$) trên mặt đó, nhiệ vụ của viên bi là dò tìm đường để đi đến điểm đích (global minimum).\nNếu chỉ nhìn vào hình trên, mọi người có thể thắc mắc: Nếu muốn đến điểm global minimum, tại sao không nhảy thẳng một phát đến đó?\nNhưng mọi việc không đơn giản như vậy, bởi vì trên thực tế, chúng ta không biết hình dạng của optimization surface như thế nào, chúng ta như một ngươi mù trên đường, không biết phương hướng. Và các thuật toán tối ưu (gradient descent là một trong số đó) chính là cây gậy trong tay, giúp chúng ta dò đường.\nCụ thể hơn 1 chút thì mỗi một điểm trên optimization surface tương ứng với một giá trị loss $L$ - chính là output của loss funtion khi đưa vào cặp giá trị ($W$, $b$). Ý tưởng của thuật toán tối ưu là cố gắng thử sử dụng các cặp giá trị ($W$, $b$) khác nhau, tính toán loss, cập nhật ($W$, $b$) sao cho giá trị loss thấp hơn \u0026hellip; Lý tưởng nhất là chúng ta có thể đạt được giá trị loss nhỏ nhất tại điểm global minimum, nhưng điều này thường khó xảy ra trong thực tế.\n2. Gradient Descent cho hàm 1 biến\nGiả sử Loss Function của chúng ta là hàm bậc 1, $f(x)$. Điểm global minimum là điểm mà tại đó $x = x^*$.\nĐạo hàm của của $f(x)$ là $f'(x)$. Nếu bạn còn nhớ, trong chương trình toán THPT, khi học về đạo hàm ta có các nhận xét:\n Nếu đạo hàm của hàm số tại thời điểm $t$, $f'(x_t) \u0026gt; 0$ thì $x_t$ nằm về phía bên phải so với $x^*$, và ngược lại. $x_t$ càng xa $x^*$ về phía bên phải thì $f'(x_t)$ càng lơn hơn 0, và ngược lại.  Từ nhận xét số 1 có thể suy ra, để điểm tiếp theo $x_{t+1}$ tiến gần về $x^*$ hơn thì cần di chuyển $x_t$ về phía bên trái, tức là phía âm, hay phía ngược dấu với đạo hàm:\n$x_{t+1} = x_t + \\Delta$ ($\\Delta$ là một đại lượng ngược dấu với đạo hàm $f'(x)$)  Từ nhận xét số 2 có thể suy ra lượng di chuyển $\\Delta$ tỉ lệ thuận với $-f'(x)$.\nTổng hợp hai nhận xét trên, ta có công thức cập nhật $x_t$ một cách đơn giản là:\n$x_{t+1} = x_t - \\eta f'(x_t)$ Hoặc viết dưới dạng đơn giản:\n$x = x - \\eta f'(x)$ Trong $\\eta$ là một số \u0026gt; 0, gọi là learning rate. Dấu trừ thể hiện viêc đi ngược chiều với đạo hàm (descent nghĩa là đi ngược).\n3. Gradient Descent cho hàm nhiều biến\nGiả sử Loss Function của chúng ta, $f(\\theta)$ là hàm nhiều biến, trong đó $\\theta$ là tập hợp các vector các tham số của model cần tối ưu. Đạo hàm của $f(\\theta)$ tại thời điểm $\\theta$ là $\\nabla_\\theta f(\\theta)$.\nTương tự hàm 1 biến, quy tắc cập nhật $\\theta$ là:\n$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta f(\\theta_t)$ Hoặc viết dưới dạng đơn giản: $\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta_t)$ Tóm lại, thuật toán GD hoạt động như sau:\n Dự đoán một điểm khởi tạo $\\nabla = \\nabla_0$. Cập nhật $\\nabla$ đến khi đạt được kết quả chấp nhận được (hoặc một điều kiện dừng nào đó). $\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta)$   với $\\nabla_\\theta f(\\theta)$ là đạo hàm của Loss Function tại $\\theta$.\n4. Stochastic Gradient Descent (SGD)\nThuật toán GD nguyên thủy có một nhược điểm to lớn là hội tụ rất chậm và yêu cầu tài nguyên tính toán rất lớn. Nguyên nhân gốc rễ của vấn đề này là do GD tính toán gradient trên toàn bộ training set. Điều này thật khó để chấp nhận nếu áp dụng với một tập dữ liệu lớn.\nMột biến thể của GD, gọi là Stochastic Gradient Descent (SGD) ra đời, khắc phục những hạn chế của GD. Thay vì tính toán và cập nhật weight matrix $W$ trên toàn bộ tập dữ liệu như cách làm của GD (cập nhật theo epoch), SGD chia nhỏ tập training thành các batchs (dữ liệu thường được xáo trộn ngẫu nhiên trước khi chia), tính toán và cập nhật $W$ theo từng batch đó (cập nhật theo batch).\nBiểu diễn theo toán học, công thức cập nhật của SGD như sau:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta;x_i;y_i)$ trong đó, $f(\\theta;x_i;y_i)$ là Loss Function với chỉ 1 cặp điểm dữ liệu (input, label) là ($x_i, y_i$).\nMặc dù ra đời từ rất lâu (1960), SGD vẫn là một thuật toán quan trọng, được sử dụng rộng rãi trong các kiến trúc DL hiện đại. Vì thế, viêc hiểu cặn cẽ về nó là một điều cần thiết khi học AI/ML.\n4.1 Mini-batch SGD\nMột câu hỏi đặt ra khi sử dụng SGD là kích thước của batch (batch_size) là bao nhiêu thì hợp lý? Theo như cách diễn giải bên trên thì có vẻ như batch_size càng nhỏ càng tốt? Và tốt nhất là batch_size = 1?\nTuy nhiên, điều này không đúng. Sử dụng batch_size \u0026gt; 1 mang lại cho chúng ta một số lợi ích nhất định. Nó giúp giảm phương sai khi cập nhật $W$, và đặc biệt hơn, nếu giá trị của batch_size là lũy thừa của 2 thì chúng ta còn hưởng lợi về tốc độ thực thi của các thư viện tối ưu trong đại đố tuyến tính. Trong các bài toán thực tế, batch_size thường nhận các giá trị 32, 64, 128, 256, tùy thuộc vào tài nguyên tính toán của bạn.\nLúc này, công thức cập nhật sẽ trở thành:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta;x_{i:i+n};y_{i:i+n})$ trong đó, $x_{i:i+n}, y_{i:i+n}$ là các cặp điểm dữ liệu (*input, label*) có vị trí từ $i$ đến $i + n -1$.\n4.2 Mở rộng của SGD\nTrong quá trình sử dụng SGD, ta có thể bắt gặp 2 kỹ thuật hỗ trợ tăng tốc độ hội tụ cho SGD. Đố là momentum và nesterov accelerated gradient (NAG).\n4.2.1 Momentum\nMomentum, hiểu theo nghĩa tiếng việt là đà, lấy đà hay quán tính. Mục tiêu của nó là đẩy nhanh tốc độ cập nhật $W$ tại những nơi mà các gradients có cùng hướng, và ngược lại. Quan sát lại hình bên trên, có thể tưởng tượng rằng nếu không có momentum, viên bi của chúng ta rất dễ bị mắc kẹt ở các local minimum, mà không sao thoát ra để tìm đến global minimum được.\nỞ phần trên, ta đã biết công thức cập nhật các tham số như sau:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta_t)$ Thêm vào momentum $V$, với:\n$V_t = \\gamma V_{t-1} - \\eta \\nabla_\\theta f(\\theta)$ ta được:\n$\\theta = \\theta - V_t$ Trong đó, $\\gamma$ là đại lượng thường được chọn giá trị 0.9, hoặc ban đầu chọn là 0.5, sau khi quá trình học diễn ra ổn định thì tăng lên 0.9. Nó hầu như không bao giờ \u0026lt; 0.5. Khi khai báo sử dụng momentum (trong tensorflow chẳng hạn), ta thường truyền vào giá trị của đại lương này.\n4.2.2 Nesterov Accelerated Gradient (NAG)\nMomentum tuy giúp ta vượt qua được các local minimum, nhưng khi tới gần global minimum, do có đà nên viên bi vẫn tiếp tục dao động thêm một khoảng thời gian nữa mới có thể dừng lại đúng điểm cần dừng. NAG ra đời để khắc phục nhược điểm này.\nÝ tưởng chính của NAG là sử dụng gradient ở thời tiếp theo, thay vì gradient ở thời điểm hiện tại khi tính lượng thay đổi của $\\theta$.\nÝ tưởng của Nesterov accelerated gradient.\nNguồn: CS231n Stanford: Convolutional Neural Networks for Visual Recognition  Công thức cập nhật sẽ như sau:\n$V_t = \\gamma V_{t-1} - \\eta \\nabla_\\theta f(\\theta - \\gamma V_{t-1}) \\theta$ $\\theta = \\theta - V_t$ Momentum là một kỹ thuật quan trọng và hiệu quả, gần như luôn luôn được sử dụng cùng với SGD. Còn đối với NAG, chúng ta ít gặp hơn. Trong khi về mặt lý thuyết, nó mang lại hiệu quả hơn momentum, nhưng trong thực tế các kiến trúc nổi tiếng như AlexNet, VGGNet, ResNet, Inception, \u0026hellip; khi train trên tập dữ liệu ImageNet, chỉ sử dụng SGD với momentum. Có lẽ NAG chỉ phù hợp với các tập dữ liệu nhỏ.\n5. Các thuật toán tối ưu khác\nNgoài SGD, hai thuật toán khác cũng rất hay được sử dụng trong các kiến trúc DL hiện đại là Adam và RMSprop. Mình sẽ có bài viết riêng về các thuật toán này. Mời các bạn đón đọc.\nTham khảo\n Pyimagesearch Dive into Deep Learning machinelearningcoban blog  ","permalink":"https://tiensu.github.io/blog/27_optimization_methods_gradient-descent/","tags":["Algorithm Optimization"],"title":"Các phương pháp Optimization - Gradient Descent"},{"categories":["Machine Learning","Deep Learning"],"contents":"Bạn có biết thuật toán kNN - một trong những thuật toán đơn giản nhất của ML? Về bản chất, nó không \u0026ldquo;học\u0026rdquo; bất cứ điều gì từ dữ liệu mà chỉ đơn giản là lưu dữ liệu bên trong model, và tại thời điểm dự đoán, nó so sánh dữ liệu cần dự đoán với dữ liệu trong tập training. Rõ ràng với cách làm việc như vậy thì ưu điểm lớn nhất của kNN là không mất thời gian training model. Không không cần quá quan tâm về độ chính xác thì ưu điểm này chính là lý do mà kNN vẫn còn được sử dụng trong một số trường hợp. Hạn chế của kNN chỉ xuất hiên khi gặp bài toán mà sử dụng lượng dữ liệu lớn. Lúc này thời gian dự đoán của kNN sẽ rất lâu, và đôi khi không thể sử dụng được trong thực tế.\nMột các tiếp cận khác của ML model mà ở đó nó có thể học được các patterns từ dữ liệu trong suốt quá trình training. Sau đó, ở giai đoạn dự đoán, ML model đó có thể thực hiện rất nhanh chóng để đưa ra kết quả. Dạng này của ML được gọi là parameterized learning.\nDưới đây là định nghĩa chính thống từ tác giả:\n“A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at the parametric model, it won’t change its mind about how many parameters it needs.” – Russell and Norvig (2009).\nCó thể nói parameterized learning là nền tằng của các thuật toán ML và DL hiện đại ngày nay. Trong bài này, chúng ta sẽ cùng xem xét chi tiết một vài khía cạnh về nó!\n1. Bốn thành phần cơ bản của Parameterized Learning\nHiểu một cách đơn giản, parameterization là quá trình định nghĩa các tham số cần thiết cho một model. Nó bao gồm 4 thành phần: Data, Score Function, Loss Function, Weight-Bias.\n1.1 Data\nData ở đây chính là input data đưa vào để model học, nó bao gồm Data Points (VD: Các raw pixel của bức anh, các trích xuất đặc trưng của đối tượng, \u0026hellip;) và nhãn kết hợp với Data Points.\nData thường được biểu diễn dưới dạng ma trận (gọi là desing matrix). Mỗi hàng của matrix đại diện cho một Data Point, trong khi mỗi cột của nó thể hiện moojtoj feature.\nVí dụ, xem xét một dataset gồm 1000 bức ảnh màu, mỗi ảnh có kích thước 96x96x3 pixels. Design Matrix cho dataset này sẽ là: $X \\subseteq R^{100 \\times (32 \\times 32 \\times 3)}$, với $X_i$ là ảnh thức $i$ trong $R$. Cùng với Design Matrix, ta cũng định nghĩa vector $y$ mà $y_i$ là nhãn cho ảnh thứ $i$ trong dataset.\n1.2 Scoring Function\nHàm này chấp nhận một input data, sau đó ánh xạ nó sang nhãn tương ứng.\nINPUT_IMAGES =\u0026gt; F(INPUT_IMAGES) =\u0026gt; OUTPUT_CLASS_LABELS\n1.3 Loss Function\nLoss Function đánh giá mức độ phù hợp giữa nhãn dự đoán (predicted label) và nhãn thực tế (ground-truth label). Giá trị của hàm này tỉ lệ nghịch với mức độ phù hợp hay độ chính xác của model. Mục tiêu của chúng ta khi training AI model là tối thiểu hóa loss funtion.\n1.4 Weights and Biases\nWeight Matrix ($W$) và vector bias ($b$) là những cái được điều chỉnh, cập nhật trong quá trình training AI model, dựa trên output của Score Function và Loss Function. Mục đích cuối cùng cũng vẫn là tăng độ chính xác phân loại của AI model.\nTiếp theo, chúng ta sẽ sử dụng 4 thành phần này để xây dựng một linear classification.\n2. Ưu điểm của Parameterized Learning\nCó 2 ưu điểm chính để sử dụng Parameterized Learning:\n Một khi đã hoàn thành viêc training model, ta có thể bỏ qua input data, chỉ giữ lại Weight Matrix $W$ và bias vector $b$. Điều này giúp giảm đáng kể kích thước của model. Tốc độ của việc phân loại sử dụng model đã trained nhanh hơn rất nhiều so với thuật toán kNN. Ta chỉ cần nhân 2 matrix $W$ và $x_i$, rồi cộng với $b$.  3. Linear Classification\nPhần này, chúng ta sẽ đi sâu hơn 1 chút về toán, tìm hiểu cách mà parameterized learning áp dụng vào machine learning.\nGiả sử chúng ta có một bộ dataset gồm $N$ ảnh kích thước ($W_{input}, H_{input}, 3$), ký hiệu là $x_i, i = 1, 2, \u0026hellip;, N$. Mỗi $x_i$ có một nhãn tương ứng $y_j, j = 1, 2, \u0026hellip;, K$. Nói một cách khác, chúng ta có $N$ Data Points, mỗi Data Point có $D = (W_{input} \\times H_{input}\\times3)$ chiều, và được chia thành $K$ nhóm phân biệt.\nTa định nghĩa Score Function thông qua một hàm tuyến tính đơn giản như sau:\n$f(x_i, W, b) = Wx_i + b$ Giả sử mỗi $x_i$ được đại diện bởi một vector cột với kích thước $[D\\times1]$ (đối với images, ta duỗi thẳng theo cả 3 chiều thành $D = (W_{input} \\times H_{input}\\times3)$ giá trị nguyên của các pixcels). Weight Matrix $W$ sẽ có kích thước $[K \\times D]$ và bias $b$ sẽ có kích thước $[K \\times 1]$. Vector bias cho phép ta dịch chuyển Score Function trên toàn bộ trên đồ thị, đóng vai trò quan trọng đối với sự thành công của AI model.\nVí dụ cụ thể đối một dataset có 3000 bức ảnh, mỗi ảnh có kích thước $[32 \\times 32 \\times 3]$ và được phân chia vào 1 trong 3 nhóm. Khi đó mỗi $x_i$ được đại diện bởi D = 32323 = 3072 pixels và có kích thước $[3072 \\times 1]$. Weight Matrix $W$ có kích thước $[3 \\times 3072]$ và bias vector $b$ có kích thước $[3 \\times 1]$.\nHình dưới minh họa linear classification Score Function $f$. Bên trái là ảnh đầu vào kích thuớc $[32 \\times 32 \\times 3]$ được \u0026ldquo;duỗi thẳng\u0026rdquo; thành 3072 pixcels (bằng cách reshape 3D array thành 1D list).\n Weight Matrix $W$ chứa 3 hàng (mỗi hàng tương ứng với 1 nhãn) và 3072 cột (mỗi cột tương ứng với 1 pixel của ảnh). Sau khi nhân 2 matrix $W$ và $x_i$, ta cộng thêm bias vector $b$ sẽ thu được output của Score Function, như bên trái của hình trên. Có 3 giá trị tương ứng với 3 nhãn: Cat, Dog và Panda.\n4. Thực hành Linear Classification với Python\nChúng ta đã có hình dung cơ bản về lý thuyết của Parameterized Learning, giờ ta sẽ bắt tay vào thực hành code để có thể hiểu hơn.\nMục đích của phần này không phải hướng dẫn việc training model, chỉ đơn giản là cài đạt Score Function bằng python mà thôi. Khi training model thì $W$ và $b$ sẽ được khởi tạo và cập nhật dần dần trong quá trình training, còn ở đây, chúng chỉ được khởi tạo 1 lần và sử dụng luôn để tính output của Score Function.\nMục tiêu của ta ở đây là nhận diện xem ảnh dưới đây là con gì trong số 3 con vật: Cat, Dog và Panda.\n Chúng ta sẽ code như sau (xem giải thích trong comment code):\n# import the necessary packages import numpy as np import cv2 # initialize the class labels and set the seed of the pseudorandom number generator so we can reproduce our results labels = [\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;panda\u0026#34;] np.random.seed(1) # randomly initialize our Weight Matrix and bias vector -- in a *real* training and classification task, these parameters would be  # *learned* by our model, but for the sake of this example, let\u0026#39;s use random values W = np.random.randn(3, 3072) b = np.random.randn(3) # load our example image, resize it, and then flatten it into our \u0026#34;feature vector\u0026#34; representation orig = cv2.imread(\u0026#34;dog.png\u0026#34;) image = cv2.resize(orig, (32, 32)).flatten() # compute the output scores by taking the dot product between the Weight Matrix and image pixels, followed by adding in the bias scores = W.dot(image) + b # loop over the scores + labels and display them Chạy code trên ta thu được kết quả trên terminal:\n[INFO] dog: 8058.57 [INFO] cat: -2926.35 [INFO] panda: 3531.41 và ảnh hiển thị:  Nhắc lại lần nữa là khi thực hiện bài toán AI trong thực tế từ đầu, chúng ta cần phải cập nhật $W$ và $b$ thông qua các thuật toán tối ưu. Thuật toán tối ưu kinh điển là Gradient Descent sẽ được tìm hiểu chi tiết trong bài viết tiếp theo.\n5. Loss Function\n5.1 Loss Function là gì?\nLoss Function, tên tiếng việt là hàm mất mát, thể hiện sai số giữa giá trị dự đoán của model và giá trị thực tế. Sai số càng nhỏ thì model dự đoán càng chính xác và ngược lại.\nMục tiêu của việc training model là cập nhật $W$ và $b$ để tối thiểu hóa giá trị của Loss Function, qua đó nâng cao độ chính xác của model.\nMột cách lý tưởng, Loss Function nên giảm dần theo thời gian trong quá trình training như hình bên dưới:\n 5.2 Multi-class SVM Loss\nMulti-class SVM Loss là sự mở rộng của Linear SVM, xuất hiện trong bài toán khi cần phân biệt nhiều labels (\u0026gt; 2). Nó sử dụng Score Function $f$ để ánh xạ mỗi Data Point thành các Scores cho mỗi nhãn.\nScore Function $f$ có dạng như sau:\n$f(x_i, W, b) = Wx_i + b$ Để phán định một model là \u0026ldquo;good\u0026rdquo; hay \u0026ldquo;bad\u0026rdquo;, ta cần thêm một Loss Function.\nTa đã biết khi tạo ra một ML model, chúng ta có Design Matrix $X$, ở đó, mỗi hàng của $X$ là một Data Point ($x_i$) mà chúng ta muốn phân loại (tìm nhãn cho Data Point đó). Ground-truth Label cho $x_i$, ký hiệu $y_i$ là vector mà chúng ta hi vọng Score Function sẽ dự đoán đúng.\nViết lại Score Function như sau:\n$s = f(x_i, W)$ Predicted Score của class j-th tại i-th Data Point sẽ là:\n$s_j = f(x_i, W)_j$ Ta định nghĩa Hinge Loss Function như sau:\n$L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} +1)$ Hàm này tính tổng các sai số giữa các Score của nhãn dự đoán so với nhãn thực tế. Ở đây, sử dụng hàm $max()$ để bỏ qua những giá trị âm của độ lệch. $x_i$ được dự đoán chính xác khi $L_i$ = 0.\nÁp dụng cho toàn bộ tập training, ta lấy trung bình của mỗi $L_i$:\n$L = \\frac{1}{N} \\sum_{i=1}^N L_i$ Một dạng khác của Loss Function mà ta có thể gặp trong một số tài liệu là Squared Hinge Loss:\n$L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} + 1)^2$ Dạng này sẽ \u0026ldquo;trừng phạt\u0026rdquo; (penalize) năng hơn Loss. Việc sử dụng dạng nào còn tùy thuộc vào dataset của bạn. Thực tế thì dạng chuẩn (không bình phương) có vẻ phổ biến hơn 1 chút. Nhưng trong nhiều bài toán, sử dụng dạng bình phương lại cho kết quả tốt hơn. Nói chung, đây sẽ là một hyperparameter mà chúng ta cần phải tuning trong quá trình xây dựng model.\nVí dụ:\nĐể hiểu rõ hơn, hãy cùng xem xét ví dụ sau:\n Chúng ta có 3 training examples cho 3 nhãn: Dogs, Cats và Pandas. Giả sử ta đã biết giá trị của $W$ và $b$, từ đó sẽ tính được giá trị của Score Fcuntion, như trong hình.\nTính các $L_i$ cho từng example:\n$L_i(Image 1) = max(0, 1.33 - 4.26 + 1) + max(0, -1.01 - 4.46 + 1) = 0$ $L_i(Image 2) = max(0, 3.76 - (-1.20) + 1) + max(0, -3.81 - (-1.20) + 1) = 5.96$ $L_i(Image 3) = max(0, -2.37 - (-2.27) + 1) + max(0, 1.03 - (-2.27) + 1) = 5.199$\nTa thấy, $L_i(Image1) = 0$, tức là Image #1 được dự đoán đoán chính xác là Dogs. Điều này cũng hợp lý vì Score của Image #1 đối với nhãn Dogs lớn hơn nhiều so với 2 nhãn còn lại.\nĐối với Image #2 và Image #3, đang được dự đoán là Dogs và Cats, tương ứng. Rõ ràng, đây là dự đoán sai. Nhìn vào $L_i(cats) và L_i(pandas)$ đều \u0026gt; 0, ta thấy khá hợp lý với những dự đoán này.\nTrong bài sau, ta sẽ học các phương pháp tối ưu để tìm ra giá trị của $W$ và $b$ để việc dự đoán sẽ đúng cho cả 3 bức ảnh.\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/26_parameterized_learning/","tags":["Machine Learning","Deep Learning"],"title":"Parameterized Learning"},{"categories":["Ebook"],"contents":"Bạn có là người thích đọc sách nhưng ngân quỹ có giới hạn, không đủ tiền để mua sách \u0026ldquo;xịn\u0026rdquo; trên amazon, hay trên các tạp chí khoa học, \u0026hellip;? Bạn đã từng trải qua cảm giác gặp một cuốn sách/bài báo rất hay. Bạn rất muốn tìm bản \u0026ldquo;full không che\u0026rdquo; để đọc nhưng không thể tìm được bản free trên mạng? (Mình đã từng mất cả ngày seach google, đăng ký tài khoản ở cả những trang web nước ngoài nhưng vẫn không tìm được. Có chăng thì chỉ là bản sample, hoặc không có code đi kèm) Lên amazon hoặc trang bán sách trực tiếp của tác giả xem thì giá bán là XXX đô la. Bạn đắn đo, không biết có nên bỏ ra số tiền như vậy để mua hay không? Nhỡ sách không hay như kỳ vọng thì sao? Rồi bạn lan man nghĩ ngợi, sáng nay bạn còn không còn đủ 10k ăn sáng, \u0026hellip; Cuối cùng là bạn \u0026hellip; thôi, ko mua nữa.\n\u0026ldquo;May mắn\u0026rdquo; là mình tìm được 2 cách lách luật để có thể sở hữu những cuốn sách như vậy. Cũng phải nhấn mạnh với mọi người là mình không phải là người chuyên đọc sách lậu, và cố tình cỗ súy cho việc này. Đối với một số sách mà mình thấy nó thực sự hay và thực sự có ích đối với mình thì mặc dù có thể download được free nhưng mình vẫn trả tiền cho tác giả. Vì mình nghĩ đơn giản, trả tiền cho tác giả thì họ mới có động lực viết ra những quyển sách hay như thế nữa cho mình đọc. Tuy nhiên, có 2 lý do mà mình quyết định viết bài này:\n Tri thức là của chung của mọi người, chia sẻ kiến thức là việc mà mọi người đều nên làm, không chỉ giúp người mà còn giúp chính mình. Những quyển sách đó có thể cũng đã được chia sẻ ở đâu đó rồi. Mình không nói thì có thể bạn cũng sẽ biết được từ một nguồn nào đó ở nơi khác.  Trong thời đại CNTT ngày nay thì mọi thông tin đều có thể tìm kiếm được từ Internet, vấn đề chủ yếu ở đây là bạn hành động như thế nào thôi. Bạn có thể phản đối hoàn toàn việc đọc sách \u0026ldquo;lậu\u0026rdquo;, làm như mình (trả tiền cho một số cuốn) hoặc dùng hoàn toàn hàng free mà không trả đồng nào. Mình không đánh giá bạn qua việc đó, mình chỉ hi vọng các bạn đọc sách một cách hiệu quả, thực sự yêu quý và trân trọng quyển sách đó, chứ đừng download về đầy máy tính rồi cả năm không động vào. Nếu mình là tác giả của những cuốn sách mà được mọi người yêu thích thì mình cũng cảm thấy được an ủi phần nào. (phần còn lại là nhận được tiền từ các bạn, :D).\nOk, lan man vậy thôi, mình sẽ đi vào vấn đề chính luôn.\n1. Download các bài báo khoa học\nKỹ năng đọc hiểu và implement theo các public papers là một kỹ năng cần thiết đối các kỹ sư AI. Thực tế, dự án mà mình vừa trải qua tại VTI, mình cần đọc hiểu bài báo sau Inception Single Shot MultiBox Detector for object detection. Thực ra thì cũng có khá nhiều các bài viết về thuật toán này rồi, nhưng mình muốn đọc paper gốc để hiểu rõ hơn ý định của tác giả nên mình đã vào IEEE để tìm. Nhưng mà trên đó, họ chỉ cho đọc mỗi phần Abstract. Nếu muốn đọc full thì phải đăng ký thành viên và phải trả phí.\n Số tiền tuy không lớn nhưng cũng không nhỏ đối với những người mà tiền trong ví lúc nào cũng chỉ đổ xăng, ăn sáng và uống trà đá như mình, :D.\nTrong trường hợp này, bạn chỉ cần truy cập vào trang web https://sci-hub.st/.\n Tại trang chủ của nó, bạn có thể nhìn thấy ngay slogan của họ: ... to remove all barriers in the way of science. Việc bạn cần làm là dán đường link bài báo mà bạn muốn tải về vào ô textbox rồi click vào nút open.\nVà đây là kết quả:\n Đến đây thì bạn có thể tải về máy tính của bạn một các dễ dàng rồi.\nĐây là cách mà mình được một GS giới thiệu cho khi học môn \u0026ldquo;Văn phong khoa học kỹ thuật\u0026rdquo; ở lớp Master. Thầy cũng nhấn mạnh rằng, đây là cách mà trong giới khoa học ngầm hiểu với nhau chứ không công khai trong bất kì tài liệu chính thức nào. Bởi vì có một thực tế rằng các nhà khoa học khi làm nghiên cứu, họ phải đọc hàng chục, thậm chí hằng trăm bài báo khoa học để tìm ý tưởng, lấy dẫn chứng, \u0026hellip; Giả sử mỗi bài báo có giá 1$ tải về thôi thì cũng là một vấn đề không nhỏ rồi.\nLiệu bạn có thắc mắc, tại sao một trang web như này lại có thể công khai tồn tại một cách hiển nhiên như thế này? Đó cũng là câu hỏi mà mình từng nghĩ đến nhưng vẫn chưa tìm được câu trả lời chính xác. Nhưng có lẽ, nên nhìn nhận vấn đề này giống như cách chúng ta sử dụng hệ điều hành Windows hay bộ công cụ Office của Microsoft. Mặc dù Microsoft biết là có rất nhiều người dùng \u0026ldquo;chùa\u0026rdquo; nhưng họ không chặn, cứ \u0026ldquo;giả ngơ\u0026rdquo; để cho mọi người dùng thoải mái. Không phải Microsoft không thể chặn, mà có lẽ là họ không muốn chặn. Phí để mua bản quyền hệ điều hành Windows khá cao so với mức thu nhập của rất nhiều người, nếu Microsoft ngăn chạn triệt để thì có thể người dùng sẽ quay lưng, chuyển sang một hệ điều hành mở khác (Ubuntu chẳng hạn). Khi người dùng đã trở nên quen với Windows rồi, Microsoft mới bắt đầu tính phí bản quyền của các doanh nghiệp, công ty. Nếu công ty, doanh nghiệp nào không mua bản quyền, thì rất dễ bị kiện bởi Microsoft, hoặc bị chính khách hàng của họ chấm dứt mối quan hệ làm ăn với công ty đó. Nếu tổ chức (trường học, viện nghiên cứu, \u0026hellip;) sử dụng các bài báo không có bản quyền để phục vụ các mục đích thuơng mại, hay công bố quốc tế thì rất có thể cũng sẽ bị kiện bởi tác giả hoặc nhà xuất bản.\nNgoài IEEE, các bài báo ở các nhà xuất bản khác như Sciencedirect, Springer, Researchgate, \u0026hellip; cũng có thể download được theo cách này.\nCó một lưu ý nhỏ là, trang https://sci-hub.st/ này có thể bị chặn, không truy cập được đối với một số mạng nội bộ ở công ty, doanh nghiệp. Khi đó bạn chỉ cần đổi qua một mạng Internet free open nào đó (4G chẳng hạn) là có thể truy cập được.\n2. Download sách\nMột trong những cuốn sách mình rất thích, đó là:\n Trên Amazon, quyển này có giá khá cao, $37.49 cho bản sách điện tử.\n Và đây là cách mà mình đã làm để download cuốn sách này với giá $0.\nVào trang https://b-ok.asia/.\n Gõ tên sách cần tìm ở ô textbox rồi click Search. Kết quả:\n Woo, có đẩy đủ các Edition luôn. Mình chọn 2nd Editon cho cập nhật.\nTrang này mình vô tình biết được trong một lần lang thang lên mạng tìm sách free. Có khá nhiều trang cho download sách, nhưng cá nhân mình thâý trang này dễ dàng và đầy đủ nhất (trong phạm vi nhu cầu của mình).\n3. Đọc các bài viết trên https://medium.com/\nTrang web này có lẽ đã khá quen thuộc với nhiều người. Các bài viết trên đó đều đến từ các chuyên gia, và người có kinh nghiệm trong từng lĩnh vực cụ thể, và được cập nhật hàng ngày. Mình cảm thấy các bài viết trên đó rất chất lượng, rất đáng để đọc. Mình đang cố gắng tập thói quen chuyển bớt thời gian lướt Facebook sang đọc Medium. :D\nTuy nhiên, https://medium.com/ chỉ cho đọc free tối đa 5 bài. Muốn đọc nhiều hơn bạn phải đăng ký thành viên, trả phí 5$/tháng. 5$ thực sự không phải là một con số lớn so với lượng kiến thức hay ho như vậy. Thế nên mình quyết định mua, không xài \u0026ldquo;chùa\u0026rdquo; nữa. :D\nDù vậy, nếu bạn vẫn muốn đọc free thì dưới đây là cách cho bạn:\nGiả sử khi bạn vào https://medium.com/, click vào 1 bài để đọc thì nhận được thông báo:\n Hãy làm theo các bước sau:\nClick vào biểu tượng Cookie trên trình duyệt:\n Chọn Cookies:\n Chọn medium.com rồi click vào nút Remove.\nQuay trở lại trình duyệt, reload lại trang. Bạn sẽ thấy bài viết đã được hiển thị đầy đủ.\nNgoài nội dung kiến thức hay và phong phú, Medium còn có tính năng recommend các bài biết hay, phù hợp với lĩnh vực bạn quan tâm, dựa trên cookies của bạn. Vì thế, nếu bạn chọn cách đọc free thì đôi khi bạn sẽ bỏ lỡ các bài viết hay và mới nhất. Hãy cân nhắc khi quyết định hành động của bạn!\nVậy là mình đã giới thiệu đến mọi người 3 cách để tiếp cận với nguồn tri thức vô tận trên Internet. Nhấn mạnh lại lần nữa là mình không ủng hộ hoàn toàn các cách làm này. Nếu bạn có điều kiện, hãy mua sách, mua tài khoản và trả tiền một cách đầy đủ cho tác giả. Nếu bạn như mình, không quá dư giả nhưng sách thì vẫn muốn đọc thì có thể cân nhắc như cách mình đã làm. Ngoài ra, nếu bạn download một cuốn sách về đọc mà trong lòng cảm thấy \u0026ldquo;áy náy\u0026rdquo; thì hãy làm một việc tốt gì đó bù lại. Chẳng hạn, tối đi làm về trên đường, gặp người ăn xin, bạn có thể cho họ chút tiền lẻ của bạn. Như vậy thì ít nhiều bạn cũng đỡ \u0026ldquo;áy náy\u0026rdquo; hơn phần nào! :)\nCuối cùng, chúc mọi người tìm đuợc những cuốn sách hay, bài báo thú vị và bổ ích cho mình!\n","permalink":"https://tiensu.github.io/blog/25_for_book_lover/","tags":["Ebook"],"title":"Dành cho người yêu sách"},{"categories":["Deep Learning","CNN"],"contents":"Tiếp tục chuỗi các bài viết về CNN, trong bài này mình sẽ chia sẻ với các bạn một số \u0026ldquo;common patterns 7 rules\u0026rdquo; trong việc xây dựng kiến trúc CNN. Nắm rõ những \u0026ldquo;patterns \u0026amp; rules\u0026rdquo; này sẽ giúp các bạn giảm thiếu thời gian và công sức khá nhiều trong các dự án của các bạn!\n3. Common Architectures \u0026amp; Training Patterns\nQua 2 bài viết trước, chúng ta đã biết, CNN được tạo thành từ 4 loại layers chủ yếu, bao gồm: CONV, POOL, RELU, và FC. Sắp xếp các layers này với nhau theo một thứ tự nhất định ta sẽ một CNN (gọi tên đầy đủ là kiến trúc CNN).\n3.1 Layers Patterns\nNói chung, hầu hết các kiến trúc CNN đều có mộ vài lớp CONV và RELU liên tiếp nhau, theo sau bởi lớp POOL. Lặp lại như thế đến khi kích thước của input volumn đủ nhỏ, rồi thêm vào một hoặc nhiều FC layers. Pattern tổng quát như sau:\nINPUT =\u0026gt; [[CONV =\u0026gt; RELU]xM =\u0026gt; POOL?]xN =\u0026gt; [FC =\u0026gt; RELU]xK =\u0026gt; FC\nKý hiệu x ở đây tức là lặp lại 1 hoặc nhiều lần, còn ? nghĩa là tùy chọn, có thể có hoặc không.\nM, N, K thường chọn theo các rules sau:\n 0 \u0026lt;= N \u0026lt;= 3 M \u0026gt;= 0 0 \u0026lt;= K \u0026lt;= 2  Ví dụ một số kiến trúc CNN áp dụng pattern tổng quát bên trên như sau:\n INPUT =\u0026gt; FC INPUT =\u0026gt; [CONV =\u0026gt; RELU =\u0026gt; POOL]x2 =\u0026gt; FC =\u0026gt; RELU =\u0026gt; FC INPUT =\u0026gt; [[CONV =\u0026gt; RELU]x2 =\u0026gt; POOL]x3 =\u0026gt; [FC =\u0026gt; RELU]x2 =\u0026gt; FC  Các kiến trúc CNN kinh điển cũng dựa trên pattern tổng quát này:\n AlexNet: INPUT =\u0026gt; [CONV =\u0026gt; RELU =\u0026gt; POOL]x2 =\u0026gt; [CONV =\u0026gt; RELU]x3 =\u0026gt; POOL =\u0026gt; [FC =\u0026gt; RELU =\u0026gt; DO]x2 =\u0026gt; SOFTMAX VGGNet: INPUT =\u0026gt; [CONV =\u0026gt; RELU]x2 =\u0026gt; POOL =\u0026gt; [CONV =\u0026gt; RELU]x2 =\u0026gt; POOL =\u0026gt; [CONV =\u0026gt; RELU]x3 =\u0026gt; POOL =\u0026gt; [CONV =\u0026gt; RELU]x3 =\u0026gt; POOL =\u0026gt; [FC =\u0026gt; RELU =\u0026gt; DO]x2 =\u0026gt; SOFTMAX  Một cách khái quát, chúng ta sẽ áp dụng các kiến trúc CNN sâu khi gặp bài toán phức tạp, nhiều labels, các đối tượng thay đổi không có quy luật. Sử dụng nhiều CONV layers trước khi áp dụng POOL layer cho phép các CONV layers học được các complex features trước khi áp dụng POOL layer để giảm kích thước của input volumn.\nNhư đã đề cập ở bài trước, một số kiến trúc CNN đã loại bỏ hoàn toàn các POOL layers phía sau CONV layers, chỉ sử dụng CONV layers để giảm kích thước của input volumn. Hơn nữa, các FC layers ở cuối cũng không còn được sử dụng, thay vào đó là average pooling. GoogLeNet, ResNet, SqueezeNet là những kiến trúc sử dụng cách này. Kết quả là giảm số lượng tham số của CNN và thời gian train cũng ngắn hơn.\nĐặc biệt hơn, GoogLeNet còn áp dụng đồng thời 3 loại filters có kích thước khác nhau (1x1, 3x3, 5x5) tại cùng 1 vị trí trong kiến trúc để học multi-level features. Những kiến trúc kiểu như này được coi là công nghệ tiên tiến trong lĩnh vực DL.\n3.2 Quy tắc ngón tay cái\nTrong phần này, chúng ta sẽ cùng xem xét một số rules khi xây dựng CNN model.\n Rule 1  Đầu tiên, images đưa vào CNN nên có chiều rộng và chiều cao bằng nhau (square) ($W_{input} = H_{input}$). Sử dụng squere images cho phép chúng ta tận dụng các lợi ích của các thư viện tối ưu trong đại số tuyến tính. Kích thước thường hay sử dụng là: 32x32, 64x64, 96x96, 224x224, 227x227, 229x229.\n Rule 2  Thứ 2, sau khi đi qua CONV layer đầu tiên, kích thước của images nên có thể chia hết cho 2. Điều này, cho phép POOL layer tiếp sau đó hoạt động theo cách hiệu quả hơn. Để áp dụng rule này, có thể điêu chỉnh kích thước của filters và stride. Nói chung, CONV layers nên có kích thước nhỏ (3x3 hoặc 5x5). Tiny filter (1x1) có thể được sử dụng để học các local features, nhưng chỉ nên áp dụng trong các kiến trúc hiện đại và phức tạp. Kích thước lớn hơn của filters (7x7 hoặc 11x11) cũng có thể xuất hiện ở CONV layer đầu tiên trong kiế trúc để giảm nhanh kích thước không gian của input volumn có kích thước \u0026gt; 200x200 pixels. Nhấn mạnh là chỉ áp dung filers có kích thước lớn ở CONV layer đầu tiên, ngược lại, input volumn sẽ giảm rất nhanh làm mất mát các features quan trọng.\n  Rule 3 Stride của CONV, S = 1 cũng nên được sử dụng cho các CONV layers đối với các input volumns có kích thước trung bình nhỏ (\u0026lt; 200x200 pixels). Sử dụng S = 2 cho các input volumns có kích thước lớn hơn, nhưng cũng chỉ nên áp dụng ở CONV layer đầu tiên. Khi S = 1 thì CONV layers làm nhiệm vụ học các features của images, trong khi POOL layers chịu trách nhiệm giảm kích thước input volumns. Tuy nhiên, nhắc lại lần nữa rằng trong các kiến trúc CNN tiên tiến, POOL layers đang dần dần được thay thể bởi CONV layers với S \u0026gt;= 2.\n  Rule 4 Cá nhân mình thường áp dụng zero-padding trong CONV layer để đảm bảo kich thước của input volumns không đổi khi đi qua CONV layer và sử dụng POOL layer để giảm kích thước input volumn. Thực nghiệm của mình cho thấy classification accuracy thường cao hơn khi sử dụng rule này. Khi làm viêc với Keras framework, bạn có thể làm điều này một cách dễ dàng bằng cách setting padding=same khi tạo CONV layer. Bạn chỉ nên sử thay POOL layer bằng CONV layer khi đã thành thạo ở mức chuyên gia trong việc thiết kế kiến trúc của CNN.\n  Rule 5\n  Đối với POOL layer, kích thước thông thường của nó trong kiến trúc CNN là 2x2, cộng với stride S = 2. Kích thước 3x3 cũng có thể sử dụng ở các layers đầu trong CNN để giảm nhanh kích thước của input volumn. Kích thước \u0026gt; 3x3 chưa từng thấy xuất hiện trong bất cứ mạng CNN nào từ trước đến giờ.\n Rule 6  Về phần Batch Normalization, như trong bài trước đã đề cập, mặc dù nó làm tăng lên đáng kể thời gian training, nhưng chúng ta vẫn nên sử dụng nó trong hầu hết các trường hợp vì những lợi ích mà nó mang lại. BN layer được đặt sau ACT layer như trong các ví dụ sau:\n  INPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; FC\n  INPUT =\u0026gt; [CONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; POOL]x2 =\u0026gt; FC =\u0026gt; RELU =\u0026gt; BN =\u0026gt; FC\n  INPUT =\u0026gt; [[CONV =\u0026gt; RELU =\u0026gt; BN]x2 =\u0026gt; POOL]x3 =\u0026gt; [FC =\u0026gt; RELU =\u0026gt; BN]x2 =\u0026gt; FC\n  Rule 7\n  Droput (DO) được đặt giữa các FC layers với xác suất ngắt kết nối các nodes là 50%. Nó cũng được khuyên sử dụng DO trong mọi kiến trúc CNN của bạn. Cá nhân mình, đôi khi cũng đặt DO ở giữa CONV và POOL layers, và điều này đôi khi cũng tỏ ra hiệu quả trong việc giảm bớt Overfitting. Bạn có thể thử-sai trong các bài toán của bạn.\nOk, đó là 7 rules mình muốn giới thiệu đến các bạn. Bằng viêc ghi nhớ những rules này, bạn sẽ bớt đau đầu hơn khi xây dựng kiến trúc CNN của riêng mình. Một khi bạn đã trở thành chuyên gian xây dựng mạng CNN theo cách truyền thống như thế này, hãy thử bỏ qua max pooling, chỉ sử dụng CONV layer để giảm kích thước không gian của input volumns và sử dụng average pooling thay thế cho FC layer để giảm độ phức tạp tính toán của CNN. Mình sẽ đề cập chi tiết hơn những kỹ thuật advances này trong các bài viết về sau.\n4. Tổng kết\nVậy là mình đã kết thúc 3 bài viết về CNN. Hi vọng với những kiến thức chia sẻ ở đây sẽ giúp ích được cho các bạn, đặc biệt là các patterns và rules ở bài số 3 này. Các bạn có thể áp dụng luôn vào trong bài toán của mình và kiểm tra sự khác biệt.\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/24_convolutional_neural_network_3/","tags":["Deep Learning","CNN"],"title":"Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 3"},{"categories":["Deep Learning","CNN"],"contents":"Vì sử dụng trực tiếp raw pixel của image nên so với CNN, FCN (Fully Connected Network) có 2 nhược điểm kích thước của image tăng lên:\n Hiệu năng giảm mạnh. Kích thước của mạng tăng nhanh. Kết quả thực nghiệm cho thấy, khi áp dụng Fully Connected Network vào bộ dataset CIFAR-10, độ chính xác chỉ đạt được 52%.  CNN, theo một cách khác, sắp xếp các layers theo dạng 3D volume với 3 chiều: Width, Height, Depth. Các neurons trong mỗi layer chỉ kết nối tới 1 small region của layer trước đó - gọi là local connectivity. Điều này giúp giảm bớt rất nhiều kích thước của mạng.\n2. Các loại layers\nCó khá nhiều các dạng layers khác nhau để xây dựng nên CNNs. Các loại dưới đây được sử dụng phổ biến:\n Convolutional (CONV) Activation (ACT, RELU, SOFTMAX) Pooling (POOL) Fully-connected (FC) Batch normalization (BN) Dropout (DO)  Sắp xếp các dạng layers trên liên tiếp nhau theo thứ tự nào đó sẽ cho ta một CNN.\nVí dụ: INPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; FC =\u0026gt; SOFTMAX\nỞ đây, ta định nghĩa một CNN đơn giản, nhận input, áp dụng \u0026ldquo;convolution layer\u0026rdquo;, sau đó là 1 \u0026ldquo;activation layer\u0026rdquo; (RELU), tiếp theo là 1 \u0026ldquo;fully-connected layer\u0026rdquo;. Cuối cùng là một \u0026ldquo;activation layer\u0026rdquo; nữa (SOFTMAX) để đạt được xác suất của output theo các nhãn cần phân loại.\nTrong số các lớp trên, chỉ có CONV và FC là chứa các tham số được cập nhật trong quá trình training. POOL có tác dụng thay đổi kích thước không gian của image khi nó di chuyển qua các lớp của CNN.\nCONV, POOL, RELU và FC là 4 layers quan trọng nhất, gần như không thể thiếu khi xây dựng CNNs.\n2.1 CONV\nCONV chứa một tập K learnable filters (ví dụ: Kernel), mỗi filter có kích thước width x height. Mặc định,width luôn luôn bằng height, trừ khi có lý do đặc biệt. Hai giá trị này có thường nhỏ (1, 3, 5, 7) nhưng K thì có thể rất lớn (4, 8, 16, 32, 64, 128, \u0026hellip;). K cũng được gọi là độ sâu (depth) của CONV layer.\nCùng xem xét forward-pass của một CNN. CONV có K filters, áp dụng vào một input volumn có kích thước WxH. Tưởng tượng rẵng, mỗi filters sẽ trượt ngang qua toàn bộ input volumn, tính toán convolution, sau đó lưu kết quả ra một mảng 2 chiều, gọi là activation map. Xem hình bên dưới:\n Sau khi áp dùng K filters lên input volumn, chúng ta thu được K, 2-dimensional activation maps. Xếp chồng (Stack) những activation maps này theo chiều sâu sẽ thu được kết qủa cuối cùng (output volumn).\n Xét về kích thước của output volumn, có 3 tham số tác động. Chúng là depth, stride và zero-padding size.\n Depth  Như đã nói ở trên, depth chính là số lượng filters trong CONV layer, có giá trị là K.\n Stride  Stride là kích thước của step khi các filters trượt qua input volumn. Giá trị của stride thường là 1 hoặc 2 (S=1 hoặc S=2), tương ứng với step là 1 hoặc 2 pixel. S nhỏ sẽ sinh ra ouput volumn lớn, và có nhiều vùng được bị trùng lặp trong quá trình trượt và tính covolution của các filters. Đối với S lớn, kết quả sẽ ngược lại.\nXem ví dụ sau:\nTrong hình bên dưới, ma trận bên trái là input volumn, ma trận bên phải là filter.\n Sử dụng S = 1 và S = 2 cho convolution, thu được kết quả tương ứng bên trái, phải trong hình sau:\n Cùng với pooling (xem phần bên dưới), stride có thể được sử dụng để giảm kích thước của input volumn.\n Zero-padding  Sử dụng stride làm giảm kích thước của input volumn. Vậy nếu muốn giữ nguyên kích thước của input volumn thì sao? Zero-padding chính là câu trả lời.\nZero-padding tức là gắn thêm (pad) viền (border) cho input volumn. Các giá trị được gắn thêm đều là 0, vì thế mà có tên zero-padding.\nXem ví dụ sau:\n Bên trái là 3x3 ouput khi áp dụng 3x3 convolution tới 5x5 input.\nBên phải là khi áp dụng zero-padding vào input, thu được input mới có kích thước 7x7. Giá trị của zero-padding trong trường hợp này là P = 1.\nBên dưới là 5x5 ouput khi áp dụng 3x3 convolution tới 7x7 input. Ta thấy kích thước 5x5 của input ban đầu được duy trì trong output.\nNếu không sử dụng zero-padding, kích thước của input volumn sẽ giảm rất nhanh, do đó không thể xây dựng CNN nhiều layers.\nCông thức tính kích thước của ouput volumn như sau: O = ((W - F + 2P)/S) + 1\nTrong đó:\n 0: kích thước của output volumn. W: kích thước input volumn F: kích thước của filter P: kích thước zero-padding S: kích thước stride  Nếu O không phải là số nguyên, cần thay đổi lai giá trị của S.\nThử áp dụng công thức này vào ví dụ bên trên:\nO = ((5 - 3 + 2*1)/1) = 5\n2.2 Activation Layers\nActivation layer thường được áp dụng sau mỗi CONV layer trong một mạng CNN. Các activation layer hay dùng là các hàm phi tuyến, giống như: ReLU, ELU, Leaky ReLU. Trong các public paper, ReLU được sử dụng rất phổ biến, gần như là mặc định. Khi viết ACT, ta ngầm hiểu đó là ReLU.\nTrên thực tế, activation layer không được coi là một layer theo đúng nghĩa. Bởi vì nó không có parameters nào được học trong quá trình huấn luyện mô hình. Trong một số diagram kiến trúc của mạng, nó có thể không xuất hiện và được ngầm hiểu rằng nó nằm ngay sau CONV layer.\nVí dụ với kiến trúc sau:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; FC\nCó thể được viết gọn thành:\nINPUT =\u0026gt; CONV =\u0026gt; FC\nMột activation layer nhận input volumn có kích thước là $W_{input}$x$H_{input}$x$D_{input}$, áp dụng activation function theo kiểu element-wise nên kích thước của output volumn đúng bằng kích thước của input volumn: $W_{input}$=$W_{output}$, $H_{input}$=$H_{output}$, $D_{input}$=$D_{output}$.\n2.3 Pooling Layers\nCó 2 phương pháp để giảm kích thước của input volumn: CONV layer (với stride \u0026gt; 1) và POOL layer.\nThông thường, POOL layer được đặt ngay sau ACT layer và trước CONV layer. Trong trường hợp không có ACT layer thì nó nằm giữa 2 CONV layer.\nVí dụ kiến trúc mạng sau:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; POOL =\u0026gt; CONV =\u0026gt; POOL =\u0026gt; FC\nChức năng đầu tiên của POOL layer là giảm kích thước không gian của input volumn một các từ từ. Điều này dẫn đến việc giảm số lượng tham số và sự phức tạp tính toán của mạng. Vì thế mà nó là một trong những phương pháp hiệu của để tránh hiện tượng Overfitting cho mạng DL.\nPOOL layer hoạt động độc lập trên mỗi depth slice của input volumn, sử dụng hàm max hoặc average. Hai cái tên có lẽ cũng đã nói lên cách thức hoạt động của chúng. Max pooling chỉ giữ lại giá trị lớn nhất trong phạm vi kích thước của nó còn average pooling thì lấy giá trị trung bình của các giá trị trong phạm vi kích thước của POOL layer. Về vị trí trong kiến trúc, trong khi max pooling thường được đặt ở giữa của kiến trúc mạng DL để giảm kích thước, còn average pooling lại hay được đặt ở các layers cuối (hoặc gần cuối) (VD: GoogLeNet, SqueezeNet, ResNet) để thay thế cho các FC layers, giúp giảm độ phức tạp cả model.\nKích thước của POOL layer hay được sử dụng là 2x2. Mặc dù vậy, với input volumn có kích thước \u0026gt; 200x200, ta có thể sử dụng kích thước 3x3 của POOL ở những layer đầu.\nBước nhảy (stride, S) của mỗi POOL layer thường là 1 hoặc 2. Hình bên dưới minh họa kết quả hoạt dộng của max pooling với S =1,2, tương ứng.\n Công thức tính kích thước của output volumn sau khi qua POOL layer như sau:\n $W_{output}$ = (($W_{input}$ - $F$)/$S$) + 1 $H_{output}$ = (($H_{input}$ - $F$)/$S$) + 1 $D_{output}$ = $D_{input}$  Trong đó:\n $W_{input}$x$H_{input}$x$D_{input}$: kích thước của input volumn. $W_{output}$x$H_{output}$x$D_{output}$: kích thước của output volumn. F: kích thước của POOL layer (cũng gọi là pool size). S: stride  Trong các bài toán thực tế, có 3 dạng max pooling thường hay được sử dụng.\n Dạng 1: (F = 3, S = 2), gọi là overlapping pooling, thường áp dụng đối với các images/input volumn có kích thước lớn (\u0026gt; 200x200 pixels) Dạng 2: (F = 2, S = 2), gọi là non-overlapping pooling, thường được áp dụng với các images/input volumn có kích thước trung bình (\u0026gt; 64x64 pixels và \u0026lt; 200x200 pixels) Dạng 3: (F = 2, S = 1), gọi là small pooling, áp dụng với các images/input volumn nhỏ (\u0026lt; 64x64 pixels)  Đến đây, có thể các bạn sẽ thắc mắc, khi nào thì dùng CONV layer, khi nào thì dùng POOL layer để giảm kích thước của input volumn?\nSpringenberg et al, trong paper Striving for Simplicity: The All Convolutional Net xuất bản năm 2014 của họ đã đề xuất loại bỏ hoàn toàn POOL layer, chỉ sử dụng CONV layer (với S\u0026gt;1). Họ đã chứng minh tính hiệu của cách tiếp cận này trên một số tập dữ liệu, bao gồm cả CIFAR-10 (small images, low number of class) và ImageNet (large input images, 1000 classes). Xu hướng này cũng xuất hiện trong kiến trúc của mạng Resnet năm 2015 và đang dần dần trở nên phổ biến hơn. Có lẽ trong tương lai không xa, chúng ta sẽ không sử dụng POOL layer (cụ thể là max pooling) trong phần giữa các kiến trúc mạng DL hiện đại nữa mà chỉ sử dụng average pooling tại các layer cuối để thay thế cho FC layer vốn cồng kềnh và phức tạp. Tuy nhiên, trước mắt thì max pooling vẫn chưa thể biến mất hoàn toàn được, nên chúng ta vẫn cần phải học, hiểu và áp dụng chúng trong việc xây dựng kiến trúc mạng DL của riêng mình, cũng như đọc hiểu các kiến trúc mạng DL kinh điển khác.\n2.4 Full-Connected (FC) Layers\nFC Layer chính là Feedforward Neural Network mà chúng ta đã tìm hiểu trong bài \u0026hellip;. Nó luôn được đặt ở cuối trong các kiến trúc mạng DL.\nVí dụ kiến trúc mạng sau,\nINPUT =\u0026gt; (CONV =\u0026gt; RELU =\u0026gt; POOL)x2 =\u0026gt; FC =\u0026gt; FC =\u0026gt; SOFTMAX.\nta đã sử dụng 2 FC layers ở gần cuối mạng, theo sau là ACT layer (SOFTMAX) để phân loại (tính toán xác suất của mỗi classes).\n2.5 Batch Normalization (BN)\nĐược giới thiệu lần đầu vào năm 2015 trong paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift của Ioffe và Szegedy, BN đã nhanh chóng trở nên phổ biến trong các kiến trúc mạng DL. Đúng như tên của nó, BN layer có tác dụng normalize input volumn trước khi đi vào layer tiếp theo.\nGiả sử, ta có input tại thời đểm i (mini-batch i) là $x_i$, sau khi đi qua BN layer, ta thu được giá trị $\\widehat{x_i}$ theo công thức:\n$\\widehat{x_i} = \\frac{x_i - \\mu_ \\beta}{\\sqrt{\\sigma^2_ \\beta + \\varepsilon}}$   Trong đó:   $\\mu_ \\beta = \\frac{1}{M}\\sum_{i=1}^m x_i$    $\\sigma^2_ = \\frac{1}{M}\\sum_{i=1}^m (x_i - \\mu_\\beta)^2$      Giá trị của $\\varepsilon$ được chọn là giá trị dương đủ nhỏ (VD: 1e-7) để tránh việc chia cho 0. Sau khi áp dụng BN, input volumn sẽ có trung bình (mean) xấp xỉ 0 và độ lệch chuẩn (variance) xấp xỉ 1 (còn gọi là zero-centered).\nKhi sử dụng model để test, ta thay thế $\\mu_\\beta$ và $\\sigma_\\beta$ bằng giá trị trung bình của chúng trong suốt quá trình training. Điều này đảm bảo cho ta có thể pass input volumn xuyên qua mạng DL mà không bị biased bởi giá trị $\\mu_\\beta$ và $\\sigma_\\beta$ tại thời điểm cuối cùng ($x_m$).\nBN layer tỏ ra hiệu quả cao trong việc làm cho quá trình training một NN ổn định hơn, giảm số lượng epochs cần thiêt để train model, và quan trọng nhất là hạn chế tình trạng Overfitting. Khi sử dụng BN, việc tuning các tham số khác của model cũng trở nên đơn giản hơn bởi vì BN đã thu hẹp đáng kế phạm vi giá trị của các weights trong mạng.\nHạn chế lớn nhất của BN có lẽ là nó làm tăng thời gian training của bạn do phải tính toán normalization và statistic tại mỗi nơi mà nó xuất hiện trong kiến trúc mạng. Thường là tăng gấp 2 đến 3 so với không sử dụng BN.\nTuy nhiên, có lẽ hạn chế trên không đáng kể so với những ưu điểm mà BN mang lại. Vì vậy, lời khuyên ở đây là nên sử dụng BN thường xuyên trong bài toán của bạn.\nCuối cùng là về vị trí đạt BN layer trong kiến trúc DL. Mặc dù trong paper gốc của tác giả BN layer được đặt trước ACT layer, nhưng điều này lại không hợp lý vê mặt thống kê. Bởi vì output của BN là zero-centered, khi đi qua ACT layer (ReLU), phần giá trị âm sẽ bị triệt tiêu. Điều này vô tình làm mất đi bản chất của BN. Thực nghiệm rất nhiều cũng đã chỉ ra rằng, đặt BN layer ở sau ACT layer cho kết quả tốt hơn (higher accuracy và lower loss) trong hầu hết mọi trường hợp. Vì thế, mặc định, hãy đặt BN layer sau ACT layer, trừ khi bạn có lý do đặc biệt nào khác.\nVí dụ về việc đặt BN layer trong kiến trúc DL:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; BN \u0026hellip;\n2.6 Dropout (DO) layer\nDO thực chất là một dạng của regularization, mục đích là để hạn chế hiện tượng Overfitting. Tại mỗi mini-batch trong quá trình train, DO layer sẽ ngẫu nhiên ngắt kết nối các inputs giữa 2 layer liên tiếp, với xác suất p.\nVí dụ về DO với p = 5 giữa 2 FC layers như hình bên dưới:\n DO chỉ hoạt động theo 1 chiều forwarding, chiều ngược lại (backwarding), các dropped connections sẽ được kết nối lại để tính toán.\nDO giúp giảm Overfitting theo cách như trên bởi vì khi đó, vai trò của các nodes trong mạng sẽ được phân phối đều hơn, không có nodes nào chịu trách nhiệm chính, nhiều hơn các nodes khác. Điều này sẽ giúp model generalize tốt hơn.\nVề vị trí trong kiến trúc mạng DL, DO layer thường được set với p = 0.5 và đặt xen kẽ 2 FC layers ở cuối.\nVí dụ:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; POOL =\u0026gt; FC =\u0026gt; DO =\u0026gt; FC =\u0026gt; DO =\u0026gt; SOFTMAX\nBài thứ 2 về CNN xin được kết thúc tại đây. Trong bài tiếp theo (cũng là bài cuối cùng về CNN), mình sẽ chia sẻ một số patterns và một số rules trong việc xây dựng kiến trúc CNN. Mời các bạn đón đọc!\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/23_convolutional_neural_network_2/","tags":["Deep Learning","CNN"],"title":"Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 2"},{"categories":["Deep Learning","CNN"],"contents":"Sau khi đã tìm hiểu cơ bản về Neural Network, chúng ta sẽ đi tìm hiểu về CNN. CNN là một dạng kiến trúc Neural Network đóng vai trò vô cùng quan trọng trong Deep Learning.\nTrong Feedfoward Neural Network, mỗi neural trong một layer được kết nối đến tất cả các nodes của layer tiếp theo. Ta gọi điều này là Fully Connected (FC) layer. Tuy nhiên, trong CNNs, FC layers chỉ được sử dụng ở 1 vài layers cuối. Các layers còn lại được gọi là convolutional layers.\nMột hàm kích hoạt (activation function) (thường là ReLU) được áp dụng tới output của các convolutional layers. Kết hợp với các dạng layers khác nhau để giảm kích thước của input. Các FC layers ở cuối có nhiệm vụ phân loại output thành các classes khác nhau.\nMỗi layer trong CNN áp dụng một tập các bộ lọc (filters) (có thể lên đến hàng trăm hoặc hàng nghìn), kết hợp kết quả lại, cho qua layer tiếp theo. Trong suốt quá trình training, giá trị của các filters được cập nhật (tương tự như trọng số weight trong Neural Network).\nTrong lĩnh vực xử lý ảnh, CNN có thể học để:\n Phát hiện biên (edges) từ raw pixel data ở layer đầu tiên. Sử dụng edges đã phát hiện để phát hiện hình dạng (shapes) đối tượng ở layer thứ 2. Sử dụng shapes để phát hiện heigher-level features trong các layers tiếp theo.  1. Hiểu rõ về Convolutions\nChúng ta sẽ cùng nhau trả lời một số câu hỏi sau:\n Convolutions là gì? Chúng lamf được những việc gì? Tại sao lại sử dụng chúng? Áp dụng chúng vào xử lý ảnh như thế nào?  Từ convolution, dịch sang tiếng việt là tích chập, nghe có vẻ phức tạp. Bạn chắc chắn đã nghe đến từ này nếu bạn học qua môn Xử lý tín hiệu sô. Tuy nhiên, convolution trong lĩnh vực xử lý lý ảnh hơi khác một chút. Không phải khi có Deep Learning, chúng ta mới sử dụng convolution, các phương pháp xử lý ảnh truyền thống đều sử dụng convolution: Edges detection, Sharpen images, Blurring and Smoothing images, \u0026hellip; Vì thế mới nói, convolution là xương sống của xử lý ảnh. Hiểu rõ convolution là điều kiện tiên quyết để bước chân vào lĩnh vựa xử lý ảnh (theo cả phương pháp truyền thống và sử dụng Deep Learning).\nNghe thì có vẻ \u0026ldquo;đao to búa lớn\u0026rdquo; vậy, nhưng thực sự không phải vậy. Convolution đơn giản chỉ là tổng của các tích đôi một của từng phần tử tron 2 ma trận. Chia nhỏ các bước ra cho dễ hiểu:\n Lấy 2 ma trận có cùng kích thước Nhân 2 ma trận đôi một (element-by-element) (không phải phép nhân ma trận trong đại số tuyến tính). Cộng kết quả của các tích lại.  Yup, đó là convolution.\n1.1 Kernel\nMột image là một ma trận nhiều chiều. Thường là 3 chiều (w, h, c) với width là số cột, height là số hàng và depth là số kênh màu. \u0026ldquo;Image matrix\u0026rdquo; thường được gọi với cái tên \u0026ldquo;big matrix\u0026rdquo;. Một ma trận khác gọi là kernel (hoặc \u0026ldquo;convolution matrix\u0026rdquo;, \u0026ldquo;tiny matrix\u0026rdquo;, filter) đặt bên trên \u0026ldquo;big matrix\u0026rdquo;, trượt từ trái sang phải, từ trên xuống dưới. Trong quá trình di chuyển, các phép toán (convolution, \u0026hellip;) được áp dụng đối với 2 ma trận đó. Sử dụng các kernel khác nhau, ta có thể đạt được các mục đích mong muốn: Blurring (average smoothing, Gaussian smoothing, \u0026hellip;), Edge detection (Laplacian, Sobel, ...), \u0026hellip;\nĐể hiểu rõ hơn, chúng ta sẽ làm thử 1 ví dụ cụ thể.\nGiả sử có \u0026ldquo;image matrix\u0026rdquo; kernel như sau:\n   Theo lý thuyết bên trên, kernel sẽ được trượt qua \u0026ldquo;image matrix\u0026rdquo; từ trái qua phải, từ trên xuống dưới. Số bước trượt thường là 1 hoặc 2. Tại bước, sau khi trượt xong, ta sẽ dừng lại, thực hiện phép convolution giữa kernel và phần \u0026ldquo;image matrix\u0026rdquo; bị che bởi kernel. Giá trị ouput được lưu trong ma trận kết quả tại vị trí trung tâm của kernel tại bước đó.\nChi tiết các bước:\n Chọn tọa độ ($x,y$) từ \u0026ldquo;image matrix\u0026rdquo;. Đặt center của kernel tại ($x,y$). Thực hiện convolution giữa kernel và phần \u0026ldquo;image matrix\u0026rdquo; bị che phủ bởi kernel. Lưu kết quả tại ($x,y$) của ma trận kết quả.  Ví dụ, với $(x,y) = (3,3)$:   Sau khi tính toán xong, ta sẽ gán giá trị 132 cho pixel tại vị trí (3,3) của ma trận kết quả. $O_{i,j}$ = 132.\n1.2 Implement Convolutions bằng python.\nGiờ hãy bắt tay vào code thôi. Việc thực hiện convolution bằng code sẽ giúp bạn hiểu sâu sắc hơn cách áp dụng convolution trong xử lý ảnh.\nTạo file convolutions.py và code như sau:\n# USAGE # python convolutions.py --image mai-ngoc.jpg # import the necessary packages from skimage.exposure import rescale_intensity import numpy as np import argparse import cv2 def convolve(image, K): # grab the spatial dimensions of the image and kernel (iH, iW) = image.shape[:2] (kH, kW) = K.shape[:2] # allocate memory for the output image, taking care to \u0026#34;pad\u0026#34; the orders of the input image so the spatial size (i.e., width and height) are not reduced pad = (kW - 1) // 2 image = cv2.copyMakeBorder(image, pad, pad, pad, pad, cv2.BORDER_REPLICATE) output = np.zeros((iH, iW), dtype=\u0026#34;float\u0026#34;) # loop over the input image, \u0026#34;sliding\u0026#34; the kernel across each (x, y)-coordinate from left-to-right and top-to-bottom for y in np.arange(pad, iH + pad): for x in np.arange(pad, iW + pad): # extract the ROI of the image by extracting the *center* region of the current (x, y)-coordinates dimensions roi = image[y - pad:y + pad + 1, x - pad:x + pad + 1] # perform the actual convolution by taking the element-wise multiplication between the ROI and the kernel, the summing the matrix k = (roi * K).sum() # store the convolved value in the output (x, y)- coordinate of the output image output[y - pad, x - pad] = k # rescale the output image to be in the range [0, 255] output = rescale_intensity(output, in_range=(0, 255)) output = (output * 255).astype(\u0026#34;uint8\u0026#34;) # return the output image return output # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-i\u0026#34;, \u0026#34;--image\u0026#34;, required=True, help=\u0026#34;path to the input image\u0026#34;) args = vars(ap.parse_args()) # construct average blurring kernels used to smooth an image smallBlur = np.ones((7, 7), dtype=\u0026#34;float\u0026#34;) * (1.0 / (7 * 7)) largeBlur = np.ones((21, 21), dtype=\u0026#34;float\u0026#34;) * (1.0 / (21 * 21)) # construct a sharpening filter sharpen = np.array(( [0, -1, 0], [-1, 5, -1], [0, -1, 0]), dtype=\u0026#34;int\u0026#34;) # construct the Laplacian kernel used to detect edge-like regions of an image laplacian = np.array(( [0, 1, 0], [1, -4, 1], [0, 1, 0]), dtype=\u0026#34;int\u0026#34;) # construct the Sobel x-axis kernel sobelX = np.array(( [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]), dtype=\u0026#34;int\u0026#34;) # construct the Sobel y-axis kernel sobelY = np.array(( [-1, -2, -1], [0, 0, 0], [1, 2, 1]), dtype=\u0026#34;int\u0026#34;) # construct an emboss kernel emboss = np.array(( [-2, -1, 0], [-1, 1, 1], [0, 1, 2]), dtype=\u0026#34;int\u0026#34;) # construct the kernel bank, a list of kernels we\u0026#39;re going to apply using both our custom `convole` function and OpenCV\u0026#39;s `filter2D` function kernelBank = ( (\u0026#34;small_blur\u0026#34;, smallBlur), (\u0026#34;large_blur\u0026#34;, largeBlur), (\u0026#34;sharpen\u0026#34;, sharpen), (\u0026#34;laplacian\u0026#34;, laplacian), (\u0026#34;sobel_x\u0026#34;, sobelX), (\u0026#34;sobel_y\u0026#34;, sobelY), (\u0026#34;emboss\u0026#34;, emboss)) # load the input image and convert it to grayscale image = cv2.imread(args[\u0026#34;image\u0026#34;]) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # loop over the kernels for (kernelName, K) in kernelBank: # apply the kernel to the grayscale image using both our custom `convolve` function and OpenCV\u0026#39;s `filter2D` function print(\u0026#34;[INFO] applying {} kernel\u0026#34;.format(kernelName)) convolveOutput = convolve(gray, K) opencvOutput = cv2.filter2D(gray, -1, K) # show the output images cv2.imshow(\u0026#34;Original\u0026#34;, gray) cv2.imshow(\u0026#34;{} - convole\u0026#34;.format(kernelName), convolveOutput) cv2.imshow(\u0026#34;{} - opencv\u0026#34;.format(kernelName), opencvOutput) cv2.waitKey(0) cv2.destroyAllWindows() Kết quả:\nTừ trái sang phải: Ảnh gốc, ảnh áp dụng \"average blur\" sử dụng 7x7 kernel convolution, và ảnh áp dụng \"average blur\" sử dụng OpenCV’s cv2.filter2D.\n 1.3 Vai trò của Convolutions trong Deep Learning\nNhư các bạn đã thấy từ phần trước, chúng ta phải tự định nghĩa (manually hand-define) các kernel cho mỗi nhiệm vụ xử lý ảnh khác nhau.\nLiệu có cách nào \u0026ldquo;tự động hóa\u0026rdquo; việc này?\nCNN chính là câu trả lời. Bằng cách sắp xếp nhiều lớp convolutions, kết hợp với \u0026ldquo;activation function\u0026rdquo;, pooling, backpropagation, \u0026hellip; CNNs có khả năng học để cập nhật giá trị của kernel, từ đó trích xuất được các đặc trưng của đối tượng trong ảnh.\nTrong bài tiếp theo, mình sẽ tìm hiểu kỹ hơn về các dạng layers khác nhau, sau đó sẽ đưa ra một số \u0026ldquo;common layer stacking patterns\u0026rdquo; được sử dụng rộng rãi trong lĩnh vực xử lý ảnh.\nSource code sử dụng trong bài này, các bạn có thể tham khảo tại github cá nhân của mình tại github\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/22_convolutional_neural_network_1/","tags":["Deep Learning","CNN"],"title":"Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 1"},{"categories":["Deep Learning","Neural Network"],"contents":"Trong quá trình tìm hiểu về mạng NN, mình thấy khá là khó hiểu, đặc biệt với các bạn không mạnh về toán. Bài này, mình sẽ diễn giải cách thức làm việc của NN một cách trực quan, dễ hiểu cho các bạn thông qua một ví dụ cụ thể.\n1. Nhắc lại lý thuyết\nGiả sử ta có mạng NN như sau:\n Quá trình training model bao gồm 2 phases:\n1.1 Forward Path\nPhase này tính toán (dự đoán) đầu ra $o_1, o_2$, tính loss.\nGiả sử activation là hàm sigmoid:  Ta sẽ tính lần lượt các đại lượng trung gian:\n $in_{h_1}$: input của $h_1$ $in_{h_2}$: input của $h_2$ $out_{h_1}$: output của $h_1$ $out_{h_2}$: output của $h_2$ $in_{o_1}$: input của $o_1$ $in_{o_2}$: input của $o_2$ $out_{o_1}$: output của $o_1$ $out_{o_2}: output của $o_2$$  Công thức tính của từng đại lượng như sau:\n$in_{h_1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1$\n$in_{h_2} = w_3 * i_1 + w_4 * i_2 + b_1 * 1$\n$out_{h_1} = sigmoid(in_{h_1}) =$$\\frac{1}{1 + e^{-in_{h_1}}}$\n$out_{h_2} = sigmoid(in_{h_2}) =$$\\frac{1}{1 + e^{-in_{h_2}}}$\n$in_{o_1} = w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1$\n$in_{o_2} = w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1$\n$out_{o_1} = sigmoid(in_{o_1}) =$$\\frac{1}{1 + e^{-in_{o_1}}}$\n$out_{o_2} = sigmoid(in_{o_2}) =$$\\frac{1}{1 + e^{-in_{o_2}}}$\n Tiếp theo là tính loss bằng cách so sánh đầu ra của mạng NN với các giá trị thực tế:\n $target_{o_1}$ $target_{o_2}$:  Công thức tính loss như sau:\n$E_{total} = \\sum_{i=1}^2 E_{o_i} = \\sum_{i=1}^2 \\frac{1}{2} (target_{o_i} - out_{o_i})^2 = E_{o_1} + E_{o_2}$\n$E_{o_1} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2$\n$E_{o_2} = \\frac{1}{2} (target_{o_2} - out_{o_2})^2$\n 1.2 Backward Path\nMục đích của phase này là cập nhật trọng số $w$ sao cho tối thiểu hóa loss.\nTa sẽ sử dụng thuật toán tối ưu Stochastic Gradient Descent (SGD) để cập nhật $w$.\nCông thức cập nhật như sau:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta)$ với:\n $\\nabla_\\theta f(\\theta)$ là đạo hàm của Loss Function tại $\\theta$ (đạo hàm từng phần theo $\\nabla$). $\\eta$ là một số \u0026gt; 0, gọi là learning rate. $\\theta$ là tập hợp các vector các tham số của model cần tối ưu. Trong trường hợp này là các trọng số $w$.  Đạo hàm từng phần của các $w$ tại output layer được tính theo quy tắc chain rule như sau:\n$\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_5}$\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_7} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_7}$/p $\\frac{\\partial E_{total}}{\\partial w_8} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_8}$\n Đạo hàm từng phần của các $w$ tại hidden layer được tính như sau:\n$\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_1}$\n$\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_2}$\n$\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_3}$\n$\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_4}$\n Sau khi tính được đạo hàm từng phần của mỗi $w$, ta áp dụng công thức phía trên để cập nhật $w$.\n2. Ví dụ áp dụng\nVẫn với kiến trúc mạng như trên, ta sẽ gán các giá trị khởi tạo cho các tham số như hình bên dưới:\n Ok, bây giờ ta sẽ bắt đầu đi tính toán.\n2.1 Fordward Path\nInput của $h_1$:\n$in_{h_1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1$\n$in_{h_1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1$\n$in_{h_1} = 0.3775$\n Input của $h_2$:\n$in_{h_2} = w_3 * i_1 + w_4 * i_2 + b_1 * 1$\n$in_{h_2} = 0.25 * 0.05 + 0.3 * 0.1 + 0.35 * 1$\n$in_{h_2} = 0.3925$\n Ouput của $h_1$:\n$out_{h_1} = $$\\frac{1}{1 + e^{-in_{h_1}}}$\n$out_{h_1} = \\frac{1}{1 + e^{-0.3775}}$\n$out_{h_1} = 0.593269992$\n Output của $h_2$:\n$out_{h_2} = $$\\frac{1}{1 + e^{-in_{h_2}}}$\n$out_{h_2} = \\frac{1}{1 + e^{-0.3925}}$\n$out_{h_2} = 0.596884378$\n Input của $o_1$:\n$in_{o_1} = w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1$\n$in_{o_1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1$\n$in_{o_1} = 1.105905967$\n Input của $o_2$:\n$in_{o_2} = w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1$\n$in_{o_2} = 0.5 * 0.593269992 + 0.55 * 0.596884378 + 0.6 * 1$\n$in_{o_2} = 1.224921404$\n Output của $o_1$:\n$out_{o_1} = $$\\frac{1}{1 + e^{-in_{o_1}}}$\n$out_{o_1} = \\frac{1}{1 + e^{-1.105905967}}$\n$out_{o_1} = 0.75136507$\n Output cuat $o_2$:\n$out_{o_2} = $$\\frac{1}{1 + e^{-in_{o_2}}}$\n$out_{o_2} = \\frac{1}{1 + e^{-1.224921404}}$\n$out_{o_2} = 0.772928465$\n Tổng lỗi:\n$E_{o_1} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2$\n$E_{o_2} = \\frac{1}{2} (0.01 - 0.75136507)^2$\n$E_{o_1} = 0.274811083$\n $E_{o_2} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2$\n$E_{o_2} = \\frac{1}{2} (0.01 - 0.772928465)^2$\n$E_{o_2} = 0.023560026$\n $E_{total} = \\sum_{i=1}^2 E_{o_i}$\n$E_{total} = 0.274811083 + 0.023560026$\n$E_{total} = 0.298371109$\n 2.2 Backward Path\nTính đạo hàm từng phần của Loss Function theo mỗi $w$.\nCác $w$ của output layer ($w_5, w_6, w_7, w_8$) có cách tính giống nhau:\n $w_5$:  $\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_5}$\n Ta biết:\n$E_{total} = \\sum_{i=1}^2 \\frac{1}{2} (target_{o_i} - out_{o_i})^2$\n$E_{total} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2 + \\frac{1}{2} (target_{o_2} - out_{o_2})^2$\n Nên:\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1})^{2-1} * (-1) + 0$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(target_{o_1} - out_{o_1})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(0.01 - 0.75136507) = 0.74136507$\n Tiếp theo, vì:\n$out_{o_1} = sigmoid(in_{o_1}) =$ $\\frac{1}{1 + e^{-in_{01}}}$\n Nên:\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.186815602$\n Và, $in_{o_1} = w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1$\n Nên:\n$\\frac{\\partial in_{o_1}}{\\partial w_5}$ $ = out_{h_1}$\n$\\frac{\\partial in_{o_1}}{\\partial w_5}$ $ = 0.593269992$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_5}$$\n$\\frac{\\partial E_{total}}{\\partial w_5}$ $ = 0.74136507 * 0.186815602 * 0.593269992$\n$\\frac{\\partial E_{total}}{\\partial w_5}$ $ = 0.082167041$\n  $w_6$:  $\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n $\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1})^{2-1} * (-1) + 0$$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(target_{o_1} - out_{o_1})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(0.01 - 0.75136507) = 0.74136507$\n $\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.186815602$\n $\\frac{\\partial in_{o_1}}{\\partial w_6} $ $= out_{h_2}$\n$\\frac{\\partial in_{o_1}}{\\partial w_6}$ $ = 0.596884378$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_6}$ $ = 0.74136507 * 0.186815602 * 0.596884378$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= 0.082667628$\n  $w_7$:  $\\frac{\\partial E_{total}}{\\partial w_7} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_7}$\n $\\frac{\\partial E_{total}}{\\partial out_{o_2}} = 0 + 2 * \\frac{1}{2} (target_{o_2} - out_{o_2})^{2-1} * (-1)$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}}$ $ = -(target_{o_2} - out_{o_2})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}}$ $ = -(0.99 - 0.772928465) = -0.217071535$\n $\\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$ $ = out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.175510053$\n $\\frac{\\partial in_{o_2}}{\\partial w_7}$ $ = out_{h_1}$\n$\\frac{\\partial in_{o_2}}{\\partial w_7} $ $= 0.593269992$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.217071535 * 0.175510053 * 0.593269992$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.022602541$\n  $w_8$:  $\\frac{\\partial E_{total}}{\\partial w_8} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_8}$\n $\\frac{\\partial E_{total}}{\\partial out_{o_2}} = 0 + 2 * \\frac{1}{2} (target_{o_2} - out_{o_2})^{2-1} * (-1)$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}} $ $= -(target_{o_2} - out_{o_2})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}} $ $= -(0.99 - 0.772928465) = -0.217071535$\n $\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.175510053$\n $\\frac{\\partial in_{o_2}}{\\partial w_8} $ $= out_{h_2}$\n$\\frac{\\partial in_{o_2}}{\\partial w_8} $ $= 0.596884378$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.217071535 * 0.175510053 * 0.596884378$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.022740242$\n Các $w$ của hidden layer ($w_1, w_2, w_3, w_4$) có cách tính giống nhau:\n $w_1$:  $\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_1}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = w_5$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = 0.4$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}}$ $ = 0.138498562 * 0.4$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} $ $= 0.055399425$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$ $ = w_7$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} $ $= 0.5$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.038098237 * 0.5$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.019049118$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_1}} $$= 0.055399425 + (-0.019049118) = 0,036350307$\n ----------------------\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_1}}})}{\\partial in_{h_1}}$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} $ $= out_{h_1}(1 - out_{h_1})$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}}$ $ = 0.59326999(1 - 0.59326999) = 0.241300709$\n ----------------------\n$\\frac{\\partial in_{h_1}}{\\partial w_1} = \\frac{\\partial (w_1 * i_1 + w_2 * i_2 + b_1 * 1)}{\\partial w_1}$\n$\\frac{\\partial in_{h_1}}{\\partial w_1}$ $ = i_1$\n$\\frac{\\partial in_{h_1}}{\\partial w_1} $ $= 0.05$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_1}$\n$\\frac{\\partial E_{total}}{\\partial w_1} $ $= 0.036350306 * 0.241300709 * 0.05$\n$\\frac{\\partial E_{total}}{\\partial w_1} $ $= 0.000438568$\n  $w_2$:  $\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_2}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = w_5$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = 0.4$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}}$ $ = 0.138498562 * 0.4$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} $ $= 0.055399425$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$ $ = w_7$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} $ $= 0.5$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.038098237 * 0.5$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.019049118$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_1}} $$= 0.055399425 + (-0.019049118) = 0,036350307$\n ----------------------\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_1}}})}{\\partial in_{h_1}}$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} $ $= out_{h_1}(1 - out_{h_1})$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}}$ $ = 0.59326999(1 - 0.59326999) = 0.241300709$\n ----------------------\n$\\frac{\\partial in_{h_1}}{\\partial w_2} = \\frac{\\partial (w_1 * i_1 + w_2 * i_2 + b_1 * 1)}{\\partial w_2}$\n$\\frac{\\partial in_{h_1}}{\\partial w_2}$ $ = i_2$\n$\\frac{\\partial in_{h_1}}{\\partial w_2} $ $= 0.1$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_2}$\n$\\frac{\\partial E_{total}}{\\partial w_2} $ $= 0.036350306 * 0.241300709 * 0.1$\n$\\frac{\\partial E_{total}}{\\partial w_2} $ $= 0.000877135$\n  $w_3$:  $\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_3}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = w_6$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = 0.45$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}}$ $ = 0.138498562 * 0.45$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} $ $= 0.062324353$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$ $ = w_8$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} $ $= 0.55$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.038098237 * 0.55$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.02095403$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_2}} $$= 0.062324353 + (-0.02095403) = 0.041370323$\n ----------------------\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_2}}})}{\\partial in_{h_2}}$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} $ $= out_{h_2}(1 - out_{h_2})$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}}$ $ = 0.596884378(1 - 0.596884378) = 0.240613417$\n ----------------------\n$\\frac{\\partial in_{h_2}}{\\partial w_3} = \\frac{\\partial (w_3 * i_1 + w_4 * i_2 + b_1 * 1)}{\\partial w_3}$\n$\\frac{\\partial in_{h_2}}{\\partial w_3}$ $ = i_1$\n$\\frac{\\partial in_{h_2}}{\\partial w_3} $ $= 0.05$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_3}$\n$\\frac{\\partial E_{total}}{\\partial w_3} $ $= 0.041370323 * 0.240613417 * 0.05$\n$\\frac{\\partial E_{total}}{\\partial w_3} $ $= 0.000497713$\n  $w_4$:  $\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_4}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = w_6$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = 0.45$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}}$ $ = 0.138498562 * 0.45$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} $ $= 0.062324353$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$ $ = w_8$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} $ $= 0.55$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.038098237 * 0.55$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.02095403$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_2}} $$= 0.062324353 + (-0.02095403) = 0.041370323$\n ----------------------\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_2}}})}{\\partial in_{h_2}}$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} $ $= out_{h_2}(1 - out_{h_2})$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}}$ $ = 0.596884378(1 - 0.596884378) = 0.240613417$\n ----------------------\n$\\frac{\\partial in_{h_2}}{\\partial w_4} = \\frac{\\partial (w_3 * i_1 + w_4 * i_2 + b_1 * 1)}{\\partial w_3}$\n$\\frac{\\partial in_{h_2}}{\\partial w_4}$ $ = i_2$\n$\\frac{\\partial in_{h_2}}{\\partial w_4} $ $= 0.1$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_4}$\n$\\frac{\\partial E_{total}}{\\partial w_4} $ $= 0.041370323 * 0.240613417 * 0.1$\n$\\frac{\\partial E_{total}}{\\partial w_4} $ $= 0.000995425$\n Đến đây ta đã tính xong các đạo hàm từng phần theo các $w$. Áp dụng SGD để cập nhật các $w$ ta được (chọn $\\eta = 0.9$):\n$w_5^+ = w_5 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_5}$\n$w_5^+ = 0.4 - 0.9 * 0.082167041$\n$w_5^+ = 0.326049663$\n-------------------------\n $w_6^+ = w_6 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_6}$\n$w_6^+ = 0.45 - 0.9 * 0.082667628$\n$w_6^+ = 0.375599135$\n-------------------------\n $w_7^+ = w_7 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_7}$\n$w_7^+ = 0.5 - 0.9 * (-0.022602541)$\n$w_7^+ = 0.520342287$\n-------------------------\n $w_8^+ = w_8 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_8}$\n$w_8^+ = 0.55 - 0.9 * (-0.022740242)$\n$w_8^+ = 0.570466218$\n-------------------------\n $w_1^+ = w_1 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_1}$\n$w_1^+ = 0.15 - 0.9 * 0.000438568$\n$w_1^+ = 0.149605289$\n-------------------------\n $w_2^+ = w_2 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_2}$\n$w_2^+ = 0.2 - 0.9 * 0.0080877135$\n$w_2^+ = 0.192721058$\n-------------------------\n $w_3^+ = w_3 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_3}$\n$w_3^+ = 0.25 - 0.9 * 0.000497713$\n$w_3^+ = 0.249552058$\n-------------------------\n $w_4^+ = w_4 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_4}$\n$w_4^+ = 0.3 - 0.9 * 0.000995425$\n$w_4^+ = 0.299104118$\n-------------------------\n Phù, như vậy là chúng ta đã cập nhật xong giá trị mới cho các trọng số $w$. Đây là những phép toán xảy ra trong mỗi lần cập nhật khi training model. Hi vọng, thông qua ví dụ trong bài này, các bạn đã có thể hiểu rõ hơn bản chất của mạng NN. Hẹn gặp lại các bạn trong các bài tiếp theo!\n3. Tham khảo\n Mattmazur Dive into Deep Learning Wikipedia  ","permalink":"https://tiensu.github.io/blog/21_neural_network_fundamentals_2/","tags":["Deep Learning","Neural Network"],"title":"Neural Network cơ bản (Phần 2)"},{"categories":["Deep Learning","Neural Network"],"contents":"Trong bài này chúng ta sẽ cùng nhau tìm hiểu lys thuyết cơ bản về mạng thần kinh nhân tạo (neural network):\n Cấu trúc của neural network. Thuật toán lan truyền (propagation) và lan truyền ngược (backpropagation).  Những kiến thức trong bài này sẽ là tiền đề để các bạn tiến xa hơn trong thế giới của Deep Learning.\n1. Neural Network là gì?\nNeural Networks là các khối (blocks) để xây dựng lên các hệ thống Deep Learning. Chúng ta sẽ bắt đầu với việc xem xét ở mức \u0026quot;high-level\u0026quot; của Neural Network, bao gồm cả mối liên hệ của chúng với não bộ của con người.\nTrong thực tế, có rất nhiều những công việc rất khó để có thể thực hiện tự động hóa bởi máy móc nhưng lại rất dễ dàng đối với các loài động vật (bao gồm cả con người). Những công việc đó thường liên quan đến việc nhận diện, phân loại đối tượng.\nVí dụ:\n Con chó của gia đình bạn có thể phân biệt được người quen (người trong gia đình bạn) và người lạ (không phải trong gia đình bạn)? Một đứa trẻ có thể nhận biết được sự khác nhau giữa xe oto con và xe oto tải.  Tại sao con chó và đứa trẻ có thể làm được những việc đó?\nCâu trả lời nằm ở cấu tạo bên trong não bộ của chúng. Não bộ của cả 2 đều chứa một mạng thần kinh sinh học kết nối đến hệ thần kinh trung tâm. Mạng này được tạo ra bởi rất nhiều các neurons kết nối với nhau.\nTừ neural là dạng tính từ của neuron, và network ngầm chỉ kiến trúc \u0026quot;graph\u0026quot; của hệ thần kinh. Do vậy, một Artificial Neural Network (ANN) là một hệ thống tính toán, cố gắng mô phỏng (bắt chước) mạng thần kinh sinh học của các loài động vật. ANN là một graph có định hướng, nó bao gồm các nodes và các connections (kết nối giữa các nodes). Mỗi node thực hiện một tính toán đơn giản nào đó, mỗi connection mang một tín hiệu từ node này đến node khác. Những tín hiệu này đi kèm theo một trọng số (weight) chỉ ra mức độ khuyếch đại hoặc giảm bớt cường độ tín hiệu đó. Giá trị weight càng lớn chứng tỏ tín hiệu đi kèm càng quan trọng đối với kết quả đầu ra và ngược lại.\nHình dưới đây là một ANN đơn giản, bao gồm một lớp input ở đầu, 2 lớp ở giữa (hidden layers) và một lớp output ở cuối. Mỗi connection mang theo một tín hiệu xuyên qua hai hidden layers. Kết quả cuối cùng được tính toán tại lớp output.\n 2. Artificial Models\nHãy xem thử một ANN cơ bản như hình bên dưới:\n ANN này thực hiện tính tổng có trọng số ($w_i$) của các input vectors($x_i$). Trong thực tế, các input vectors có thể là pixcel của images, hay các rows của một dataset dạng tabular.\nMỗi $x_i$ kết nối với một neuron thông qua vector trọng số $w_i$.\nDiễn giải bằng công thức toán học thì output của ANN này sẽ là:\n $y = f(w_1x_1 + w_2x_2 + ... + w_nx_n)$ $y = f(\\sum_{i=1}^n w_ix_i)$ $y = f(net)$. Với net = $\\sum_{i=1}^n w_ix_i$  Nói chung, dù diễn đạt theo cách nào đi nữa thì ý tưởng chung vẫn là áp dụng hàm activate (f) vào tổng có trọng số của các input vectors.\n3. Activation Functions\n a) Step function  Hàm activation đơn giản nhất có lẽ là Step function. Hàm này được sử dụng bởi thuật toán Perceptron (sẽ đề cập ở phần sau).\n Hàm này luôn nhận giá trị 1 nếu $\\sum_{i=1}^n$ $w_i$$x_i$ \u0026gt;= 0 và nhận giá trị 0 trong trường hợp còn lại.\n Vấn đề của step function là nó giá trị của nó không có sự khác biệt khi net \u0026gt;=0 hoặc net \u0026lt; 0. Điều này có thể dẫn đến một số vấn đề khi huấn luyện neural network.\n b) Sigmoid function   y = tf.nn.sigmoid(x) d2l.plot(x.numpy(), y.numpy(), \u0026#39;x\u0026#39;, \u0026#39;sigmoid(x)\u0026#39;, figsize=(5, 2.5))  So với step function, sigmoid function có một số ưu điểm sau:\n Giá trị của nó liên tục và phân biệt nhau tại một nơi. Đồ thị của nó đối xứng qua trục y.  Tuy nhiên, có 2 nhược điểm lớn nhất của sigmoid function là:\n Output của nó không tập trung quanh điểm gốc tọa độ. Càng xa gốc tọa độ, giá trị của nó tiệm cận với giá trị bão hòa. Điề u này vô tình triệt tiêu gradient, vì delta của gradient vô cùng nhỏ.  Đạo hàm của sigmoid function như sau:\n with tf.GradientTape() as t: y = tf.nn.sigmoid(x) d2l.plot(x.numpy(), t.gradient(y, x).numpy(), \u0026#39;x\u0026#39;, \u0026#39;grad of sigmoid\u0026#39;, figsize=(5, 2.5))   c) Tanh function  Hàm này giải quyết được nhược điểm thứ nhất của sigmoid function.\n y = tf.nn.tanh(x) d2l.plot(x.numpy(), y.numpy(), \u0026#39;x\u0026#39;, \u0026#39;tanh(x)\u0026#39;, figsize=(5, 2.5))   d) ReLU (Rectified Linear Unit) funtion   Hàm này nhận giá trị 0 khi inputs \u0026lt; 0, nhưng sẽ tăng tuyến tính khi inputs \u0026gt;= 0.\nThực tế chứng minh, ReLU function hoạt động tốt hơn hẳn so với các hàm kể trên. Bắt đầu từ năm 2015, nó được sử dụng thường xuyên trong Deep Learning.\nx = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32) y = tf.nn.relu(x) d2l.plot(x.numpy(), y.numpy(), \u0026#39;x\u0026#39;, \u0026#39;relu(x)\u0026#39;, figsize=(5, 2.5))  ReLU function vẫn có nhược điểm. Khi inputs \u0026lt; 0, nó nhận giá trị 0. Như vậy thì không thể tính được gradient tại những điể m đó. Thực tế thì cũng hiếm khi có trường hợp nào mà inputs lại có giá trị \u0026lt; 0. Tuy nhiên, để giải quyết triệt để vấn đề thì lại sinh ra một hàm mới:\n e) Leaky ReLU function   Đây là một biến thể của ReLU funtion, nó nhận một giá trị khác 0 (thường rất nhỏ) khi inputs \u0026lt; 0. Giá trị của $\\alpha$ rất nhỏ và được cập nhật trong quá trình huấn luyện neural network.\n  f) ELU (Exponential Linear Units) function   Khác với ReLU function, giá trị của $\\alpha$ trong ELU function được cố định từ đầu (lúc xây đựng kiến trúc mạng). Giá trị thông thường của $\\alpha$ là 1.\n Nên sử dụng activation function nào?\nViệc có nhiều hơn 1 activation function đôi khi làm cho bạn bối rối khi lựa chọn sử dụng cái nào, không sử dụng cái nào?\nLời khuyên của mình như sau:\n Bắt đầu với ReLU để đặt được một baseline accuracy. (Hầu hết các public papers đều làm như vậy) Thử chuyển qua sử dụng các biến thể của ReLU: Leaky ReLU, ELU.  Trong các dự án thực tế thì mình thường làm theo các bước:\n Sử dụng ReLU Tune các hyper-parameters khác: architecture, learning rate, regularization strength, ... Ghi lại các giá trị accuracy. Một khi đã tương đối thoả mãn về accuracy, chuyển qua ELU. Độ chính xác thường sẽ tăng khoảng 1-5% tùy thuộc dataset.  Cách này chỉ là kinh nghiệm cá nhân của mình, và không có gì đảm bảo đúng trong mọi trường hợp. Bạn có thể tham khảo hoặc không. Hãy luôn nhớ thử-sai mọi khả năng có thể cho bài toán của bạn.\n4. Feedfoward Network Architecture\nKiến trúc ANN thì có rất nhiều, nhưng phổ biến nhất là dạng feedfoward network.\n Trong kiến trúc này, một connection giữa 2 nodes chỉ được phép đi theo chiều từ layer $i$ tới layer $i+1$ (vì thế mà có tên feedfoward). Không có chiều từ layer $i+1$ đến layer $i$ hoặc bất kỳ chiều nào khác. Khi có thêm chiều từ layer $i+1$ đến layer $i$ (feedback connection) thì ta được kiến trúc RNN (Recurrent Neural Network). Feedfoward network được sử dụng chủ yếu trong các bài toán về Computer Vision (mạng CNN là một ví dụ điển hình) , trong khi feedback network lại được sử dụng chủ yếu trong các bài toán về Natural Language Processing.\nANN được sử dụng cho cả 3 dạng bài toán: supervised, unsupervised, and semi-supervised. Một số ví dụ điển hình là classification, regression, clustering, vector quantization, pattern association, ...\nTrong bài tiếp theo, mình sẽ minh họa cách thức cập nhật trong số của mạng ANN thông qua một ví dụ rất dễ hiểu. Mời các bạn đón đọc.\n3. Tham khảo\n Pyimagesearch Dive into Deep Learning Wikipedia  ","permalink":"https://tiensu.github.io/blog/20_neural_network_fundamentals_1/","tags":["Deep Learning","Neural Network"],"title":"Neural Network cơ bản (Phần 1)"},{"categories":["Deep Learning"],"contents":"Bài viết này nhằm mục đích tổng hợp, tóm tắt lại các thuật toán của Deep Learning, giúp bạn đọc có cái nhìn toàn cảnh và hiểu rõ hơn về Deep Learning.\n1. Deep Learning (DL) là gì?\nTheo Wikipedia: \u0026ldquo;Deep learning (also known as deep structured learning or differential programming) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised\u0026rdquo;.\nHiểu theo một cách khác thì DL là một tập hợp các thuật toán mô phỏng lại cách thức hoạt động của bộ não của con người trong việc phân tích dữ liệu, nhằm tạo ra các models cái mà được sử dụng cho việc đưa ra quyết định dựa trên dữ liệu đầu vào. Về bản chất, DL bao gồm nhiều layers, mỗi layer có thể coi là một mạng Neural Network (NN).\nGiống như não bộ của con người, NN có chứa các neurons. Mỗi neurons nhận các tín hiệu ở đầu vào, nhân chúng với các trọng số (weights), tổng hợp chúng lại, và sau cùng là áp dụng các hàm kích hoạt (thường là non-linear) lên chúng. Các neurons được sắp xếp thành các layers liên tiếp nhau (stack).\nCác kiến trúc mạng DL đều sử dụng thuật toán Backpropagation để tính toán và cập nhật các trọng số của nó thông qua quá trình huấn luyện. Về mặt ý tưởng, dữ liệu được đưa vào mạng DL, sinh ra output, so sánh output với giá trị thực tế (sử dụng loss function) rồi điều chỉnh trọng số theo kết quả so sánh đó.\nViệc điều chỉnh trọng số là một quá trình tối ưu, thường sử dụng thuật toán SGD. Ngoài SGD, một số thuật toán khác cũng hay được sử dụng. Đó là Adam, Radam, SRMprop.\n2. Các thuật toán DL\n 2.1 Convolution Neural Network (CNN)\nCNN sử dụng phép toán tích chập để tạo liên kết giữa các layers trong mạng NN. Mỗi neuron, thay vì kết nối đến tất cả các neurons khác thì sẽ chỉ kết nối đến một vài neurons đại diện. Điều này giúp cho CNN học được các mối liên hệ không gian của dữ liệu tốt hơn.\nĐó là lý do mà vì sao CNN lại được sử dụng rất phổ biến trong các bài toán về Computer Vision như phân loại hình ảnh, phát hiện đối tượng trong ảnh/video, \u0026hellip;\nMột vài kiến trúc mạng CNN kinh điển có thể kể đến như: VGG, ResNet, MobiNet, InceptionNet, Yolo, SSD, \u0026hellip;\n 2.2 Recurrent Neural Network (RNN)\nRNN là thuật toán rất phù hợp với các loại dữ liệu có mối liên hệ về thời gian như time serial forecasting hay trong các bài toán về xử lý ngôn ngữ tự nhiên (NLP), bởi vì một phần output của nó ở thời điể m này được đưa trở lại thành input ở thời điểm tiếp theo. Do cách thức làm việc đặc biệt như vậy mà nó có có năng ghi nhớ được thông tin trong quá khứ và sử dụng những thông tin đó vào việc dự đoán kết quả.\nMột số kiến trúc mạng kinh điển của RNN có thể kể đến như: GRU, LSTM, \u0026hellip;\n 2.3 AutoEncoders\nAutoEncoders là kiến trúc mạng bao gồm 2 thành phần: Encoder và Decoder. Encoder nhận input và mã hóa nó thành các vector trong một không gian có số chiều ít hơn. Decoder sử dụng các vertors đó để giải mã (xây dựng lại) nó thành dữ liệu ban đầu.\nAutoEncoders sử dụng chủ yếu trong các bài toán về giảm chiều dữ liệu, nén dữ liệu, \u0026hellip;\n 2.4 Generative Adversarial Networks (GAN)\nGAN cũng bao gồm 2 thành phần: Generator và Discriminator. Generator sinh ra dữ liệu giả và Discriminator sẽ cố gắng phân biệt dữ liệu mới sinh ra đó là giả hay thật. Hai thành phần này được huấn luyện cùng với nhau, chúng sẽ cạnh tranh nhau để cuối cùng tạo ra được model GAN tốt nhất.\nGAN được dùng để sinh ra dữ liệu mới từ dữ liệu ban đầu, ví dụ: sinh ra các mẫu thời trang mới, các mẫu nhân vật hoạt hình mới, \u0026hellip;\n 2.5 Transformer\nTransformer là kiến trúc mạng mới được phát triển và được sử dụng rộng rãi gần đây trong các bài toán NLP khi nó chứng minh được sự vượt trội của nó so với RNN. Có được này là nhờ vào một cơ chế, gọi là Attention, tức là chỉ tập trung và một số vị trí cụ thể thay vì toàn bộ vị trí trong dữ liệu.\nTransformer cũng bao gồm một số bộ encoders và một số bộ decoders được đặt cạnh nhau (stacked) cùng với các lớp attentions.\n Hiện nay, BERT và GPT-3 là 2 pre-trained model của transformer được sử dụng rất nhiều trong lĩnh vực NLP.\n2.6 Graph Neural Network (GNN)\nDL nói chung thường làm việc với dữ liệu có cấu trúc. Tuy nhiên, trong thực tế có rất nhiều dữ liệu ở dạng phi cấu trúc (unstructed data) và được sắp xếp dưới dạng graph. Ví dụ như là mạng xã hội, cấu tạo phân tử trong hóa học, \u0026hellip;\nGNN chính là mạng NN mô hình hóa graph data. Chúng sẽ nhận diện các mối liên kết của các nodes trong graph và output ra các vector đặc trưng của dữ liệu đó, giống như embedding. Output này được sử dụng làm input cho các NN khác.\n3. Các thuật toán DL trong các bài toán Computer Vision \u0026amp; Natural Language Processing\n3.1 DL trong CV\n  Image Classification  Đây là bài toán nhận diện đối tượng trong ảnh thuộc về class nào. Số lượng class có thể là 2 (binary) hoặc nhiều hơn (multiple).\nCác kiến trúc mạng kinh điển như VGG (VGG16, VGG19), ResNet, InceptionNet, MobiNet, AlexNet \u0026hellip; giải quyết rất tốt bài toán này.\n Object Detection  Đây là bài toán mở rộng của image classification vì không chỉ phân loại mà còn phải định vị được đối tượng trong ảnh.\nCác kiến trúc NN giải quyết vấn đề này bao gồm: Họ R-CNN (R-CNN, Fast R-CNN, Faster R-CNN), Yolo (YOLOv2, YOLOv3, YOLOv4), SSD, EfficientNet, \u0026hellip;\n  Semantic Segmentation  Nếu như Image Classification là bài toán gán nhãn cho toàn bộ bức ảnh thì Semantic Segmentation có thể coi là bài toán gán nhãn cho từng pixcel trong ảnh đó. Tất cả các pixel thuộc cùng một class (hay các objects thuộc cùng một class) được coi như là một thực thể (thể hiện bằng màu sắc giống nhau).\n Một số kiến trúc mạng như Full Connected Networks FCN, UNET, \u0026hellip; giải quyết tốt bài toán này.\n Instance Segmentation  Cũng giống như Semantic Segmentation là gán nhãn cho từng pixel trong ảnh, tuy nhiên Instance Segmentation coi mỗi đối tượng (cùng hoặc khác class) là các thực thể riêng biệt (thể hiện bằng màu sắc khác nhau).\n Kiến trúc Mask_RCNN nổi tiếng nhất cho viêc giải quyết bài toán này.\n Face Recognition  Đây là bài toán nhận diện khuôn mặt của người trong video/ảnh. Nó bao gồm 2 công đoạn: - Đầu tiên là phát hiện vùng chứa khuôn mặt (bài toán Object Detection), - Sau đó nhận diện xem khuôn mặt đó là của ai (bài toán classification)\nMột số thuật toán nổi tiếng giải quyết cho công đoạn 2 là: FaceNet, VGGFace, MTCNN, \u0026hellip; Còn đối với công đoạn 1 có thể sử dụng OpenCV.\n  Optical Character Recogniton (OCR)  Đây là bài toán nhận diện ký tự trong hình ảnh. Tương tự như bài toán Face Detection, nó cũng bao gồm 2 công đoạn: - Phát hiện vùng chứa ký tự trong ảnh (bài toán object detection) - Nhận diện từng ký tự trong vùng chứa đó.\nMột số thuật toán nổi bật là: - Công đoạn 1: Craft, Yolo, \u0026hellip; - Công đoạn 2: CRNN, \u0026hellip;\nMột số thư viện hỗ trợ bài toán OCR: Tesseract, EasyOCR\n  Humand Pose  Humand Pose hay Pose Estimation là bài toán xác định vị trí của các khớp nối của con người trong ảnh/video.\n PostNet là thuật toán nối tiếng cho bài toán này.\n3.2 DL trong NPL\n Các thuật toán Word Embedding  Word Embedding là quá trình biến đổi các từ thành các vector đại diện, làm đầu vào cho các DL model. Việc biến đổi này có tính đến hoàn cảnh và ngữ nghĩa của từ đó trong câu. Một số thuật toán phổ biến:\na. Word2Vec: Nó hoạt động theo theo 1 trong 2 cách, dự đoán từ dựa vào những tù xung quanh của nó (CBOW) hoăc dự đoán các từ xung quanh của 1 từ (Skip-Gram). Các từ được đưa vào Word2Vec theo dạng One-hot-encoding.\n b. Glove: Là một mô hình khác mở rộng ý tưởng của Word2Vec bằng cách kết hợp nó với các kỹ thuật phân tích thừa số ma trận.\n c. Contextual Word Embeddings: Sử dụng 2 layers bi-directional LSTM để embedding các từ, cho phép tận dụng sự phụ thuộc của nó với các từ trước đó.\nd. Transformer: Đây là phương pháp tiên tiến nhất hiện nay, cho phép tập trung vào một vài vị trí cụ thể của từ trong câu thay vì toàn bộ như khi sử dụng LSTM.\n  Sequence Modeling  Mô hình này giải quyết hầu hết các bài toán trong NLP như Machine Translation, Speech Recognition, Autocompletion và Sentiment Classification. Nó có khả năng xử lý cả một chuỗi đầu vào thay vì từng từ một.\n Còn rất nhiều kiến trúc, thuật toán, model nữa mà trong bài này chưa thể kể hết được. Nhưng hi vọng qua đây, các bạn cũng có được cái nhìn tổng quát về các thuật toán, model, kiến trúc phổ biến trong các bài toán DL.\nTrong các bài viết tiếp theo đi chi tiết vào một số thụât toán với các ứng dụng cụ thể. Mời các bạn đón đọc!\n4. Tham khảo\n Dive Into Deep Learning AI Summer  ","permalink":"https://tiensu.github.io/blog/19_deep_learning_algorithms_summary/","tags":["Deep Learning"],"title":"Tổng hợp các thuật toán Deep Learning"},{"categories":["Machine Learning","Data Science"],"contents":"Bạn thường nghe nói Data Scientist là nghê sexy nhất thế kỷ 21, với mức lương cao ngất ngưởng, tạo ra những sản phầm có tầm ảnh hưởng lớn, được mọi người ngưỡng mộ, blabla. Và thế là bạn quyết định chuyển hướng sang học làm data scientist. Bạn lao vào học toán (đại số tuyến tính, xác suất thống kê, đạo hàm, tích phân, \u0026hellip;), học lập trình (python, R, \u0026hellip;), học các thuật toán ML, cách sử dụng các thư viện ML. Thậm chí có bạn còn chơi lớn, học luôn cao học về nghành này vì \u0026ldquo;hình như\u0026rdquo; ngành này yêu cầu phải có trình độ cao học trở lên (bản thân mình chính là 1 ví dụ của trường hợp này, :D). OK, mình không có ý kiến gì về lựa chọn của bạn cả. Trong bài viết này, mình chỉ muốn chia sẻ một số điều mà mình cảm nhận được sau một thời gian làm trong nghành với danh xưng data scientist này. Những điều này là sự khác biệt giữa lý thuyết các bạn học được và thực tế công việc mà các bạn sẽ trải qua. Bạn sẽ tìm được cho mình câu trả lời cho câu hỏi \u0026ldquo;Thế nào là một data scientist giỏi? Và thế nào là một data scientist xuất sắc?\u0026rdquo;.\n1. \u0026ldquo;Tóm lại, câu chuyện bạn muốn kể là gì?\u0026quot;\nMột trong những câu hỏi tôi thường nghe sau mỗi lần kết thúc buổi seminar của CEO (hoặc một người nào đó ở vị trí tương đương) là:\n\u0026ldquo;Tóm lại, câu chuyện bạn muốn kể là gì?\u0026quot;\nThú thực, lần đầu tiên khi nghe câu hỏi này, tôi có phần bối rối. Tôi không hiểu tại sao họ lại nhấn mạnh vào \u0026ldquo;câu chuyện\u0026rdquo;? Tại sao họ không hỏi:\n Những gì tôi đã trình bày trong slide. Kết quả tôi đạt được là gì. Tôi đã làm như thế nào. \u0026hellip;  Trước khi thực sự hiểu được sự quan trọng của kỹ năng kể chuyện (telling story), tôi đã trải qua khá nhiều sai lầm \u0026hellip;\nHoặc là stackeholders (thường là những người non-techical) không hiểu những gì bạn đang nói, đang trình bày, hoặc là nội dung bạn đang truyền tải chưa đủ để thuyết phục họ, thúc đẩy họ đưa ra quyết định hay hành động cụ thể nào (take actions) \u0026hellip;\nVà tôi quyết định cải thiện kỹ năng kể chuyện của mình \u0026hellip;\nKhi tôi đã đạt được sự tiến bộ nhất đinh, rất nhiều thứ đã thay đổi \u0026hellip;\nStackeholders đã bắt đầu hiểu những gì mà tôi đang diễn giải. Và họ đã có những hành động cụ thể \u0026hellip;\nNếu bạn muốn trở thành môt data scientist giỏi, hãy tập trung vào kiến thức kỹ thuật.\nCòn nếu bạn muốn trở thành một data scientist xuất sắc, hãy tập trung vào kỹ năng kể chuyện.\nVậy \u0026hellip; làm thế nào để học kỹ năng kể chuyện? Lời khuyên của tôi là hãy học từ Vox. Họ là những bậc thầy về kỹ năng này, họ luôn luôn có thể giải thích bất kì vấn đề phức tạp nào một cách rất đơn giản và dễ hiểu. Bạn có thể xem thử video bên dưới của họ. Hãy quan sát cách họ giải thích các hiện tượng và vấn đề xã hội theo cách kể chuyện trực quan nhất để ai ai cũng có thể hiểu được.\n  Tóm tắt lại một vài lời khuyên rất hay cho chúng ta để nâng cao kỹ năng kể chuyện. Những điều này không chỉ đúng trong phạm vi data science mà nó còn đúng cho tất cả các lĩnh vực khác.\n𝟏) Lãnh đạo có ít thời gian hơn chúng ta:\nVới vai trò lãnh đạo cấp cao, họ phải xử lý nhiều việc quan trọng hơn.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Hãy đi thẳng vào vấn đề chính Trình bày ngắn gọn, có cấu trúc rõ ràng Trình bày xong, hãy đưa ra lời kiến nghị hành động với họ.  𝟐) Lãnh đạo dễ mất kiên nhẫn\nLãnh đạo cấp cao có quá nhiều việc cần phải xử lý một lúc và vì vậy họ dễ mất kiên nhẫn.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Cho họ thông tin tổng quát là bạn sẽ trình bày trong bao lâu. Nếu bạn trình bày trong 30 phút, thì bạn hãy rút ngắn thời gian nói xuống 15 phút, 15 còn lại để hỏi đáp. Đó là nguyên tắc 50:50.  𝟑) Chào đón sự cắt ngang\nNhững khán giả cấp cao thường xuyên ngắt ngang giữa chừng những bài thuyết trình.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Chuẩn bị bài với cấu trúc rõ ràng để khi “được chen ngang”, bạn nắm rõ là đang nói phần nào.  𝟒) Luôn có các slide phụ lục Nhà lãnh đạo cấp cao cần nghe một bức tranh tổng thể.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Thuyết trình tổng thể về ý tưởng theo một cấu trúc rõ ràng có chủ đích. Chuẩn bị sẵn các số liệu và phần “phụ lục”. Khi trình bày xong, chia sẻ thêm các số liệu này, hoặc mở lên khi có câu hỏi.  2. Data sạch ư? Mơ đi nhé!\nBạn tự hào rằng mình đã từng tham gia rất nhiều cuộc thi trên Kaggle, thâm chí còn leo lên top 10, top 5. Hãy quên điều đó đi, vì các dự án trong thực tế không \u0026ldquo;màu hồng\u0026rdquo; như vậy đâu. Trên Kaggle, bạn đuợc cung cấp sẵn bộ dữ liệu tương đối đầy đủ và sạch sẽ, còn trên thực tế thì hầu như bạn chỉ được đưa cho một mớ bòng bong hỗ độn. Khách hàng bảo, \u0026ldquo;đấy, bọn tao chỉ có như này, mày dùng như nào thì dùng\u0026rdquo;. Trường hợp xấu hơn, chúng ta bắt đầu dự án chả có tý data nào cả. Xấu hơn nữa là chúng ta không biết tìm kiếm data ở đâu, tìm kiếm như thế nào \u0026hellip; Quả thực rất khó khăn.\nThế mới nói, data collection và data integrity là hai trong số những bước quan trọng nhất của bất kỳ dự án data science nào. Garbage in, garbage out (đầu vào là rác thì đầu ra cũng là rác), câu nói này chắc ai làm trong lĩnh vực này đều đã từng nghe. Nhưng có lẽ không phải ai cũng thực sự hiểu được ý nghĩa của nó.\n Data collection: tức là thu thập dữ liệu. Để thu thập được dữ liệu thì cần phải hiểu rõ yêu cầu bài toán là gì, dữ liệu nào đã có, dữ liệu nào còn thiếu, cấu trúc của dữ liệu ra sao, số lượng tối thiểu như thế nào, cần chuẩn bị những công cụ dụng cụ gì để lấy dữ liệu (ví dụ, phải có camera để quay video, chụp ảnh)\u0026hellip; Ngoài ra, không phải lúc nào cũng có thể thu thập đầy đủ dữ liệu ngay từ ban đầu. Dữ liệu có thể đến trong suốt quá trình phát triển dự án hoặc vận hành sản phẩm. Vì vậy, phải có chiển lựợc quản lý dữ liệu hợp lý để dễ dàng cho viêc sử dụng, báo cáo thống kê về sau. Data integrity: tức là tính toàn vẹn của dữ liệu. Bạn cần phải kiểm tra kỹ lưỡng xem dữ liệu thu được có ý nghĩa hay không, có đầy đủ hay không bằng cách tự đặt câu hỏi và trả lời, hoặc nhờ sự trợ giúp từ các bên có liên quan và hiểu biết.  Nếu hai bước này làm không tốt thì cho dù bạn có áp dung bao nhiêu kỹ thuật cao siêu của data clearning, EDA, và tuning model thì kết qủa cũng sẽ không thể tốt đẹp như bạn mong đơi.\n3. Soft skills \u0026gt; Technical skills\nBạn đã từng tự đặt ra câu hỏi:\nNhững kỹ năng gì cần học để có thể làm việc được trong lĩnh vực data science?\nTheo ý kiến cá nhân của tôi, technical skills (lập trình, thống kê, \u0026hellip;) là kỹ năng bắt buộc và nên được ưu tiên học trước tiên khi bắt đầu bước chân vào thế giới data science.\nMột khi đã có đủ tự tin về technical skills của mình, chúng ta nên tập trung vào xây dựng và cải thiện soft skills (giao tiếp, làm việc nhóm, kể chuyện, \u0026hellip;). Khi học, bạn thường học một mình, rất ít khi giao tiếp hoặc làm cùng người khác. Nhưng dự án trong thực tế thường rất lớn, gồm nhiều phần, nhiều công đoạn. Và bạn thường chỉ làm một trong số những công đoạn đó. Nếu bạn là team-leader hay PM, bạn phải giao tiếp với các phòng ban bộ phận khác để lấy thông tin, trình bày kết quả với lãnh đạo, phân chia nhiệm vụ giữa các thành viên, \u0026hellip; Nếu bạn là nhân viên bình thường thì bạn vẫn cần phải trao đổi với các thành viên khác trong dự án, trình bày giải pháp kỹ thuật của mình. Rất may là những soft skills này khá giống với những dự án thuộc lĩnh vực khác, nên nếu bạn là người đã có kinh nghiệm làm việc ở những lĩnh vực khác chuyển sang data science thì bạn không cần quá lo lắng. Ngược lại, đối với các bạn sinh viên mới ra trường thì cần đặc biệt lưu ý hơn về việc phát triển soft skills của mình.\nCó một câu nói khá nổi tiếng của W.Edwards Deming:\n\u0026ldquo;Without data you\u0026rsquo;re just another person with an opinion\u0026rdquo;\nTạm dich: Nếu không có số liệu chứng minh cụ thể, thì tất cả chỉ là ý kiến, suy đoán của bạn mà thôi. (giống như trong các vụ điều tra, phá án. Lập luận, suy đoán có thuyết phục, hợp lý đi bao nhiêu chăng nữa mà không có bằng chứng cụ thể thì cũng vô nghĩa, không thể bắt được hung thủ).\nÁp dụng vào lĩnh vực data science này thì, có data chỉ là bước đầu tiên. Điều quan trọng là làm thế nào để sử dụng data đó để sinh ra những phán quyết, hành động (business decisions) mà mang lại lợi ích cho tổ chức.\nThử thay đổi câu nói trên một chút cho phù hợp với ngữ cảnh:\n\u0026ldquo;Without storytelling skills you\u0026rsquo;re just another persion with data\u0026rdquo;\nBạn có thể phân tích dữ liệu một cách xuất sắc.\nBạn có thể tạo ra một model tốt nhất trên thế giới.\nBạn thuộc làu làu cuốn Code Complete và viết code đẹp như trong phim.\nNhưng nếu bạn không thể sử dụng những kết quả đó để thuyết phục stackeholders để đưa ra business decisions, thì những nỗ lực của bạn vẫn chỉ nằm trên những slides PowerPoints mà thôi.\nHơi buồn, nhưng đó là sự thật!\n4. \u0026ldquo;What pattern observed behind?\u0026quot;\nĐối với hầu hết các bài toán, trừ khi bạn đang làm việc trong một công ty lớn về công nghệ (cutting-edge technology company), những models phức tạp thường không phải lựa chọn đầu tiên để phân tích và dự đoán.\nBạn cần phải thực sự hiểu những gì diễn ra đằng sau model và kết quả của chúng ta để thuyết phục các stackeholders, bởi vì họ có thể hỏi bạn:\n Tại sao lại không phát hiện ra lỗi này? Tại sao lỗi này lại bị nhầm sang lỗi kia? Tại sao độ chính xác lại thấp như vậy? Nếu xảy ra trường hợp này thì phải làm như thế nào? \u0026hellip;  Stackeholders không thể \u0026ldquo;nhắm mắt\u0026rdquo; cho sử dụng một model trong các sản phẩm thực tế khi mà họ chưa hiểu rõ (blackbox model), vì như thế nguy cơ thất thoát tiền bạc là rất lớn.\nĐây có lẽ cũng là một trong những lý do mà các models đơn giản như decision tree, logistic regression hay kNN vần còn còn được sử dụng khá nhiều trong các bài toán công nghiệp.\n5. Luôn luôn nhìn bức tranh tổng quát\nĐây có lẽ là một trong những lỗi phổ biến nhất khi bắt đầu với data science. Mọi người thường quá chú trọng vào việc làm sao train được model tốt, viết code cho đẹp mà không để ý đến việc model sẽ chạy trong thực tế như thế nào, tốc độ đáp ứng ra sao, \u0026hellip;\nNói chung ngay từ lúc bắt đầu dự án data science, bạn nên nhìn một cách tổng quát bài toán, từ việc hiểu rõ yêu cầu (cả function và non-function), chuẩn bị dữ liệu, huấn luyện model, triển khai model trong thực tế và cập nhật model. Có được cái nhìn rõ ràng về bức tranh lớn, mạch suy nghĩ và làm việc của bạn sẽ được xuyên suốt từ đầu đến cuối và không bị \u0026ldquo;khớp\u0026rdquo; khi chuyển tiếp giữa các giai đoạn với nhau.\n 6. Kết luận\nTrong bất kỳ lình vực nào, lý thuyết và thực tế luôn luôn có sự khác biệt. Sự khác biệt này càng rõ hơn trong phạm vi data science nói riêng. Hiễu được những điều này chính là bạn đã có được cái nhìn về bức tranh lớn của data science.\nHi vọng những điều chia sẽ trong bài viết này sẽ giúp ích cho các bạn trên con đường chinh phục sexy job. Hẹn gặp lại trong những bài viết sau!\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/18_data_scientist_theory_and_real/","tags":["Machine Learning","Data Science"],"title":"Nghề Data Scientis - Lý thuyết và thực tế - Sự khác biêt"},{"categories":["Machine Learning","XGBoost"],"contents":"Trong quá trình training, XGBoost thường xuyên phải thực hiện công việc chọn lựa ngẫu nhiên tập dữ liệu con (subsamples) từ tập dữ liệu gốc ban đầu. Các kỹ thuật để làm việc này được gọi bằng cái tên Stochastic Gradient Boosting (SGB).\nTrong bài này chúng ta sẽ cùng tìm hiểu về SGB và tuning SGB để tìm ra kỹ thuật phù hợp với bài toán.\n1. Tuning Row Subsampling\nRow subsampling liên quan đến việc chọn ngẫu nhiên các samples từ tập train set. Trong XGBoost, giá trị của row subsampling được chỉ ra bởi tham số subsample. Giá trị mặc định là 1, nghĩa là sử dụng toàn bộ tập train set, không subsampling.\nTiếp tục sử dụng tập Otto dataset, chúng ta sẽ grid-search tham số subsample với các giá trị như sau: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0].\nCó 9 giá trị của subsample, mỗi model sẽ được đánh giá sử dụng 10-fold cross-validation. Như vậy, có 9x10=90 models cần phải trained.\nCode đầy đủ như sau:\n# XGBoost on Otto dataset, tune subsample from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] param_grid = dict(subsample=subsample) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(subsample, means, yerr=stds) pyplot.title(\u0026#34;XGBoost subsample vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;subsample\u0026#39;) pyplot.ylabel(\u0026#39;Accuracy\u0026#39;) pyplot.savefig(\u0026#39;subsample.png\u0026#39;) Kết quả chạy:\nFitting 10 folds for each of 9 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 3.3min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 12.3min finished Best: 0.999919 using {\u0026#39;subsample\u0026#39;: 0.2} 0.999887 (0.000126) with: {\u0026#39;subsample\u0026#39;: 0.1} 0.999919 (0.000081) with: {\u0026#39;subsample\u0026#39;: 0.2} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.3} 0.999919 (0.000108) with: {\u0026#39;subsample\u0026#39;: 0.4} 0.999919 (0.000108) with: {\u0026#39;subsample\u0026#39;: 0.5} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.6} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.7} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.8} 0.999887 (0.000103) with: {\u0026#39;subsample\u0026#39;: 1.0} Độ chính xác của model đạt được bằng 0.999919 tại điểm subsample = 0.2, hay subset của mỗi model = 30% train set.\nĐồ thị bên dưới thể hiện mối quan hệ giữa subsample và accuracy.\n 2. Tuning Column Subsampling by Tree\nChúng ta cũng có thể tạo ra một tập ngẫu nhiên các input features để sử dụng cho mỗi decision tree. Trong XGBoost, điều này được cấu hình thông qua tham số colsample_tree. Giá trị mặc định của nó là 1, tức là tất cả các input features đều được sử dụng cho mỗi tree.\nTa sẽ thử tune tham số này với tập giá trị như sau: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\nCode đầy đủ:\n# XGBoost on Otto dataset, tune colsample_bytree from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] param_grid = dict(colsample_bytree=colsample_bytree) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(colsample_bytree, means, yerr=stds) pyplot.title(\u0026#34;XGBoost colsample_bytree vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;colsample_bytree\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;colsample_bytree.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 9 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 3.0min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 10.6min finished Best: 0.999887 using {\u0026#39;colsample_bytree\u0026#39;: 1.0} 0.998578 (0.000484) with: {\u0026#39;colsample_bytree\u0026#39;: 0.1} 0.999855 (0.000152) with: {\u0026#39;colsample_bytree\u0026#39;: 0.2} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.3} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.4} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.5} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.6} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.7} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.8} 0.999887 (0.000103) with: {\u0026#39;colsample_bytree\u0026#39;: 1.0} Độ chính xác của XGBoost đạt được là 0.999887 tại colsample_bytree = 1.0. Điều này có nghĩa rằng trong trường hợp này, subsampling column không mang lại giá trị nào.\nĐồ thị thể hiện mối quan hệ giữa subsampling column và accuracy.\n 3. Tuning Column Subsampling by Split\nThay vì subsampling column cho mỗi tree, ta có thể subsampling column ở mức node (hay Split). Tức là tại mỗi node, ta sẽ subsampling column để tìm ra 1 tập ngẫu nhiên các input features để quyết định hướng đi tiếp theo. Đây chính là điểm khác biệt giữa Random Forest và Bagging meta-data mà ta đã đề cập đến trong bài 2 của chuỗi các bài viết về XGBoost.\nSubsampling column ở mức node được cấu hình thông qua tham số colsample_bylevel. Ta sẽ tiến hành tune tham số này với giá trị thay đổi từ 10% đến giá trị mặc định ban đầu của nó (100%).\nCode đầy đủ như bên dưới:\n# XGBoost on Otto dataset, tune colsample_bylevel from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() colsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] param_grid = dict(colsample_bylevel=colsample_bylevel) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(colsample_bylevel, means, yerr=stds) pyplot.title(\u0026#34;XGBoost colsample_bylevel vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;colsample_bylevel\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;colsample_bylevel.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 9 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 2.6min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 9.3min finished Best: 0.999919 using {\u0026#39;colsample_bylevel\u0026#39;: 0.3} 0.999903 (0.000129) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.1} 0.999903 (0.000079) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.2} 0.999919 (0.000081) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.3} 0.999903 (0.000107) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.4} 0.999887 (0.000103) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.5} 0.999903 (0.000107) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.6} 0.999903 (0.000107) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.7} 0.999871 (0.000121) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.8} 0.999887 (0.000103) with: {\u0026#39;colsample_bylevel\u0026#39;: 1.0} colsample_bylevel = 0.3 cho ta model với độ chính xác cao nhất, 0.999919.\nĐồ thị thể hiện mối quan hệ giữa colsample_bylevel và accuracy.\n 6. Kết luận\nNhư vậy là chúng ta đã biêt cách tuning các kỹ thuật subsample hay stochastic gradient boosting của XGBoost. Nếu có phần cứng đủ mạnh, các bạn có thể tune tất cả các tham số đồng thời với nhau. Khi đó, độ chính xác của model có thể tăng lên 1 chút. Nhưng cái giá phải trả là thời gian train sẽ rất lâu. :D\nĐây cũng là bài cuối cùng trong loạt bài viết về XGBoost model. Hi vọng qua những bài viết của mình, các bạn có thể hiểu hơn về XGBoost và tự tin hơn khi làm viêc với nó. Hẹn mọi người ở những chủ để tiếp theo! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/17_tuning_subsampling/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 14: Tuning Subsample"},{"categories":["Machine Learning","XGBoost"],"contents":"Một vấn đề còn tồn tại của XGBoost là khả năng học trên tập dữ liệu huấn luyện một cách rất nhanh chóng. Điều này đôi khi dễ dẫn đến hiện tượng Overfitting, mặc dù XGBoost đã sử dụng regularization. Một cách hiệu quả để điều khiển quá trình học của XGBoost là sử dụng learning_rate (hay eta).\nTrong bài này, chúng ta sẽ cùng nhau tune learning_rate, learning_rate kết hợp với số lượng trees để tìm ra giá trị tối ưu của hai tham số này.\n1. Tuning Learning_Rate\nChúng ta tiếp tục sử dụng Otto dataset trong bài này. Sử dụng giá trị mặc định của số lượng trees là 100, ta sẽ đánh giá sự phù hợp của mỗi giá trị learning_rate trong tập sau: [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nCó 6 giá trị của learning_rate, kết hợp với 10-fold cross-validation \u0026ndash;\u0026gt; Có 60 models được trained.\nCode tuning như sau:\n# XGBoost on Otto dataset, Tune learning_rate from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] param_grid = dict(learning_rate=learning_rate) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(learning_rate, means, yerr=stds) pyplot.title(\u0026#34;XGBoost learning_rate vs Log Loss\u0026#34;) pyplot.xlabel(\u0026#39;learning_rate\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;learning_rate.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 6 candidates, totalling 60 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 26 tasks | elapsed: 9.1min [Parallel(n_jobs=-1)]: Done 60 out of 60 | elapsed: 13.8min finished Best: 0.999887 using {\u0026#39;learning_rate\u0026#39;: 0.001} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.2} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.3} Giá trị learning_rate tối ưu tìm được là 0.001.\nĐồ thị bên dưới thể hiện mối qua hệ giữa learning_rate và độ chính xác của model.\n 2. Tuning Learning_Rate và số lượng decision tree\nNói chung, khi có nhiều trees được thêm vào XGBoost, những trees thêm vào sau nên sử dụng giá trị learning_rate nhỏ. Ta sẽ kiểm tra nhận định này thông qua quá trình tuning như sau:\n Số lượng trees (n_estimators) = [100, 200, 300, 400, 500] learning_rate = [0.0001, 0.001, 0.01, 0.1]  Có 5 giá trị của n_estimators và 4 giá trị của learning_rate, kết hợp với 10-fold cross-validation ta có 200 models cần train.\nCode đầy đủ như dưới đây:\n# XGBoost on Otto dataset, Tune learning_rate and n_estimators from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot import numpy # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() n_estimators = [100, 200, 300, 400, 500] learning_rate = [0.0001, 0.001, 0.01, 0.1] param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot results scores = numpy.array(means).reshape(len(learning_rate), len(n_estimators)) for i, value in enumerate(learning_rate): pyplot.plot(n_estimators, scores[i], label=\u0026#39;learning_rate: \u0026#39; + str(value)) pyplot.legend() pyplot.xlabel(\u0026#39;n_estimators\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;n_estimators_vs_learning_rate.png\u0026#39;) Sau khoảng 2 tiếng chờ đơi thì chúng ta cũng thu được kết quả:\nFitting 10 folds for each of 20 candidates, totalling 200 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 6.2min [Parallel(n_jobs=-1)]: Done 168 tasks | elapsed: 58.2min [Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 67.6min finished Best: 0.999887 using {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 100} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 100} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 200} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 300} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 400} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 500} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 100} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 200} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 300} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 400} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 500} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 100} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 200} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 300} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 400} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 500} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 100} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 200} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 300} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 400} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 500} Ta có thể thấy, kết quả tốt nhất của model đạt được tại learning_rate=0.001 và n_estimators=100. Tuy nhiên, kết quả này cũng không có sự khác biệt đáng kể so với những trường hợp khác. Bạn có thể thử nghiệm với các metrics đánh giá khác (F1-score, precition, recall, log_loss) để nhìn thấy sự khác biệt rõ hơn.\nBên dưới là đồ thị thể hiện mối quan hệ của mỗi learning_rate với các giá trị khác nhau của n_estimators.\n 3. Kết luận\nỞ bài viết này, chúng ta đã tiến hành tuning XGBoost model với 2 hyper-parameters là learning_rate và n_estimators.\nBài viết tiếp theo chúng ta sẽ tiếp tục tune thêm một tham số khác là subsample. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/16_tuning_learning_rate_and_number_decition_tree/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 13: Tuning Learning_Rate và số lượng của Decision Tree"},{"categories":["Machine Learning","XGBoost"],"contents":"Ý tưởng cơ bản của thuật toán Gradient Boosting là lần lượt thêm các decision trees nối tiếp nhau. Tree thêm vào sau sẽ cố gắng giải quyết những sai sót của tree trước đó. Câu hỏi đặt ra là bao nhiêu trees (weak learner hay estimators) là đủ?\nTrong bài nãy, hãy cùng nhau tìm hiều cách lựa chọn số lượng và kích thước của các trees phù hợp với từng bài toán của các bạn.\n1. Tune số lượng của decision tree\nThông thường khi sử dụng GBM, ta thường chọn số lượng trees tương đối nhỏ. Có thể là vài chục, vài trăm, hoặc vài nghìn. Nguyên nhân có lẽ là vì tăng số lượng trees lên nhiều hơn, hiệu năng của model cũng không tăng, thậm chí còn giảm đi so với khi sử dụng số lượng trees ít hơn.\nMình sẽ sử dụng Otto dataset để minh họa việc tuning số lượng trees. Ở đây mình sử dụng 10-fold cross-validation, số lượng trees trong khoảng [50, 400, 50] -\u0026gt; Có 80 models được train.\nSố lượng của trees được chỉ ra bởi giá trị của tham số n_estimators.\n# XGBoost on Otto dataset, Tune n_estimators from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() n_estimators = range(50, 400, 50) param_grid = dict(n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(n_estimators, means, yerr=stds) pyplot.title(\u0026#34;XGBoost n_estimators vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;n_estimators\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;_estimators.png\u0026#39;) Kết quả:\nBest: -0.001155 using {\u0026#39;n_estimators\u0026#39;: 100} -0.001160 (0.001059) with: {\u0026#39;n_estimators\u0026#39;: 50} -0.001155 (0.001053) with: {\u0026#39;n_estimators\u0026#39;: 100} -0.001156 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 150} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 200} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 250} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 300} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 350} Số lượng trees phù hợp nhất là 100, neg_log_loss đạt được tại đó là -0.001055. Hiệu năng của model không được cải thiện khi tăng số lượng trees từ 100 lên 350.\nĐồ thị bên dưới thể hiện mối quan hệ giữa số lượng trees và inverted logarihmic:\n 2. Tune kích thước của decision tree\nKích thước của tree hay còn gọi là số lớp (layers) hay độ sâu (depth) của tree đó. Nếu tree quá nông (shallow) sẽ dẫn đến underfitting vì model chỉ học được rất ít chi tiết từ dữ liệu. Ngược lại, tree quá sâu (deep) thì model lại học quá nhiều chi tiết từ dữ liệu -\u0026gt; overfitting.\nKích thước của tree được chỉ ra bởi giá trị của tham số max_depth. Ta sẽ thử grid-seach tham số này theo phạm vi [1, 11, 2].\nCode đầy đủ như bê dưới:\n# XGBoost on Otto dataset, Tune max_depth from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(✬Agg✬) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() max_depth = range(1, 11, 2) print(max_depth) param_grid = dict(max_depth=max_depth) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(max_depth, means, yerr=stds) pyplot.title(\u0026#34;XGBoost max_depth vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;max_depth\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;max_depth.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 5 candidates, totalling 50 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 26 tasks | elapsed: 5.0min [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 7.8min finished Best: -0.001136 using {\u0026#39;max_depth\u0026#39;: 5} -0.001319 (0.001100) with: {\u0026#39;max_depth\u0026#39;: 1} -0.001153 (0.001066) with: {\u0026#39;max_depth\u0026#39;: 3} -0.001136 (0.001077) with: {\u0026#39;max_depth\u0026#39;: 5} -0.001150 (0.001063) with: {\u0026#39;max_depth\u0026#39;: 7} -0.001150 (0.001063) with: {\u0026#39;max_depth\u0026#39;: 9} Quan sát ouput, ta thấy rằng max_depth = 5 cho kết quả tốt nhất. Tăng giá trị này lên 7 hoặc 9, hiệu năng của model không những không được cải thiện mà còn kém đi.\nĐồ thị thể hiện mối quan hệ của kích thước tree và neg_log_loss.\n 3. Tune đồng thời số lượng và kích thước của decision tree\nCó một mối liên hệ giữa số lượng và kích thước của mỗi tree. Nhiều tree hơn thì kích thước của mỗi tree sẽ nhỏ hơn. Ngược lại, ít tree hơn thì kích thước của mỗi tree sẽ lớn hơn.\nĐể tìm ra cặp giá trị (n_estimators, max_depth) phù hợp, ta sẽ thử grid-search như sau:\n n_estimators: (50, 100, 150, 200) max_depth: (2, 4, 6, 8) 10-fold cross-validation -\u0026gt; 4x4x10 = 160 models  Code đầy đủ bên dưới:\n# XGBoost on Otto dataset, Tune n_estimators and max_depth from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot import numpy # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() n_estimators = [50, 100, 150, 200] max_depth = [2, 4, 6, 8] print(max_depth) param_grid = dict(max_depth=max_depth, n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot results scores = numpy.array(means).reshape(len(max_depth), len(n_estimators)) for i, value in enumerate(max_depth): pyplot.plot(n_estimators, scores[i], label=\u0026#39;depth: \u0026#39; + str(value)) pyplot.legend() pyplot.xlabel(\u0026#39;n_estimators\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;n_estimators_vs_max_depth.png\u0026#39;) Kết quả:\nBest: -0.001131 using {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 100} -0.001266 (0.001112) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 50} -0.001249 (0.001101) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 100} -0.001248 (0.001101) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 150} -0.001247 (0.001100) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 200} -0.001141 (0.001094) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 50} -0.001131 (0.001088) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 100} -0.001132 (0.001089) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 150} -0.001132 (0.001089) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 200} -0.001160 (0.001059) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 50} -0.001155 (0.001053) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 100} -0.001156 (0.001054) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 150} -0.001155 (0.001054) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 200} -0.001155 (0.001068) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 50} -0.001150 (0.001063) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 100} -0.001150 (0.001064) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 150} -0.001150 (0.001064) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 200} Từ kết quả ta thấy kết quả tốt nhất đạt được tại max_depth=4 và n_estimators=100, tương tự như kết quả của 2 lần tuning riêng rẽ 2 tham số ở bên trên.\nĐồ thị thể hiện mối quan hê của mỗi max_depth với các giá trị của n_estimators.\n Kết quả thể hiện trên đồ thị cũng minh họa cho nhận định về mối quan hệ giữa số lượng và kích thước của tree mà ta đã nói bên trên.\n6. Kết luận\nQua bài viết này, chúng ta đã biết cách tuning XGBoost model, sử dụng phương pháp grid-search (hỗ trợ bởi thư viện scikit-learn) để tìm được số lượng và kích thước của trees phù hợp với bài toán đặt ra ban đầu. Ngoài phương pháp này, còn có 1 phương pháp khác cũng rất hiệu quả là bayes (sử dụng định luật bayes). Phương pháp này thước được các ông lớn AWS, Google, \u0026hellip; sử dụng trong các dịch vụ về AI của họ.\nBài viết tiếp theo chúng ta sẽ tiếp tục tune learning_rate đồng thời với số lượng của tree trong XGboost model. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/15_tuning_number_and_size_decision_tree/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 12: Tuning số lượng và kích thước của Decision Tree"},{"categories":["Machine Learning","XGBoost"],"contents":"Thư viện XGBoost được thiết kế để tận dụng tối đa sức mạnh của phần cứng hệ thống, bao gồm tất cả CPU cores và bộ nhớ. Trong bài viết này, ta sẽ cùng nhau tìm hiểu cách thiết lập một server trên AWS để train XGBoost model, sao cho vừa nhanh, vừa rẻ! :D\nBài viết gồm 4 phần:\n Tạo tài khoản AWS Chạy AWS EC2 Instance Kết nối đến EC2 Instance và chạy code train XGBoost model Đóng AWS EC2 Instance  Chú ý quan trọng: Sẽ mất khoảng 1-2$ chi phí để sử dụng các dịch vụ của AWS trong bài viết này.\n1. Tạo tài khoản AWS\n(Nếu bạn đã có tài khoản AWS, hãy bỏ qua bước này!)\n Truy cập vào màn hình console của AWS. Tại đây ta có thể đăng nếu đã có tài khoản hoặc đăng ký tài khoản mới nếu chưa có.    Bạn cần cung cấp một số thông tin cần thiết, và đặc biệt là phải có một thẻ credit còn hiệu lực để có thể tiến hành tạo tài khoản. Các công đoạn khác, hãy làm theo chỉ dần trên màn hình.  2. Chạy AWS EC2 Instance\nChúng ta sẽ sử dụng dịch vụ EC2 để chạy XGBoost.\n Đăng nhập vào AWS console. Sau khi đăng nhập thành công, danh sách các dịch vụ của AWS sẽ hiển thị. Chọn EC2.    Click vào nút Launch EC2 Instance. Click vào Community AMIs    Nhập ami-1c40bf7d vào Search community AMIs và chọn Select.    Chọn EC2 Instance type là r4.8xlarge (32 cores CPU). Click Review and Launch. Click Launch. Chọn Create a new key pair, điền tên của key là xgboost-key và click Download Key Pair`.   Lưu file key vào máy tính ở local, sau đó click Launch EC2 Instances.\n Click View EC2 Instances. Chờ khoảng 3 phút và kiểm tra trạng thái của EC2 Instance.   Nếu trạng thái là Running thì tức là đã tạo EC2 Instance thành công. Ta cũng để ý thấy địa chỉ public IP của EC2 Instance là: 54.92.106.10. Tiếp theo ta sẽ sử dụng địa chỉ này để kết nối đến EC2 Instance từ localhost thông qua giao thức SSH.\n3. Kết nối đến EC2 Instance và chạy code\n3.1 Kết nối đến EC2 Instance qua giao thức ssh\n Trên máy tính local (mình dùng Ubuntu), mở cửa sổ Terminal và gõ lệnh:  $ cd Documents #Thư mục chứa key file $ chmod 600 xgboost-key.pem $ ssh -i xgboost.pem fedora@54.92.106.10 Nếu đây là lần đầu kết nối đến EC2 Instance, sẽ có 1 cảnh báo xuất hiện. Gõ yes.\nNếu kết nối thành công, màn hình Terminal sẽ xuất hiện như sau:\n  Kiểm ra số lượng CPU cores  $cat /proc/cpuinfo | grep processor | wc -l Kết quả:\n32 3.2 Cài đặt các thư viện cần thiết\n Cài đặt GCC, Python và SciPy  sudo dnf install gcc gcc-c++ make git unzip python python3-numpy python3-scipy python3-scikit-learn python3-pandas python3-matplotlib  Cài đặt Cmake  XGBoost yêu cầu cmake \u0026gt;= 3.13. Nếu bạn cài bằng bằng lệnh dnf install cmake thì phiên bản của make la 3.9. Để cài cmake \u0026gt;= 3.13, bạn phải cài build từ source.\n$ wget https://github.com/Kitware/CMake/releases/download/v3.15.2/cmake-3.15.2.tar.gz $ tar -zxvf cmake-3.15.2.tar.gz $ cd cmake-3.15.2 $ ./bootstrap $ make $ sudo make install Kiểm tra GCC:\n$gcc --version Kết quả:\n[fedora@ip-172-31-37-253 ~]$ gcc --version gcc (GCC) 6.3.1 20161221 (Red Hat 6.3.1-1) Kiểm tra Python:\n$ python3 --version Kết quả:\nPython 3.5.1 Kiểm tra SciPy:\n$ python3 -c \u0026quot;import scipy;print(scipy.__version__)\u0026quot; $ python3 -c \u0026quot;import numpy;print(numpy.__version__)\u0026quot; $ python3 -c \u0026quot;import pandas;print(pandas.__version__)\u0026quot; $ python3 -c \u0026quot;import sklearn;print(sklearn.__version__)\u0026quot; Kết quả:\n0.16.1 1.11.0 0.18.0 0.17.1 Kiểm tra Cmake\n$ cmake --version Kết quả:\n3.15.1 3.3. Cài đặt thư viện XGBoost\n$ pip3 install xgboost==1.1 Tại thời điểm viết bài, phiên bản mới nhất của xgboost là 1.2. Nhưng vì phiên bản của python=3.5 nên bạn chỉ có thể sử dụng được phiên bản 1.1 của xgboost.\nKiểm tra:\n$ python3 -c \u0026quot;import xgboost;print(xgboost.__version__)\u0026quot; Kết quả:\n1.1.0 4. Train XGBoost model\nTương tự như ở bài 9, chúng ta cũng sẽ sử dụng Otto dataset để kiểm tra khả năng của XGBoost model theo số lượng cores của CPU.\n Tạo thư mục xgboost trên máy local, tạo file check_num_threads.py với code như sau:  # Otto multi-core test from pandas import read_csv from xgboost import XGBClassifier from sklearn.preprocessing import LabelEncoder from time import time # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # evaluate the effect of the number of threads results = [] num_threads = [1, 16, 32] for n in num_threads: start = time() model = XGBClassifier(nthread=n) model.fit(X, label_encoded_y) elapsed = time() - start print(n, elapsed) results.append(elapsed) Copy file train.csv ở bài trước vào thư mục xgboost.\n Tại cửa sổ Terminal của máy local, copy thư mục xgboost lên EC2 Instance:  $ scp -i xgboost-key.pem -r xgboost fedora@54.92.106.10:/home/fedora  Tại của sổ Terminal kết nôi tới EC2 Instance, tiến hành chạy code:  $ cd xgboost python3 check_num_threads.py Kết quả thu được:\n1 70.75178146362305 16 6.106862545013428 32 5.045598745346069 Sử dụng 32 cores, mất 5s để train XGBoost model với tập dữ liệu tương đối lớn. Đây quả là một kết quả ấn tượng. :D\nBonus:: Trong trường hợp việc train model mất nhiều thời gian hơn mà chẳng may bạn bị mất kết nối đến EC2 Instance giữa chừng thì thế nào? Bạn phải chạy train lại từ đầu ư? Quả là mất thời gian phải không? Giải pháp để ngăn chặn tình huống này, hoặc là bạn chạy lệnh train ở chế độ background process và ghi kết quả ra một file như lệnh sau:\n$ nohup python script.py \u0026gt;script.py.out 2\u0026gt;\u0026amp;1 \u0026amp; hoặc bạn cũng có thể sử dụng Tmux.\n5. Tắt EC2 EC2 Instance\nTiết kiệm là quốc sách hàng đầu, hãy luôn nhớ tắt EC2 EC2 Instance mỗi khi sử dụng xong. Bản thân mình đã từng một lần quên không tắt trong vài ngày. Kết quả là con số trên hóa đơn AWS tháng đó làm mình buồn mất cả tuần, :).\nĐể tắt EC2 EC2 Instance, đơn giản là làm theo như hình vẽ sau:\n  Chọn 1-\u0026gt;2-\u0026gt;3 nếu bạn muốn tắt tạm thời (khi nào muốn dùng thì khởi động lên). Chọn 1-\u0026gt;2-\u0026gt;4 nếu bạn muốn xóa hẳn EC2 EC2 Instance này.  6. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách cài đặt vào cấu hình EC2 Instance để train XGBoost model trên AWS.\nBài viết tiếp theo sẽ thiên về lý thuyết một chút, chúng ta sẽ tìm hiểu cách cấu hình hyper-parameters cho gradient boosting model. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/14_train_xgboost_models_on_aws/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 11: Train XGBoost model trên AWS"},{"categories":["Machine Learning","XGBoost"],"contents":"Thư viện XGBoost được thiết kế để làm việc h iệu quả vớicơ chế xử lý song song trên nhiều core (multithreading) của phần cứng, cả trong quá trình train và dự đoán. Hãy cùng nhau tìm hiểu cơ chế đó thông qua bài viết này.\n1. Chuẩn bị dataset\nChúng ta sẽ sử dụng Otto Group Product Classification Challenge dataset để minh họa cơ chế multithreading của thư viện XGBoost. Để download dataset này, bạn cần đăng nhập vào Kaggle. có 2 file là train.csv và test.csv. Vì chỉ có file train.csv là có nhãn nên ta sẽ sử dụng file này. Download file train.csv về máy tính (dạng train.csv.zip). Giải nén nó ra và đặt trong thư mục làm việc của bạn.\nDataset này bao gồm khoảng 94.000 sản phẩm và 93 input featuresđược chia thành 10 nhóm (ví dụ: thời trang, điện tử, \u0026hellip;). Mục tiêu là xây dựng một model để phân loại một sản phẩm mới vào các nhóm này. Cuộc thi này đã kết thúc vào 05/2015 và người chiến thắng cũng sử dụng XGBoost để tạo model.\n2. Ảnh hưởng của số lượng threads đến thời gian train model\nXGBoost được viết bằng C++, sử dụng OpenMP API để phát triển cơ chế xử lý song song. Trong một số trường hợp, bạn có thể phải compile mại XGBoost mới có thể sử dụng cơ chế song song này. Chi tiết về việc cài đặt XGBoost, có thể xem tại đây.\nHai lớp XGBClassifier và XGBRegressor trong thư viện XGBoost cung cấp tham số nthread để chỉ ra số lượng threads mà XGBoost sử dụng trong quá trình train. Mặc định thì giá trị của tham số này là -1, tức là sử dụng tất cả các core của hệ thống.\nmodel = XGBClassifier(nthread=-1) Bây giờ chúng ta sẽ xây dựng một số XGBoost models khác nhau theo số lượng threads sử dụng và đo đặc thời gian train của mỗi model.\nCode snippet:\n# evaluate the effect of the number of threads results = [] num_threads = [1, 2, 3, 4, 5, 6, 7, 8] for n in num_threads: start = time.time() model = XGBClassifier(nthread=n) model.fit(X_train, y_train) elapsed = time.time() - start print(n, elapsed) results.append(elapsed) Áp dụng vào bộ dữ liệu Otto (bạn có thể thay đổi giá trị của mảng num_threads theo máy tính của bạn):\nfrom matplotlib import pyplot # load data data = read_csv(\u0026#39;test.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # evaluate the effect of the number of threads results = [] num_threads = [1, 2, 3, 4] for n in num_threads: start = time() model = XGBClassifier(nthread=n) model.fit(X, label_encoded_y) elapsed = time() - start print(n, elapsed) results.append(elapsed) # plot results pyplot.plot(num_threads, results) pyplot.ylabel(\u0026#39;Speed (seconds)\u0026#39;) pyplot.xlabel(\u0026#39;Number of Threads\u0026#39;) pyplot.title(\u0026#39;XGBoost Training Speed vs Number of Threads\u0026#39;) pyplot.show() Chạy code trên thu được kết quả:\n1 50.13007640838623 2 26.999273538589478 3 18.629448890686035 4 15.561982154846191 5 13.940500497817993 6 12.550707578659058 7 13.075348854064941 8 12.36113166809082 và đồ thị thể hiện mối quan hệ giữa số lượng threads và thời gian train model.\n Có thể thấy rõ xu hướng giảm của thời gian train model khi số lượng threads tăng lên. Nếu kết quả chạy trên máy của bạn không giống như vậy, bạn cần phải xem xét lại cách enable cơ chế song song của XGBoost như link tham khảo mình đề cập bên trên.\n3. Ảnh hưởng của số lượng threads đến thời gian cross-validation\nk-fold cross-validation hỗ trợ bởi thư viện scikit-learn cũng có cơ chế xử lý song song tương tự như XGBoost. Tham số n_jobs của hàm cross_val_crore() chỉ ra số lượng threads sử dụng. Mặc định, n_jobs=1 tức là chỉ sử dụng 1 core (1 thread). Ta có thể gán giá trị -1 cho nó để sử dụng tất cả cores của hệ thống.\nresults = cross_val_score(model, X, label_encoded_y, cv=kfold,scoring=\u0026#39;neg_log_loss\u0026#39;, n_jobs=-1, verbose=1) Đến đây lại xuất hiện một câu hỏi, chúng ta nên chọn phương án nào trong 3 phương án sau:\n Disable multithreading của XGBoost, enable multithreading của cross-validation. Enable multithreading của XGBoost, disable multithreading của cross-validation. Enable multithreading của XGBoost, enable multithreading của cross-validation.  Để trả lời câu hỏi này, không gì chính xác hơn là chúng ta sẽ code cho cả 3 cách và so sánh thời gian thực thi của mỗi cách:\n# Otto, parallel cross validation from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import cross_val_score from sklearn.preprocessing import LabelEncoder import time # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # prepare cross validation kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) # Single Thread XGBoost, Parallel Thread CV start = time.time() model = XGBClassifier(nthread=1) results = cross_val_score(model, X, label_encoded_y, cv=kfold, coring=\u0026#39;neg_log_loss\u0026#39;, n_jobs=-1) elapsed = time.time() - start print(\u0026#34;Single Thread XGBoost, Parallel Thread CV: %f\u0026#34; % (elapsed)) # Parallel Thread XGBoost, Single Thread CV start = time.time() model = XGBClassifier(nthread=-1) results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=\u0026#39;neg_log_loss, n_jobs=1) elapsed = time.time() - start print(\u0026#34;Parallel Thread XGBoost, Single Thread CV: %f\u0026#34; % (elapsed)) # Parallel Thread XGBoost and CV start = time.time() model = XGBClassifier(nthread=-1) results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=\u0026#39;neg_log_loss\u0026#39;, n_jobs=-1) elapsed = time.time() - start print(\u0026#34;Parallel Thread XGBoost and CV: %f\u0026#34; % (elapsed)) Kết quả cuối cùng:\nSingle Thread XGBoost, Parallel Thread CV: 101.820478 Parallel Thread XGBoost, Single Thread CV: 455.847770 Parallel Thread XGBoost and CV: 98.794466 Rõ ràng, phương án thứ 3 sử dụng thời gian ít nhất. Bây giờ bạn đã biết câu trả lời rồi phải không? :D\n4. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách cấu hình cơ chế multithreading của XGBoost model. Chúng ta cũng nhận thức được ảnh hưởng của số lượng threads (số lượng cores) đến thời gian train model, từ đó biết cách kiểm tra xem hệ thống có hỗ trợ cơ chế xử lý song song của XGBoost hay không? Cuối cùng, cách cấu hình tốt nhất cho cả XGBoost và cross-validation để giảm thời gian thực thi cũng đã được tìm ra.\nỞ bài viết tiếp theo chúng ta sẽ tìm hiểu cách scale-up XGBoost model để sử dụng nhiều cores của hệ thống hơn trên AWS cloud. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/13_multithreading-xgboost/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 10: Cấu hình Multithreading cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Overfitting vẫn luôn là một vấn đề làm đau đầu những kỹ sư AI. Trong bài viết này chúng ta sẽ cùng tìm hiểu cách thức monitor (giám sát) performance (hiệu năng) của XGBoost model trong suốt quá trình train. Từ đó cấu hình early stopping để quyết định khi nào thì nên dừng lại quá trình này để tránh hiện tượng overfitting. Bài viết gồm 2 phần:\n Monitor hiệu năng của XGBoost model thông qua learning curve (đường cong học tập). Cấu hình early stopping.  1. Giám sát hiệu năng của XGBoost model\nĐể monitor porformance của XGBoost model, ta cần cung cấp cả train set, test set và một metric (chỉ tiêu đánh giá) khi train model (gọi hàm model.fit()). Ví dụ, để tính toán error metric trên tập test set, sử dụng code snippet sau:\neval_set = [(X_test, y_test)] model.fit(X_train, y_train, eval_metric=\u0026#34;error\u0026#34;, eval_set=eval_set, verbose=True) XGBoost model hỗ trợ một số metric như sau:\n rmse: root mean squared error. mae: mean absolute error. logloss: binary logarithmic loss. mlogloss: multiclass log loss (cross entropy). error: classification error. auc: area under ROC curve. Danh sách đầy đủ các metrics, các bạn có thể xem tại đây.  Code dưới đây minh hoạ việc monitor performance trong quá trình train một XGBoost model trên tập dữ liệu Pima Indians onset of diabetes.\n# monitor training performance from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on training data model = XGBClassifier() eval_set = [(X_test, y_test)] model.fit(X_train, y_train, eval_metric=\u0026#34;error\u0026#34;, eval_set=eval_set, verbose=True) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Ở đây, ta sử dụng 67% dữ liệu cho việc train, 33% còn lại cho viêc đánh giá model. Error metric được tính toán tại cuối mỗi vòng lặp (sau khi mỗi boosted tree được thêm vào model). Cuối cùng, độ chính xác của model được in ra.\nKết quả hiển thị trên màn hình như sau:\n[0]\tvalidation_0-error:0.28347 [1]\tvalidation_0-error:0.25984 [2]\tvalidation_0-error:0.25591 [3]\tvalidation_0-error:0.24803 [4]\tvalidation_0-error:0.24409 [5]\tvalidation_0-error:0.24803 [6]\tvalidation_0-error:0.25591 [7]\tvalidation_0-error:0.24803 [8]\tvalidation_0-error:0.25591 [9]\tvalidation_0-error:0.24409 ... [89]\tvalidation_0-error:0.26378 [90]\tvalidation_0-error:0.27165 [91]\tvalidation_0-error:0.26772 [92]\tvalidation_0-error:0.27165 [93]\tvalidation_0-error:0.26378 [94]\tvalidation_0-error:0.27165 [95]\tvalidation_0-error:0.26378 [96]\tvalidation_0-error:0.25984 [97]\tvalidation_0-error:0.26378 [98]\tvalidation_0-error:0.25984 [99]\tvalidation_0-error:0.25984 Accuracy: 74.02% Quan sát kết quả ta thấy performance của model không thay dổi quá nhiều trong suốt quá trình train. Thậm chí đến cuối quá trình, performance còn kém hơn so với nửa đầu.\nĐể có cái nhìn tường minh hơn, hãy thể hiện performance của model trên đồ thị. Ta sẽ monitor performace của model trên cả train set và test set:\neval_set = [(X_train, y_train), (X_test, y_test)] model.fit(X_train, y_train, eval_metric=\u0026#34;error\u0026#34;, eval_set=eval_set, verbose=True) Performance của model trên mỗi tập evaluation set được lưu bởi model sau khi train kết thúc. Để truy cập giá trị performace này, sử dụng hàm model.evals_result():\nresults = model.evals_result() print(results) Kết quả in ra sẽ giống như sau:\n{ \u0026#39;validation_0\u0026#39;: {\u0026#39;error\u0026#39;: [0.259843, 0.26378, 0.26378, ...]}, \u0026#39;validation_1\u0026#39;: {\u0026#39;error\u0026#39;: [0.22179, 0.202335, 0.196498, ...]} } validation_0 và validation_1 theo thứ tự tương ứng với hai tập validation set mà ta đã định nghĩa trong tham số eval_set khi gọi hàm fit().\nError metric được truy cập như sau:\nresults[\u0026#39;validation_0\u0026#39;][\u0026#39;error\u0026#39;] results[\u0026#39;validation_1\u0026#39;][\u0026#39;error\u0026#39;] Thêm nữa, bạn có thể lựa chọn nhiều metrics để đánh giá model bằng cách cung cấp một mảng các giá trị metric tới tham số eval_metric của hàm fit(). Giá trị của các metric thu được sau đó đươc thể hiện trên đồ thị, gọi là learning curve.\nCode đầy đủ dưới đây minh họa việc thu thập giá tị của các metrics và thể hiện trên learning curve:\n# plot learning curve from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on training data model = XGBClassifier() eval_set = [(X_train, y_train), (X_test, y_test)] model.fit(X_train, y_train, eval_metric=[\u0026#34;error\u0026#34;, \u0026#34;logloss\u0026#34;], eval_set=eval_set, verbose=True) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) # retrieve performance metrics results = model.evals_result() epochs = len(results[\u0026#39;validation_0\u0026#39;][\u0026#39;error\u0026#39;]) x_axis = range(0, epochs) # plot log loss fig, ax = pyplot.subplots() ax.plot(x_axis, results[\u0026#39;validation_0\u0026#39;][\u0026#39;logloss\u0026#39;], label=\u0026#39;Train\u0026#39;) ax.plot(x_axis, results[\u0026#39;validation_1\u0026#39;][\u0026#39;logloss\u0026#39;], label=\u0026#39;Test\u0026#39;) ax.legend() pyplot.ylabel(\u0026#39;Log Loss\u0026#39;) pyplot.title(\u0026#39;XGBoost Log Loss\u0026#39;) pyplot.show() # plot classification error fig, ax = pyplot.subplots() ax.plot(x_axis, results[\u0026#39;validation_0\u0026#39;][\u0026#39;error\u0026#39;], label=\u0026#39;Train\u0026#39;) ax.plot(x_axis, results[\u0026#39;validation_1\u0026#39;][\u0026#39;error\u0026#39;], label=\u0026#39;Test\u0026#39;) ax.legend() pyplot.ylabel(\u0026#39;Classification Error\u0026#39;) pyplot.title(\u0026#39;XGBoost Classification Error\u0026#39;) pyplot.show() Chạy code trên, error và logloss metric trên cả 2 tập train set và test set được in ra. Ta có thể bỏ qua điều này bằng cách truyền giá trị False (giá trị mặc định) cho tham sô verbose khi gọi hàm fit(). Hai đồ thị được tạo ra. Đồ thị đầu tiên thể hiện logloss của XGBoost model đối với mỗi epoch (iteration) trong quá trình train.\n Đồ thị thứ 2 hiển thị error metric của mỗi epoch.\n Quan sát cả 2 đồ thị trên ta có một nhận xét rằng, nếu dừng train sớm hơn tại epoch \u0026lt; 100 thì performace của model sẽ tốt hơn. Đây chính là tiền đề của kỹ thuật early stopping mà chúng ta sẽ tìm hiểu ngay sau đây.\n2. Cấu hình early stopping cho XGBoost model\nEarly stopping là một kỹ thuật khá phổ biến áp dụng cho các ML model phức tạp để tránh hiện tượng overfitting. Nó làm việc bằng cách monitor performance của model trên tập test set trong suốt quá trình train và buộc quá trình này dừng lại một khi performance của model không được cải thiện sau một số epochs nhất định.\nTrên đồ thị learning curve, điểm bắt đầu overfitting là điể mà tại đó performace của model trên tập test set bắt đầu giảm trong khi performance của model trên tập train set vẫn tiếp tục tăng.\nĐể cấu hình early stopping cho XGBoost model, cần cung cấp thêm giá trị cho tham số early_stopping_rounds khi gọi hàm fit(). Ý nghĩa của nó là chỉ ra số lượng epochs mà quá trình húân luyện trải qua mà performance không có sự cải thiện nào.\nVí dụ:\neval_set = [(X_test, y_test)] model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\u0026#34;logloss\u0026#34;, eval_set=eval_set, verbose=True) Nếu có nhiều evaluation sets hoặc nhiều metrics được cung cấp, early stopping sẽ sử dụng cái cuối cùng trong danh sách.\nCode đầy đủ cấu hình early stopping như bên dưới:\n# early stopping from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() eval_set = [(X_test, y_test)] model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\u0026#34;logloss\u0026#34;, eval_set=eval_set, verbose=True) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Kết quả chạy code:\n[0]\tvalidation_0-logloss:0.60491 Will train until validation_0-logloss hasn\u0026#39;t improved in 10 rounds. [1]\tvalidation_0-logloss:0.55934 [2]\tvalidation_0-logloss:0.53068 [3]\tvalidation_0-logloss:0.51795 [4]\tvalidation_0-logloss:0.51153 [5]\tvalidation_0-logloss:0.50935 [6]\tvalidation_0-logloss:0.50818 [7]\tvalidation_0-logloss:0.51097 [8]\tvalidation_0-logloss:0.51760 [9]\tvalidation_0-logloss:0.51912 [10]\tvalidation_0-logloss:0.52503 [11]\tvalidation_0-logloss:0.52697 [12]\tvalidation_0-logloss:0.53335 [13]\tvalidation_0-logloss:0.53905 [14]\tvalidation_0-logloss:0.54546 [15]\tvalidation_0-logloss:0.54613 [16]\tvalidation_0-logloss:0.54982 Stopping. Best iteration: [6]\tvalidation_0-logloss:0.50818 Accuracy: 74.41% Quá trình train model dừng lại ở epoch 16 (gần với những gì mà chúng ta phán đoán dựa trên đồ thị learning curve) và model đạt được metric thấp nhất tại epoch 6. Việc chọn giá trị của tham số early_stopping_rounds thường dựa vào quan sát trên đồ thị learning curve. Nếu bạn không biết thì có thể chọn giá trị mặc định là 10.\n3. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách monitor performance của XGBoost model trong quá trình train và cấu hình early stopping để hạn chế hiện tượng overfitting của model.\nỞ bài viết tiếp theo chúng ta sẽ tìm hiểu cách cấu hình XGBoost model để tận dụng hết tài nguyên của phần cứng khi train model và khi sử model để dự đoán. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/12_early_stopping/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 9: Cấu hình Early_Stopping cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Feature selection hay lựa chọn features là một bước tương đối quan trọng trước khi train XGBoost model. Lựa chọn đúng các features sẽ giúp model khái quát hóa vấn đề tốt hơn (low variance) -\u0026gt; đạt độ chính xác cao hơn.\nTrong bài viết này, hãy cùng xem xét về cách dùng thư viện XGBoost để tính importance scores và thể hiện nó trên đồ thị, sau đó lựa chọn các features để train XGBoost model dựa trên importance scores đó.\n1. Tính và hiển thị importance score trên đồ thị\n1.1 Cách 1\nModel XGBoost đã train sẽ tự động tính toán mức độ quan trọng của các features. Các giá trị này được lưu trong biến feature_importances_ của model đã train. Kiểm tra bằng cách:\nprint(model.feature_importances_) Thể hiện các features importance lên đồ thị:\n# plot pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_) pyplot.show() Code dưới đây minh họa đầy đủ việc train XGBoost model trên tập dữ liệu Pima Indians onset of diabetes và hiển thị các features importances lên đồ thị:\n# plot feature importance manually from numpy import loadtxt from XGBoost import XGBClassifier from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] y = dataset[:,8] # fit model on training data model = XGBClassifier() model.fit(X, y) # feature importance print(model.feature_importances_) # plot pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_) pyplot.show() Chạy code trên, importance score được in ra:\n[0.10621197 0.2424023 0.08803366 0.07818192 0.10381887 0.1486732 0.10059207 0.13208601] và đồ thị:\n 1.2 Cách 2\nNhược điểm của cách này là các importance scores được sắp xếp theo thứ tự của các features trong tập dataset. Điều này làm cho chúng ta khó quan sát trong trường hợp số lượng features lớn. Liệu có thể sắp thứ tự các importance scores này theo giá trị của chúng được hay không? Câu trả lời là có thể. Thư viện XGBoost có một hàm gọi là plot_importance() giúp chúng ta thực hiện việc này.\n# plot feature importance plot_importance(model) pyplot.show() Code dưới đây minh họa đầy đủ việc train XGBoost model trên tập dữ liệu Pima Indians onset of diabetes và hiển thị các features importances lên đồ thị:\n# plot feature importance using built-in function from numpy import loadtxt from XGBoost import XGBClassifier from XGBoost import plot_importance from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] y = dataset[:,8] # fit model on training data model = XGBClassifier() model.fit(X, y) # plot feature importance plot_importance(model) pyplot.show() Chạy code ví dụ bên trên thu được kết quả:\n Quan sát đồ thị ta thấy, các features được tự động đặt tên từ f0 đến f7 theo thứ tự của chúng trong mảng dữ liệu input X. Từ đồ thị có thể kết lụân rằng:\n f6 có importance score cao nhất - 333 f4 có importance score thấp nhất - 124  Nếu có bảng mô tả dữ liệu, ta có thể ánh xạ f4, f6 thành tên các features tương ứng.\n2. Lựa chọn features (feature selection) theo importance scores\nThư viện scikit-learn cung cấp lớp SelectFromModel cho phép lựa chọn các features để train model. Lớp này yêu cầu 2 tham số bắt buộc:\n model: model đã được train trên toàn bộ dataset. threshold: ngưỡng để lựa chọn features. Chỉ những features có importance score không nhỏ hơn ngưỡng mới được lựa chọn. Sau khi gọi hàm transform() thì lớp SelectFromModel sẽ chuyển đổi tập dữ liệu ban đầu thành tập dữ liệu nhỏ hơn chỉ bao gồm các features được chọn.  # select features using threshold selection = SelectFromModel(model, threshold=thresh, prefit=True) select_X_train = selection.transform(X_train) Sau khi có tập dữ liệu mới, ta tiến hành train và đánh giá model mới tạo ra như bình thường.\n# train model selection_model = XGBClassifier() selection_model.fit(select_X_train, y_train) # eval model select_X_test = selection.transform(X_test) y_pred = selection_model.predict(select_X_test) Trong các bài toán thực tế, ta thường không biết chính xác giá trị nào của threshold là phù hợp. Vì vậy mà ta sẽ tuning giá trị này bằng phương pháp grid-seach (mình sẽ có 1 bài viết riêng giải thích chi tiết về các phương pháp tuning hyper-parameters. Ở đây, bạn chỉ cần hiểu một cách đơn giản là kiểm tra với nhiều giá trị của threshold để chọn ra giá trị tốt nhất). Chúng ta sẽ bắt đầu kiểm tra với tất cả features, kết thúc với feature quan trọng nhất.\nCode hoàn chỉnh như bên dưới:\n# use feature importance for feature selection from numpy import loadtxt from numpy import sort from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.feature_selection import SelectFromModel # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on all training data model = XGBClassifier() model.fit(X_train, y_train) # make predictions for test data and evaluate predictions = model.predict(X_test) accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) # Fit model using each importance as a threshold thresholds = sort(model.feature_importances_) for thresh in thresholds: # select features using threshold selection = SelectFromModel(model, threshold=thresh, prefit=True) select_X_train = selection.transform(X_train) # train model selection_model = XGBClassifier() selection_model.fit(select_X_train, y_train) # eval model select_X_test = selection.transform(X_test) predictions = selection_model.predict(select_X_test) accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Thresh=%.3f, n=%d, Accuracy: %.2f%%\u0026#34; % (thresh, select_X_train.shape[1], accuracy*100.0)) Chạy code trên thu được kết quả như sau:\nAccuracy: 74.02% Thresh=0.088, n=8, Accuracy: 74.02% Thresh=0.089, n=7, Accuracy: 71.65% Thresh=0.098, n=6, Accuracy: 71.26% Thresh=0.098, n=5, Accuracy: 74.41% Thresh=0.100, n=4, Accuracy: 74.80% Thresh=0.136, n=3, Accuracy: 71.26% Thresh=0.152, n=2, Accuracy: 71.26% Thresh=0.240, n=1, Accuracy: 67.32% Có thể thấy rằng độ chính xác của model cao nhất trên tập dữ liệu gồm 4 features quan trọng nhất và thấp nhất trên tập dữ liệu chỉ gồm một feature.\nTuning theo kiểu grid-seach như này đặc biệt hiệu quả trong trường hợp bộ dữ liệu lớn.\n3. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách thể hiện importance score của các features trên đồ thị và sử dụng importance score để lựa chọn các features sao cho model đạt được độ chính xác cao nhất.\nBài viết tiếp theo ta sẽ tìm hiểu cách giám sát (monitor) hiệu năng của model trong quá trình train và cấu hình early stop (dừng train khi model đáp ứng một tiêu chí nào đó). Hai kỹ thuật này rất cần thiết để train một XGBoost model tốt. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/11_feature-selection/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 8: Lựa chọn features cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Giả sử bạn đã train xong một XGBoost model đạt được độ chính xác rất cao. Câu hỏi đặt ra là làm sao lưu lại model đó để sử dụng về sau (không phải mất công train lại model mỗi khi cần sử dụng)?\nTrong bài viết này, chúng ta hãy cùng tìm hiểu cách thức lưu một XGBoost model thành 1 file sử dụng Python pickle API. Nội dung bài viết gồm 2 phần chính:\n Lưu và sử dụng XGBoost model bằng thư viện pickle. Lưu và sử dụng XGBoost model bằng thư viện joblib.  1. Lưu và sử dụng XGBoost model bằng thư viện pickle.\nPickle là một cách chuẩn chỉ để lưu một dối tượng trong Python thành một file. Cách sử dụng tương đối đơn giản.\n Lưu model thành file  # save model to file pickle.dump(model, open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;wb\u0026#34;))  Gọi model đã lưu để sử dụng  # load model from file loaded_model = pickle.load(open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;rb\u0026#34;)) Ví dụ dưới đây mình họa việc train một XGBoost model trên tập dữ liệu Pima Indians onset of diabetes, lưu model thành file và gọi model đã lưu để dự đoán.\n# Train XGBoost model, save to file using pickle, load and make predictions from numpy import loadtxt from XGBoost import XGBClassifier import pickle from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) # save model to file pickle.dump(model, open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;wb\u0026#34;)) print(\u0026#34;Saved model to: pima.pickle.dat\u0026#34;) # some time later... # load model from file loaded_model = pickle.load(open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;rb\u0026#34;)) print(\u0026#34;Loaded model from: pima.pickle.dat\u0026#34;) # make predictions for test data predictions = loaded_model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Kết quả:\nSaved model to: pima.pickle.dat Loaded model from: pima.pickle.dat Accuracy: 74.02% 2. Lưu và sử dụng XGBoost model bằng thư viện joblib.\nJoblib là một phần của hệ sinh thái SciPy, nó cũng hỗ trợ việc lưu ML model thành file rât dễ dàng, sử dụng cấu trúc dữ liệu của NumPy. Ưu điểm của viêc sử dụng joblib so với pickle là nó hoạt động khá nhanh, đặc biệt với những model có kích thước lớn. Cách sử dụng:\n Lưu model thành file  # save model to file joblib.dump(model, \u0026#34;pima.joblib.dat\u0026#34;)  Sử dụng model đã lưu  # load model from file loaded_model = joblib.load(\u0026#34;pima.joblib.dat\u0026#34;) Ví dụ dưới đây mình họa việc train một XGBoost model trên tập dữ liệu Pima Indians onset of diabetes, lưu model thành file và gọi model đã lưu để dự đoán.\n# Train XGBoost model, save to file using joblib, load and make predictions from numpy import loadtxt from XGBoost import XGBClassifier from joblib import dump from joblib import load from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) # save model to file dump(model, \u0026#34;pima.joblib.dat\u0026#34;) print(\u0026#34;Saved model to: pima.joblib.dat\u0026#34;) # some time later... # load model from file loaded_model = load(\u0026#34;pima.joblib.dat\u0026#34;) print(\u0026#34;Loaded model from: pima.joblib.dat\u0026#34;) # make predictions for test data predictions = loaded_model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Output của đoạn code trên:\nSaved model to: pima.joblib.dat Loaded model from: pima.joblib.dat Accuracy: 74.02% 3. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách thức lưu XGBoost model thành file sử dụng pickle và joblib, sau đó gọi lại model đã lưu từ file để dự đoán.\nBài viết tiếp theo sẽ tìm hiểu cách tính toán và lựa chọn các features tốt nhất cho việc train XGBoost model.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/10_save-load-xgboost-model/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 7: Lưu và sử dụng XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Ta đã biết, XGBoost thực chất là tập hợp gồm nhiều decision tree. Việc thể hiện mỗi decision tree đó trên đồ thì sẽ giúp chúng ta hiểu sâu sắc hơn quá trình boosting khi đưa vào một tập dữ liệu. Trong bài này, hãy cùng tìm hiểu cách thức thể hiện đó từ một XGBoost model đã được train.\n1. Vẽ một decision tree đơn lẻ\nXGBoost Python API cung cấp một hàm cho việc vẽ các decision tree của một XGBoost model đã train, đó là plot_tree(). Hàm này nhận một tham số đầu tiên chính là model cần thể hiện.\nplot_tree(model) Đồ thị vẽ ra bởi hàm này có thể được lưu dưới dạng file hoặc hiển thị trên màn hình bằng cách sử dụng hàm pyplot.show() của thư viện matplotlib. Yêu cầu là thư viện graphviz đã được cài đặt.\nĐể minh họa cho việc này, hãy cùng tạo một một XGBoost model và train nó trên tập dữ liệu Pima Indians onset of diabetes dataset. Code đầy đủ như bên dưới:\n# plot decision tree from numpy import loadtxt from XGBoost import XGBClassifier from XGBoost import plot_tree from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] y = dataset[:,8] # fit model on training data model = XGBClassifier() model.fit(X, y) # plot single tree plot_tree(model) pyplot.show() Đoạn code bên trên sẽ tạo ra một đồ thị của decision tree đầu tiên trong model (index 0). Các feature và feature value được thể hiện trên đồ thị.\n Một vài quan sát:\n Các features được đặt tên tự động từ f1 đến f5 tương ứng với các feature indices trong dataset. Trong mỗi node, hai hướng trái phải được phân biệt bằng màu sắc. Bên trái là màu xanh, trong khi bên phải là màu đỏ.  2. Một số tùy chọn\nNgoài tham số model cần vẽ là bắt buộc, hàm plot_tree() còn nhận vào một vài tham số tùy chọn khác:\n num_trees: Chỉ số tree muốn vẽ. Giá trị mặc định là 0. Ví dụ:  plot_tree(model, num_trees=4) sẽ vẽ boosted tree thứ 5.\n rankdir: Hướng của đồ thị. Ví dụ: LR là left-to-right. Mặc định là UT - top-to-bottom.  Ví dụ:\nplot_tree(model, num_trees=0, rankdir=\u0026#39;LR\u0026#39;) sẽ cho kết quả như sau:\n 3. Kết luận\nTrong bài này, chúng ta đã tìm hiểu cách vẽ các decision tree của một XGBoost model đã train. Đây là cách rất hay giúp chúng ta có cái nhiều sâu hơn vào bên trong của model, hiểu rõ hơn cách thức mà model hoạt động.\nTrong bài tiếp theo, chúng ta sẽ tìm hiểu cách lưu lại XGBoost model để train và sử dụng model đã lưu để dự đoán trên một mẫu data mới.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/09_visualize-xgboost-model/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 6: Trực quan hóa XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Mục đích của việc phát triển mô hình dự đoán là tạo ra một mô hình có độ chính xác cao khi kiểm tra trên bộ dữ liệu độc lập với dữ liệu train (gọi là unseen data). Trong bài viết này, chúng ta cùng tìm hiểu hai phương pháp đánh giá một XGBoost model:\n Sử dụng train và test dataset. Sử dụng k-fold cross-validation. Bạn hoàn toàn có thể áp dụng những phương pháp trong bài này cho những ML models khác. Tại vì dạo này mình đang tìm hiểu vê XGBoost model nên mình lấy XGBoost model làm ví dụ thôi.  1. Phương pháp 1: Sử dụng train-test set\nĐây là phương pháp đơn giản nhất để đánh giá một ML model. Từ tập dữ liệu ban đầu, ta chia thành 2 phần, gọi là train set và test set theo tỉ lệ nhất định (thường là 7:3, 8:2 hoặc thậm chí 9:1 tùy theo kích thước của tập và đặc trưng của tập dữ liệu). Sau đó, tiến hành train model trên train set rồi sử dụng model đã train đó để dự đoán trên tập test set. Dựa trên kết quả của dự đoán để đưa ra đánh giá chất lượng của model.\nƯu điểm của phương pháp này là nhanh. Nó sẽ phù hợp để áp dụng khi bài toán của bạn đáp ứng ít nhất 1 trong 2 tiêu chí sau:\n Tập dữ liệu có kích thước lớn (hàng triệu mẫu) và có cơ sở để tin rằng cả 2 phần dữ liệu đều đại diện đầy đủ cho tất cả các khía cạnh của vấn đề cần dự đoán (để chắc chắn hơn về điều này, ta có thể xáo trộn ngẫu nhiên tập dữ liệu trước khi chia) Thuật toán train của model rất lâu để hội tụ.  Nếu điều kiện thứ 2 không thỏa mãn mà ta vẫn sử dụng phương pháp này thì sẽ gặp phải vấn đề high variance. Tức là khi 2 tập train set và test set chứa những đại diện khác nhau của vấn đề cần dự đoán thì kết quả đánh giá trên tập test set không thể hiện đúng chất lượng của model.\nThư viện scikit-learn cung cấp hàm train_test_split() giúp chúng ta thực hiện việc chia dữ liệu:\n# split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) Source code dưới đây sử dụng Pima Indians onset of diabetes dataset để train XGBoost model và đánh giá model theo phương pháp này:\n# train-test split evaluation of XGBoost model from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Chạy code trên thu được kết quả:\nAccuracy: 74.02% 2. Phương pháp 2: k-fold cross-validation\nCross-validation là phương pháp mở rộng của phương pháp bên trên để hạn chế được vấn đề high variance. Các bước tiến hành của nó như sau:\n Xáo trộn dữ liệu một cách ngẫu nhiên. Chia tập dữ liệu ban đầu thành k phần (k=5,10,\u0026hellip;), mỗi phần gọi là một fold. - train model trên k-1 fold và đánh giá trên fold còn lại. Lặp lại k lần bước bên trên để mỗi fold trong tập dữ liệu đều có cơ hội trở thành test set. Sau khi toàn bộ quá trình kết thúc ta sẽ có k kết quả đánh giá khác nhau, kêt quả cuối cùng sẽ được tổng hợp dựa vào trung bình (mean) và độ lệch chuẩn (standard deviation) của k kết quả đó.  Phương pháp này cho kết quả đánh giá tin cậy hơn so với phương pháp sử dụng train-test set bởi vì nó được train và đánh giá nhiều lần trên các tập dữ liệu khác nhau. Việc lựa chọn k cũng cần phải xem xét sao cho kích thước của mỗi fold đủ lớn để dữ liệu trong mỗi fold mang tính đại diện cao về mặt thống kê của toàn bộ dữ liệu. Thực nghiệm cho thấy k=5 hoặc k=10 là lựa chọn tốt nhất cho hầu hết các trường hợp. Hãy sử dụng 2 giá trị này trước khi thử nghiệm với các giá trị khác.\nThư viện scikit-learn cung cấp lớp KFold để sử dụng phương pháp này. Đầu tiên, khai báo đối tượng KFold và chỉ ra giá trị của k. Sau đó sử dụng hàm cross_val_score() để bắt đầu đánh giá model.\nkfold = KFold(n_splits=10, random_state=7) results = cross_val_score(model, X, Y, cv=kfold) Code đầy đủ như bên dưới:\n# k-fold cross validation evaluation of XGBoost model from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # CV model model = XGBClassifier() kfold = KFold(n_splits=10, random_state=7) results = cross_val_score(model, X, Y, cv=kfold) print(\u0026#34;Accuracy: %.2f%%(%.2f%%)\u0026#34; % (results.mean()*100, results.std()*100)) Chạy code trên thu được kết quả:\nAccuracy: 73.96% (4.77%) Nếu bài toán là multi-classification hoặc dữ liệu bị mất cân bằng (imbalanced) giữa các lớp (số lượng mẫu giữa các lớp chênh lệch nhau lớn) thì ta có thể sử dụng lớp  StratifiedKFold thay vì KFold của thư viện scikit-learn. Việc làm này có tác dụng làm cho sự phân phối dữ liệu trong mỗi fold giống nhau hơn, nâng cao hiệu quả của model.\n# stratified k-fold cross validation evaluation of XGBoost model from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import cross_val_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # CV model model = XGBClassifier() kfold = StratifiedKFold(n_splits=10, random_state=7) results = cross_val_score(model, X, Y, cv=kfold) print(\u0026#34;Accuracy: %.2f%%(%.2f%%)\u0026#34; % (results.mean()*100, results.std()*100)) Chạy code trên được output là:\nAccuracy: 73.57% (4.39%) Có thể thấy ràng variance có sự giảm nhẹ trong kết quả.\n3. Lựa chọn phương pháp đánh giá nào?\n Nói chung, k-fold cross-validation là phương pháp tốt nhất cho việc đánh giá hiệu năng của một ML model trong hầu hết mọi trường hợp. Sử dụng stratified cross-validation để đảm bảo sự thống nhất về mặt phân phối dữ liệu khi dữ liệu có nhiều lớp cần dự đoán và bị mất cân bằng giữa các lớp. Sử dụng train-test set trong trường hợp thuật toán train mất nhiều thời gian để hội tụ và số lượng mẫu của dữ liệu rất lớn.  Lời khuyên hợp lý nhất là hãy thử nghiệm nhiều lần và tìm ra phương pháp phù hợp với bài toán của bạn sao cho nhanh nhất có thể. Phương pháp được coi là phù hợp khi nó kết quả đánh giá của nó đáp ứng đúng (hoặc gần đúng) yêu cầu bài toán đặt ra ban đầu. Lời khuyên cuối cùng (từ kinh nghiệm thực tế của tác giả):\n Sử dụng 10-fold cross-validation cho bài toán regression. Sử dụng stratified 10-fold-validation cho bài toán classification.  4. Kết luận\nTrong bài viết này, chúng ta đã cùng tìm 2 phương pháp phổ biến đánh gía hiệu quả của một ML model nói chung, XGBoost mode nói riêng:\n Sử dụng train-test set Sử dụng k-fold cross-validation Ngoài ra, mình cũng đưa vài lời khuyên cho các bạn trong viêc lựa chọn phương pháp nào để áp dụng trong bài toán của các bạn!  Trong bài tiếp theo, chúng ta sẽ tìm hiểu cách thức visualize XGBoost model để hiểu sâu hơn bản chất của nó.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/08_evaluate-xgbosst-models/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 5: Đánh giá hiệu năng của XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"XGBoost là một thuật toán thuộc họ Gradient Boosting. Những ưu điểm vượt trội của nó đã được chứng minh qua các cuộc thi trên kaggle. Dữ liệu đầu vào cho XGBoost model phải ở dạng số. Nếu dữ liệu không ở dạng số thì phải được chuyển qua dạng số (numeric) trước khi đưa vào XGBoost model để train. Có một vài phương pháp để thực hiện việc này, hãy cùng nhau điểm qua trong phần còn lại của bài viết.\nSau khi xem hết bài viết này, bạn sẽ biết:\n Làm thế nào để mã hóa chuỗi (string) đầu ra cho việc phân loại? Làm thế nào để mã hóa dữ liệu đầu vào kiểu \u0026ldquo;categorical\u0026rdquo; sử dụng mã hóa one-hot (one hot encoding). Làm thế nào để tự động xử lý vấn đề thiếu dữ liệu trong dataset (missing data)?  1. Mã hóa chuỗi đầu ra\nChúng ta sẽ sử dụng bài toán phân loại hoa Iris để minh họa cho vấn đề này. Đưa vào các số liệu đo đặc các thành phần của hoa Iris, cần dự đoán hoa Iris đó thuộc loại nào (đầu ra dự đoán của model là các chuỗi ký tự).\nHãy xem ví dụ của dataset:\n5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa XGBoost sẽ không thể mô hình hóa bài toán này bởi vì nó yêu cầu dữ liệu đầu vào phải ở dạng số. Chúng ta có thể dễ dang chuyển đổi từ kiểủ chuổi (string) sang kiểu số (integer) sử dụng lớp LabelEncoder của thư viện sklearn. Ba giá trị đầu ra kiểu string (Iris-setosa, Iris-versicolor, Iris-virginica) sẽ được ánh xạ thành các giá trị số tương ứng (0, 1, 2):\n# encode string class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) Chúng ta cần lưu lại bộ mã hóa này để chuyển đổi ngược lại từ số sang chuỗi trong quá trình dự đoán.\nDưới đây là toàn bộ code minh họa việc việc đọc dataset, mã hóa dữ liệu đầu ra, train XGBoost model và đánh giá độ chính xác của model đó:\n# multiclass classification from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder # load data data = read_csv(\u0026#39;iris.csv\u0026#39;, header=None) dataset = data.values # split data into X and y X = dataset[:,0:4] Y = dataset[:,4] # encode string class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y,test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Chạy đoạn code trên, được kết quả như sau:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, objective='multi:softprob', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 92.00% 2. Mã hóa one-code dữ liệu kiểu Categorical\nRất nhiều bộ dataset chứa các dữ liệu kiểu categorical. Trong phần này, chúng ta sử dụng bộ dataset breast cancer để làm việc. Bộ dataset này miêu tả thông tin của y tế của các bệnh nhân, nhãn của nó chỉ ra bệnh nhân đó có bị ung thư hay không.\nVí dụ của bộ dữ liệu này như bên dưới:\n'40-49','premeno','15-19','0-2','yes','3','right','left_up','no','recurrence-events' '50-59','ge40','15-19','0-2','no','1','right','central','no','no-recurrence-events' '50-59','ge40','35-39','0-2','no','2','left','left_low','no','recurrence-events' '40-49','premeno','35-39','0-2','yes','3','right','left_low','yes','no-recurrence-events' '40-49','premeno','30-34','3-5','yes','2','left','right_up','no','recurrence-events' Chúng ta nhìn thấy rằng tất cả 9 giá trị input đều ở là kiểu categorical và được thể hiện ở dạng string. Đây cũng là bài toán phân lớp nhị phân và nhãn cần dự đoán cũng đang ở dạng string. Vì vậy, ta có thể sử dụng lại cách tiếp cận ở phần trước, chuyển các giá trị dạng string sang integer sử dụng LabelEncoder.\n# encode string input values as integers features = [] for i in range(0, X.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(X[:,i]) features.append(feature) encoded_x = numpy.array(features) encoded_x = encoded_x.reshape(X.shape[0], X.shape[1]) Khi sử dụng LabelEncoder, XGBoost có thể hiểu rằng các giá trị encoded integer của mỗi input feature có mối quan hệ thứ tự. Ví dụ, đối với input feature breast-quad, giá trị left-up được mã hóa là 0, left-low được mã hóa là 1. Điều này thực tế là không đúng trong trường hợp này. Để tránh điều này, ta phải ánh xạ các giá trị integer thành 1 giá trị kiểu binary. Ví dụ, các giá trị của biến đầu vào breast-quad là:\nleft-up left-low right-up right-low central được ánh xạ thành 5 giá trị binary tương ứng:\n1,0,0,0,0 0,1,0,0,0 0,0,1,0,0 0,0,0,1,0 0,0,0,0,1 Điều này được gọi là OneHotEncoder. Ta có thể mã hóa OneHot tất cả các input features kiểu categorical sử dụng lớp OneHotEncoder của thư viện scikit-leaen:\nonehot_encoder = OneHotEncoder(sparse=False, categories=✬auto✬) feature = onehot_encoder.fit_transform(feature) OneHot Encoder cho tất cả input features của dữ liệu:\n# encode string input values as integers columns = [] for i in range(0, X.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(X[:,i]) feature = feature.reshape(X.shape[0], 1) onehot_encoder = OneHotEncoder(sparse=False, categories=✬auto✬) feature = onehot_encoder.fit_transform(feature) columns.append(feature) # collapse columns into array encoded_x = numpy.column_stack(columns) Nếu biết chắc chắn răng một input feature nào đó có mối quan hệ về thứ tự, có thể bỏ qua OneHot Encoder mà chỉ cần LabelEncoder cho feature đó.\nToàn bộ source code của phần này:\n# binary classification, breast cancer dataset, label and one hot encoded from numpy import column_stack from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder # load data data = read_csv(✬datasets-uci-breast-cancer.csv✬, header=None) dataset = data.values # split data into X and y X = dataset[:,0:9] X = X.astype(str) Y = dataset[:,9] # encode string input values as integers columns = [] for i in range(0, X.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(X[:,i]) feature = feature.reshape(X.shape[0], 1) onehot_encoder = OneHotEncoder(sparse=False, categories=✬auto✬) feature = onehot_encoder.fit_transform(feature) columns.append(feature) # collapse columns into array encoded_x = column_stack(columns) print(\u0026#34;X shape: : \u0026#34;, encoded_x.shape) # encode string class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(encoded_x, label_encoded_y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Chạy code trên ta được kết quả:\n('X shape: : ', (286, 43)) XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 68.42% 3. Giải quyết vấn đề missing data\nXGBoost có thể tự động học cách để đưa ra cách giải quyết tốt nhất cho vấn đề missing data. Trên thực tế, XGBoost được thiết kế để làm việc với sparse data, giống như one hot encoded data ở phần trước. Chi tiết hơn về các kỹ thuật xử lý missing data của XGBoost, có thể tham khảo Section 3.4 Sparsity-aware Split Finding trong bài báo XGBoost: A Scalable Tree Boosting System.\nTrong phần này, ta sẽ sử dụng Horse Colic dataset để minh họa khả năng xử lý missing data của XGBoost. Trong dataset này, tỷ lệ mising data tương dối lớn, rơi vào khoảng 30%.\nDễ dàng đọc được dataset này với thư viện Pandas:\ndataframe = read_csv(\u0026#34;horse-colic.csv\u0026#34;, delim_whitespace=True, header=None) dataframe.head() Ta có thể quan sát thấy rằng, missing data được đánh dấu bằng dấu ?.\n 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 0 2 1 530101 38.50 66 28 3 3 ? 2 5 4 4 ? ? ? 3 5 45.00 8.40 ? ? 2 2 11300 0 0 2 1 1 1 534817 39.2 88 20 ? ? 4 1 3 4 2 ? ? ? 4 2 50 85 2 2 3 2 2208 0 0 2 2 2 1 530334 38.30 40 24 1 1 3 1 3 3 1 ? ? ? 1 1 33.00 6.70 ? ? 1 2 0 0 0 1 3 1 9 5290409 39.10 164 84 4 1 6 2 2 4 4 1 2 5.00 3 ? 48.00 7.20 3 5.30 2 1 2208 0 0 1 4 2 1 530255 37.30 104 35 ? ? 6 2 ? ? ? ? ? ? ? ? 74.00 7.40 ? ? 2 2 4300 0 0 2 Thay ? bởi giá trị 0:\n# set missing values to 0 X[X == \u0026#39;?\u0026#39;] = 0 # convert to numeric X = X.astype(\u0026#39;float32\u0026#39;) Bài toán đối với dataset này là Binary Classification, nhãn bao gồm 2 giá trị là 1 và 2. Ta dễ dàng chuyển sang 0 và 1 sử dụng LabelEncoder:\n# encode Y class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) Code đầy đủ:\n# binary classification, missing data from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder # load data dataframe = read_csv(\u0026#34;horse-colic.csv\u0026#34;, delim_whitespace=True, header=None) dataset = dataframe.values # split data into X and y X = dataset[:,0:27] Y = dataset[:,27] # set missing values to 0 X[X == \u0026#39;?\u0026#39;] = 0 # convert to numeric X = X.astype(\u0026#39;float32\u0026#39;) # encode Y class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Output:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 82.83% Hãy kiểm tra khả năng của XGBoost bằng cách thử thay missing value với các giá trị khác nhau:\n Thay missing value bởi 1  X[X == \u0026#39;?`] = 1 Kết quả:\nAccuracy: 81.82%  Thay missing value bởi NaN  X[X == \u0026#39;?`] = 1 Kết quả:\nAccuracy: 83.84%  Thay thế missing value bằng giá trị trung bình (mean) của toàn bộ feature đó  # impute missing values as the mean imputer = SimpleImputer() imputed_x = imputer.fit_transform(X) Source đầy đủ:\n# binary classification, missing data, impute with mean import numpy from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import Imputer # load data dataframe = read_csv(\u0026#34;horse-colic.csv\u0026#34;, delim_whitespace=True, header=None) dataset = dataframe.values # split data into X and y X = dataset[:,0:27] Y = dataset[:,27] # set missing values to NaN X[X == \u0026#39;?\u0026#39;] = numpy.nan # convert to numeric X = X.astype(✬float32✬) # impute missing values as the mean. imputer = Imputer() imputed_x = imputer.fit_transform(X) # encode Y class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(imputed_x, label_encoded_y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Kết quả:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 81.82% Có thể thấý rằng, đối với bài toán này, giải pháp thay thế missing value bằng NaN mang lại kết quả tốt nhất. Trong các bài toán thực tế, chúng ta cũng cần phải thử-sai nhiều cách khác nhau để chọn được phương pháp tối ưu cho bài toán đó.\n4. Kết luận\nTrong bài viết này chúng ta đã cùng nhau tìm hiểu các cách để chuẩn bị dữ liệu cho việc train XGBoost model. Cụ thể:\n Mã hóa dữ liệu kiểu string bằng LabelEncoder Mã hóa dữ liệu kiểu categorical bằng OneHot Encoder Xử lý missing data  Trong bài tiếp theo, chúng ta sẽ tìm hiểu các phương pháp đánh giá hiệu năng của XGBoost model.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/07_data-preparation-for-gradient-boosting/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 4: Chuẩn bị dữ liệu cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"XGBoost là một thuật toán rất mạnh mẽ, tối ưu hóa về tốc độ và hiệu năng cho việc xây dựng các mô hình dự đoán. Một thống kê chỉ ra rằng, hầu hết những người chiến thắng trong các cuộc thi trên Kaggle đều sử dụng thuật toán này. Trong bài viết này, hãy cùng nhau xây dựng một mô hình XGBoost đơn giản để có thể hiểu được cách thức làm việc của nó.\nNội dung bài viết chia thành các phần:\n Cài đặt thư viện XGBoost Chuẩn bị dữ liệu Train XGBoost model Đánh giá XGBoost model Nguồn tham khảo  1. Cài đặt thư viện XGBoost\nCó 2 cách để cài đặt thư viện XGBoost. Sử dụng pip hoặc biên dịch từ mã nguồn:\n1.1 Sử dụng pip để cài đặt:\npip install XGBoost Để cập nhật thư viện, sử dụng lệnh sau:\npip install --upgrade XGBoost 1.2 Biên dịch từ mã nguồn\nSử dụng cách này nếu muốn cài đặt phiên bản mới nhất của XGBoost.\ngit clone --recursive https://github.com/dmlc/XGBoost cd XGBoost cp make/minimum.mk ./config.mk make -j8 cd python-package sudo python setup.py install Tại thời điểm viết bài, phiên bản của XGBoost là 1.2\nChuẩn bị dữ liệu  Trong bài viết này, chúng ta sẽ sử dụng dataset về bênh tiểu đường của Ấn Độ. Dataset bao gồm 8 features, miêu tả chi tiết tình trạng của mỗi bệnh nhân và một feature tương ứng chỉ ra bênh nhân có bị tiểu đường hay không. Chi tiết về dataset này, bạn có thể tham khảo trên UCI Machine Learning Repository website\nĐây là một dataset khá đơn giản bởi vì tất cả các features của nó đều đã ở dạng số và vấn đề chỉ là \u0026ldquo;binary classification\u0026rdquo;.\n6,148,72,35,0,33.6,0.627,50,1 1,85,66,29,0,26.6,0.351,31,0 8,183,64,0,0,23.3,0.672,32,1 1,89,66,23,94,28.1,0.167,21,0 0,137,40,35,168,43.1,2.288,33,1 Tải dataset và đặt nó trong thư mục làm việc hiện tại của bạn với tên là pima-indians-diabetes.csv.\nTiếp theo, load dataset từ file vừa tải về để chuẩn bị cho training và evaluating XGBoost model.\n Import các thư viện sử dụng:  from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score  Load csv file  dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;)  Chia dataset thành dữ liệu input (X) và output (Y)  X = dataset[:, 0:8] y = dataset[:, 8]  Chia X và y thành data training và data testing  Training data được sử dụng để train XGBoost model, trong khi testing data được sử dụng để đánh giá độ chính xác của model đó. Để làm điều này, ta có thể sử dụng hàm train_test_split() trong thư viện scikit-learn.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_seed=42) Đến đây, dữ liệu đã được chuẩn bị sẵn sàng cho việc train XGBoost model.\n2. train XGBoost model\nThư viện XGBoost cung cấp một \u0026ldquo;Wrapper class\u0026rdquo; cho phép sử dụng XGBoost model tương tự như như làm việc với thư viện scikit-learn. XGBoost model trong thư viện XGBoost là XGBClassifier.\nTạo XGBoost model và thực hiện train:\nmodel = XGBClassifier() model.fit(X_train, y_train) Ở đây, chúng ta đang sử dụng giá trị mặc định của các tham số. Mình sẽ có các bài việc về việc *tuning papameters\u0026quot; cho XGBoost model, mời các bạn đón đọc.\nBạn có thể quan sát các tham số sử dụng trong model bằng lệnh sau:\nprint(model) 3. Đánh giá XGBoost model\nĐể sử dụng model đã train để dự đoán, sử dụng hàm model.predict():\npredictions = model.predict(X_test) Ta có thể đánh giá độ chính xác của model bằng cách so sánh kết quả dự đoán của model với kêt quả thực tế. Hàm accuracy_score() giúp chúng ta thực hiện việc này:\naccuracy = accuracy_score(y_test, predictions) print(\u0026#39;Accuracy: %.2f%%\u0026#39; % (accuracy*100)) Kết quả cuối cùng:\nAccuracy: 77.95% Kết quả khá tốt đối với bài toán này.\n5. Tổng kết\nTrong bài viết này, chúng ta đã xây dựng XGBoost model sử dụng thư viện XGBoost. Cụ thể, chúng ta đã học:\n Cách cài đặt thư viện XGBoost Chuẩn bị dữ liệu train model Đánh giá model  Trong bài tiếp theo, chúng ta sẽ bàn luận về một số phương pháp chuẩn bị dữ liệu train cho XGBoost model.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/06_build-xgboost-model/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 3: Xây dựng XGBoost model"},{"categories":["Machine Learning","Ensemble Learning","XGBoost"],"contents":"Tiếp tục phần 2 của loạt bài tìm hiểu toàn cảnh về Ensemble Learning, trong phần này ta sẽ đi qua một số thuât toán thuộc nhóm Bagging và Boosting.\n Các thuật toán thuộc nhóm Bagging bao gồm:  Bagging meta-estimator Random forest   Các thuật toán thuộc họ Boosting bao gồm:  AdaBoost Gradient Boosting (GBM) XGBoost (XGBM) Light GBM CatBoost    Để minh họa cho các thuật toán kể trên, mình sẽ sử dụng bộ dữ liệu Loan Prediction Problem.\n1. Bagging techniques\n1.1 Bagging meta-estimator\nBagging meta-estimator là thuật toán sử dụng cho cả 2 loại bài toán classification (BaggingClassifier) và regression (BaggingRegressor).\nCác bước thực hiện của thuật toán như sau:\n Bước 1: Tạo ngẫu nhiên các N bags từ tập train set. Bước 2: Tạo N objects của lớp BaggingClassifier và train trên mỗi bag, độc lập với nhau. Bước 3: Sử dụng các objects đã trained để dự đoán trên tập test set.  Code cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.ensemble import BaggingClassifier from sklearn import tree from sklearn.preprocessing import LabelEncoder #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1)) model.fit(x_train, y_train) accuracy = model.score(x_test,y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Kết quả:\nAccuracy: 77.83% Đối với bài toán regression, thay BaggingClassifier bằng BaggingRegressor.\nMột số tham số:\n base_estimator: Định nghĩa thuật toán mà base model sử dụng. Mặc định là decision tree. n_estimators: Định nghĩa số lượng base models. Mặc định là 10. max_samples: Định nghĩa số lượng mẫu data tối đa trong mỗi bag. Mặc định là 1. max_features: Định nghĩa số lượng features tối đa sử dụng trong mỗi bag. Mặc định là 1. n_jobs: Số lượng jobs chạy song song cho cả quá trình train và predict. Mặc định là 1. Nếu giá trị bằng -1 thì số jobs bằng số cores của hệ thống. random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi BaggingClassifier thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh các models với nhau.  1.2 Random Forest\nCác thức hoạt động của Random Forest gần giống Bagging meta-estimator, chỉ khác một điều duy nhất là tại mỗi node của tree trong Decision Tree, nó tạo ra một tập ngẫu nhiên các features và sử dụng tập này đê chọn hướng đi tiếp theo. Trong khi đó, Bagging meta-estimator sử dụng tất cả features để chọn đường.\nCode ví dụ:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = RandomForestClassifier() model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Output:\nAccuracy: 79.86% Một số tham số:\n n_estimators: Số lượng decition trees (base models). Mặc định là 100 (đối với phiên bản scikit-learn từ 0.22) và 10 (đối với phiên bản \u0026lt; 0.22). criterion: Chỉ ra hàm được sử dụng để quyết định hướng đi tại mỗi node của tree. Tham số này có thể nhận 1 trong 2 giá trị {\u0026ldquo;gini\u0026rdquo;, \u0026ldquo;entropy\u0026rdquo;}. Giá trị mặc định là \u0026ldquo;gini\u0026rdquo;. max_features: Số lượng features được sử dụng tại mỗi node để tìm đường đi tiếp theo. Một số giá trị thường được sử dụng là:  auto/sqrt: max_features = sqrt(n_features). Đây là giá trị mặc định. log2: max_features = log2(n_features). None: max_features = n_features.   max_depth: Độ sâu của mỗi tree. Mặc định, các nodes sẽ được mở rộng tận khi tất cả các leaves chứa ít hơn min_samples_split mẫu (samples). min_sample_split: Số lượng mẫu tối thiểu tại mỗi leaf node để có thể tiếp tục mở rộng tree. Giá trị mặc định là 2. min_samples_leaf: Số lượng mẫu tối thiểu tại mỗi leaf node. Mặc định là 1. max_leaf_nodes: Số lượng leaf node tối đa của mỗi tree. Giá trị mặc định là không có giới hạn số lượng. n_jobs: Số lượng jobs chạy song song. Mặc định là 1. Gán giá trị -1 để sử dụng tất cả các cores của hệ thống. random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi RandomForestClassifier thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh các models với nhau.  2. Boosting techniques\n2.1 AdaBoost\nAdaBoost là thuật toán đơn giản nhất trong họ Boosting, nó cũng thường sử dụng decision tree để làm base model.\nThuật toán thực hiện như sau:\n Bước 1: Ban đầu, tất cả các mẫu dữ liệu được gán cho cùng một giá trị trọng số (weight). Bước 2: Lựa chọn ngẫu nhiên một tập dữ liệu con (tập S) từ tập dữ liệu ban đầu (tập D) và train decition tree model trên tập dữ liệu con này. Bước 3: Sử dụng model đã trained, tiến hành dự đoán trên toàn tập D. Bước 4: Tính toán lỗi (error) bằng cách so sánh giá trị dự đoán và giá trị thực tế. Bước 5: Gán giá trị weight cao hơn cho những mẫu dữ liệu có error cao hơn. Bước 6: Lặp lại bước 2,3,4,5 đến khi error không đổi hoặc số lượng tốí đa của weak learner đạt được.  Code mẫu cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn import tree from sklearn.ensemble import AdaBoostClassifier from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = AdaBoostClassifier(random_state=1) model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Kết quả:\nAccuracy: 72.22% Đối với bài toán regression, thay AdaBoostClassifier bằng AdaBoostRegressor.\nMột vài tham số quan trọng:\n base_estimator: Chỉ ra weak learner là gì. Mặc định sử dụng decition tree. n_estimators: Số lượng của weak learners. Mặc định là 50. learning_rate: Điều chỉnh mức độ đóng góp của mỗi weak learner đến kết quả cuối cùng. random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi AdaBoostClassifier thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh các models với nhau.  2.2 Gradient Boosting (GBM)\nĐể giúp mọi người dễ hình dung, mình sẽ trình bày ý tưởng của GBM thông qua ví dụ sau:\nCho bảng dữ liệu bên dưới:\n   ID Married Gender City Monthly Income Age (target)     1 Y F Hanoi 51.000 35   2 N M HCM 25.000 24   3 Y F Hanoi 70.000 38   4 Y M HCM 53.000 30   5 N M Hanoi 47.000 33    Bài toán đạt ra là cần dự đoán tuổi dựa trên các input features: Tình trạng hôn nhân, giới tính, thành phố sinh sống, thu nhập hàng tháng.\n Bước 1: Train decition tree model thứ nhất trên tập dữ liệu bên trên. Bước 2: Tính toán lỗi dựa theo sai số giữa giá trị thưc tế và giá trị dự đoán.     ID Married Gender City Monthly Income Age (target) Age (prediction 1) Error 1     1 Y F Hanoi 51.000 35 32 3   2 N M HCM 25.000 24 32 -8   3 Y F Hanoi 70.000 38 32 6   4 Y M HCM 53.000 30 32 -2   5 N M Hanoi 47.000 33 32 1     Bước 3: Một decition tree model thứ 2 được tạo, sử dụng cùng input features với model trước đó, nhưng target là Error 1. Bước 4: Giá trị dự đoán của model thứ 2 được cộng với giá trị dự đoán của model thứ nhất.     ID Age (target) Age (prediction 1) Error 1 (new target) Prediction 2 Combine (Pred1+Pred2)     1 35 32 3 3 35   2 24 32 -8 -5 27   3 38 32 6 3 35   4 30 32 -2 -5 27   5 33 32 1 3 35     Bước 5: Giá trị kết hợp bở bước 3 coi như là giá trị dự đoán mới. Ta tính lỗi (Error 2) dựa trên sai số giữa giá trị này và giá trị thực teses.     ID Age (target) Age (prediction 1) Error 1 (new target) Prediction 2 Combine (Pred1+Pred2) Error 2     1 35 32 3 3 35 0   2 24 32 -8 -5 27 -3   3 38 32 6 3 35 3   4 30 32 -2 -5 27 3   5 33 32 1 3 35 -3     Bước 6: Lặp lại bước 2-5 ho đến khi số lượng weak learner đạt được hoặc giá trị lỗi không đổi.  Code ví dụ cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree from sklearn.ensemble import GradientBoostingClassifier from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = GradientBoostingClassifier(learning_rate=0.01,random_state=1) model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Output:\nAccuracy: 78.47% Đối với bài toán regression, thay GradientBoostingClassifier thành GradientBoostingRegressor.\nMột số tham số quan trọng:\n min_sample_split: Số lượng mẫu tối thiểu tại mỗi leaf node để có thể tiếp tục mở rộng tree. Giá trị mặc định là 2. min_samples_leaf: Số lượng mẫu tối thiểu tại mỗi leaf node. Mặc định là 1. max_depth: Độ sâu của mỗi tree. Nên xem xét tham số này khi tuning model. Giá trị mặc định là 3. max_features: Số lượng tối đa features xem xét khi tìm đường mở rộng tree. Những features này được chọn ngẫu nhiên.  2.3 XGBoost\nXGBoost (extreme Gradient Boosting) là phiên bản cải tiến của Gradient Boosting. Ưu điểm vượt trội của nó được chứng minh ở các khía cạnh:\n  Tốc độ xử lý\n XGBoost thực hiện tinh toán song song nên tốc độ xử lý có thể tăng gấp 10 lần so với GBM. Ngoài ra, XGboost còn hỗ trợ tính toán trên Hadoop.    Overfitting\n XGBoost áp dụng cơ chế Regularization nên hạn chế đáng kể hiệ tượng Overfitting (GBM không có regularization).    Sự linh hoạt\n XGboost cho phép người dùng sử dụng hàm tối ưu và chỉ tiêu đánh giá của riêng họ, không hạn chế ở những hàm cung cấp sẵn.    Xử lý missing value\n XGBoost bao gồm cơ chế tự động xử lý missing value bên trong nó. Vì thế, có thể bỏ qua bước này khi chuẩn bị dữ liệu cho XGBoost.    Tự động cắt tỉa\n Tính năng tree pruning hộ trợ việc tự động bỏ qua những leaves, nodes không mang giá trị tích cực trong quá trình mở rộng tree.    Chính vì những ưu điểm đó mà hiệu năng của XGBoost tăng lên đáng kể so với các thuật toán ensemble learning khác. Nó được sử dụng ở hầu hết các cuộc thi trên Kaggle cũng như Hackathons.\nCode ví dụ cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree import xgboost as xgb from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = xgb.XGBClassifier(random_state=1, eta=0.01) model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Kết quả:\nAccuracy: 82% Đối với vài toán regression, sử dụng XGBRegressor thay vì XGBClassifier.\nMột số tham số quan trọng:\n n_thread: Số lượng cores của hê thống được sử dụng để chạy model. Giá trị mặc định là -1, XGBoost sẽ tự động phát hiện và sử dụng tất cả các cores. eta: Tương tự learning_rate trong GBM. Giá trị mặc định là 0.3. max_depth: Độ sâu tối đa của decision tree. Giá trị mặc định là 6. colsample_bytree: Tương tự max_features của GBM. lambda: L2 regularization. Giá trị mặc định là 1. alpha: L1 regularization. Giá trị mặc định là 0.  2.4 Light GBM\nTại sao chúng ta vẫn cần thuật toán này khi mà ta đã có XGBoost rất mạnh mẽ rồi?\nSự khác nhau nằm ở kích thước của dữ liệu huấn luyện. Light GBM đánh bại tất cả các thuật toán khác khi tập dataset có kích thước cực lớn. Thực tế chứng minh, nó cần ít thời gian đê xử lý hơn trên tập dữ liệu này (Có lẽ vì thế mà có chứ light - ánh sáng). Nguyên nhân sâu xa của sự khác biệt này nằm ở cơ chế làm viêc của Light GBM. Trong khi các thuật toán khác sử dụng cơ chế level-wise thì nó lại sử dụng leaf-wise.\nHình dưới đây minh họa sự khác nhau giữa 2 cơ chế level-wise và leaf-wise:\n   Như chúng ta thấy, leaf-wise chỉ mở rộng tree theo 1 trong 2 hướng so với cả 2 hướng của level-wise, tức là số lượng tính toán của Light GBM chỉ bằng 1/2 so với XGBoost.\nCode ví dụ cho bài toán classifier:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree import lightgbm as lgb from sklearn.preprocessing import LabelEncoder #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = lgb.LGBMClassifier() model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Trong trường hợp regression, sử dụng LGBMRegressor thay cho LGBMClassifier.\nMột số tham số quan trọng:\n num_leaves: Số lượng leaves tối đa trên mỗi node. Giá trị mặc định là 31 max_depth: Độ sâu tối đa của mỗi tree. Mặc định là không có giới hạn. learing_rate: learning rate của mỗi tree. Mặc định là 0.1. n_estimators: Số lượng weak learners. Mặc định là 100. n_jobs: Số lượng cores của hê thống được sử dụng để chạy model. Giá trị mặc định là -1, XGBoost sẽ tự động phát hiện và sử dụng tất cả các cores.  2.5 CatBoost\nKhi làm việc với tập dữ liệu mà có số lượng lớn input features kiểu categorical, nếu chúng ta áp dụng one-hot encoding thì số chiều dữ liệu sẽ tăng lên rất nhanh (theo hàm mũ e).\nCatBoost ra đời chính là để gánh vác sứ mệnh giải quyết những bài toán như vậy (CatBoost = Categories + Boosting). Khi làm việc với CatBoost, chúng ta không cần thực hiện one-hot encoding.\nCode ví dụ cho classification:\n# importing required libraries import pandas as pd import numpy as np from catboost import CatBoostClassifier from sklearn.metrics import accuracy_score # read the train and test dataset train_data = pd.read_csv(\u0026#39;train-data.csv\u0026#39;) test_data = pd.read_csv(\u0026#39;test-data.csv\u0026#39;) # Now, we have used a dataset which has more categorical variables # hr-employee attrition data where target variable is Attrition  # seperate the independent and target variable on training data train_x = train_data.drop(columns=[\u0026#39;Attrition\u0026#39;],axis=1) train_y = train_data[\u0026#39;Attrition\u0026#39;] # seperate the independent and target variable on testing data test_x = test_data.drop(columns=[\u0026#39;Attrition\u0026#39;],axis=1) test_y = test_data[\u0026#39;Attrition\u0026#39;] # find out the indices of categorical variables categorical_var = np.where(train_x.dtypes != np.float)[0] model = CatBoostClassifier(iterations=50) # fit the model with the training data model.fit(train_x,train_y,cat_features = categorical_var,plot=False) # predict the target on the train dataset predict_train = model.predict(train_x) # Accuray Score on train dataset accuracy_train = accuracy_score(train_y,predict_train) print(\u0026#39;\\naccuracy_score on train dataset : {:.2f}%\u0026#39;.format(accuracy_train*100)) # predict the target on the test dataset predict_test = model.predict(test_x) # Accuracy Score on test dataset accuracy_test = accuracy_score(test_y,predict_test) print(\u0026#39;\\naccuracy_score on test dataset : {:.2f}%\u0026#39;.format(accuracy_test*100)) Kết quả:\naccuracy_score on train dataset : 91.41% accuracy_score on test dataset : 86.05% Thay CatBoostRegressor cho CatBoostClassifier trong bài toán regression.\nMột số tham số quan trọng:\n loss_function: Định nghĩa loss_function sử dụng để training model. iterations: Số lượng weak learner. learning_rate: Learning rate của mỗi tree. depth: Độ sâu của mỗi tree.  3. Kết luận\nChúng ta đã cùng nhau đi qua 2 phần khá dài để tìm hiểu về Ensemble Learning. Rất nhiều khía cạnh đã được bàn bạc và kèm theo code ví dụ. Hi vọng các bạn đã có cái nhìn rõ hơn về Ensemble Learning. Trong các bài tiếp theo, mình sẽ đi sâu hơn về XGBoost, một thuật toán mạnh mẽ, chiến thắng trong hầu như mọi cuộc thi Kaggle. Hãy đón đọc!\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/05_comprehensive_guide_to_ensemble_model_2/","tags":["Machine Learning","Ensemble Learning","XGBoost"],"title":"XGBoost - Bài 2: Toàn cảnh về Ensemble Learning - Phần 2"},{"categories":["Machine Learning","Ensemble Learning","XGBoost"],"contents":"1. Giới thiệu về Ensemble Learning\nGiả sử chúng ta có một bài toán phân loại sản phẩm sử dụng ML. Team của bạn chia thành 3 nhóm, mỗi nhóm sử dụng một thuật toán khác nhau và đánh giá độ chính xác trên tập validation set:\n Nhóm 1: Sử dụng thuật toán Linear Regression. Nhóm 2: Sử dụng thuật toán k-Nearest Neighbour. Nhóm 3: Sử dụng thuật toán Decision Tree. Độ chính xác của mỗi nhóm lần lượt là 70%, 67% và 76%. Điều này hoàn toàn dễ hiểu bởi vì 3 models làm việc theo những các khác nhau. Ví dụ, Linear Regression cố gắng tìm ra mối quan hệ tuyến tính giữa các điểm dữ liệu, trong khi Decision Tree thì lại dựa vào mỗi quan hệ phi tuyến để liên kết dữ liệu.  Có cách nào kết hợp kết quả cả 3 models để tạo ra kết quả cuối cùng hay không?\n Câu hỏi này là tiền đề cho một phương pháp, một họ các thuật toán hoạt động rất hiệu quả trong các bài toán ML. Đó là Ensemble Learning hay Ensemble Models.\nHình dưới đây thể hiện bức tranh tổng quát về Ensemble Learning.\n 2. Basic Ensemble Techniques\nỞ mức độ cơ bản, có 3 kỹ thuật là:\n Max Voting Averaging Weighted Averaging Mặc dù đơn giản nhưng những kỹ thuật này lại tỏ ra hiệu quả trong một số trường hợp nhất định. Hãy cùng tìm hiểu kỹ hơn về chúng.  2.1 Max Voting\nKỹ thuật này hay được sử dụng cho bài toán phân lớp, ở đó, nhiều models được sử dụng để dự đoán cho mỗi mẫu dữ liệu. Kết quả dự đoán của mỗi model được xem như là một vote. Cái nào có số vote cao nhất thì sẽ là kết quả dự đoán cuối cùng. Nói cách khác, đây là kiểu bầu chọn theo số đông, được áp dụng rất nhiều trong cuộc sống, chính trị, \u0026hellip;\nLấy ví dụ, đợt vừa rồi, công ty của bạn tổ chức khám sức khỏe cho nhân viên tại bệnh viện X. Sau khi khám xong, phòng tổ chức nhân sự (TCNS) lấy ý kiến mọi người về chất lượng khám bệnh để xem năm sau có tiếp tục khám ở bênh viên X đó nữa không. Bảng dưới là ý kiến của 5 người được chọn ngẫu nhiên trong số toàn bộ nhân viên.\n   Người 1 Người 2 Người 3 Người 4 Người 5     Có Không Không Có Có    Có 3 ý kiến muốn tiêp tục khám ở bệnh viện X vào năm sau, và 2 ý kiến muốn đổi bênh viện khác. Căn cứ theo max voting thì phòng TCNS sẽ tiếp tục chọn bệnh viên Xlà nơi khám bệnh cho nhân viên cho năm tiếp theo.\nCode minh họa:\nx_train, y_train, x_test, y_test = get_data() model_1 = DecisionTreeClassifier() model_2 = KNeighborsClassifier() model_3= LogisticRegression() model_1.fit(x_train,y_train) model_2.fit(x_train,y_train) model_3.fit(x_train,y_train) pred_1=model_1.predict(x_test) pred_2=model_2.predict(x_test) pred_3=model_3.predict(x_test) final_pred = np.array([]) for i in range(0,len(x_test)): final_pred = np.append(final_pred, mode([pred_1[i], pred_2[i], pred_3[i]])) Thư viện scikit-learn có module VotingClassifier giúp chúng ta đơn giản hóa việc này:\nfrom sklearn.ensemble import VotingClassifier x_train, y_train, x_test, y_test = get_data() model_1 = LogisticRegression(random_state=1) model_2 = DecisionTreeClassifier(random_state=1) model = VotingClassifier(estimators=[(\u0026#39;lr\u0026#39;, model_1), (\u0026#39;dt\u0026#39;, model_2)], voting=\u0026#39;hard\u0026#39;) model.fit(x_train,y_train) model.score(x_test,y_test) 2.2 Averaging\nTương tự như kỹ thuật Voting, Averaging cũng sử dụng kết quả dự đoán của nhiều models. Tuy nhiên, ở bước quyết định kết quả cuối cùng, giá trị trung bình của tất cả kêt quả của các models được lựa chọn.\n Tiếp tục với ví dụ ở trên, một đề nghị khác của phòng TCNS là yêu cầu nhân viên chấm điểm chất lượng khám bệnh của bênh viện X, theo thang điểm từ 1 đến 5.\nBảng kết quả trả lời của 5 người ngẫu nhiên:\n   Người 1 Người 2 Người 3 Người 4 Người 5     2 4 3 5 4    Điểm đánh giá cuối cùng sẽ là: (2+4+3+5+4)/5 = 3.6\nCode ví dụ:\nx_train, y_train, x_test, y_test = get_data() model_1 = tree.DecisionTreeClassifier() model_2 = KNeighborsClassifier() model_3 = LogisticRegression() model_1.fit(x_train,y_train) model_2.fit(x_train,y_train) model_3.fit(x_train,y_train) pred_1 = model1.predict_proba(x_test) pred_2 = model2.predict_proba(x_test) pred_3 = model3.predict_proba(x_test) final_pred =(pred1+pred2+pred3)/3 2.3 Weighted Average\nĐây là kỹ thuật mở rộng của averaging. Mỗi model được gắn kèm với một trọng số tỷ lệ với mức độ quan trọng của model đó. Kết quả cuối cùng là trung bình có trọng số của tất cả kết quả của các models.\n Vẫn với ví dụ ở mục 2.2, nhưng trong số 5 người được hỏi thì người thứ nhất có vợ là bác sĩ, người thứ 2 có mẹ là y tá, người thứ 3 có người yêu là sinh viên trường y. Vì vậy, ý kiến của 3 người này rõ ràng có giá trị hơn so với 2 người còn lại. Ta đánh trọng số cho mỗi người như bảng dưới (hàng thứ 2 là trọng số, hàng thứ 3 là điểm đánh giá):\n   Người 1 Người 2 Người 3 Người 4 Người 5     1 0.8 0.5 0.3 0.3   2 4 3 5 4    Điểm đánh giá cuối cùng sẽ là: (21 + 40.8 + 30.5 + 50.3 + 4*0.3)/5 = 1.88\nCode minh họa:\nx_train, y_train, x_test, y_test = get_data() model_1 = DecisionTreeClassifier() model_2 = KNeighborsClassifier() model_3 = LogisticRegression() model_1.fit(x_train,y_train) model_2.fit(x_train,y_train) model_3.fit(x_train,y_train) pred_1 = model1.predict_proba(x_test) pred_2 = model2.predict_proba(x_test) pred_3 = model3.predict_proba(x_test) final_pred=(pred_1*0.3 + pred_2*0.3 + pred_3*0.4) 3. Advanced Ensemble techniques\nĐã có basic thì chắc chắn phải có advanced, phải không mọi người. :D\nCó 4 kỹ thuật của Ensemble Learning được xếp vào nhóm advanced:\n Stacking Blending Bagging Boosting  Chúng ta tiếp tục đi qua lần lượt từng kỹ thuật này:\n3.1 Stacking\nHãy xem các bước thực hiện của kỹ thuật này:\n Bước 1: Train model A (base model) theo kiểu cross-validation với k=10. Bước 2: Tiếp tuc train model A trên toàn bộ train set. Bước 3: Sử dụng model A để dự đoán trên test set. Bước 4: Lặp lại bước 1,2,3 cho các base model khác. Bước 5:  Kết quả dự đoán trên train set của các base models được sử dụng như là input features (ensemble train set) để train stacking model. Kết quả dự đoán trên test set của các base models được sử dụng như là test set (ensemble test set) của stacking model.   Bước 6: Train và đánh giá stacking model sử dụng ensemble train set và ensemble test set.  Code minh họa ý tưởng:\n# We first define a function to make predictions on n-folds of train and test dataset. This function returns the predictions for train and test for each model. def Stacking(model,train,y,test,n_fold): folds=StratifiedKFold(n_splits=n_fold,random_state=1) test_pred = np.empty((test.shape[0],1),float) train_pred = np.empty((0,1),float) for train_indices,val_indices in folds.split(train,y.values): x_train,x_val = train.iloc[train_indices],train.iloc[val_indices] y_train,y_val = y.iloc[train_indices],y.iloc[val_indices] model.fit(X=x_train,y=y_train) train_pred = np.append(train_pred,model.predict(x_val)) test_pred = np.append(test_pred,model.predict(test)) return test_pred.reshape(-1,1),train_pred # Now we’ll create two base models – decision tree and knn. model_1 = DecisionTreeClassifier(random_state=1) test_pred_1 ,train_pred_1 = Stacking(model=model_1, n_fold=10, train=x_train, test=x_test, y=y_train) train_pred_1 = pd.DataFrame(train_pred_1) test_pred_1 = pd.DataFrame(test_pred_1) model_2 = KNeighborsClassifier() test_pred_2, train_pred_2 = Stacking(model=model_2, n_fold=10, train=x_train,test=x_test, y=y_train) train_pred_2 = pd.DataFrame(train_pred_2) test_pred_2 = pd.DataFrame(test_pred_2) # Create a final model, logistic regression, on the predictions of the decision tree and knn models. df = pd.concat([train_pred_1, train_pred_2], axis=1) df_test = pd.concat([test_pred_1, test_pred_2], axis=1) model = LogisticRegression(random_state=1) model.fit(df,y_train) model.score(df_test, y_test) Đoạn code trên chỉ minh họa stack model với 2 levels. Decision Tree và kNN là level 0, còn Logistic Regression là level 1. Bạn hoàn toàn có thể thử nghiệm với nhiều levels hơn.\n3.2 Blending\nCác bước thực hiện phương pháp này như sau:\n Buớc 1: Chia dataset thành train set, validation set và test set. Bước 2: Base model được train trên train set. Bước 3: Sử dụng base model để dự đoán trên validation set và test set. Bước 4: Lặp lại bước 2,3 cho các base models khác. Bước 5:  Validation set và các kết quả dự đoán trên validation set của các base models được sử dụng như là input features (ensemble train set) của blending model. Test set và các kết quả dự đoán trên test set của các base models được sử dụng như là test set (ensemble test set) của blending model.   Bước 6: Train và đánh giá blending model sử dụng ensemble train set và ensemble test set.  Code minh họa ý tưởng:\n# build two models, decision tree and knn, on the train set in order to make predictions on the validation set. model_1 = DecisionTreeClassifier() model_1.fit(x_train, y_train) val_pred_1 = model1.predict(x_val) test_pred_1 = model1.predict(x_test) val_pred_1 = pd.DataFrame(val_pred_1) test_pred_1 = pd.DataFrame(test_pred_1) model_2 = KNeighborsClassifier() model_2.fit(x_train, y_train) val_pred_2 = model_2.predict(x_val) test_pred_2 = model2.predict(x_test) val_pred_2 = pd.DataFrame(val_pred_2) test_pred_2 = pd.DataFrame(test_pred_2) # Combining the meta-features and the validation set, a logistic regression model is built to make predictions on the test set. df_val = pd.concat([x_val, val_pred_1,val_pred_2], axis=1) df_test = pd.concat([x_test, test_pred1,test_pred_2], axis=1) model = LogisticRegression() model.fit(df_val, y_val) model.score(df_test, y_test) 3.3 Bagging\nBagging (Bootstrap Aggregating) khác với hai kỹ thuật trên ở chỗ, nó sử dụng chung 1 thuật toán cho tất cả các base models. Tập dataset sẽ được chia thành các phần khác nhau (bags) và mỗi base model sẽ được train trên mỗi bag đó.\nCác bước thực hiện của bagging như sau:\n Bước 1: Chia tập dữ liệu ban đầu thành nhiều phần khác nhau (bags). Bước 2: Tạo các base models (weak learner) và train chúng trên các bags. Các base model được train song song và độc lập với nhau. Bước 3: Kết quả dự đoán cuối cùng được quyết định bằng cách kết hợp kết quả từ các base models.   3.4 Boosting\nNếu như các base models được train độc lập với nhau trong phương pháp bagging, thì ở phương pháp boosting, chúng lại được train một cách tuần tự. Base model sau được train dựa theo kết quả của base model trước đó để cố gắng sửa những lỗi sai tồn tại ở model này.\nCác bước tiến hành như sau:\n Bước 1: Tạo một tập dữ liệu con (tập A) từ tập dữ liệu ban đầu (tập D). Bước 2: Gán cho mỗi điểm dữ liệu trong tập A một trọng số w có giá trị giống nhau. Bước 3: Tạo một base model X và train trên tập A. Bước 4: Sử dụng model X để dự đoán trên toàn bộ tập D. Bước 5: Tính toán sai số dự đoán dựa vào kết quả dự đoán và kết quả thực tế. Bước 6: Gán giá trị w cao hơn cho những điểm dữ liệu bị dự đoán sai. Bước 7: Lặp lại bước 1,2,3,4,5,6 đối với base model mới, Y. Bước 8: Model cuối cùng (boosting model) sẽ là trung bình có trọng số của tất cả các base models.  Mỗi base model được gọi là một weak learner. Chúng sẽ không hoạt động tốt trên toàn bộ tập D, nhưng khi kết hợp nhiều weak learners ta được một strong learner. Strong learner này chắc chắn sẽ hiệu quả trên tập D. Ta nói, các weak learners đã boost performance cho strong learner.\nBagging và Boosting là 2 kỹ thuật quan trọng, hiệu quả. Có một số thuật toán đã được phát triển dựa trên nền tảng của chúng. Đặc biệt là thuật toán XGBoost. Trong bài tiếp theo, chúng ta sẽ đi chi tiết hơn về các thuật toán này.\nMời các bạn đón đọc!\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/04_comprehensive_guide_to_ensemble_model_1/","tags":["Machine Learning","Ensemble Learning","XGBoost"],"title":"XGBoost - Bài 1: Toàn cảnh về Ensemble Learning - Phần 1"},{"categories":["Machine Learning","Ensemble Learning","XGBoost"],"contents":"XGBoost là một thuật toán rất được quan tâm gần đây vì những ưu điểm vượt trội của nó so với các thuật toán khác. Vì vậy, mình quyết định sẽ viết một chuỗi các bài về chủ để này.\nNội dung các bài viết sẽ chủ yếu tập trung vào code thực hành, sẽ có (ít) lý thuyết toán để các bạn đỡ bị đau đầu. :D\nDanh sách các bài viết (sẽ cập nhật dần dần):\n Bài 1: Toàn cảnh về Ensemble models - Phần 1 Bài 2: Toàn cảnh về Ensemble models - Phần 2 Bài 3: Xây dựng model XGBoost đầu tiên Bài 4: Chuẩn bị dữ liệu cho XGBoost model Bài 5: Các phương pháp đánh giá độ chính xác của XGBoost model Bài 6: Trực quan hóa XGBoost model Bài 7: Lưu và sử dụng XGBoost model Bài 8: Lựa chọn Features cho XGBoost model Bài 9: Cấu hình Early_Stopping cho XGBoost model Bài 10: Cấu hình Multi-Threading cho XGBoost model Bài 11: Train XGBoost model trên AWS Bài 12: Tuning số lượng và kích thước của Decision Tree Bài 13: Tuning Learning_Rate và số lượng của Decision Tree Bài 14: Tuning Subsample  ","permalink":"https://tiensu.github.io/blog/03_xgboost-model-serial-introduction/","tags":["Machine Learning","Ensemble Learning","XGBoost"],"title":"XGBoost - Giới thiệu chuỗi bài viết về thuật toán XGBoost"},{"categories":["Machine Learning"],"contents":"Bài viết này nhằm mục đích tổng hợp, tóm tắt lại các thuật toán của Machine Learning, giúp bạn đọc có cái nhìn toàn cảnh và hiểu rõ hơn về Deep Learning.\nCác thuật toán ML, nhìn chung có thể phân loại theo một trong 2 cách:\n Theo cách thức \u0026ldquo;học\u0026rdquo; của thuật toán Theo cách thức làm việc của thuật toán  Cả 2 cách phân loại đều hợp lý, bạn có thể chọn tùy ý. Trong bài bài này, mình sẽ đi sâu hơn theo cách thứ 2. Cũng phải nói thêm rằng, cho dù phân loại theo cách nào thì cũng đều mang tính chất tương đối, vì một thuật toán có thể thuộc nhiều nhóm khác nhau, tùy thuộc vào dữ liệu đưa vào huấn luyện model.\n 1. Phân loại theo cách \u0026ldquo;học\u0026rdquo; (Learning Style)\n1.1 Học có giám sát (Supervised Learning)\nTrong cách học này, dữ liệu đưa vào huấn luyện model, gọi là input data, đi kèm với một nhãn đã biết trước (input data đã được dánh nhãn). Ví dụ như là spam/not-spam, giá cổ phiếu tại 1 thời điểm, \u0026hellip;\nTrong quá trình training, output của model được so sánh với nhãn. Nếu có sự sai khác, model sẽ cố gắng cập nhật các trọng số của nó để giảm sự sai khác đó đến một mức nào đó thỏa mãn yêu cầu bài toán.\nCác vấn đề có thể giải quyết theo cách này: Phân lớn, hồi quy.\nMột số thuật toán thuộc loại này: Logistic Regression, Backpropagation, \u0026hellip;\n 1.2 Học không giám sát (Unsupervised Learning)\nInput data không được đánh nhãn theo cách học này. Model được huấn luyện bằng cách giảm cấu trúc phức tạp của dữ liệu, tìm ra các đặc trưng, các mối liên hệ tương quan trong dữ liệu.\nCác vấn đề có thể giải quyết theo cách này: phân cụm, giảm chiều dữ liệu.\nMột số thuật toán thuộc loại này: Apriori, K-Means, \u0026hellip;\n 1.3 Học bán giám sát (Semi-supervised)\nInput data bao gồm cả 2 loại: đã đánh nhãn và không đánh nhãn.\nModel sẽ sử dụng kết hợp cả 2 cách học giám sát và không giám sát trong quá trình huấn luyện. Dựa vào kết quả dự đoán của model trên dữ liệu chưa đánh nhãn, nhà phát triển sẽ tốn ít công sức hơn trong việc đánh nhãn cho những dữ liệu đó. Độ chính xác của model sẽ được cải thiện dần dần khi có nhiều dữ liệu được đánh nhãn hơn.\nThực tế, tất cả các thuật toán đều có thể thuộc thể loại này vì không phải lúc nào cũng có đầy đủ dữ liệu được đánh nhãn ngay từ đầu.\n 2. Phân loại theo cách làm việc\n2.1 Regression Algorithms\nCác thùât toán được xếp vào nhóm này khi nhãn của dữ liệu là các giá trị liên tục. Ví dụ: nhiệt độ, giá tiền, diện tích, \u0026hellip;\nMột số thuật toán:\n Ordinary Least Squares Regression (OLSR) Linear Regression Logistic Regression Stepwise Regression Multivariate Adaptive Regression Splines (MARS) Locally Estimated Scatterplot Smoothing (LOESS)   2.2 Classification Algorithms\nCác thuật toán thụộc nhóm này khi nhãn của dữ liệu chỉ bao gồm một số lượng hữu hạn các giá trị. Ví dụ: Spam/not-spam, hình dạng (tròn, vuông, tam giác), \u0026hellip;\nMột số thuật toán:\n Linear Classifier Support Vector Machine (SVM) Kernel SVM Sparse Representation-based classification (SRC)   2.3 Instance-based Algorithms\nCác thuật toán thuộc nhóm này không \u0026ldquo;học\u0026rdquo; gì từ dữ liệu. Khi nào cần dự đoán nhãn cho dữ liệu mới, chúng sẽ quét toàn bộ dữ liệu ban đầu và tính toán tương quan với dữ liệu mới để quyết định nhãn.\nMột số thuật toán:\n k-Nearest Neighbor (kNN) Learning Vector Quantization (LVQ) Self-Organizing Map (SOM) Locally Weighted Learning (LWL)   2.4 Regularization Algorithms\nCác thuật toán có thể được mở rộng theo cách \u0026ldquo;trừng phạt\u0026rdquo; model dựa trên độ phức tạp của chúng, làm cho model trở nên đơn giản hơn, kết quả là \u0026ldquo;học\u0026rdquo; tốt hơn.\nMột số thuật toán:\n Ridge Regression Least Absolute Shrinkage and Selection Operator (LASSO) Elastic Net Least-Angle Regression (LARS)   2.5 Decision Tree\nĐây là phương pháp xây dựng model dựa vào trực tiếp giá trị thực tế của input data. Tùy theo các điều kiện cụ thể áp dụng vào input data mà model sẽ đưa ra các quyết định khác nhau. Trong ML, các thuật toán thuộc nhóm này được sử dụng khá phổ biến.\nMột số thuật toán: Classification and Regression Tree (CART)\n Iterative Dichotomiser 3 (ID3) C4.5 and C5.0 (different versions of a powerful approach) Chi-squared Automatic Interaction Detection (CHAID) Decision Stump M5 Conditional Decision Trees   2.6 Bayesian Algorithms\nĐây là họ các thụât toán áp dụng định luật Bayes trong xác suất thống kê.\nMột số thuật toán:\n Naive Bayes Gaussian Naive Bayes Multinomial Naive Bayes Averaged One-Dependence Estimators (AODE) Bayesian Belief Network (BBN) Bayesian Network (BN)   2.7 Clustering Algorithms\nDựa trên số lượng cụm (nhóm, lớp) cho trước, các thuật toán clusering sẽ phân bổ các điểm dữ liệu về từng lớp, dựa trên sự tương quan giữa các điểm dữ liệu đó với nhau.\nMột số thuật toán:\n k-Means k-Medians Expectation Maximisation (EM) Hierarchical Clustering   2.8 Association Rule Learning Algorithms\nCác thuật toán này tập trung vào việc tìm ra các quy tắc kết hợp giữa các điểm dữ liệu để sinh ra dữ liệu mới, hoặc dữ liệu tồn tại trong tập ban đầu.\nMột số thuật toán:\n Apriori algorithm Eclat algorithm   2.9 Artificial Neural Network Algorithms (ANN)\nĐược truyền cảm hứng từ cấu tạo não bộ của các loài động vật, các thuật toán này mô phỏng lại cách làm viêc của các bộ não đó. Chúng được cấu tạo gồm các layers và các nerurons liên kết với nhau.\nMột số thuật toán:\n Perceptron Multilayer Perceptrons (MLP) Back-Propagation Stochastic Gradient Descent Hopfield Network Radial Basis Function Network (RBFN)   2.10 Deep Learning (DL) Algorithms\nCác thuật toán DL là sự nâng cấp, mở rộng của thuật toán ANN. Chúng bao gồm các mạng ANN phức tạp hơn, giải quyết các bài toán với lượng dữ liệu lớn hơn.\nMột số thuật toán:\n Convolutional Neural Network (CNN) Recurrent Neural Networks (RNNs) Long Short-Term Memory Networks (LSTMs) Stacked Auto-Encoders Deep Boltzmann Machine (DBM) Deep Belief Networks (DBN)   Chi tiết hơn về các thuật toán ở nhóm này, mình sẽ đề cập trong bài tiếp theo. Mời các bạn đón đọc.\n2.11 Dimensionality Reduction Algorithms\nĐôi khi dữ liệu quá phức tạp sẽ làm giảm khả năng học của các ML model. Các thuật toán này sẽ giúp giải quyết vấn đề này bằng cách giảm bớt số chiều của dữ liệu (giảm độ phức tạp của dữ liệu).\nMột số thuật toán:\n Principal Component Analysis (PCA) Principal Component Regression (PCR) Partial Least Squares Regression (PLSR) Sammon Mapping Multidimensional Scaling (MDS) Projection Pursuit Linear Discriminant Analysis (LDA) Mixture Discriminant Analysis (MDA) Quadratic Discriminant Analysis (QDA) Flexible Discriminant Analysis (FDA)   2.12 Ensemble Algorithms\nEnsemble là phương pháp sử dụng kết hợp nhiều thuật toán khác nhau để tạo thành một thuật toán mới. Mỗi cách kết hợp khác nhau sẽ cho ra các thuật toán khác nhau. Trong ML, các thuật toán thuộc nhóm này được sử dụng rất phổ biến, đạt hiệu quả rất cao.\nMột số thuật toán:\n Boosting Bootstrapped Aggregation (Bagging) AdaBoost Weighted Average (Blending) Stacked Generalization (Stacking) Gradient Boosting Machines (GBM) Gradient Boosted Regression Trees (GBRT) Random Forest   2.13 Recommendation System Algorithms\nĐúng như tên gọi, đây là các thuật toán giải quyết bài toán khuyến nghị người dùng làm một việc gì đó bằng cách đưa cho họ những cái mà họ có thể quan tâm. Chúng thường được áp dụng trong các trang web thương mại điện tử, các ứng dụng xem phim trực tuyến, \u0026hellip;\nMột số thuật toán:\n Content based Collaborative filtering   2.14 Các thuật toán khác\nCòn rất nhiều thuật toán chưa được liệt kê bên trên, đó là những thuật toán giải quyết các bài toán cụ thể. Có thể kể ra một số như sau:\n Feature selection algorithms Algorithm accuracy evaluation Performance measures Optimization algorithms \u0026hellip;  Vậy là mình đã giới thiệu đến các bạn các thuật toán ML mà các bạn có thể gặp trong quá trình học ML. Hi vọng rằng các bạn đã có cái nhìn tổng quát về chúng, làm tiền đề để đi sâu hơn trong các bài toán ML về sau.\nTrong các bài viết tiếp theo, mình sẽ tổng hợp lại các thuật toán Deep Learning, sau đó sẽ đi chi tiết vào một số thụât toán với các ứng dụng cụ thể. Mời các bạn đón đọc!\n3. Tham khảo\n Machinelearningmastery Simplilearn  ","permalink":"https://tiensu.github.io/blog/02_machine_learning_algorithms_summary/","tags":["Machine Learning"],"title":"Tổng hợp các thuật toán Machine Learning"},{"categories":["Machine Learning","Project Management"],"contents":"Trong bất kỳ dự án nào, đứng ở góc độ của nhà đầu tư và người quản lý, họ đều muốn biết được mốc thời gian dự án có thể được hoàn thành trước khi thực sự bắt đầu dự án. Bởi vì thông tin này giúp cho họ đưa ra quyết định về ngân sách dành cho dự án, khả năng thu hồi vốn và sinh lời (ROI). Cuối cùng là kết luận xem dự án có đáng để đầu tư hay không?\nCác dự án trong lĩnh vực phát triển phần mềm cũng không ngoại lệ. Đối với các dự án phần mềm thông thường, hiện nay, có rất nhiều công cụ, kỹ thuật có thể giúp chúng ta lên kế hoạch, ước lượng khối lượng công việc ban đầu tuơng đối dễ dàng và đầy đủ. Tuy nhiên, các dự án AI sẽ có một vài điểm khác biệt cần lưu ý trong quá trình thực hiện các công đoạn như ước lượng chi phí, lập kế hoạch quản lý dự án. Trong bài viết này, chúng ta sẽ cùng bàn chiêt tiết về các vấn đề đó. Nội dung của mỗi phần được tham khảo từ nhiều nguồn, kết hợp với kinh nghiệm thực tế của bản thân.\n1. Ví dụ về 2 loại dự án phần mềm Loại dự án 1: Xây dựng một website bán hàng hoặc một ứng dụng mobile Ví dụ, dự án phát triên một website bán hàng. Yêu cầu của dự án là thiết kế các form, layout, button, database, và xử lý các hành vi của người dùng khi mua hàng. Những hành vi này thực chất là một chuỗi các bước được thực hiện tuần tự để đạt được một mục tiêu cụ thể nào đó. Sau đó, anh em developers chỉ cần code theo đúng các bước như vậy.\nLoại dự án 2: Phát triển một phần mềm nhận diện các giống mèo khác nhau trong ảnh Để thực hiện dự án này theo cách thông thường, cần phải hiểu rõ đặc tính sinh học, giải phẫu học của loài mèo, trích xuất những thông tin đó từ trong bức ảnh. Đây là một công việc không hề dễ dàng, và trong nhiều trường hợp không thể thực hiện được. Thay vì thế, ta có thể xây dựng một mô hình học máy (Machine Learning model - ML model) và cho nó \u0026ldquo;học\u0026rdquo; những đặc tính khác nhau của từng loài mèo. Từ đó, nó có thể dễ dàng phân loại các loài mèo khác nhau khi đưa cho nó một bức ảnh về mèo.\n2. Sơ lựơc về AI model Về bản chất, AI model là một tập hợp các công thức toán học, cùng với rất nhiều các tham số có thể điều chỉnh được trong suốt quá trình train (gọi là các parameters, phân biệt với hyper-parameters là các tham số không thay đổi thông qua quá trình train). Có thể hình dung một cách đơn giản, train một AI model giống như việc một người giáo viên đưa cho một học sinh bức ảnh và nói đó là con mèo. Người học sinh đóng vai trò là AI mode, sẽ cố gắng học những đặc điểm của bức ảnh để biết đó là con mèo. Còn người giáo viên chính là người train.\nCó rất nhiều thuật toán AI khác nhau, mỗi loại lại sử dụng những phép toán khác nhau, kiểu và số lượng các parameters cũng khác nhau. Điều này dẫn đến các AI model tương ứng cũng cho ra kết quả khác nhau trên cùng 1 tập dữ liệu. Và nhiệm vụ chính của người kỹ sư AI chính là làm sao xây dựng được AI model có độ chính xác cao nhất, thời gian xử lý nhanh nhất đối với 1 tập dữ liệu nhất định. Để xây dựng lên một AI model tốt, cần phải thực hiện rất nhiều xác thực nghiệm, thí nghiệm trên các thuật toán khác nhau, kết hợp với việc điều chỉnh các hyper-parameters hợp lý. Quá trình này gọi là \u0026ldquo;thử-sai\u0026rdquo; (try - error). Điều này nghe thì rất đơn giản nhưng thực tế thì không hề dễ dàng.\nVòng đời của một AI model có thể tóm tắt trong 6 bước:\n  Bussiness Understanding Hiểu yêu cầu bài toán, hiểu vấn đề của khách hàng. Data Understanding \u0026amp; Collection Hiểu dữ liệu, cần dữ liệu như thế nào để train được model tốt. Khách hàng có thể có rất nhiều dữ liệu dạng thô, nhưng họ không biết cách sử dụng, khai thác. Mình phải là người hướng dẫn họ. Data Preparation Dữ liệu thu thập được cần phải được chuẩn hóa trước khi đưa vào train AI model. Modeling Có dữ liệu rồi, lựa chọn thuật toán phù hợp và tiến hành train mô hình. Quá trình này có thể kéo dài vì chúng ta phải \u0026ldquo;thử-sai\u0026rdquo; rất nhiều lần. Evaluation Model sau khi train xong cần được đánh giá dựa theo 1 tiêu chí cụ thể nào đó. Nếu chưa thỏa mãn thì lại quay lại các bước trước đó. Deployment Sau khi model được train thoả mãn yêu cầu, nó sẽ được đem vào triển khai trong sản phẩm thực tế. Vòng đời của một AI model là tuần hoàn khép kín vì nó luôn phải được cập nhật theo dữ liệu mới. Để làm tốt các bước 1,2 thì cần phải trải qua kinh nghiệm thực tế để có cảm quan tốt về vấn đề, đồng thời có thể phải cần hiểu về quy trình nghiệp vụ của từng bài toán. Các bước còn lại, có khá nhiều kỹ thuật, phuơng pháp xử lý. Mình sẽ giới thiệu trong các bài sau.  3. Tại sao dự án AI khó ước lượng? Một người kỹ sư phần mềm với kinh nghiệm lâu năm của mình có thể dễ dàng ước lượng được thời gian cần thiết để có thể hoàn thành một công việc, bởi vì họ đã làm những công việc tương tự như thế rất nhiều lần rồi.\nMột người kỹ sư AI có kinh nghiệm, đã từng xây dựng nhiều AI model, độ chính xác lên đến 90%. Tuy nhiên, nếu được hỏi mất bao nhiêu thời gian để họ có thể phát triển AI model tương tự, đạt được độ chính xác như thế, họ sẽ rất khó để trả lời. Tại sao lại có sự khác biệt đó? Có một câu nói rất nổi tiếng trong giới \u0026ldquo;AI\u0026rdquo; rằng: No free launch. Điều này ngụ ý là không có một công thức, một cách tiếp cận hay một phuơng pháp nào chung cho các vấn đề trong AI. ML model của bạn có thể đạt độ chính xác cao đối với vấn đề A, nhưng nếu bạn sử dụng cách thức train ML model đó để tạo ra ML model cho vấn đề B, không có gì đảm bảo rằng ML mới cũng hoạt động tốt.\nMột vấn đề tối quan trọng nữa, đó là dữ liệu. Có thể nói dữ liệu là vấn đề sống còn của ML model. Thống kê chỉ ra rằng một người kỹ sư AI dành đến 80% thời gian của họ để làm việc với dữ liệu. 20% thời gian còn lại dành cho việc train và triển khai ML model. Nếu bạn chưa có chút hiểu biết gì về dữ liệu bạn cần sử dụng thì mọi ước lượng của bạn có thể coi như là vô nghĩa. \u0026ldquo;Garbage in, garbage out\u0026rdquo; - hãy nhớ điều này.\nNgoài ra, bạn cũng nên biết rằng mỗi lần điều chỉnh, cập nhật ML model đều phải dựa trên kết quả của lần train trước đó. Vì thế mà việc xây dựng một bản kế hoạch chi tiết trong thời gian dài cho 1 dự án AI là điều không thực tế.\n4. Rủi ro của dự án AI/ML Có 3 vấn đề cần lưu ý (risks) trong một dự án AI: 1. Chưa xác định được cách thức xây dựng ML model phù hợp: Chọn thuật toán, chọn tham số, \u0026hellip; 2. Thiếu dữ liệu train ML model 3. Khó lập kế hoạch chi tiết\nVấn đề đầu tiên có thể được giải quyết bằng cách thực hiện điều tra nghiên cứu tính khả thi của ML model trước khi thực sự bắt đầu dự án. Thậm chí, việc này có thể coi là một dự án PoC, làm tiền đề cho dự án thực sự phía sau. Trong giai đoạn PoC, chúng ta sẽ thử nghiệm các thuật toán, các bộ tham số, các cách thức tiếp cận khác nhau để giải quyết một vấn đề nhỏ nhưng tiêu biểu cho dự án lớn. PoC không chỉ giúp chúng ta xác định được các khả năng có thể và không thể của ML model đối với yêu cầu của dự án mà còn chỉ ra được những yêu cầu cụ thể đối với dữ liệu đầu vào. Thông qua PoC, người kỹ sư AI sẽ hiểu rõ hơn về nghiệp vụ, từ đó rút ngắn được danh sách các thuật toán PoC cần thử nghiệm. Và thậm chí là không cần dùng đến ML model cũng có thể giải quyết được yêu cầu của bài toán đặt ra. Nếu mà kết quả của PoC không được như mong đợi, không chọn ra được bất kỳ phuơng pháp nào đáp ứng yêu cầu dự án thì hoặc là cần phải dành thêm thời gian để nghiên cứu hoặc là quyết định dừng dự án lại, tránh lãng phí thời gian và tiền bạc. Sau khi đã giải quyết được vấn đề đầu tiên thì vấn đề thứ 2 đã sáng tỏ hơn. Chúng ta đã biết là cần dữ liệu như thế nào. Việc còn lại chỉ là làm thế nào để có được dữ liệu đó. Thông thường, khi đưa ra bài toán, khách hàng thường đã có dữ liệu (dạng thô) rồi. Người kỹ sư AI phải làm sao sử dụng được dữ liệu đó một cách hiệu quả và hợp lý để tăng được độ chính xác của ML model. Trong trường hợp, khách hàng không có sẵn dữ liệu thì chúng ta phải hướng dẫn họ cách thức thu thập dữ liệu, sao cho đơn giản, nhanh chóng và chính xác dữ liệu được mong đợi. Tuy nhiên, việc làm này có một hạn chế đó là việc thu thập dữ liệu trong một thời gian ngắn thường sẽ khó có thể bao quát hết các trường hợp xảy ra trong thực tế nghiệp vụ của khách hàng (vấn đề bias dữ liệu trong AI). Điều này chắc chắn sẽ làm giảm độ chính xác của ML model khi triển khai trong thực tế. Tất nhiên, theo thời gian, ML model sẽ được cập nhật theo dữ liệu thực tế thì độ chính xác cũng sẽ được tăng lên. Nhưng làm sao để khách hàng hiểu và chấp nhận điều này thì lại là một vấn đề không đơn giản. :D Vấn đề còn lại, giải quyết bằng cách áp dụng nguyên lý Agile-Scrum trong quản trị dự án. Chúng ta lập kế hoạch ngắn hạn cho từng Sprin. Kế hoạch của Sprin tiếp theo sẽ phụ thuộc vào kết quả của Sprin trước đó. Trong quá trình thực hiện dự án, cũng nên liên tục trao đổi, chia sẻ thông tin với khách hàng để điều chỉnh kế hoạch cho phù hợp, đảm bảo dự án đang đi đúng hướng.\n5. Kết luận Vấn đề về chất lượng dữ liệu và việc không có phương pháp chính xác ngay từ đầu yêu cầu phải có giai đoạn \u0026ldquo;research\u0026rdquo; - nghiên cứu tìm hiêu (PoC) trước khi chính thức bắt đầu một dự án AI. Tuy nhiên, những rủi ro này thường không đuợc xem xét trong quá trình ước lựong và lập kế hoạch cho một dự án AI. Cần nhấn mạnh một điều rằng, dự án AI thuộc thể loại \u0026ldquo;experiment-driven\u0026rdquo;, tức là dự án phải trải qua rất nhiều \u0026ldquo;experiments\u0026rdquo; - thực nghiệm, thí nghiệm. Và hành động tiếp theo phụ thuộc vào kết quả của hành động trước đó. Nếu bạn đã từng trải qua những dự án AI, hãy chia sẻ kinh nghiệm của bạn dưới phần bình luận! Rất vui nếu được tiếp thu những ý kiến đóng góp của mọi người!\nBài viết có tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/01_ai-project-planing/","tags":["Machine Learning","Project Management"],"title":"Lưu ý khi lập kế hoạch cho một dự án AI"}]