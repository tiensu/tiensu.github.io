[{"categories":["Audio Classification","Speech Recognition","Speech-to-Text"],"contents":"Bên cạnh Images, Text thì Sound cũng là một dạng dữ liệu mà chúng ta thường gặp trong đời sống hàng ngày. Trong chuỗi 5 bài tiếp theo, chúng ta sẽ tìm hiểu chi tiết về 2 bài toán sử dụng Audio làm dữ liệu đầu vào là Audio Classification và Speech Recognition (Speech to Text), sử dụng các mô hình Deep Learning.\n Bài 1: Giới thiệu chung về bài toán Audio Clasifidation Bài 2: Mel Spectrograms Bài 3: Feature Optimization and Augmentation Bài 4: Xây dựng Audio Classification model bằng PyTorch Bài 5: Tìm hiểu bài toán Speech Recognition (Speech to Text)  Nội dung của bài đầu tiên bao gồm một số lý thuyết chung về Audio, các ứng dụng liên quan đến Sound. Chúng ta cũng tìm hiểu về Spectrogram và một số kỹ thuật, kiến trúc mô hình để làm việc với Sound.\n1. Kiến thức chung về Sound Signal\nTừ hồi học phổ thông chúng ta đã biết, Sound là một dạng tín hiệu được sinh ra từ sự thay đổi áp suất không khí, bắt nguồn từ một dao động cơ học nào đó. Cường độ của sự thay đổi áp suất này có thể đo được, và nó chính là biên độ (Applitude) của Sound Signal.\nSound Signal thường lặp đi lặp lại theo một chu kỳ T, đồ thị của nó có dạng sóng.  Giá trị nghịch đảo của chu kỳ T, ký hiệu là f, gọi là tần số của Sound Signal. Nó thể hiện mức độ dao động của Signal trong thời gian 1s (bằng số đỉnh của Signal trong 1s). Đơn vị của f là Hertz.\nTrong thực tế, đồ thị của Sound Signal thường không đơn giản dạng Sin như vậy, mà phức tạp hơn rất nhiều. Tuy nhiên, chúng vẫn có dạng sóng và có chu kỳ. Ví dụ, đồ thị của một dụng cụ âm nhạc như dưới đây:  Nhìều Sound Signal có thể được tổng hợp thành một Sound Signal duy nhất.\nVề mặt cảm thụ sinh học, mỗi Sound Signal có một đặc trưng riêng, gọi là âm sắc (timbre). Tai người có thể phân biệt được các Sound khác nhau dựa vào âm sắc của các Sound đó.\n2. Số hóa (Digitize) Sound Signal\nNguyên bản, Sound Signal là một dạng tín hiệu tương tự (Analog Signal) liên tục theo thời gian. Tuy nhiên, để thuận lợi trong việc lưu trữ, xử lý và truyền tải, Sound Signal được chuyển sang dạng Số (Digital Signal). Việc chuyển đổi này phải đảm bảo không làm mất mát quá nhiều thông tin so với tín hiệu gốc, và từ tín hiệu đã chuyển đổi có thể dễ dàng khôi khục lại gần như nguyên vẹn tín hiệu ban đầu. Số hóa Sound Signal được thực hiện bằng cách lấy giá trị biên độ của nó tại các vị trí cách đều nhau trong mỗi chu kỳ.  Mỗi vị trí như vậy được gọi là một mẫu (Sample). Ta có khái niệm Tần số lấy mẫu (Sample Rate) là số lượng mẫu trong 1s.\nMột câu hỏi đặt ra là giá trị của Sample Rate là bao nhiêu là hợp lý. Hai nhà khoa học Nyquist và Shannon đã đồng thời, độc lập đưa ra một định lý, gọi là Định lý lấy mẫu Nyquist–Shannon, về việc xác đinh giá trị của Sample Rate. Định lý phát biểu như sau:\nMột hàm số tín hiệu x(t) không chứa bất kỳ thành phần tần số nào lớn hơn hoặc bằng một giá trị fm có thể biểu diễn chính xác bằng tập các giá trị của nó với chu kỳ lấy mẫu T = 1/(2fm).\nTừ định lý này có thể dễ dàng suy ra, Tần số lấy mẫu phải thoả mãn điều kiện $f_s$ ≥ $2f_m$. Tần số giới hạn $f_s/2$ này được gọi là tần số Nyquist và khoảng $(-f_s/2; f_s/2)$ gọi là khoảng Nyquist. Thực tế, tín hiệu trước khi lấy mẫu sẽ bị giới hạn bằng một bộ lọc để tần số tín hiệu nằm trong khoảng Nyquist.\nĐịnh lý Nyquist–Shannon được áp dụng cho tín hiệu nói chung chứ không phải chỉ riêng tín hiệu âm thanh.\nĐến đây, chúng ta cần phân biệt được 2 khái niệm Audio và Sound. Sound có nguồn gốc là các dao động cơ học lan truyền trong các môi trường đàn hồi (rắn, lỏng, khí), còn Audio được sinh ra từ các thiết bị điện tử thông qua quá trình lấy mẫu, ghi âm, \u0026hellip;\n3. Chuẩn bị dữ liệu Audio cho Deep Learning model\nHãy nhớ lại khoảng 10 năm trước, khi mà các kỹ thuật Deep Learning còn chưa phát triển, chúng ta phải sử dụng các phương pháp thống kê, xử lý truyền thống để có thể trích xuất được các đặc trưng làm đầu vào cho các thuật toán Machine Learning. Cụ thể, đối với lĩnh vực Computer Vision, chúng ta sử dụng các kỹ thuật xử lý ảnh như phát hiện biên, ngưỡng nhị phân, \u0026hellip; Đối với lĩnh vực NLP, chúng ta sử dụng phương pháp N-gram, Term Frequency, \u0026hellip;\nDữ liệu Audio cũng không ngoại lệ, chúng ta phải thực hiện rất nhiều công việc tay chân, sữ dụng các phương pháp phân tích ngữ âm, âm vị, \u0026hellip; để có thể tạo được các vectors đặc trưng của chúng.\nNgày nay, với sự trợ giúp của Deep Learning, việc trích xuất đặc trưng để chuẩn bị dữ liệu cho các mô hình học máy trở nên đơn giản hơn rất nhiều. Riêng đối với dữ liệu Audio, chúng ta sẽ chuyển chúng sang dạng Image và sử dụng kiến trúc CNN kinh điển để xử lý chúng. Từ Audio chuyển sang Image, điều này nghe có vẻ rất kỳ lạ, nhưng thực tế đó lại là cách làm rất bình thường trong các bài toán xử lý dữ liệu Audio. Để hiểu rõ hơn về vấn đề này, trước tiên chúng ta cần tìm hiểu Spectrum và Spectrogram.\nHình dưới đây thể hiện Spectrum của một đoạn nhạc. Trục tung là giá trị biên độ, trục hoành là giá trị tần số của mỗi tín hiệu thành phần.  Tần số có giá trị nhỏ nhất được gọi là tần số Cơ bản. Các tần số khác là bội số của tần số cơ bản được gọi là Sóng hài (harmonics). Ví dụ, nếu tần số cơ bản là 200Hz, thì các sóng hài của nó là 400Hz, 600Hz, \u0026hellip; 3.1 Spectrum\nNhư đã nói ở trên, một Sound Signal trong thực tế thường là sự tổng hợp của nhiều tín hiệu thành phần khác nhau. Ví dụ, tiếng nói của chúng ta bao gồm cả các tạp âm (Noise) xung quanh. Mỗi tín hiệu thành phần đó lại có tần số khác nhau, và do vậy, tổng hợp các tần số thành phần ta có tần số của Sound Signal.\nSpectrum chính là tập hợp các tần số của các tín hiệu thành phần tạo nên Sound Signal của chúng ta.\n3.2 Miền thời gian và miền Tần số\nMỗi Sound Signal đều có 2 miền giá trị: Thời gian và Tần số. Trong mỗi miền đó, Sound Signal được thể hiện theo cách khác nhau.   Thể hiện trong miền thời gian Trong miền thời gian, Sound Signal mô tả sự thay đổi của biên độ theo thời gian. Biên độ nằm trên trục tung và thời gian nằm trên trục hoành (xem đồ thị bên trên). Thể hiện trong miền tần số Trong miền tần số, Sound Signal mô tả sự thay đổi của biên độ theo tần số. Biên độ nằm trên trục tung và tần số nằm trên trục hoành (xem đồ thị bên trên).  3.3 Spectrogram\nỞ mục 3.2, ta đã nói đến mỗi liên hệ giữa biên độ với thời gian và biên độ với tần số. Hơn thế nữa, tần số và và thời gian cũng có mối liên hệ với nhau. Đồ thị thể hiện mối liên hệ này gọi là Spectrogram, trong đó, trục X thể hiện thời gian và trục Y thể hiện tần số. Nói một cách dễ hiểu thì Spectrogram thể hiện sự thay đổi của tần số theo thời gian. Không chỉ có vậy, độ lớn của biên độ cũng được Spectrogram thể hiện thông qua màu sắc. Màu sắc càng sáng thì thì biên độ càng lớn và ngược lại.\nTrong 2 hình bên dưới, hình thứ nhất thể hiện Soung Signal trong miền thời gian. Tại t = 600000, biên độ có giá trị lớn nhất. Hình thứ 2 là Spectrogram của Sound Signal đó. Tương ứng với giá trị lớn nhất của biên độ là vùng màu sắc sáng nhất trên Spectrogram.\n Có thể nói, Spectrogram là cách thể hiện tốt nhất của Sound Signal, dưới dạng một hình ảnh, bởi vì nó mang đầy đủ thông tin về thời gian, tần số và biên độ. Và do vậy, Spectrogram được sử dụng làm dữ liệu đầu vào cho các Deep Learning model, như một bức ảnh thông thường.\nSpectrogram được sinh ra bằng cách áp dụng phép biến đổi Fourier lên một Signal để phân tách Signal đó thành các tần số thành phần. Có lẽ đa số chúng ta đã quên mất cách tính Fourier bằng tay như thế nào, mặc dù chúng ta đã được học ở bậc đại học. Rất may là có một số thư viên của Python sẽ giúp chúng ta thực hiện Fourier Transform một cách dễ dàng.\n4. Audio Deep Learning models\n Hầu hết các Deep Learning models đều sử dụng Spectrograms để làm đặc trưng cho Audio. Luồng xử lý sẽ như sau:\n Chuyển đổi Raw Audio sang Spectrogram. Áp dụng một số kỹ thuật tăng cường dữ liệu. Các kỹ thuật này cũng có thể áp dụng cho Raw Audio. Bước này không bắt buộc. Xây dựng CNN model và huấn luyện nó. Output của CNN là các Feature Maps. Tùy vào bài toán cụ thể mà chúng ta sinh ra các dạng Output khác nhau.  Audio Classification: Cho Feature Maps đi qua một bộ phân lớp (FC, SVM, \u0026hellip;) để sinh ra nhãn cho Audio Speech to Text: Cho Feature Maps đi qua RNN để sinh ra Text tương ứng với Audio.    5. Một số bài toán mà Deep Learning có thể giải quyết đối với dữ liệu Audio\nAudio luôn tồn tại xung quanh chúng ta: giọng người nó, tiếng kêu động vật, tiếng máy móc hoạt động, \u0026hellip; Áp dụng Deep Learning, chúng ta có thể giải quyết được một số bài toán sau:\n5.1 Audio Classification\nĐây có lẽ là bài toán phổ biến nhất của Audio. Input là 1 đoạn Audio, Output là nhãn của nó.  Một số ứng dụng thực tế của bài toán này:\n Phát hiện sự hư hỏng của máy móc thông qua tiếng kêu khi hoạt động bình thường và khi hư hỏng. Phát hiện trộm dựa vào phát hiện tiếng đập vỡ cữa kính Phát hiện bệnh thông qua tiếng ho, tiếng thở \u0026hellip;  5.2 Audio Separation and Segmentation\nBài toán này liên quan đến viêc tách riêng các tín hiệu thành phần từ tín hiệu gốc ban đầu.  Một số ví dụ ứng dụng thực tế:\n Tách riêng giọng của mỗi người trong 1 cuộc họp. Tách riêng âm thanh của một loại nhạc cụ trong một buổi hòa nhạc. Tách giọng ca sĩ khỏi bài hát.  5.3 Music Generation and Music Transcription\nCũng giống như GAN hay Language Model, một số Deep Learning model có thể sinh ra các bản nhạc theo thể loại, loại nhạc cụ hay thậm chí là phong cách của một nhạc sĩ cụ thể. Đó gọi là Music Generation.\nỞ chiều ngược lại, từ một bản nhạc dạng Audio, Deep Learning model có thể dịch ngược lại thành Text, kèm theo giai điệu, node nhạc, \u0026hellip;  5.4 Voice Recognition\nThực chất, đây cũng là bài toán Audio Classification nhưng được áp dụng cho giọng nói của con người.  Một số ứng dụng thực tế của bài toán này:\n Phân biệt giọng nam và giọng nữ Nhận diện tên người thông qua giọng nói của họ Nhận diện cảm xúc thông qua giọng nói của họ \u0026hellip;  5.5 Speech to Text and Text to Speech\nĐối với giọng nói của con người, chúng ta có thể đi sâu hơn một chút. Đó là không chỉ nhận dạng người nói là ai mà còn có thể hiểu được người đó nói gì. Điều này được gọi là bài toán Speech to Text, tức chuyển Audio thành văn bản.  Theo hướng ngược lại, chúng ta có bài toán Text to Speech, hay Speech Synthesic, tức là chuyển đổi một văn bản thành dạng Audio.\nCả 2 bài toán này đều có rất nhiều ứng dụng hữu ích trong cuộc sống, ví dụ như trợ lý ảo thông mình Siri, Alexa, Cortana hay Google Home.\n6. Kết luận\nBài đầu tiên trong chuỗi các bài viết về Audio Deep Learning này, chúng ta đã tìm hiểu ở mức khái quát lý thuyết về Sound Signal, các số hóa Sound Signal, cách tạo dữ liệu cho Deep Learning model sử dụng Spectrogram. Cuối cùng, chúng ta cũng điểm qua một số bài toán cụ thể mà Deep Learning có thể giải quyết đối với dữ liệu Audio.\nỞ bài tiếp theo, mình sẽ cùng các bạn tìm hiểu chi tiết về Mel Spectrogram, một dạng biến đổi khác từ Spectrogram mà có thể giúp cho các Deep Learning model học tốt hơn. Mời các bạn đón đọc.\n4. Tham khảo\n[1] Ketan Doshi, \u0026ldquo;Audio Deep Learning Made Simple (Part 1): State-of-the-Art Techniques\u0026rdquo;, Available online: https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504 (Accessed on 20 May 2021).\n[2] Wikipedia, \u0026ldquo;Định lý lấy mẫu Nyquist–Shannon\u0026rdquo;, Available online: https://vi.wikipedia.org/wiki/%C4%90%E1%BB%8Bnh_l%C3%BD_l%E1%BA%A5y_m%E1%BA%ABu_Nyquist%E2%80%93Shannon (Accessed on 20 May 2021).\n","permalink":"https://tiensu.github.io/blog/67_audio_deep_learning_part_1/","tags":["Audio Classification","Speech Recognition","Speech-to-Text"],"title":"Giới thiệu chung về các bài toán Sound/Audio sử dụng Deep Learning"},{"categories":["Object Detection"],"contents":"Object Detection là một trong những bài toán rất phổ biến trong lĩnh vực Computer Vision. Ngày nay, với sự giúp đỡ của các thuật toán Deep Learning, bài toán này đã được giải quyết khá tốt. Các thuật toán nối tiếng có thể kể ra ở đây là: R-CNN, Fast R-CNN, Faster R-CNN, YOLOv1, YOLOv2, YOLOv3, YOLOv4, YOLOv5, SSD, EfficientDet. Trong bài này, hãy cùng nhau tóm tắt lại các thuật toán Deep Learning đó nhé!\n1. Các thuật toán Object Detection\n1.1 R-CNN\nĐây có thể coi là thuật toán Deep Learning đầu tiên giải quyết bài toán Computer Vision. Trước đó, cũng có một vài thuật toán khác (không phải Deep Learning), như Exhaustive Search, \u0026hellip; Nhược điểm chung của các thuật toán này là chúng yêu cầu tài nguyên tính toán rất lớn, thời gian xử lý cũng rất lâu.\nĐể giải quyết những yếu điểm đó, R-CNN đề xuất sử dụng phương pháp Selective Search để trích xuất thông tin từ khoảng 2000 khu vực trên bức ảnh (mỗi khu vực được gọi là Region Proposal). Các thông tin được trích xuất sau đó sẽ được đưa qua một mạng CNN để xác định vị trí cũng như phân loại đối tượng.  Mặc dù vậy, R-CNN vẫn cần trung bình khoảng 50s để xử lý một bức ảnh. Nếu ảnh có nhiều đối tượng thì thời gian xử lý còn lâu hơn nữa.\n1.2 Fast R-CNN\nFast R-CNN ra đời để giải quyết hạn chế của R-CNN. Thay vì chia bức ảnh thành các Region Proposals và đưa chúng vào mạng CNN như R-CNN, Fast R-CNN đưa toàn bộ bức ảnh vào mạng CNN một lần để sinh ra Feature Map. Từ Feature Map này, các Region Proposals mới được nhận diện và đưa vào mạng FC. Cuối cùng, Softmax được sử dụng để dự đoán nhãn cho mỗi đối tượng và tính toán tọa độ các Bouding Boxs của chúng.  Fast R-CNN nhanh hơn R-CNN vì mạng CNN chỉ hoạt động một lần trên mỗi bức ảnh.\n1.3 Faster R-CNN\nTương tự như Fast R-CNN, toàn bộ bức ảnh cũng được đi qua mạng CNN đẻ sinh ra Feature Map. Điểm khác ở đây là Faster R-CNN sử dụng một Region Proposal Network (RPN) thay vì Selective Search để sinh ra các Region Proposals. Tiếp đó, ROI Pooling nhận đầu vào là các Region Proposals đó để sinh ra nhãn dự đoán và tọa độ của Bounding Box cho mỗi đối tượng trong ảnh.  So sánh về mặt thời gian xử lý giữa 3 thuật toán trong hộ R-CNN, có sự giảm dần từ R-CNN -\u0026gt; Fast R-CNN -\u0026gt; Faster R-CNN.  1.4 YOLO - You Look Only Once\nCác thuật toán trong họ R-CNN có thể được phân loại là nhóm Two Stages Detector, bởi vì cách làm việc của chúng bao gồm 2 bước (2 stages). Đầu tiên, chúng lựa chọn ROI (Region of Interest) trong bức ảnh. Sau đó, chúng phân loại các ROI đó sử dụng mạng CNN. Đó chính là nguyên nhân làm cho tốc độ thực thi của những thuật toán đó tương đối chậm.\nYOLO là một thuật toán Single Stage Detecto (Single Shot Detector), nghĩa là chúng sẽ dự đóan nhãn và vị trí của đối tượng trong toàn bộ bức ảnh chỉ với một lần chạy thuật toán duy nhất. Và tất nhiên, cách làm việc này giúp cho thời gian xử lý của YOLO rất nhanh, phù hợp với các ứng dụng cần chạy Realtime.\nCách làm viêc của YOLO có thể tóm tắt như sau:\n Chia bức ảnh thành các Cells. Ví dụ: 19x19, 13x13, \u0026hellip; Mỗi Cell chịu trách nhiệm dự đoán ra b Bounding Box. b = 3, 5, 7, \u0026hellip; Ouput của việc dự đoán bao gồm:  Tọa độ trung tâm của Bounding Box ($b_x, b_y$) Chiều rộng của Bounding Box ($b_w$) Chiều cao của Bounding Box ($b_h$) Nhãn của đối tượng (nếu có) trong Bounding Box (c) Xác suất có đối tượng trong Bounding Box ($p_c$)     Hầu hết các Cell và Bounding Box không chứa đối tượng. Quy định một giá trị ngưỡng cho $p_c$ để loại bỏ bớt những Bounding Box này. Cách này gọi là NMS (Non-Max Suppression).  Darknet github phổ biến nhất của YOLO. Bạn có thể sử dụng nó cho các dự án của mình.\nTiếp sau thành công của YOLOv1, các phiên bản tiếp theo YOLOv2, YOLOv3, YOLOv4, YOLOv5 lần lượt ra đời, phiên bản sau kế thừa ưu điểm và hạn chế nhược điểm của phiên bản trước đó.\n  YOLOv1:\n 24 Conv layers, 2 FC layers -\u0026gt; tổng cộng có 26 layers. Nhược điểm chính là khả năng phát hiện những đối tượng kích thước nhỏ rất kém. Chi tiết xem tại đây    YOLOv2:\n Thêm BN layers -\u0026gt; tổng cộng có 30 layers. Có thêm 3 Anchor Boxes. Không có FC layers. Kích thước của Training Image được Scale trong khoảng 320-608. Mỗi đối tượng có thể có nhiều nhãn. Vẫn gặp khó khăn khi phát hiện những đối tượng có kích thước nhỏ. Chi tiết xem tại đây    YOLOv3:\n Tổng cộng 106 layers. Thay đối cách tính Loss Function. Sử dụng 9 Anchor Boxes. Phát hiện đối tượng kích thước nhỏ khá tốt. Chi tiết xem tại đây    YOLOv4:\n Sử dụng CSPDARKNET53 model làm Backbone. Neck — Spatial pyramid pooling and Path Aggregation Network Head — Class subnet and Box subnet, as well as in YOLOv3 Hỗ trợ rất nhiều Frameworks: TensorFlow, OpenCV, OpenVINO, PyTorch, TensorRT, ONNX, CoreML, etc. So với YOLOv3,mAP và FPS của YOLOv4 tăng tương ứng 10% và 12%. So với EfficientDet, YOLOv4 nhanh hơn khoảng 2 lần. Chi tiết xem tại đây       YOLOv5:  Ra đời chỉ vài ngày sau YOLOv4 Có 4 phiên bản khác nhau: YOLOv5s, YOLOv5m, YOLOv5l, YOLOv5x. Độ chính xác tăng dần và tốc độ giảm dần theo thứ tự đó. So sánh với YOLOv4:  \u0026ldquo;If you’re a developer looking to incorporate near realtime object detection into your project quickly, YOLOv5 is a great choice. If you’re a computer vision engineer in pursuit of state-of-the-art and not afraid of a little more custom configuration, YOLOv4 in Darknet continues to be most accurate.\u0026rdquo; CSPDarknet53s-YOSPP gets 19.5% faster model inference speed and 1.3% higher AP than YOLOv5l.      1.5 SSD - Single Shot Detector\nSSD cũng là một thuật toán thuộc nhóm Single Stage Detector. Ảnh đầu vào được cho đi qua mạng VGG-16 để trích xuất Feature Maps với các hệ số Scale khác nhau. Tiếp theo, nó dự đoán nhãn và vị trí của đối tượng sử dụng Conv layers. Output là một tập các Bounding Box và xác suất có đối tượng bên trong đó. Cuối cùng áp dụng NMS để loại bỏ đi những Bounding Box không phù hợp.\nChi tiết về SSD, bạn có thể tham khảo ở đây\n Khá ngạc nhiên là mặc dù ra đời từ năm 2015 nhưng đến nay vẫn không có thêm phiên bản mới nào của SSD, trong khi YOLO thì ra phiên bản mới liên tục.\nVề tốc độ xử lý và độ chính xác thì nói chung SSD ngang ngửa với YOLOv3 và chỉ chịu thua YOLOv4. Các bạn có thể xem so sánh chi tiết ở đây, và ở đây nữa\nNgoài các thuật toán kể trên, còn có 1 số thuật toán Object Detection khác như EfficientDet, RatinaNet, \u0026hellip; Bạn có thể đọc thêm về những thuật toán này từ các nguồn trên Internet. Đến thời điểm hiện tại, ý kiến của cá nhân mình thì YOLOv4 đang là State-of-Art cho bài toán Object Detection, cả về tốc độ và độ chính xác. Mình luôn áp dụng YOLOv4 đầu tiên trong các dự án về Object Detection của mình, nếu có vấn đề gì thì mới chuyển qua thuật toán khác.\n2. Metrics đánh giá Object Detection model\nĐể đánh giá một Classification model, ta thường sử dụng các Metrics: Precision, Recall, F1-Score, Accuracy.\nĐể đánh giá một Regression model, ta thường sử dụng các Metrics: Root Mean Square (RMS), Mean Square Error (MSE), Mean Absoluate Error (MAE), \u0026hellip; Nhược\nĐể đánh giá một Object Detection model, ta dùng Metrics nào?\nĐể trả lời câu hỏi này, hãy nhớ lại 2 mục đích của một Object Detection model là:\n Classification: Phân loại đối tượng trong bức ảnh thuộc vào lớp nào? Localization: Xác định chính xác vị trí của đối tượng trong bức ảnh.  Vì vậy, Metrics đánh giá một Object Detection model cần phải kết hợp cả 2 mục tiêu này. Hiện nay, Mean Average Precision (mAP) được sử dụng rộng rãi để đánh giá các Object Detection model. Về bản chất, mỗi một lớp (class - nhãn) trong bài Object Detection model có một giá trị AP riêng, mAP là trung bình của các giá trị AP đó. Vì AP được tính cho mỗi lớp nên trong cách tính AP dưới đây, bạn có thể thấy rằng ta chỉ sử dụng kết quả dự đoán Bounding Box mà không sử dụng kết quả dự đoán nhãn của model.\n2.1 Intersection over Union - IoU\nTrước tiên, cần hiểu khái niệm IoU. Như cái tên của nó đã gợi nhớ, IoU là tỉ số giữa phần giao nhau và phần hợp nhau của 2 Bounding Box. Ground Trust(GT - Bounding Box mà ta vẽ bằng tay khi đánh nhãn đối tượng) Bounding Box và Bounding Box mà model dự đoán ra được.  Giá trị IoU nằm trong khoảng [0;1]. IoU = 0 ngụ ý rằng model dự đoán sai hoàn toàn, IoU = 1 ngụ ý rằng model dự đoán chính xác hoàn toàn.\nThông thường, sẽ có một giá trị ngưỡng (threshold) của IoU để xác định Bounding Box dự đoán ra có được chấp nhận hay không? Threshold thường là 0.5 hoặc 0.75. Nếu IoU \u0026gt; Threshold, Bounding Box là hợp lệ và ngược lại.\n2.2 Precision và Recall\nPrecision và Recall được xác định thông qua công thức: $Precision = \\frac{TP}{TP + FP}$\n$Recall = \\frac{TP}{TP + FN}$\n Sử dụng IoU và Threshold, ta có thể xác định được TP, FP, TN, FN như sau:\n Nếu IoU \u0026gt;= Threshold, Bounding Box được coi là TP - True Positive. Nếu IoU \u0026lt; Threshold, Bounding Box được coi là FP - False Positive. Nếu có đối tượng trong bức ảnh nhưng model không phát hiện được (không dự đoán được Bounding Box) \u0026ndash;\u0026gt; FN - False Negative. Mọi phần trong bức ảnh mà không có đối tượng và model cũng dự đoán là không có đối tượng \u0026ndash;\u0026gt; TN - True Negative.  Precision and Recall được tính cho mỗi đối tượng trong bức ảnh. Chúng ta cũng cần xem xét đến giá trị Confidence Score trả ra từ model. Các Bounding Box có Confidence lớn hơn một giá trị ngưỡng được coi là Positive và ngược lại.\n2.3 Tính toán mAP\n Bước 1 - Vẽ đồ thị Precision - Recall Thể hiện 2 giá trị Precision và Recall trên đồ thị PR, Precision trên trục y, Recall trên trục x.    Mong đợi rằng đồ thị PR (đường màu xanh) sẽ có dạng đơn điệu giảm dần từ trái sang phải theo giá trị của Precision. Nếu không, chúng ta sẽ phải sử dụng giá trị interpolation - nội suy của Pricsion (đường màu đỏ) - $p_{interp}$. Giá trị $interp_p$ tại mỗi vị trí bằng giá trị lớn nhất của Precision ở bên phải của vị trí đó. Vì AP được tính bằng trung bình của 11 điểm $p_{interp}$ nên ta thể hiện 11 điểm trên đồ thị PR, tương ứng với Recall từ 0.0, 0.1, \u0026hellip;, 1.0\n  Bước 2 - Tính AP $AP = \\frac{1}{11}\\sum_{r \\in {0,0.1,0.2,...,1}}p_{interp}(r) = \\frac{1}{11} (1 + 0.8 + 0.6 + 0.5 + 0.5 + 0.5 + 0.45 + 0.48 + 0.48 + 0.35 + 0.2) = 0.53$\n   Bước 3 - Tính mAP mAP được tính bằng trung bình của tất cả các AP của mỗi class. $mAP = \\frac{1}{n}\\sum_{i=1}^n AP_i$\n   Trong đó, $n$ là số class mà model có thể phát hiện.\n3. Kết luận\nBài này, mình đã giới thiệu đến các bạn những kiến thức khái quát về các thuật toán Object Detection và một Metrics mAP để đánh giá các thuật toán đó. Hi vọng các bạn sẽ có được cái nhìn toàn diện để có thể lựa chọn được thuật toán phù hợp với bài toán của mình. Để hiểu sâu hơn về mỗi thuật toán đó, mình khuyên các bạn nên đọc kỹ bài báo gốc của nó và bắt tay vào train/test lại chúng trên bộ dữ liệu của mình. Có rất nhiều hướng dẫn trên Internet giúp các bạn làm điều này.\nỞ bài tiếp theo, chúng ta sẽ chuyển qua tìm hiểu về bài toán Sematics Segmentation. Mời các bạn đón đọc.\n4. Tham khảo\n[1] Renu Khandewal, \u0026ldquo;Evaluating performance of an object detection model\u0026rdquo;, Available online: https://towardsdatascience.com/evaluating-performance-of-an-object-detection-model-137a349c517b (Accessed on 12 May 2021).\n[2] Jędrzej Świeżewski, \u0026ldquo;YOLO Algorithm and YOLO Object Detection: An Introduction\u0026rdquo;, Available online: https://appsilon.com/object-detection-yolo-algorithm/ (Accessed on 12 May 2021).\n[3] Analytics Vidhya, \u0026ldquo;A Review of Object Detection Models\u0026rdquo;, Available online: https://medium.com/analytics-vidhya/a-review-of-object-detection-models-f575c515655cl (Accessed on 12 May 2021).\n","permalink":"https://tiensu.github.io/blog/66_object_detection_summary/","tags":["Object Detection"],"title":"Tóm tắt các thuật toán Object Detection"},{"categories":["OCR","CTC","Text Recognition"],"contents":"1. CRNN Model\nConvolutional Recurrent Neural Network (CRNN) là một kiến trúc được thiết kế chuyên biệt để giải quyết nhiệm vụ Text Recognition trong bài toán OCR. Xuất hiện trong bài báo An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition xuất bản năm 2015, cho đến nay, nó vẫn được coi là một trong những model hiệu quả nhất cho việc thực hiện Text Recognition.  CRNN, đúng như cái tên của nó, là sự kết hợp giữa CNN và RNN. Đây là một sự cộng sinh rất hợp lý bởi vì nhiệm vụ của nó là nhận một bức ảnh đầu vào và cho ra một văn bản chứa trong bức ảnh đó. Nhắc đến xử lý ảnh thì CNN chắc chắn không thể thiếu, và xử lý văn bản thì RNN cũng là ứng cử viên nặng ký. Kiến trúc của CRNN chia thành 3 phần rõ rệt.\n1.1 Convolutional Layers\nẢnh đầu vào được cho đi qua các lớp Conv, sinh ra các Feature Maps. Các Feature Maps sau đó lại được chia ra thành một chuỗi của các Feature Vectors (các TimeSteps), gọi là Feature Sequence.  1.2 Recurrent Layers\nFeature Sequence được đưa vào các lớp Bidirectional LSTM, sinh ra một chuỗi các ký tự (Seq2Seq), mà mỗi một ký tự tương ứng với một TimeStep trong Feature Sequence. Về lý thuyết thì đây chính là văn bản đầu ra cần xác định. Tuy nhiên, Feature Maps không phải lúc nào cũng được chia chính xác thành các Feature Vectors, mà mỗi Feature Vector chứa đúng 1 ký tự cần nhận diện, nên chuỗi đầu ra của LSTM cũng rất lộn xộn: trùng lặp, không có ký tự, \u0026hellip;  1.3 Transcription Layers\nNhiệm vụ của phần này là xử lý đầu ra của Recurrent Layers, sắp xếp lại các ký tự, loại bỏ các lỗi tồn tại (alignment) để đưa ra kết quả cuối cùng.  Một số hướng tiếp cận của Transcription Layers như sau:\n Cách 1 - Quy định mỗi ký tự tương ứng với một số cố định TimeSteps. Ví dụ, chúng ta có 10 TimeSteps, tương ứng với Output là State thì ta sắp xếp các TimeSteps như sau: \u0026ldquo;S, S, T, T, A, A, T, T, E, E\u0026rdquo;. Cách này đơn giản nhưng chỉ đúng khi kích thước, kiểu chữ, \u0026hellip; của văn bản không đổi. Cách 2 - Đánh nhãn cho mỗi TimeStep như hình bên dưới, rồi huấn luyện model sử dụng dữ liệu này.    Cách này có vẻ chính xác hơn, nhưng nhược điểm là tốn rất nhiều thời gian cho việc tạo dữ liệu.\n Cách 3 - Sử dụng Connectionist Temporal Classification (CTC)  CTC ra đời từ năm 2006 để giải quyết vấn đề Alignment (chính là vấn đề ánh xạ từ Output của RNN ra kết quả cuối cùng) cho các bài toán OCR, Speech Recognition, \u0026hellip; một cách hiệu quả. Chúng ta sẽ tìm hiểu chi tiết về nó trong phần 2 dưới đây.\nĐể nâng cao hiệu quả của CRNN, chúng ta có thể thêm vào một lớp Attention ở giữa hai lớp CNN và RNN. Cơ chế hoàn toàn giống như trong bài Tìm hiểu cơ chế Attention trong mô hình Seq2Seq.  2. Connectionist Temporal Classification\nCTC đơn giản là một Loss Function được sử dụng để huấn luyện các Deep Learning model. Mục tiêu của nó là tìm ra cách ánh xạ (alignment) giữa một Input X và Output Y. Nó không yêu cầu Aligned Data (dữ liệu được gắn nhãn cụ thể cho mỗi TimeStep), bởi vì nó có thể đưa ra được xác suất cho mỗi khả năng Align từ X sang Y. Nó chỉ yêu cầu đầu vào là một hình ảnh (ma trận Feature của hinh ảnh) và đoạn Text tương ứng với hình ảnh đó.  Thực chất, bản thân CTC cũng ko biết chính xác cách Alignment giữa X và Y. Nó làm việc theo kiểu Work Arround, tức là nếu đưa cho nó X thì nó sẽ trả lại cho ta tất cả các khả năng của Y, kèm theo xác suất chính xác của mỗi khả năng đó. Chúng ta cần huấn luyện model sao cho:  Trong đó $Y^*$ là kết quả đầu ra cuối cùng mà ta mong đợi, nó càng giống với nhãn (Ground Trust - GT) càng tốt.\nQuá trình làm việc của CTC bao gồm 3 bước:\n2.1 Encoding Text\nLý tưởng thì mỗi TimeStep sẽ tương ứng với một ký tự. Nhưng nếu không phải vậy thì sao, nếu một ký tự tồn tại trong cả 2 TimeSteps thì sao? Khi đó, kết quả sẽ xuất hiện các ký tự trùng nhau. CTC giải quyết vấn đề này bằng cách gộp tất cả các ký tự trùng nhau thành một. Ví dụ: ttiien ssuu -\u0026gt; tien su.\nTuy nhiên, nếu làm như vậy thì những từ mà bản thân nó có các ký tự trùng nhau thì sao? Ví dụ: hello, sorry, \u0026hellip; Để tiếp tục xử lý vấn đề, CTC sử dụng một ký tự giả, gọi là blank và ký hiệu là \u0026ldquo;-\u0026rdquo;. Trong khi mã hóa Text, nếu gặp 2 ký tự trùng nhau, CTC sẽ chèn thêm kí tự blank vào giữa chúng. Ví dụ: meet -\u0026gt; mm-ee-ee-t, mmm-e-ee-tt. Trong quá trình Decoding Text, nếu gặp ký tự blank này thì CTC hiểu rằng phải giữ lại cả 2 ký tự 2 bên ký hiệu đó.\n2.2 Loss Calculate\nLoss được tính toán cho mỗi Training Sample (một cặp ảnh và GT Text tương ứng). Nó là tổng tất cả các Scores của tất cả các khả năng Alignments của GT Text. Giả sử chúng ta có một ma trận Score là Ouput của CRNN như sau:  Ma trận này gồm 2 TimeSteps, và GT Text có 3 ký tự: a, b và ký tự blank (-). Tổng Score tại mỗi TimeStep bằng 1. Giả sử:\n Các khả năng Alignment của ký tự a là: aaa, a\u0026ndash;, a-, aa-, -aa, \u0026ndash;a \u0026ndash;\u0026gt; Score của *a = 0.4x0.3x0.4 + 0.4x0.7x0.6 + 0.4x0.7 + 0.4x0.3x0.6 + 0.1x0.3x0.4 + 0.1x0.7x0.4 = 0.608. * \u0026ndash;\u0026gt; Loss = $-\\log_{10}0.6084 = 0.216$ Các khả năng Alignment của ký tự b là: bbb, b\u0026ndash;, b-, bb-, -bb, \u0026ndash;b \u0026ndash;\u0026gt; Score của b = 0.5x0.0x0.0 + 0.5x0.7x0.6 + 0.5x0.7 + 0.5x0.0x0.6 + 0.1x0.0x0.0 + 0.1x0.7x0.0 = 0.56 \u0026ndash;\u0026gt; *Loss = * $\\log_{10}0.56 = 0.25$ Các khả năng Alignment của ký tự blank là: \u0026ndash;, \u0026mdash; -\u0026gt; Score của blank = 0.1x0.7 + 0.1x0.7x0.6 = 0.112 \u0026ndash;\u0026gt; Loss = $-\\log_{10}0.112 = 0.95$  Tổng Loss = 0.216 + 0.25 + 0.95 = 1.416.\nChúng ta cần tối thiểu hóa giá trị Loss này trong quá trình huấn luyện model, sử dụng thuật toán Backpropagation và SGD.\n2.3 Decoding Text\nQuá trình Decoding một Unseen Image diễn ra như sau:\n Tìm đường đi tối ưu nhất từ Score Matrix bằng cách chọn các ký tự có Score cao nhất tại mỗi TimeStep. Xóa bỏ các ký tự trống, ký tự trùng lặp.  Xem thử ví dụ sau:  Có 3 ký tự là a. b, - và 5 TimeSteps. Các ký tự trên đường đi tối ưu nhất là: aaa-b. Sau khi loại bỏ ký tự trùng và khoảng trắng ta được ab.\n3. Kết luận\nBài này, mình đã giới thiệu đến các bạn những kiến thức khái quát về CRNN model, về CTC Loss dùng cho nhiệm vụ Text Recognition của bài toán OCR: kiến trúc, cách làm việc \u0026hellip;\nSouce Code ví dụ của CRNN + CTC, các bạn có thể tham khảo tại đây.\nỞ bài tiếp theo, chúng ta sẽ chuyển qua tìm hiểu về bài toán Object Detection. Mời các bạn đón đọc.\n4. Tham khảo\n[1] B.Shi, X.Bai, C.Yao, An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition. arXiv preprint arXiv:1507.05717.\n[2] TheAILearner, \u0026ldquo;CTC – Problem Statement\u0026rdquo;, Available online: https://theailearner.com/2021/03/10/ctc-problem-statement/ (Accessed on 07 May 2021).\n[3] Harald Scheidl, \u0026ldquo;An Intuitive Explanation of Connectionist Temporal Classification\u0026rdquo;, Available online: https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c (Accessed on 07 May 2021).\n[4] Siddhant, \u0026ldquo;Explanation of Connectionist Temporal Classification\u0026rdquo;, Available online: https://sid2697.github.io/Blog_Sid/algorithm/2019/10/19/CTC-Loss.html (Accessed on 07 May 2021).\n","permalink":"https://tiensu.github.io/blog/65_ocr_crnn_model_ctc_loss_function/","tags":["OCR","CRNN","CTC","Text Recognition"],"title":"Text Recognition với CRNN và CTC"},{"categories":["OCR","Text Detection"],"contents":"Trước khi có sự ra đời của học sâu trong bài toán Text Detection, hầu hết các phương pháp đều khó thực hiện trong các tình huống phức tạp của dữ liệu thực tế. Các phương pháp này sử dụng những kỹ thuật xử lý ảnh truyền thống và thủ công, chúng thường có nhiều giai đoạn và kết thúc với hiệu suất tổng thể không thực sự thuyết phục. Trong bài viết này, chúng ta sẽ tìm hiểu một thuật toán dựa trên học sâu (EAST) để phát hiện văn bản bằng một mạng nơron duy nhất.\n1. Giới thiệu EAST\nEAST - pipeline được giới thiệu trong bài báo EAST: An Efficient and Accurate Scene Text Detector vào năm 2017. Nó có thể phát hiện Text theo cả 2 cấp độ: Words và Line. Nó cũng không hạn chế quá nhiều điều kiện của ảnh đầu vào, ảnh có thể xoay, mờ, nhiễu (nói vậy không có nghĩa là ta có thể bỏ qua bước Pre-processing Image); Text trên ảnh có thể thuộc một trong 2 loại: Structure và Unstructure. Tại thời điểm ra đời, EAST vượt trội hơn hẳn so với các phương pháp trước đó.\n2. Kiến trúc của EAST\n Kiến trúc tổng thể của EAST gồm 3 phần:\n  Feature Extractor Stem - Phần này làm nhiệm vụ trích xuất đặc trưng từ ảnh đầu vào theo 4 cấp độ, từ $f_1$ đến $f_4$. Trong bài báo, tác giả sử dụng 2 Pre-trained model là VGG16 và PVANet cho thí nghiệm của mình.\n  Feature Merging Branch - Các Features từ phần trên được cho đi qua các lớp Unpool, sau đó được tập hợp lại, cuối cùng là đi qua lần lượt các lớp 1x1 Conv, 3x3 Conv. Mục đích của việc làm này là để EAST có thể dự đoán được Text ở những khu vực nhỏ.    Output Layer - Ouput Layer chứa Score Map và Geometry Map. Score Map cho ta biết xác suất xuất hiện Text trong một khu vực, còn Geometry Map chứa thông tin về tọa độ Bounding Box của khu vực đó. Bounding Box có thể được đưa ra dưới dạng Rotated Box (tọa độ của điểm top-left, chiều dài, chiều rộng, góc quay) hoặc Quadrangle (đầy đủ tọa độ 4 điểm của hình chữ nhật).\n  3. Loss Function\nLoss Function của EAST là tổng Loss của Score Map và Geometry Map. $L = L_s + \\lambda_gL_g$\n Trong đó, $L_s$ là Loss Function của Score Map, còn $L_g$ là Loss Function của Geomatry Map. Hằng số $\\lambda_g$ là trọng số thể hiện mức độ quan trong của $L_g$, nó nhận giá trị trong khoảng [0,1]. Trong bài báo thì tác giả đang sử dụng $\\lambda_g$ = 1.\n4. Non-max Suppression Mergin Stage\nOutput của Geomatry Map đối với một khu vực có thể có rất nhiều, Chỉ một Output lớn hơn giá trị ngưỡng quy định bởi Locality-Aware NSM mới được giữ lại.\n5. Sử dụng EAST model\nCó khá nhiều Source Code của EAST trên Github. Trong phần này, mình sẽ sử dụng Github của argman để làm thực nghiệm.\n5.1 Clone EAST Repository\n$ git clone https://github.com/argman/EAST.git   5.2 Compile LANMS\nEAST sử dụng Locality-Aware NMS (LSNMS) thay vì NSM chuẩn, trong Github này, LSNMS được viết bằng C++. Để nó có thể làm việc được với Python, chúng ta cần biên dịch nó thành thư viện .so\n Cài đặt công cụ cần thiết:  $ sudo apt-get install build-essential   Mở file init.py trong thư mục lanms, và Comment Out 2 dòng 7 và 8 như hình:    Chạy lệnh sau để biên dịch\n  $ cd EAST/lanms/ $ make 5.3 Test EAST model\nTrước tiên cần phải có EAST model. Nếu có đủ dữ liệu của riêng mình, chúng ta có thể huấn luyện nó lại từ đầu như hướng dẫn trên Github argman. Ở đây, mình chọn Pre-trained EAST model, download tại đây.\n Chuyển đổi Code từ TF 1.xx sang TF 2.xx Source Code trên Github được viết bằng Tensorflow phiên bản 1.xx. Thời điểm hiện tại, hầu hết các model mới đều chuyển sang sử dụng Tensorflow phiên bản 2.xx. Để chạy được với Tensorflow 2.xx, chúng ta cần làm thêm 1 bước chuyển đổi Code từ TF 1.xx sang TF 2.xx.  $ tf_upgrade_v2 --intree . --outtree tf2_version --reportfile report.txt Source Code mới tương thích với TF 2.xx được di chuyển hết vào thư mục tf2_version. Tuy nhiên, vẫn còn 1 lỗi mà chưa thể chuyển đổi tự động được, ta phải sửa bằng tay.\nDòng 60, file nets/resnet_v1.py: from tensorflow.contrib import slim -\u0026gt; import tf_slim as slim\nDòng 43, file nets/resnet_utils.py: slim = tf.contrib.slim -\u0026gt; import tf_slim as slim\nTất nhiên, ta phải cài thêm thư viện tf_slim: pip install tf_slim.\n Chạy EAST model  Thực hiện lệnh sau:\n$ cd tf2_version $ python eval.py --test_data_path=training_samples/ --gpu_list=0 --checkpoint_path=east_icdar2015_resnet_v1_50_rbox/ --output_dir=outputs/ Ở đây, ta dùng cách ảnh trong thư mục training_samples để test, kết quả được lưu ra thư mục outputs.\n Hiện tại, EAST đã được tích hợp sẵn trong OpenCV phiên bản 4.xx. Chúng ta có thể dễ dàng sử dụng nó mà không cần phải làm nhiều bước dài dòng như thế này. Tham khảo cách sử dụng EAST trong OpenCV tại đây.\n6. Kết luận\nBài này, mình đã giới thiệu đến các bạn những kiến thức khái quát về EAST model, dùng cho nhiệm vụ Text Detection của bài toán OCR: ưu/nhược điểm, kiến trúc, cách làm việc và cách sử dụng nó từ Source Code trên Github.\nỞ bài tiếp theo, mình giới thiệu về model CRNN và CTC để thực hiện Text Recognition. Mời các bạn đón đọc.\n7. Tham khảo\n[1] X.Zhou, C.Yao, H.Wen, et al, EAST: An Efficient and Accurate Scene Text Detector. arXiv preprint arXiv:1704.03155.\n[2] TheAILearner, \u0026ldquo;Efficient and Accurate Scene Text Detector (EAST)\u0026rdquo;, Available online: https://theailearner.com/2019/10/19/efficient-and-accurate-scene-text-detector-east/ (Accessed on 06 May 2021).\n[3] TheAILearner, \u0026ldquo;Implementation of EAST\u0026rdquo;, Available online: https://theailearner.com/2021/03/10/implementation-of-east/ (Accessed on 06 May 2021).\n","permalink":"https://tiensu.github.io/blog/64_ocr_text_detection_east/","tags":["OCR","Text Detection"],"title":"Text Detection với EAST"},{"categories":["Data Imbalance"],"contents":"1. Thế nào là Data Imbalance?\nMất cân bằng dữ liệu (Data Imbalance) là một vấn đề thường xuyên gặp phải đối với các bộ dữ liệu trong thực tế. Nó xảy ra khi số lượng mẫu dữ liệu (hình ảnh, bản ghi, \u0026hellip;) của mỗi lớp có sự khác nhau lớn. Nguyên nhân của vấn đề này thường do đặc thù từng bài toán. Trong khi các mẫu dữ liệu của lớp này rất dễ dàng thu thập được thì ngược lại, các mẫu dữ liệu của lớp kia rất khó để tập hợp, hoặc phải mất một thời gian rất dài. Ví dụ, các bài toán về dự đoán lỗi trong giao dịch ngân hàng, bài toán về phân loại bệnh nhân ung thư, \u0026hellip;\nMất cân bằng dữ liệu ảnh hưởng rất lớn đến kết quả dự đoán của model trong bài toán phân loại. Model thường có xu hướng dự đoán kết quả nghiêng về lớp có nhiều dữ liệu hơn (bias model).\n2. Giải quyết vấn đề Data Imbalance\nĐể giải quyết vấn đề này, cách làm triệt để và chính xác nhất, tất nhiên là thu thập thêm dữ liệu cho các lớp có lượng dữ liệu ít hơn. Tuy nhiên, như đã nói ở phần nguyên nhân, việc có đủ lượng dữ liệu cần thiết của một số lớp là rất khó, thậm chí không thể thực hiện được. Thay vào đó, chúng ta sẽ sử dụng một số phương pháp để hạn chế ảnh hưởng của vấn đề này.\nMình sẽ trình bày các phương pháp hạn chế ảnh hưởng của Data Imbalance thông qua một ví dụ cụ thể.\n2.1 Chuẩn bị dữ liệu\nDữ liệu được sử dụng trong bài này là tập NIH Chest X-ray\n11. Tham khảo\n[1] Krizhevsky, A., Sutskever, I., \u0026amp; Hinton, G. E. (2017). Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6), 84-90.\n[2] Simonyan, K., \u0026amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.\n[3] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., \u0026hellip; \u0026amp; Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).\n[4] He, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n[5] Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., \u0026amp; Houlsby, N. (2019). Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370, 6(2)\n[6] Huang, G., Liu, Z., Van Der Maaten, L., \u0026amp; Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).\n[7] Tan, M., \u0026amp; Le, Q. V. (2019). Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946.\n[8] Xie, Q., Luong, M. T., Hovy, E., \u0026amp; Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).\n[9] Pham, H., Xie, Q., Dai, Z., \u0026amp; Le, Q. V. (2020). Meta pseudo labels. arXiv preprint arXiv:2003.10580.\n[10] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., \u0026amp; Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).\n[11] Nikolas Adaloglou, \u0026ldquo;Best deep CNN architectures and their principles: from AlexNet to EfficientNet\u0026rdquo;, Available online: https://theaisummer.com/cnn-architectures/ (Accessed on 02 May 2021).\n","permalink":"https://tiensu.github.io/blog/ddd_data_imbalance_in_classification_task/","tags":["Data Imbalance"],"title":"Mất cân bằng dữ liệu - làm sao để giải quyết?"},{"categories":["OCR"],"contents":"1. OCR là gì?\nOCR, viết tắt của Optical Character Recognition là một phương pháp chuyển đổi các văn bản, ký tự xuất hiện trong hình ảnh, hay các tài liệu scaned thành định dạnh mà máy tính có thể hiểu được. Từ đó, chúng ta có thể dễ dàng chỉnh sửa, tìm kiếm, và thực hiện rất nhiều công việc khác. Nếu đưa cho máy tính một hình ảnh chứa các văn bản, ký tự thì máy tính chỉ coi đó là một bức ảnh đại diện bởi ma trận các giá trị của từng pixel trong ảnh đó.\n2. Ứng dụng của OCR\nOCR có rất nhiều ứng dụng trong thực tế. Có thể kể ra đây một số ví dụ sau:\n- Automatic Data Entry - Tự động nhập liệu\nĐây có lẽ là ứng dụng phổ biến và quan trọng nhất của OCR. Trước đây, đối với những số liệu trong một hình ảnh hay tài liệu scan, để đưa vào máy tính xử lý, con người phải nhập thủ công bằng tay. Việc này rất mất thời gian và nhàm chán. Ngày nay, với sự hỗ trợ của OCR, quá trình này diễn ra hoàn toàn tự động, nhanh chóng, dễ dàng, độ chính xác cao.  - Nhận diện biển số xe\nÁp dụng trong các bãi đỗ xe, tự động nhận diện biển số giúp giảm thời gian quản lý cho cả người lái xe và nhân công bảo vệ.  - Xe tự lái\nOCR giúp xe tự động nhận diện biển số để đi theo đúng chỉ dẫn.  - Book Scanning\nOCR giúp chuyển sách giấy thành sách điện từ một cách dễ dàng.  Và còn rất rất nhiều ứng dụng khác nữa.\n3. OCR Pipeline\nOCR hoạt động theo một Pipeline như sau:  3.1 Image Pre-processing\nĐây là bước tiền xử lý hình ảnh trước khi đưa vào cho model học tập. Các images có thể bị mờ, bị nhiễu, bị lệch, \u0026hellip; Nếu để nguyên như vậy đưa vào model thì kết quả sẽ rất kém. Nhiệm vụ của Image Pre-processing là cố gắng loại bỏ những lỗi như vậy.  3.2 Text Detection\nNhư cái tên đã chỉ ra, nhiệm vụ của bước này là tìm ra khu vực trong hình ảnh chứa ký tự/văn bản.  Có 2 kiểu ký tự/văn bản trong hình ảnh mà bài toán OCR có thể giải quyết:\n Ký tự/văn bản có cấu trúc: Là những hình ảnh tương đối rõ ràng, background cố định, font chữ cố định, màu sắc cố định, ký tự/văn bản được tổ chức ngay ngắn theo hàng/cột, \u0026hellip; Ví dụ, trang sách.    - Ký tự/văn bản phi cấu trúc: Là những hình ảnh có ký tự/văn bản xuất hiệu không sự thống nhất về màu sắc, vị trí, kiểu chữ, \u0026hellip; Ví dụ: bảng quảng cáo.  Rõ ràng, giải quyết kiểu thứ 2 khó hơn rất nhiều so với kiểu thứ nhất.\nĐể thực hiện nhiệm vụ Text Detection, có thể tiếp cận theo 3 cách:\n Cách 1 - Phát hiện từng ký tự một (Character-by-Character). Cách 2 - Phát hiện từng từ một (Word-by-Word). Cách 3 - Phát hiện từng dòng một (Line-by-Line).   Nhìn chung, hầu hết các hệ thống OCR đều sử dụng cách tiếp cận thứ 2 hoặc 3. Cách 1 chậm và độ chính xác thấp hơn.\nVề mặt kỹ thuật, có 2 cách có thể sử dụng:\n Cách 1- Sử dụng các kỹ thuật xử lý ảnh cơ bản (truyền thống) Cách này sử dụng các bộ lọc (filters) để tách rời các ký tự ra khỏi nền của bức ảnh, sau đó áp dụng các kỹ thuật phát hiện biên, Contours để thu được vị trí của từng ký tự riêng rẽ. Một số cái tên điển hình sử dụng nguyên lý này là Stroke Width Transform (SWT), Maximally Stable Regions (MSER).  Trong điều kiện tương đối lý tưởng, dữ liệu sạch sẽ, ít nhiễu thì phương pháp này tỏ ra khá hiệu quả, độ chính xác cao và dễ thực hiện. Tuy nhiên, trong thực tế, rất khó để đảm bảo những điều kiện như vậy.\n Cách 2 - Sử dụng kỹ thuật Deep Learning Sử dụng DL, nói chung là hiệu quả hơn rất nhiều so với phương pháp bên trên, bởi vì chúng có khả năng học từ dữ liệu nên không bị ảnh hưởng quá nhiều bởi các yếu tố môi trường. Một số model nổi bật là Connectionist Text Proposal Network (CTPN), Efficient and Accurate Scene Text Detector (EAST), \u0026hellip; Các Object Detection models khác như SSD, Yolo, Faster RCNN cũng có thể thực hiện được nhiệm vụ này.  Ở cách 2 này, nếu chia nhỏ hơn nữa thì có thể thành: Simplified pipeline và Multi-steps.  Hai model CTPN và EAST đều thuộc nhóm Simplified Pipeline. Trong các bài tiếp theo, chúng ta sẽ tìm hiểu kỹ hơn EAST model.\n3.3 Text Recognition\nCác ký tự/văn bản trong từng khu vực phát hiện ở bước bên trên sẽ được nhận diện cụ thể ở bước này.  Tương tự như Text Detection, ở đây cũng có 2 phương pháp giải quyết là dùng kỹ thuật xử lý ảnh cơ bản và dùng kỹ thuật Deep Learning.\nĐối với cách thứ nhất, sau khi tách riêng được từng ký tự ra khỏi nền, sẽ cho chúng đi qua mội bộ phân lớp để nhận diện. Cách này xử lý ở mức Characters, phụ thuộc nhiều vào kết quả của Text Detection và môi trường nên độ chính xác không cao.\nCách thứ 2 hiện nay đã chứng tỏ được tính ưu việt của nó so với cách thứ nhất. Hai model nổi bật sử dụng DL cho nhiệm vụ này là:\n CRNN - Connectionist Temporal Classification (CTC) based. Attention-based.  Trong các bài tiếp theo, chúng ta sẽ tìm hiểu kỹ hơn về 2 model này.\n3.4 Restructing\nỞ bước cuối cùng này, ký tự/văn bản sau khi nhận dạng xong sẽ được sắp xếp lại đúng theo vị trí của nó như trong hình ảnh bản đầu. Mục đích của việc làm này là để thuận tiện trong việc trích chọn ra các thông tin cần thiết, dựa vào vị trí tương đối của chúng với nhau.  4. Kết luận\nBài này, mình đã giới thiệu đến các bạn những kiến thức tổng quát về bài toán OCR: OCR là gì, các bước thực hiện như thế nào, có các phương pháp gì, ưu/nhược điểm của từng phương pháp.\nỞ bài tiếp theo, mình giới thiệu về model EAST để thực hiện Text Detection. Mời các bạn đón đọc.\n5. Tham khảo\n[1] TheAILearner, \u0026ldquo;Optical Character Recognition: Introduction and its Applications\u0026rdquo;, Available online: https://theailearner.com/2021/03/10/optical-character-recognition-introduction-and-its-applications/ (Accessed on 04 May 2021).\n[2] TheAILearner, \u0026ldquo;Optical Character Recognition Pipeline\u0026rdquo;, Available online: https://theailearner.com/2019/05/28/optical-character-recognition-pipeline/ (Accessed on 04 May 2021).\n[3] TheAILearner, \u0026ldquo;OOptical Character Recognition Pipeline: Text Detection\u0026rdquo;, Available online: https://theailearner.com/2021/01/28/optical-character-recognition-pipeline-text-detection/ (Accessed on 04 May 2021).\n[4] TheAILearner, \u0026ldquo;Optical Character Recognition Pipeline: Text Recognition\u0026rdquo;, Available online: https://theailearner.com/2019/05/29/optical-character-recognition-pipeline-text-recognition/ (Accessed on 04 May 2021).\n","permalink":"https://tiensu.github.io/blog/63_ocr_introduction/","tags":["OCR"],"title":"Giới thiệu bài toán OCR"},{"categories":["CNN"],"contents":"Năm 2012, Alexnet ra đời với độ chính xác trên tập dữ liệu ImageNet được công bố là 63.3%. Từ đó đến nay, trải qua gần 9 năm phát triển, có rất nhiều kiến trúc mới của CNN nối tiếp nhau ra đời, cái sau tốt hơn cái trước. Thời điểm hiện tại, EfficientNet có lẽ là kiến trúc đạt được độ chính xác trên ImageNet cao nhất, lên đến hơn 90% khi huấn luyện bằng phương pháp Teacher-Student.  Bài viết này, mục đích là nhìn lại toàn bộ quá trình phát triển đó của CNN, không chỉ đưa ra số liệu, bảng biểu, đồ thị mà còn tóm tắt lại nguyên lý cơ bản của mỗi kiến trúc CNN.\nSimone Bianco, năm 2018, đã đưa ra một tóm tắt về Top Performing CNNs Model, thể hiện như hình dưới đây:  Trong hình trên, trục Y thể hiện độ chính xác của model trên tập ImageNet, trục X (Floating Point Operations Per Second - FLOPS) chỉ ra mức độ phức tạp của model. Bán kính của vòng tròn càng lớn, model càng có nhiều tham số. Từ tổng kết này, rõ ràng rằng không phải cứ có nhiều tham số thì độ chính xác sẽ cao hơn.\n1. Một số thuật ngữ sử dụng trong bài\nĐể tránh làm các bạn bối rối khi theo dõi bài viết, mình sẽ giải thích trước một số thuật ngữ được sử dụng ở đây:\n Wider network - Network có nhiều Feature Maps (Filters). Deeper network - Network có nhiều Convolutional layers. High Resolution network - Network nhận Input Image có độ phân giải lớn (Spatial resolutions).    2. AlexNet: ImageNet Classification with Deep Convolutional Neural Networks (2012)\nAlexnet được tạo thành từ 5 Conv Layers, bắt đầu từ 11x11 kernel, giảm dần đến 3x3 kernel. Nó là kiến trúc CNN đầu tiên sử dụng Max-Pooling layers, ReLU Activation function, và Dropout. Alexnet được sử dụng cho bài toán phân loại hình ảnh, số lượng nhãn lên đến 1000. Đó là một điều rất bất ngờ tại thời điểm bấy giờ.\nChúng ta có thể tạo ra Alexnet chỉ với khoảng 35 dòng Pytorch code:\nclass AlexNet(nn.Module): def __init__(self, num_classes: int = 1000) -\u0026gt; None: super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: x = self.features(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.classifier(x) return x Alexnet cũng là mô hình đầu tiên huấn luyện thành công trên tập ImageNet, đạt được Top-5 Error Rate là 15.3%.\n3. VGG (2014)\nKiến trúc VGG xuất hiện trong bài báo Very Deep Convolutional Networks for Large-Scale Image Recognition vào năm 2014. Đây là nghiên cứu đầu tiên cung cấp bằng chứng không thể phủ nhận rằng chỉ cần thêm nhiều lớp Conv trong kiến trúc sẽ tăng hiệu quả quả model. Tuy nhiên, giả định này chỉ đúng đến một thời điểm nhất định. Các tác giả của bài báo chỉ sử dụng các Filters có kích thước 3x3, trái ngược lại với Alexnet. Ảnh đầu vào để huấn luyện model là ảnh RGB có kích thước 224x224.\nVGG được đặc trưng bởi sự đơn giản của nó, chỉ sử dụng các lớp Conv với Kernel 3 × 3 xếp chồng lên nhau theo chiều sâu ngày càng tăng. Việc giảm kích thước được xử lý bằng cách sử dụng Max-pooling. Ba Fully-Connected layers, trong đó 2 lớp đầu, mỗi lớp có 4.096 nodes, lớp còn lại có 1000 nodes (tương ứng với 1000 classes), được theo sau bởi một bộ phân loại Softmax.  Có 2 phiên bản của VGG thường hay được sử dụng là VGG16 và VGG19. Các con số 16, 19 chỉ ra số Weights layers của mỗi model (cột D và E trong bảng trên). Ở thời điểm năm 2014 thì 16 và 19 layers được xem là rất deep rồi. Bây giờ thì chúng ta có kiến trúc ResNet có số lượng layers từ 50-200.\nVGG có 2 nhược điểm:\n Thời gian huấn luyện rất lâu nếu bạn ko có GPU. Dung lượng của model sau khi huấn luyện xong rất lớn (VGG16 là khoảng 533MB, còn VGG19 khoảng 574MB). Điều này làm cho VGG khó triển khai trên các thiết bị có bộ nhớ khiêm tốn.  VGG vẫn thi thoảng được sử dụng trong một số ứng dụng như Image Classification, Feature Extraction, \u0026hellip; nhưng nhìn chung thì các kiến trúc nhỏ nhẹ (SqueezeNet, GoogleNet, \u0026hellip;) vẫn được ưu chuộng hơn.\n4. InceptionNet/GoogleNet (2014)\nSau VGG, bài báo Going Deeper with Convolutions viết bởi Christian Szegedy cũng tạo ra một bước đột phá lớn. Bài báo ra đời xuất phát từ suy nghĩ rằng việc tăng độ sâu của model không phải các duy nhất làm cho nó tốt hơn. Tại sao không mở rộng model trong khi vẫn cố gắng duy trì sự tính toán ở mức độ ổn định?\nKiến trúc của GoogleNet bao gồm nhiều Inception Module, mỗi Module hoạt động như một multi-level feature extractor (bộ trích xuất đặc trưng nhiều tầng) bằng cách sử dụng các Filters có kích thước khác nhau: 1x1, 3x3, 5x5. Output của các Filters sau đó được tổng hợp lại trước khi đưa vào Module tiếp theo.  Filter 1x1 đặt trước cac Filters 3x3 và 5x5 để giảm số lượng Input Channel, từ đó giảm giúp chi phí tính toán của kiến trúc GoogleNet.\n4.1 Inception V2\nTrong bài báo năm 2014 thì kiến trúc này có tên là GoogleNet, đến bài báo Rethinking the Inception Architecture for Computer Vision (2015), với một chút cải tiến để tăng hiệu quả, nó được đặt tên là Inception V2 và Inception V3.\nSự cải tiến của Inception V2 so với GoogleNet thể hiện ở 2 điểm:\n  Thay thế Filter 5x5 bằng 2 Filters 3x3 chồng lên nhau, mục đích là để tăng tốc độ xử lý vì theo lý thuyết, thời gian để một Filter 5x5 tính toán băng 2.78 lần so với Filter 3x3.    Tách Filter nxn thành 1xn và nx1. Ví dụ, với Filter 3x3 sẽ tương đương với 1x3 và 3x1. Theo thực nghiệm thì việc làm này sẽ giảm được khoảng 33% chi phí tính toán.    4.2 Inception V3\nInception V3 tiếp tục cải tiển từ Inception V2:\n Sử dụng RMSProp Optimizer. Thêm Filter 7x7. Sử dụng BatNorm sau các FC layers. Sử dụng Label Smoothing.  Dung lượng của Inception V3 khá nhỏ so với VGG, chỉ khoảng 96MB.\nCode thực hiện GoogleNet bằng Pytorch như sau:\nimport torch import torch.nn as nn class InceptionModule(nn.Module): def __init__(self, in_channels, out_channels): super(InceptionModule, self).__init__() relu = nn.ReLU() self.branch1 = nn.Sequential( nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0), relu) conv3_1 = nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0) conv3_3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1) self.branch2 = nn.Sequential(conv3_1, conv3_3,relu) conv5_1 = nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0) conv5_5 = nn.Conv2d(out_channels, out_channels, kernel_size=5, stride=1, padding=2) self.branch3 = nn.Sequential(conv5_1,conv5_5,relu) max_pool_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) conv_max_1 = nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0) self.branch4 = nn.Sequential(max_pool_1, conv_max_1,relu) def forward(self, input): output1 = self.branch1(input) output2 = self.branch2(input) output3 = self.branch3(input) output4 = self.branch4(input) return torch.cat([output1, output2, output3, output4], dim=1) model = InceptionModule(in_channels=3,out_channels=32) inp = torch.rand(1,3,128,128) print(model(inp).shape) 5. ResNet: Deep Residual Learning for Image Recognition (2015)\nTừ sau khi VGG ra đời, người ta đã từng nghĩ rằng cứ thêm nhiều lớp Conv thì model sẽ hoạt động tốt hơn. Nhiều người trong số họ cũng thử tiến hành các thực nghiệm với số lớp Conv nhiều hơn của VGG. Tuy nhiên, tất cả đều gặp phải một vấn đề, đó là Vanishing Gradient.\nResNet ra đời đã giải quyết được phần nào vấn đề này. Ý tưởng của nó là đưa vào trong kiến trúc của mình các Identity Shortcut Connection hay Skip Connection, để sử dụng thông tin của các layers trước đó cho layer hiện tại. Nhờ vậy mà hạn chế được hiện tượng Vanishing Gradient khi số lớp Conv tăng lên.  Với việc áp dụng ý tưởng này, số lớp Conv của Resnet có thể tăng đến con sô 150 lớp (Resnet-150).  Torchvision cung cấp sẵn một số Pre-trained của các phiên bản Resnet, bạn có thể import trực tiếp vào và sử dụng chúng.\nimport torchvision pretrained = True # A lot of choices :P model = torchvision.models.resnet18(pretrained) model = torchvision.models.resnet34(pretrained) model = torchvision.models.resnet50(pretrained) model = torchvision.models.resnet101(pretrained) model = torchvision.models.resnet152(pretrained) model = torchvision.models.wide_resnet50_2(pretrained) model = torchvision.models.wide_resnet101_2(pretrained) Bản thân mình không thích từ Skip Connection hay từ dịch nghĩa bỏ qua kết nối vì thực tế ResNet có bỏ qua kết nối nào đâu (nhìn vào hình minh họa thấy rất rõ ràng). Chẳng qua là nó thêm đường tắt, bắc cầu từ các lớp Conv trước đó đến chính nó. Do vây, dùng từ Shortcut Connection mới chính xác, phản ánh đúng bản chất của ResNet.\n6. DenseNet: Densely Connected Convolutional Networks (2017)\nDenseNet tiếp tục giải quyết vấn đề cố hữu khi sử dụng nhiều lớp Conv, đó là Vanishing Gradient.  Đối với mạng CNN truyền thống (VGG, \u0026hellip;) thì Input của một lớp chính là Ouput của lớp ngay trước đó. $x_i = H_i(x_{i-1})$\n ResNet mở rộng hành vi này bằng cách thêm vào thông tin của một lớp trước đó nữa (không nhất thiết là lớp ngay trước mà có thể trước vài lớp) thông qua Shortcut Connection. $x_i = H_i(x_{i-1} + x_{i-n})$\n DenseNet tiếp tục mở rộng, nó tổng hợp thông tin của tất cả các lớp trước đó làm Input cho lớp hiện tại. $x_i = H_i([x_0, x_1, ..., x_{i-1}])$\n Tạo DenseNet model trong Torchvision như sau:\nimport torchvision model = torchvision.models.DenseNet( growth_rate = 16, # how many filters to add each layer (`k` in paper) block_config = (6, 12, 24, 16), # how many layers in each pooling block num_init_features = 16, # the number of filters to learn in the first convolution layer (k0) bn_size= 4, # multiplicative factor for number of bottleneck (1x1 cons) layers drop_rate = 0, # dropout rate after each dense conv layer num_classes = 30 # number of classification classes ) print(model) # see snapshot below Ban đầu, DenseNet được đề xuất để sử dụng cho bài toán Image Classification, nhưng về sau nó còn được sử dụng cho rất nhiều bài toán khác, như thống kê dưới đây.  7. Big Transfer (BiT): General Visual Representation Learning (2020)\nBiT là một biến thể của ResNet. Cả ba phiên bản của nó (small, medium và large) đều dựa trên ResNet152. BiT-large sử dụng ResNet152x4 và được huấn luyện trên tập JFT chứa khoảng 300M hình ảnh đã đánh nhãn, lớn hơn rất nhiều so với ImageNet.\nĐóng góp lớn nhất của kiến trúc này là việc sử dụng các Normalization Layers. Tác giả đã sử dụng Group Normalization và Weight Standardization thay vì Batch Normalization.  8. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (2020)\nEfficientNet được đề xuất bởi Mingxing Tan và Quoc V. Le tại Google trong bài báo EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. Kết quả nghiên cứu của các tác giả chỉ ra rằng nó đạt được dộ chính xác tốt hơn nhiều so với các kiến trúc CNN trước đó.  Ý tưởng của EfficientNet là thay vì tìm ra một kiến trúc tối ưu từ đầu thì nó xuất phát từ một Base model F, sau đó dần dần mở rộng, cải tiến nó dần lên.\nTuy nhiên, hãy nhớ lại một số vấn đề cần chú ý trong các kiến trúc từ trước đến giờ khi Scale-up từng thành phần riêng lẻ (Individual Scaling):\n Deeper Network có thể nắm bắt được nhiều các Features phức tạp hơn nhưng rất khó huấn luyện do vấn đề Vanishing Gradient. Wider Network có thể nắm bắt được nhiều các Featureschi tiết hơn nhưng cũng khó huấn luyện do vấn đề Saturate Gradient. High Resolution Network cũng có thể nắm bắt được nhiều các Featureschi tiết hơn nhưng độ chính xác giảm dần khi gặp những hình ảnh có độ phân giải thấp hơn.  Rút kinh nghiệm từ những vấn đề trên, EfficientNet tiến hành Scale-up đồng thời cả 3 thành phần, gọi là Compound Scaling.\nĐể tìm ra các hệ số Scale-up cho 3 thành phần đó, các nhà nghiên cứu đã sử dụng phương pháp chia tỷ lệ kết hợp. Grid-search được áp dụng để tìm mối quan hệ giữa các chiều có tỷ lệ khác nhau của Base-model trong điều kiện hạn chế tài nguyên cố định. Sử dụng chiến lược này, tác giả đã tìm được các hệ số tỷ lệ thích hợp cho mỗi chiều để có thể tăng lên. Từ các hệ số này, Base-model có thể được Scale-upe lên theo kích thước mong muốn.  Việc áp dụng Compound Scaling rõ ràng đã cải thiện được hiệu quả đáng kể so với Individual Scaling.  9. Noisy Student Training: Self-training with Noisy Student improves ImageNet classification (2020)\nXuất hiện sau EfficientNet một thời gian ngắn, Noisy Student Training đưa ra một phương pháp huấn luyện mới, sử dụng EfficientNet làm kiến trúc nền tảng. làm tăng đáng kể độ chính xác trên tập dữ liệu ImageNet.\nPhương pháp này bao gồm 4 bước như sau:\n Bước 1 - Huấn luyện một Teacher model trên tập dữ liệu đã được gán nhãn (tập A). Bước 2 - Sử dụng Teacher model để sinh ra nhãn cho 300M ảnh chưa có nhãn (pseudo labels) (tập B) Bước 3 - Huấn luyện Student model trên tổng dữ liệu (tập A và B). Bước 4 - Lặp lại bước 1 bằng cách coi Student model như là Teacher model.  Về mặt lý thuyết, Student model sẽ hiệu quả hơn Teacher model vì nó được huấn luyện trên nhiều dữ liệu hơn. Ngoài ra, một lượng lớn nhiễu (Noise) cũng được thêm vào trong quá trình huấn luyện Student model để giúp nó học hiệu quả hơn từ tập B.\nMột số kỹ thuật như Dropout, Data Augmentation, \u0026hellip; cũng được áp dụng.  10. Meta Pseudo-Labels (2021)\nQuay lại phương pháp Noisy Student Training, một vấn đề phát sinh là nếu như các Pseudo Labels không chính xác thì Student model sẽ không thể cải thiện được so với Teacher model, thậm chí là tồi hơn. Vấn đề này được gọi bằng cái tên Sự xác nhận sai lệch trong gán nhãn giả (confirmation bias in pseudo-labeling).\nĐể khác phục vấn đề này, một ý tưởng mới xuất hiện, đó là thiết kế một cơ chế phản hồi từ Student model đến Teacher model, để Teacher model sinh ra nhãn đúng hơn.  Bằng cách này, cả Teacher và Student đều tham gia vào quá trình huấn luyện cùng nhau, giúp nhau học tập tốt hơn - Together to better (Ý tưởng của phương pháp này nghe hơi giống với cách thức mà model GAN hoạt động nhỉ, :D).\n11. Kết luận\nBảng sau so sánh các kiến trúc CNN đã trình bày từ đầu đến giờ, về các khía canh: số lượng tham số, độ chính xác trên tập ImageNet và năm công bố.  Có thể rút ra một số nhận xét sau:\n Model DenseNet có ít tham số nhất, model BiT-L có nhiều tham số nhất. Model Meta Pseudo Labels đạt được độ chính xác cao nhất trên tập ImageNet. Không phải cứ nhiều tham số hơn thì độ chính xác cao hơn.  Trong bài này, chúng ta đã cũng nhau nhìn lại chặng đường phát triển của các kiến trúc CNN thông qua một số models tiêu biểu. Hầu hết các models đều được cung cấp dưới dạng Pre-trained trong Keras hay Torchvision. Bạn có thể thử sử dụng chúng và Fine-tune trên tập dữ liệu của bạn để so sánh và đánh giá kết quả, giữa các models với nhau và so với việc tạo và huấn luyện model từ đầu.\nTrong 3 bài tiếp theo, mình sẽ giới thiệu về bài toán OCR và hướng dẫn huấn luyện OCR model. Mời các bạn đón đọc.\n11. Tham khảo\n[1] Krizhevsky, A., Sutskever, I., \u0026amp; Hinton, G. E. (2017). Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6), 84-90.\n[2] Simonyan, K., \u0026amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.\n[3] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., \u0026hellip; \u0026amp; Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).\n[4] He, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n[5] Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., \u0026amp; Houlsby, N. (2019). Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370, 6(2)\n[6] Huang, G., Liu, Z., Van Der Maaten, L., \u0026amp; Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).\n[7] Tan, M., \u0026amp; Le, Q. V. (2019). Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946.\n[8] Xie, Q., Luong, M. T., Hovy, E., \u0026amp; Le, Q. V. (2020). Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10687-10698).\n[9] Pham, H., Xie, Q., Dai, Z., \u0026amp; Le, Q. V. (2020). Meta pseudo labels. arXiv preprint arXiv:2003.10580.\n[10] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., \u0026amp; Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).\n[11] Nikolas Adaloglou, \u0026ldquo;Best deep CNN architectures and their principles: from AlexNet to EfficientNet\u0026rdquo;, Available online: https://theaisummer.com/cnn-architectures/ (Accessed on 02 May 2021).\n","permalink":"https://tiensu.github.io/blog/62_cnn_architecture_summary/","tags":["CNN"],"title":"CNN - Một hành trình phát triển từ 63.3% đến 90.2%"},{"categories":["RNN","LSTM","Attention","Transformer","BERT"],"contents":"Cuối năm 2018, các nhà nghiên cứu tại Google AI Language đã công bố một mô hình với tên gọi BERT trong bài báo BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Kế thừa kiến trúc của Transformers đã từng công bố trước đó, BERT đạt được hiệu quả rất cao so với các mô hình khác trong các tác vụ của NLP. Từ đó, có rất nhiều phiên bản của BERT ra đời, tập trung giải quyết các bài toán NLP trong từng lĩnh vực, ngôn ngữ cụ thể. Có thể nói Transformers và BERT đã mở ra một kỷ nguyên mới cho lĩnh vực AI nói chung và NLP nói riêng.\nKỹ thuật chính mà BERT sử dụng là sử dụng chỉ phần Encoder Stack của Transformers và áp dụng phương pháp huấn luyện 2 chiều (bidirectional training). Trước đó thì các Language Model thường chỉ huấn luyện 1 chiều (nondirectional training), từ trái qua phải hoặc từ phải qua trái. Các tác giả của bài báo đã chỉ ra rằng, bằng việc sử dụng bidirection training có thể thu được nhiều thông tin về ngữ nghĩa của mỗi từ trong Input Sequence hơn so với nondirectional training.\nTrong bài này, chúng ta sẽ cùng tìm hiểu về nó.\n1. Tại sao lại cần BERT?\nCũng như trong Computer Vision (*CV) trong những thách thức lớn nhất trong NLP là thiếu dữ liệu đào tạo. Nhìn chung, có rất nhiều dữ liệu văn bản có sẵn, nhưng nếu chúng ta muốn giải quyết các bài toán đặc thù của mình thì chúng ta phải tự tạo các bộ dữ liệu dành riêng cho bài toán đó. Việc đó quả thực mất rất nhiều thời gian. Để giúp thu hẹp khoảng cách về dữ liệu này, các nhà nghiên cứu đã phát triển các kỹ thuật khác nhau để đào tạo các mô hình biểu diễn ngôn ngữ có mục đích chung bằng cách sử dụng vô số văn bản trên các website (Pre-trained model). Sau đó, các mô hình Pre-trained này có thể được tinh chỉnh trên các tập dữ liệu nhỏ hơn dành riêng cho nhiệm vụ cụ thể. Cách tiếp cận này cải thiện độ chính xác lớn rất nhiều so với việc huấn luyện model từ đầu trên các bộ dữ liệu nhỏ. Kỹ thuật này được gọi bằng cái tên Transfer Learning giống như trong CV. BERT là một bổ sung gần đây cho kỹ thuật này trong việc tạo ra các Pre-trained model của NLP. Các Pre-trained BERT model có thể dễ dàng tải về miễn phí, sau đó được sử dụng hoặc là để trích xuất Features từ dữ liệu văn bản như đề cập trong bài báo ELMO, hoặc Fine-tune trên tập dữ liệu riêng của một nhiệm vụ cụ thể.\n2. Ý tưởng của BERT\nHãy nói về Language Modeling. Nhiệm vụ của nó là \u0026ldquo;tìm từ tiếp theo trong câu\u0026rdquo;. Ví dụ, trong câu sau: The woman went to the store and bought a ...\n Language Modeling có thể hoàn thành việc này bằng cách đưa ra một dự đoán rằng, 80% từ còn thiếu là \u0026ldquo;bag\u0026rdquo; và 20% là \u0026ldquo;water\u0026rdquo;. Cách làm việc của Language Modeling là nhìn vào Input Sequence từ trái qua phải hoặc từ phải qua trái, gọi là Unidirection, hoặc là từ cả 2 phía, gọi là Bidirection. Các cách tiếp cận này khá hiệu quả đối với bài toán dự đoán từ tiếp theo, như ví dụ trên.\nĐến với BERT, nó cũng có thể được coi là một Language Modeling, nhưng cách làm việc của nó có một vài điểm khác với trước đó:\n BERT dựa trên kiến trúc của Transformers, tức là cũng tiếp cận Input Sequence theo cả 2 hướng, nhưng tại cùng một thời điểm (trước đó là lần lượt từ trái qua phải rồi từ phải qua trái, hoặc ngược lại). Cách này có thể gọi là Nondirection. Hơn nữa, việc sử dụng kiến trúc của Tranformers làm cho BERT có thể hiểu ngữ nghĩa của cả câu tốt hơn (xem lại bài viết về Transformers) so với kiến trúc LSTM/GRU. Thay vì dự đoán từ tiếp theo, BERT sử dụng một kỹ thuật mới, gọi là Mask LM (MLM). Ý tưởng là \u0026ldquo;mask\u0026rdquo; ngẫu nhiên một số từ trong câu và sau đó cố gắng dự đoán chúng.  Để tạo ra Word Embedding, chúng ta có thể sử dụng một trong 2 phương pháp: Context-Free hoặc Context-Based.\n Trong Context-Based - lại được chia thành 3 cách: Unidirection, Bidirection và Nondirection. Context-Free- kiểu như Word2Vec hay Glove, sinh ra các Word Embedding dựa hoàn toàn vào từ điển từ (Vocabulary Dictionary).  Ví dụ:\n Từ bank trong câu bank account sẽ có Word Embedding giống hệt với từ bank trong câu bank of the river nếu sử dụng Context-Free. Trong câu I accessed the bank account, nếu sử dụng Unidirection thì Word Embedding của từ bank sẽ được sinh ra dựa trên các từ I accessed the. Còn nếu sử dụng Bidirection/Nondirection thì sẽ dựa trên các từ I accessed the \u0026hellip; account.  3. Cách làm việc của BERT\nKiến trúc của BERT kế thừa kiến trúc của Transformers. Kiến trúc đầy đủ của Transformers bao gồm 2 thành phần:\n Encoder Stack - nhận Input Sequence và sinh ra Context Vector đại diện cho Input Sequence đó. Decoder Stack - sinh ra Output Sequence dựa vào Context Vector.  Bởi vì nhiệm vụ của BERT là sinh ra Vector đại diện (Sequence Embedding hay Context Vector) của câu nên nó chỉ cần phần Encoder Stack.\nCũng giống như Transformers, BERT cũng yêu cầu Positional Encoding thêm vào Input Sequence. Ngoài ra, nó còn yêu cầu thêm một số thành phần khác:\n Token Embedding: Một CLS Token được thêm vào tại vị trí đầu tiên của Input Sequence, và SEP Token được thêm vào cuối mỗi câu trong Input Sequence. Segment Embedding: Một ký hiệu chỉ ra từ nào thuộc về câu nào nếu trong Input Sequence có nhiều câu. Positional Encoding: Chỉ ra vị trí của từ trong Input Sequence.    Để huấn luyện BERT model, chúng ta sử dụng 2 chiến lược như sau:\n3.1 Masked LM (MLS)\nÝ tưởng của LMS khá đơn giản: Lựa chọn ngẫu nhiên 15% các từ trong Input Sequence, thay thế chúng bằng [MASK] token, sau đó toàn bộ qua BERT model để dự đoán các từ được Masked dựa trên mỗi liên hệ trước sau với các từ còn lại. Về mặt kỹ thuật, các bước để thực hiện dự đoán như sau:\n Thêm một lớp Classification ngay sau Ouput của Encoder Stack. Nhân Ouput của Encoder Stack với Embedding Matrix để chuyển chúng sang miền của Vocabulary. Tính toán xác suất của mỗi từ trong Vocabulary với hàm Softmax.    Ý tưởng này tuy đơn giản dễ thực hiện nhưng lại gặp phải một vấn đề. Đó là BERT model sẽ chỉ dự đoán khi gặp [MASK] token trong Input Sequence, trong khi đó, chúng ta muốn nó phải dự đoán trong bất cứ trường hợp nào, có hoặc không có [MASK] token. Để giải quyết vấn đề này, trong số 15% số từ được lựa chọn ngẫu nhiên kia:\n 80% được thay thế bằng [MASK] token. 10% được thay thế bằng token ngẫu nhiên. 10% còn lại được giữ nguyên, không thay đổi.  Trong khi huấn luyện BERT model, Loss Function chỉ áp dụng đối với các dự đoán của Masked token và bỏ qua dự đoán của các Non-masked tokens khác. Thời gian huấn luyện cũng lâu hơn khá nhiều so với các model sử dụng phương pháp Nondirection training.\n3.2 Next Sentence Prediction (NSP)\nĐể hiểu được mối quan hệ giữa 2 câu, BERT sử dụng một kỹ thuật gọi là NSP. Trong quá trình huần luyện, một cặp câu được đưa vào, model sẽ học để dự đoán câu thứ 2 trong cặp câu Input đó có phải là câu tiếp theo của câu thứ nhất hay không? (ý tưởng nghe cũng khá giống với Siamese Network trong CV, :D).\nCụ thể, dữ liệu huấn luyện sẽ có 50% số cặp là liên tiếp (câu thứ 2 là tiếp theo của câu thứ nhất), 50% số cặp còn lại là 2 câu rời rạc nhau.  Để dự đoán xem câu thứ 2 có phải là tiếp theo của câu thứ nhất hay không, model sẽ làm như sau:\n Toàn bộ Input Sequence được đưa qua model. Ouput của [CLS] token được chuyển về vector kích thước 2x1 thông qua một lớp Classification đơn giản. Tính toán xác suất của mỗi nhãn sử dụng hàm Softmax.  MLM và NSP được sử dụng song song trong quá trình huấn luyện BERT model, với mục tiêu là tối thiểu hóa Loss Function kết hợp của cả 2 chiến lược đó. Đây là một ví dụ của câu nói nổi tiếng - Together to better.\n4. Một số thông tin về Pre-trained BERT model\n Có 2 phiên bản của Pre-trained BERT model:  BERT-Base: 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters BERT-Large: 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters    Thời gian huấn luyện mỗi phiên bản như sau:   Nếu có đủ dữ liệu huấn luyện, thời gian huấn luyện lâu hơn sẽ cho độ chính xác cao hơn. Ví dụ, đối với chiến lược Mask LM, BERT-Base model đạt được độ chính xác cao hơn 1% khi huấn luyện 1M Steps, so với 500K Steps (cùng batch_size).    6. Cách sử dụng BERT để Fine-tune model\nBERT có thể sử dụng cho rất nhiều tác vụ trong NLP:\n Classification task: Thêm một Classification layer ngay sau Ouput của Encoder Stack cho [CLS] token. Question Answering task: Một Q\u0026amp;A model nhận một câu hỏi và nhiệm vụ của nó là tìm ra câu trả lời trong Corpus. Sử dụng BERT, nó có thể được huấn luyện bằng cách học từ các cặp câu trong Input Sequence để dự đoán xem cặp câu đó có phải là một cặp câu hỏi - trả lời hay không? Named Entity Recognition (NER): Model nhận một câu và được yêu cầu là đánh dấu các dạng Entities khác nhau (Person, Organization, Date, \u0026hellip;) xuất hiện trong câu đó. Sử dụng BERT, model này có thể được huấn luyện bằng cách cho Ouput vector của mỗi Token đi qua một Classification layer để dự đoán xem đó có phải là một Entity hay không?  Trong quá trình Fine-tune, hầu hết các Hyper-parameters của BERT model được giữ nguyên. Các tác giả của bài báo đã đưa ra một số chỉ dẫn về các Hyper-parameters cần quan tâm, thay đổi để đạt được kết quả tốt nhất.\n Dropout – 0.1 Batch Size – 16, 32 Learning Rate (Adam) – 5e-5, 3e-5, 2e-5 Number of epochs – 3, 4  Các bạn có thể đọc chi tiết trong Section 3.5 \u0026amp; 4 của bài báo đó.\n7. Một số Pre-trained BERT model cho các tác vụ (lĩnh vực) cụ thể\nBERT là mã nguồn mở, có nghĩa là bất kỳ ai cũng có thể sử dụng nó. Google tuyên bố rằng người dùng có thể huấn luyện một hệ thống Question\u0026amp;Answering chỉ mất khoảng 30 phút nếu sử dụng TPU trên Cloud và vài giờ nếu sử dụng GPU. Nhiều tổ chứcc, nhóm nghiên cứu và các nhóm riêng biệt của Google đang tinh chỉnh kiến trúc mô hình BERT để tối ưu hóa hiệu quả của nó hoặc chuyên môn hóa nó cho một số nhiệm vụ nhất định. Một số ví dụ bao gồm:\n patentBERT - BERT model fine-tuned thực hiện nhiệm vụ phân loại các sáng kiến theo các nhóm khác nhau. docBERT - BERT model fine-tuned thực hiện nhiệm vụ phân loại các văn bản nói chung. bioBERT - BERT model fine-tuned thực hiện tạo Vector Embedding cho các tài liệu trong lĩnh vực sinh học. VideoBERT - BERT model fine-tuned thực hiện gán nhãn Video trên Youtube. SciBERT - BERT model fine-tuned thực hiện tạo Vector Embedding cho các tài liệu trong lĩnh vực khoa học. G-BERT - BERT model fine-tuned kết hợp với phương pháp Hierarchical Representations để đưa ra các khuyến nghị trong lĩnh vực y học. TinyBERT by Huawei - Phiên bản rút gọn của BERT để chạy nhanh hơn. DistilBERT by HuggingFace - Tương tự TinyBERT. PhoBERT - BERT model fine-tuned dành riêng cho các bài toán NLP sử dụng tiếng Việt, được công bố bởi VinAI.  8. Kết luận\nBERT chắc chắn là một bước đột phá trong việc giải quyết các bài toán NLP. Nó cho phép tiếp cận và tinh chỉnh nhanh các tham số, layers để áp dụng vào một loạt các bài toán thực tế . Trong bài này, chúng ta đã mô tả một số đặc điểm chính của BERT mà không đi quá sâu về mặt toán học. Nếu bạn muốn tìm hiểu tường tận, chi tiết hơn, bạn nên tìm đọc bài báo gốc của tác giả. Một tài liệu tham khảo hữu ích khác là mã nguồn BERT và các Pre-trained model của nó, bao gồm 103 ngôn ngữ và được nhóm nghiên cứu phát hành rộng rãi dưới dạng mã nguồn mở.\nỞ bài tiếp theo, có lẽ mình sẽ quay lại một số chủ đề của Computer Vision. Mời các bạn đón đọc.\n9. Tham khảo\n BERT paper Samia Rani Horev Yashu Seth Ben Lutkevich  ","permalink":"https://tiensu.github.io/blog/61_bert/","tags":["RNN","LSTM","Attention","Transformer","BERT"],"title":"BERT - Bidirectional Encoder Representations from Transformers"},{"categories":["RNN","LSTM","Attention","Transformer"],"contents":"Nếu bạn là dân ngoại đạo, bạn cũng có thể đã từng nghe về Transformers. Đó là một bộ phim bom tấn, liên tục lập kỷ lục phòng vé tại thời điểm nó ra mắt. Tuy nhiên, Transformers mình muốn nói ở đầy là một AI model. Được giới thiệu lần đầu vào năm 2017 trong bài báo Attention is all you need, cũng như bộ phim kia, nó cũng lập tức gây chấn động cộng động NLP lúc bấy giờ bởi hiệu năng của nó hơn hẳn so với các kiến trúc mô hình tồn tại trước đó trong các thử nghiệm được công bố.\n1. So sánh Transformers và họ hàng nhà RNN (LSTM, GRU)\nĐầu tiên, chúng ta thử so sánh Transformers với họ hàng RNN để thấy được ưu điểm của nó, và hiểu tại sao nó lại được yêu mến đến vậy.\nNhư chúng ta đã biết, kiến trúc Encoder-Decoder với RNN truyền thống tồn tại 2 nhược điểm:\n Không có khả năng tận dụng hết thông tin ngữ nghĩa của Input và Targer Sequence, đặc biệt là trong trường hợp Input Sequence có chiều dài lớn (Với cơ chế Attention, khả năng này có tốt hơn một chút nhưng vẫn chưa đủ). Vì phải xử lý tuần tự từng TimeStep một nên thời gian tính toán rất lâu.  Transformers giải quyết được 2 nhược điểm đó bằng cách:\n Sử dụng cơ chế Self-Attention (nhiều tầng Self-Attentions) để nắm bắt tốt hơn thông tin ngữ nghĩa của Input và Targer Sequence. Xử lý song song tất cả các TimeStep cùng một lúc \u0026ndash;\u0026gt; Giảm được rất nhiều thời gian tính toán.    2. Kiến trúc và thành phần của Transformers\nTransformer tỏ ra vượt trội trong việc xử lý dữ liệu văn bản vốn có tính chất tuần tự. Nó lấy một chuỗi văn bản làm đầu vào và tạo ra một chuỗi văn bản khác. Ví dụ như bài toán Machine Translation hay Text Summarization.\nVề thành phần cấu tạo, Transformers bao gồm một nhóm các bộ Encoders (Encoder stack) và một nhóm các bộ Decoder (Decoder stack). Ngoài ra còn có các thành phần Embedding, Encoding, Mask, \u0026hellip; khác để xử lý dữ liệu đầu vào và đầu ra.  2.1 Positional Encoder\nViệc xử lý đồng thời tất cả cá từ trong câu một lượt mang lại khả năng tính toán nhanh chóng cho Transformers, nhưng nó lại vô tình làm mất thông tin về vị trí của các từ trong câu đó. Để khắc phục vấn đề này, thông tin về vị trí của từ được mã hóa thành Positional Encoding (PE) vector để làm đầu vào cho Transformers.  PE được tính như sau: $PE_{(pos,2i)} = sin(\\frac{pos}{1000^{\\frac{2i}{d}}})$\n$PE_{(pos,2i+1)} = cos(\\frac{pos}{1000^{\\frac{2i}{d}}})$\n Trong đó, $pos$ là vị trí của từ trong câu, còn $i$ là chỉ số của các phần tử trong PE vector, $d$ là số chiều của Work/Token Embedding (cũng bằng với kích thước của PE).  Trong khi $d$ cố định thì $pos$ và $i$ thay đổi. PE cuối cùng là tổng hợp của tất cả các PE với sự thay đổi của $pos$ và $i$ đó.   Chi tiết thêm về Positional Encoding, các bạn có thể tham khảo tại đây.\n2.2 Masking\n Mục đích chính của Masking là che giấu (Mask) đi phần thông tin của Token phía sau, chỉ cho phép Decoder sử dụng thông tin của Token hiện tại và trước đó khi tạo Ouput. Ví dụ như trong một kỳ thi, ta cần phải che giấu đáp án đi, chỉ cho phép thí sinh sử dụng kiến thức của họ để làm bài.\nXem xét cách tính Output của Self-Attention như hình dưới đây:  Có 4 bước, chúng ta sẽ đi chi tiết mỗi bước.\n Bước 1    Ma trận Query (Q) được tạo từ Input (X) và ma trận trọng số của Q ($W^Q$). Tương tự cho Key (K) và Value (V).\nX có kích thước là (2,4), trong đó 2 là chiều dài của Input Sequence (số từ trong câu), 4 là số chiều của Word Embedding của 1 từ.\n$W^Q$ có kích thước là (4,4), trong đó 4 là số chiều của Word Embedding của 1 từ, 4 là do chúng ta giả sử $W^Q$ là ma trận vuông cho dễ xử lý. Thực tế có thể phức tạp hơn.\nKết quả, các ma trận Q, V, K có kích thước (2, 4).\n Bước 2    Đây chính là công thức tính Output của Self-Attention.\n Bước 3    Ta biểu diễn ma trận $I = Q \\times K^T$ bởi các giá trị A, B, C, D. Nhận thấy rằng, A chỉ phụ thuộc vào Embedding Token ở vị trí đầu tiên: $A = q_1 \\times k_1 = x_1*W^Q \\times k_1$).\nTrong khi đó, B phụ thuộc vào Embedding ở vị trí thứ nhất và thứ hai: ($B = q_1 \\times k_2 = x_1W^A \\times x_2W^K$).\n($x_1, x_2$ là các Embedding của Token thứ nhất và thứ hai).\n Bước 4    Làm tương tự cho ma trận F. Ta cũng nhận thấy B' phụ thuộc vào Embedding Token của cả hai vị trí 1 và 2. Để ngăn chặn điều này, ta thêm vào Mask như sau:  Ở đó:   Ta được:  Và cuối cùng là:  Như vậy, sau khi thêm Mask vào thì Output F của Decoder đã ko còn xuất hiện thành phần Embedding Token ở vị trí số 2.\n2.3 Scaled Dot-Product Attention\nĐây chính các phép tính toán của Self-Attention.  Lần lượt từng bước như sau:\n  Matmul - phép toán Matrix Dot-Product giữa ma trận Query và chuyển vị của ma trận Key. $MatMul(Q,K) = Q.K^T$\n   Scale - Ouput của phép toán Dot-Product có thể là một giá trị rất lớn, có thể làm rối loạn hoạt động của hàm Softmax. Do vậy, ta Scale chúng bằng cách chia cho hệ số $\\sqrt(d_k)$.\n  Mask - như đã đề cập ở mục 2.2.\n  Softmax - Đưa giá trị về một phân phối xác suất trong khoảng [0,1]. $Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt(d_k)})V$\n   2.4 Multi-Head Self-Attention\n Xem lại bài trước.\n2.5 Point-Wise Feed Forward Network and Residual Dropout\n Point-Wise Feed Forward Network Block, về cơ bản là một phép biến đổi tuyến tính hai lớp được sử dụng giống nhau trong toàn bộ kiến trúc mô hình, thường là sau các khối Attention.\nĐể áp dụng Regularization, một Dropout được áp dụng tại đầu ra của mỗi Sub-layer, trước khi nó được đưa vào làm Input cho Sub-layer tiếp theo.\n3. Huấn luyện mô hình Transformers\nDữ liệu huấn luyện bao gồm 2 thành phần: Input Sequence và Target Sequence. Quá trình huấn luyện diễn ra như sau:\n Bước 1 - Input Sequence được biến đổi thành Embedding, cùng với Positional Encoding để đưa vào Encoder Stack. Bước 2 - Encoder Stack xử lý và đưa ra một Vector đại diện của Input Sequence, gọi là Context Vector. Vector này sau đó được đưa sang cho Decoder Stack. Bước 3 - Target Sequence được bổ sung thêm tiền tố START_ (*còn gọi là start-of-sentence token*), chuyển đổi thành Embedding, cùng với Positional Decoding, đưa vào Decoder Stack. Bước 4 - Decoder Stack xử lý, sinh ra một Vector đại diện của Target Sequence. Bước 5 - Output Layer chuyển Vector đại diện này một Output Sequence. Bước 6 - Transformers Loss Function so sánh Output Sequence với Target Sequence. Loss sẽ được sử dụng để sinh ra Gradients để cập nhật trọng số mô hình trong quá trình Back-propagation.    Cách thức huấn luyện như thế này còn được gọi với cái tên là Teacher Forcing. Target Sequence ở đây đóng vai trò là Teacher để Force mô hình hoạt động đúng như mong muốn. Cá nhân mình thấy, nó khá giống với phương pháp Supervise Learning mà chúng ta đã quen thuộc.\n4. Sử dụng mô hình Transformers để dự đoán (Inference) 3 Quá trình dự đoán của Transformers có một chút khác biệt so với quá trình huấn luyện của nó. Chúng ta chỉ có Input Sequence, không có Target Sequence. Mục đích của Inference là sinh ra Output Sequence từ Input Sequence.\nTrong mô hình Seq2Seq, Output của Decoder Stack được sinh ra trong một vòng lặp, Ouput từ TimeStep trước được đưa vào làm Input của TimeStep tiếp theo. Vòng lặp kết thúc khi Output là Token kết thúc (_END). Còn trong mô hình Transformers, tại mỗi TimeStep, toàn bộ Output tại các thời điểm trước đó được đưa vào làm Input cho thời điểm tiếp theo, thay vì chỉ sử dụng Output cuối cùng.\nToàn bộ quá trình Inference diễn ra như sau:\n Bước 1 - Input Sequence đuọc chuyển thành Embedding, cùng với Positional Decoding, đưa vào Encoder Stack. Bước 2 - Encoder Stack xử lý và tạo ra Context Vector. Vector này được chuyển sang cho Decoder Stack. Bước 3 - Phía Decoder Stack, sử dụng một Input Sequence rỗng (*chỉ có một Start Token - START_*), chuyển sang Embedding, cùng với Positional Encoding, đưa vào Decoder Stack. Bước 4 - Decoder Stack xử lý, cùng với Context Vector từ Encoder Stack, sinh ra Vector đại diện của Output Sequence. Bước 5 - Output Layer biến đổi Vector đại diện này thành Output Sequence. Bước 6 - Từ cuối cùng trong Output Sequence đặt vào vị trí thứ 2 (sau Start Token) của Input Sequence của Decoder Stack. Sau đó, Intput Sequence mới này lại được đưa vào Decoder Stack. Bước 7 - Lặp lại từ bước 4-6 cho đến khi bắt gặp từ cuối cùng trong Ouput Sequence là End Token (_END).    5. Ứng dụng của Transformers\nTransformers được sử dụng rất rộng rãi ở hầu hết các bài toán trong lĩnh vực NLP. Với mỗi bài toán, chúng ta sẽ sử dụng một biến thể khác của Transformers.\n  Language models - Đây là bài toán sinh ra từ mới cho câu (sáng tác nhạc, làm thơ, viết truyện, \u0026hellip;). Ở đây, chỉ thành phần Encoder Stack của Transformers được sử dụng, như là một bộ trích xuất đặc trưng (Sequence Embedding) của Input Sequence. Đầu ra của Encoder Stack được đưa vào Language Model để cho ra một xác suất cho mỗi từ trong từ điển. Từ có xác suất cao nhất là kết quả cuối cùng.    Text Classification - Đây là bài toán phân loại văn bản thành các chủ đề, nhãn, \u0026hellip; khác nhau. Tương tự Language Model, chúng ta cũng chỉ sử dụng phần Encoder Stack cho ứng dụng này. Đầu ra của Encoder Stack được đưa vào bộ phân lớp, cho ra xác suất của từng nhãn. Nhãn có xác suất cao nhất sẽ được công nhận là kết quả chung cuộc.    Seq2Seq models - Đây là lớp bài toán bao gồm Machine Translation, Text Summarization, Question-Answering, Named Entity Recognition, Speech Recognition, \u0026hellip;\n  Kiến trúc đầy đủ của Transformers được sử dụng trong các ứng dụng này.\n5. Kết luận\nTrong bài này, chúng ta đã cùng nhau tìm hiểu khá chi tiết về kiến trúc Transformers, cũng như các ưu/nhược điểm và ứng dụng của nó.\nỞ bài tiếp theo, mình sẽ giới thiệu về của BERT, một state of the art language model for NLP. Mời các bạn đón đọc.\n6. Tham khảo\n Ketan Doshi Ketan Doshi datascience Rohan Jagtap Samuel Kiebaum Attention is all you need  ","permalink":"https://tiensu.github.io/blog/60_transformer/","tags":["RNN","LSTM","Attention","Transformer"],"title":"Transformers - Nhưng không phải là kẻ hủy diệt ..."},{"categories":["RNN","LSTM","Attention","Transformer"],"contents":"Kiến trúc Transformer với xương sống là Self-Attention đã làm mưa làm gió trong cộng đồng NLP trong 1-2 năm gần đây. Nó đạt được độ chính xác rất cao trong hầu hết các bài toán NLP. Trong bài này, hãy cùng nhau tìm hiểu kỹ hơn về cơ chế Self-Attention, làm tiền đề cho việc tìm hiểu kiến trúc Transformer ở bài sau.\n1. Self-Attention là gì?\nChúng ta đều biết rằng Word Embedding là vector đại diện cho ngữ nghĩa của một từ trong câu. Những từ mà có nghĩa tương tự nhau thì vector của chúng cũng sẽ gần giống nhau và ngược lại. Tuy nhiên, trong một câu, ý nghĩa của các từ riêng lẻ không đại diện cho cả câu đó. Ví dụ, xét câu: The bank of a river. Hai từ bank và river, nếu tách riêng thì có ý nghĩa hoàn toàn khác nhau, và nếu mang ý nghĩa đó của chúng vào câu thì sai hoàn toàn.\nCơ chế Self-Attention được đề xuất trong bài báo Attention is all you need có thể giải quyết tốt vấn đề này. Ý tưởng làm việc của nó là so sánh các từ với nhau đôi một, bao gồm cả chính nó (self) để tìm ra mức độ quan trọng của mỗi từ mà nó nên chú ý tới (thể hiện qua trọng số).\n2. Minh họa cách làm việc của Self-Attention\nCơ chế Attention có thể được hiểu như là sự kết hợp giữa một Query và một cặp Key-Value để cho ra một Output. Công thức tính như sau: $Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) \\times V$\n Trong đó, $\\frac{1}{\\sqrt(d_k)}$ là hệ số tỷ lệ. $d_k$ là số chiều của Key.\n2.1 Bước 1 - Chuẩn bị Input\nGiả sử, chúng ta có 3 Inputs (3 từ trong Input Sequence), mỗi Input là một vector 4 chiều như sau:   2.2 Bước 2 - Khởi tạo trọng số (Weight) cho Key, Query và Value\nMỗi một Input sẽ được đại diện bởi 3 đại lượng: key, query và value có số chiều tùy ý. Giả sử, 3 đại lượng này có số chiều là 3. Để tạo ra chúng, ta cần khởi tạo các trọng số cho từng đại lượng. Vì Input có số chiều là 4, (Key, Query, Value) có số chiều là 3 nên các Weights phải có kích thước 4x3. Chúng thường có giá trị nhỏ, được khởi tạo ngẫu nhiên sử dụng một trong số các phân phối Gaussian, Xavier, Kaiming, \u0026hellip;\n  Weight cho Key:    Weight cho Query:    Weight cho Value:    2.3 Bước 3 - Tính Key, Query, Value\nKey, Query, Value đạt được bằng cách nhân (dot product) Input với trọng số tương ứng của chúng.\n Key:      Query:      Value:     Trong thực tế, một Bias Vector có thể được thêm vào khi tính các giá trị Key, Query và Value.\n2.4 Bước 4 - Tính Attention Scores\n Để tính Attention Scores cho Input1, ta nhân (dot product) Query của nó với tất cả các Keys.  Từ bước 4 đến bước 7, mình chỉ tính cho Input1 làm đại diện. Hai Inputs còn lại được tính tương tự.\n2.5 Bước 5 - Tính Softmax Để đơn giản, mình bỏ qua hệ số tỉ lệ $\\frac{1}{\\sqrt(d_k)}$  Đưa Attention Scores qua hàm Softmax ta được:  2.6 Bước 6 - Tính Weighted Values\n Softmaxed Attention Score nhân với tất cả các Values ta được Weighted Values.  2.7 Bước 7 - Tính tổng Weighted Values\n Tính tổng Weighted Values theo kiểu element-wise ta được Output1 vector:  [2.0, 7.0, 1.5] chính là bộ trọng số thể hiện sự tương quan của Input1 với từng Input (bao gồm chính nó).\n2.8 Bước 8 - Lặp lại từ bước 4-7 cho Input2 \u0026amp; Input3\nChúng ta đã tính xong Output1, lặp lại các bước từ 4-7 đối với Input2 \u0026amp; Input3 ta được Output2 \u0026amp; Output3.  Chú ý - Kích thước của Query và Key phải luôn bằng nhau để có thể thực hiện được dot product, còn kích thước của Value có thể khác. Kich thước của Output sẽ giống với kích thước của Value.\n3. Multi-Head Self-Attention\nTrong kiến trúc của Transformer, mỗi một Self-Attention module được gọi là một Head. Việc sử dụng nhiều Heads đồng thời gọi là Multi-Head.  Mỗi Head nhận vào một Input $x$ (Token Embedding và Positional Decoding) và cho ra: $Head_i = Attention_i(x) = softmax(\\frac{Q_iK_i^T}{\\sqrt(d_k)})V_i$\n Trong đó, $d_k = d_k$(trong trường hợp một Head) /(số lượng Head).\nSau khi có được các Output của từng Head, ta sẽ tổng hợp chúng lại thành 1 Output duy nhất. $MultiHead(Q,K,V) = Concat(Head_1, Head_2, ..., Head_h)W^0$\n $W^0$ là ma trận có chiều rộng bằng với chiều rộng của ma trận Input, mục đích sử dụng của nó là để đưa kích thước của Output về bằng với kích thước của Input.\n4. So sánh Attention và Self-Attention\nNếu bạn vẫn còn mơ hồ giữa Attention và Self-Attention thì mình sẽ liệt kê những điểm khác nhau giữa chúng cho bạn.\n Attention thường sử dụng kết hợp với RNN/LSTM/GRU để cải thiện hiệu năng của mô hình hiện tại. Self-Attention thay thế hoàn toàn RNN/LSTM/GRU. Attention thường xuất hiện trong kiến trúc có đủ 2 thành phần Encoder và Decoder để truyền thông tin giữa chúng. Ngược lại,Self-Attention thường chỉ áp dụng trong phạm vi một thành phần, hoặc Encoder hoặc Decoder. Attention chỉ có thể sử dụng 1 lần trong một kiến trúc mô hình, trong khi đó, Self-Attention có thể áp dụng nhiều lần (VD: 18 lần trong Transformer). Self-Attention, như tên gọi, làm nhiệm vụ mô hình hóa mối quan hệ giữa các từ trong một cùng 1 chuỗi (Query, Key, Value xuất phát từ cùng 1 nguồn), còn Attention thì là 2 chuỗi khác nhau. Attention có thể kết nối 2 kiểu Input Sequence khác nhau (VD: text \u0026amp; image), Self-Attention thì chỉ làm việc với 1 loại. Cơ chế Multi-Head thường áp dụng cho Self-Attention. Nhưng về mặt lý thuyết, nó cũng có thể được sử dụng cho Attention. Tương tự, Query/Key/Value thường sử dụng đối với Self-Attention, nhưng nó cũng có thể được áp dụng cho Attention.  5. Kết luận\nTrong bài này, chúng ta đã cùng nhau tìm hiểu khá chi tiết về cơ chế Self-Attention cũng như khái niệm Multi-Head Self-Attention.\nỞ bài tiếp theo, mình sẽ giới thiệu về mô hình Transformer. Mời các bạn đón đọc.\n6. Tham khảo\n Praphul Singh Raimi Karim datascience Attention is all you need  ","permalink":"https://tiensu.github.io/blog/59_self-attention/","tags":["RNN","LSTM","Attention","Transformer"],"title":"Self-Attention và Multi-head Sefl-Attention trong kiến trúc Transformer"},{"categories":["RNN","LSTM","Attention"],"contents":"Trong bài này, mình sẽ giải thích qua về kiến trúc Encoder-Decoder với mô hình Seq2Seq. Sau đó, chúng ta sẽ tìm hiểu chi tiết về cơ chế Attention áp dụng trong kiến trúc đó.\n1. Giới thiệu mô hình Sequence to Sequence (Seq2Seq)\nMô hình Seq2Seq được giới thiệu lần đầu vào năm 2014 bởi Google. Mục đích của nó là ánh xạ một Input Sequence Data có chiều dài cố định thành một Output Sequence Data có chiều dài cố định. Chiều dài của 2 Sequence Data không nhất thiết phải giống nhau. Ví dụ khi dịch câu có 5 từ What are you doing now? từ tiếng Anh sang câu có 7 ký tự 今天你在做什麼？ trong tiếng Trung Quốc.\nMô hình Seq2Seq có thể giải quyết các bài toán sau:\n Text Summarization - Đây là bài toán tóm tắt nội dung của một văn bản dài thành một đoạn văn bản ngắn hơn. Kể từ khi được Google giới thiệu năm 2014, nó đã trở nên khá phổ biến. Machine Translation - Dịch văn bản giữa các ngôn ngữ khác nhau. Google Translate chính là một sản phẩm của bài toán này. Image/Video Captioning - Đưa cho máy tính một bức ảnh hoặc một video, nó sẽ trả lại cho bạn một (hoặc một vài) câu miêu tả nội dung của bức ảnh / Video đó. Mình đang nghĩ rằng phần thi đầu tiên của kỳ thi TOEIC (phần thi miêu tả tranh) có thể chính là một ứng dụng thực tế của bài toán này. Speech Recognition - Bài toán trong lĩnh vực Audio, còn được gọi là Speech To Text, tức chuyển đổi âm thanh thành văn bản. Music Generation - Đây là một bài toán rất thú vị, máy tính có thể sáng tác nhạc cho bạn. Nghe chắc sẽ rất ngầu! :D Recommendation Engine - Hệ thống khuyến nghị có lẽ đã không còn xa lạ với mọi người. Có rất nhiều các để tạo ra nó, và mô hình Seq2Seq với kiến trúc Encoder-Decoder cũng là một trong số đó, cho kết quả rất khả quan. Chatbot - Hay còn gọi là hệ thống Question-Answer. Siri hay Alexa là ví dụ thực tế.  Các bài toán kể trên đều có chung một đặc điểm là chúng sử dụng dữ liệu ở dạng chuỗi (tuần tự), bao gồm nhiều TimeSteps. Đó có thể là văn bản, âm thanh, tín hiệu, \u0026hellip; Ví dụ đối với văn bản thì mỗi TimeStep có thể hiểu là một từ trong văn bản đó.\n2. Kiến trúc của mô hình Seq2Seq\nMô hình Seq2Seq bao gồm 2 thành phần: Encoder và Decoder. Mỗi một thành phần bao gồm nhiều NN Layers xếp chồng lên nhau (stack). NN Layer có thể là CNN, RNN, LSTM. GRU, \u0026hellip; Trong bài này, mình sẽ lấy ví dụ là LSTM.  2.1 Quá trình huấn luyện\nTrong quá trình huấn luyện, mỗi thành phần sẽ thực hiện nhiệm vụ như sau:\n  Encoder - Đọc vào toàn bộ Input Sequence, lần lượt từng TimeStep tại các LSTM Cell. Tại TimeStep $t$, Output ra của các Cell là Hidden State ($h_t$) và Cell State ($C-t$), gọi chung là Internal State. Internal State của TimeStep trước được sử dụng cùng với Input của TimeStep hiện tại để làm đầu vào cho Cell hiện tại. Internal State ($h_0, c_0$) được khởi tạo ngẫu nhiên. Internal State của Cell cuối cùng của Encoder được sử dụng làm đầu vào cho Decoder. Chi tiết về Internal State của LSTM Cell, bạn có thể xem lại bài này của mình.\n  Decoder - Đọc vào toàn bộ Target Sequence, lần lượt từng TimeStep. Khác với Encoder, Target Sequence được thêm vào tiền tố START_ và hậu tố _END để chỉ ra điểm bắt đầu và kết thúc của nó. Internal State ban đầu ($h_0, s_0$) của Decoder được khởi tạo bằng với Intern State của Cell cuối cùng trong Encoder. Tại mỗi TimeStep $t$, Decoder sẽ đọc vào một từ trong văn bản Target Sequence, cho ra ra một từ dự đoán ($y'_t$) và Internal State ($h_t, c_t$). Internal State này cũng sẽ được sử dụng cho TimeStep tiếp theo, còn $y'_t$ sẽ được dùng để tính toán lỗi với Target Sequence, sau đó Backpropagation sẽ cập nhật lại các trọng số của model theo lỗi đó. Internal State ở Cell cuối cùng của Decoder được loại bỏ vì không dùng đến.\n   2.2 Quá trình dự đoán\nQuá trình dự đoán của Encoder vẫn giống như quá trình huấn luyện nó. Còn đối với Decoder, quá trình dự đoán diễn ra như sau:\n Internal State ban đầu ($h_0, s_0$) của Decoder được khởi tạo bằng với Intern State của Cell cuối cùng trong Encoder. Input của Decoder luôn bắt đầu bằng START_. LSTM Cell của Decoder sinh ra mỗi từ tại mỗi TimeStep. Internal State của mỗi TimeStep được sử dụng cho TimeStep tiếp theo. Từ dự đoán sinh ra tại mỗi TimeStep ($y'_t) được chuyển thành Input cho TimeStep tiếp theo. Quá trình dự đoán kết thúc khi Decoder dự đoán ra $y'_t$ là _END.   3. Hạn chế của mô hình Seq2Seq với kiến trúc Encoder-Decoder\nKiến trúc Encoder-Decoder làm việc rất hiệu quả đối với Input Sequence có chiều dài nhỏ, nhưng hiệu năng sẽ giảm dần khi kích thước của Input Sequence tăng lên. Giả sử, Encoder nhận vào một Input Sequence {$x_1, x_2, \u0026hellip;, x_n$} và mã hóa thành các vectors có chiều dài cố định {$h_1, h_2, \u0026hellip;, h_n$}, gọi là Hidden State hay Context Vector. Chỉ có Context Vector cuối cùng $h_n$ mới được sử dụng cho bộ Decoder để dự đoán Output, dẫn đến thông tin của toàn bộ Input Sequence không được sử dụng đầy đủ (mất thông tin). Attention xuất hiện như là một giải pháp hữu hiệu để giải quyết vấn đề này.\n4. Giới thiệu Attention\nAttention là một kỹ thuật được Bahdanau et al., 2014 và Luong et al., 2015 giới thiệu trong các bài báo của họ. Ý tưởng của nó là cho phép Decoder sử dụng thông tin của toàn bộ Input Sequence, nhưng chỉ tập trung vào những phần quan trọng tại mỗi TimeStep. Nói một cách cụ thể và dễ hiểu hơn, Attention thực chất là cơ chế tạo ra một Context Vector bằng cách tính trung bình có trọng số của toàn bộ Internal State của Input Sequence trong bộ Encoder: $c_i = \\sum_{j=1}^n\\alpha_{ij}h_j$\n  Trong đó:\n $\\alpha_{ij}$ là trọng số của TimeStep $j$ của Decoder và TimeStep $i$ của Encoder. Nói cách khác, Output thứ $j$ của Decoder nên chú ý một lượng $alpha_{ij}$ đến Input thứ $i$ của Encoder. $h_i$ là Hidden State tại TimeStep $i$ của Encoder. $n$ là chiều dài của Input Sequence.  $\\alpha_{ij}$ được tính bằng cách lấy Softmax của Attention Score ($e_{ij}$): $\\alpha_{ij} = softmax(e_{ij}) = \\frac{exp(e_{ij})}{\\sum_{k=1}^m exp(e_{ik}}$\n$e_{ij} = f(s_{i-1}, h_j) = AlignScore(s_{i-1}, h_j)$\n Trong đó:\n $h_{i-1}$ là Hidden State tại TimeStep $i-1$ của Decoder. $s_j$ là Hidden State tại TimeStep $j$ của Encoder.  Context Vector $c_{ij}$ sau đó được sử dụng để Decoder tính ra Output $y_i$.\n 5. Bahdanau Attention \u0026amp; Luong Attention\nNhư bên trên đã giới thiệu, hai nhóm tác giả đã giới thiệu 2 loại Attention khác nhau, gọi là Bahdanau Attention và Luong Attention.  Xét về nguyên lý hoạt động thì 2 dạng Attention này đều giống nhau. Sự khác nhau của chúng nằm ở kiến trúc và cách tính toán của mỗi loại.\n5.1 Bahdanau Attention\n Bahdanau Attention còn được gọi là Additive Attention, được tạo ra bởi Dzmitry Bahdanau trong bài báo vào năm 2014. Mục tiêu của nó là cải thiện hiệu năng của mô hình Seq2Seq bằng cách thay đổi đầu vào của Decoder với các thông tin từ Input Sequence. Các bước tiến hành như sau:\n Tạo Encoder Hidden State - Encoder sinh ra Hidden State tại mỗi TimeStep. Tính toán Alignment Score giữa Decoder Hidden State ở TimeStep trước đó với mỗi Encoder Hidden State. Chú ý rằng, Encoder Hidden State ở TimeStep cuối cùng được sử dụng như là Decoder Hidden State ở TimeStep đầu tiên. Tính toán Softmax của Alignment Score - Giá trị của Alignment Score ở bước trên được đưa về khoảng giá trị [0,1] bằng cách sử dụng hàm Softmax. Tính toán Context Vector - Encoder Hidden State và Alignment Score tương ứng của nó được nhân với nhau để tạo thành Context Vector cho mỗi TimeStep. Tính toán Output của Decoder - Các Context Vectors được cộng lại với nhau, rồi cộng với vào Decoder Output và Decoder Hidden State tại TimeStep trước đó, để sinh ra Decoder Output mới tại TimeStep hiện tại. Lặp lại bước 2-5 đối với mỗi TimeStep của Decoder đến tận khi Decoder Output là _END hoặc chiều dài của Output Sequence đặt đến giá trị tối đa quy định trước.    5.2 Luong Attention\n Luong Attention được đề xuất bởi Thang Luong trong bài báo của anh ấy và đồng nghiệp. Nó còn có tên khác là Multiplicative Attention, kế thừa từ Bahdanau Attention. Hai điểm khác biết chủ yếu giữa Luong Attention và Bahdanau Attention là:\n Cách tính toán Alignment Score. Có 3 phương pháp tính Aligment Score trong Luong Attention so với 1 phương pháp của Bahdanau Attention. Vị trí của Attention trong kiến trúc Encoder-Decoder.  Các bước thực hiện Luong Attention như sau:\n Tạo Encoder Hidden State - Encoder sinh ra Hidden State tại mỗi TimeStep. Tạo Decoder Hidden State - Decoder Hidden State và Decoder Output của TimeStep trước đó được đưa qua Decoder RNN Cell để sinh ra Decoder Hidden State tại TimeStep hiện tại. Tính toán Alignment Score - Sử dụng Decoder Hidden State ở bước trên và Encoder Hidden State để tính Alignment Score. Tính toán Softmax của Alignment Score - Giá trị của Alignment Score ở bước trên được đưa về khoảng giá trị [0,1] bằng cách sử dụng hàm Softmax. Tính toán Context Vector - Encoder Hidden State và Alignment Score tương ứng của nó được nhân với nhau để tạo thành Context Vector. Tính toán Output của Decoder - Context Vector được cộng vào Decoder Output và Decoder Hidden State tại TimeStep trước đó, để sinh ra Decoder Output mới tại TimeStep hiện tại. Lặp lại bước 2-6 đối với mỗi TimeStep của Decoder đến tận khi Decoder Output là _END hoặc chiều dài của Output Sequence đặt đến giá trị tối đa quy định trước.  Như chúng ta thấy, thứ tự các bước của Luong Attention khác so với Bahdanau Attention.\n6. Global/Soft Attention \u0026amp; Local/Hard Attention\nPhụ thuộc vào việc có bao nhiêu Encoder Hidden State tham gia vào quá trình tạo Context Vector cho Decoder mà chúng ta có thể chia Attention thành 2 loại: Global/Soft Attention và Local/Hard Attention.  6.1 Global/Soft Attention\nGlobal Attention, tên khác là Soft Attention là loại Attention mà ở đó toàn bộ Encoder Hidden State đều được sử dụng để tính toán Context Vector tại mỗi TimeStep.  6.2 Local/Hard Attention\nGlobal Attention có một nhược điểm là nó yêu cầu tài nguyên tính toán khá lớn, nhất là đối với các bài toán mà Input Sequence có chiều dài lớn. Đó chính là lý do Local/Hard Attention ra đời. Nó giải quyết vấn đề của Global Attention bằng cách chỉ sử dụng một số lượng nhất định Encoder Hidden State thay vì tất cả.  7. Mở rộng của Attention (Extended Attention)\nCác loại Attention mà chúng ta nói từ đầu đến giờ chỉ hoạt động với kiến trúc Encoder-Decoder (có đủ 2 thành phần Encoder và Decoder), tức là phải có cả Input Sequence và Target Sequence như trong bài toán Machine Translation hay Text Summarization. Để áp dụng vào bài toán mà chỉ có một thành phần Encoder (chỉ có Input Sequence, không có Target Sequence) hoặc ngược lại, như Text Classification, chúng ta phải sử dụng các dạng mở rộng của Attention. Có 3 loại Extended Attention là: Self-Attention, Multi-head Self-Attention và Hierarchical Attention.  Chúng ta sẽ tìm hiểu kỹ hơn về Self-Attention và Multi-head Sefl-Attention trong bài tiếp theo. Còn Hierachical Attention, các bạn đọc thêm tại đây.\n8. Một số dạng Alignment Score Function\nBảng dưới đây tổng hợp một số dạng Alignment Score Function:  9. Ví dụ về cách làm việc của Attention\nTrong phần này, chúng ta sẽ minh họa cách làm việc của Attention thông qua một ví dụ trực quan để có thể hiểu rõ hơn về nó.\n9.1 Bước 1 - Chuẩn bị Encoder Hidden State\nGiả sử chúng ta có 4 Encoder Hidden States (màu xanh) và Decoder Hidden State đầu tiên (màu vàng).  9.2 Bước 2 - Tính Alignment Score\nTính Aligment Score, sử dụng Dot Product Function giữa Decoder Hidden State và Encoder Hidden States (xem mục 8).   Theo kết quả trên, chúng ta đạt được Alignment Score cao nhất là 60 tại TimeStep thứ 2 của Encoder (Hidden State là [5,0,1]). Điều này có nghĩa là Output tiếp theo của Decoder sẽ chịu ảnh hưởng nhiều của Hidden State này.\n9.3 Bước 3 - Cho Alignment Score qua Softmax Function\nTiếp theo, chúng ta đưa Alignment Scores đi qua hàm Softmax, thu được các giá trị trong khoảng [0,1].   9.4 Buớc 4 - Tính Context Vector của mỗi TimeStep\nVector Context được tính bằng cách nhân Encoder Hidden State với Alignment Score (đã đi qua hàm Softmax) tương ứng của nó.   9.5 Bước 5 - Tính tổng của các Context Vector\nCác Context Vector tại mỗi TimeStep được cộng lại với nhau, tạo thành 1 Context Vector chung cho toàn bộ Input Sequence.   9.6 Bước 6 - Sử dụng Context Vector cho Decoder\nĐến đây, ta đã được Context Vector đầy đủ của toàn bộ Input Sequence. Chúng ta sẽ đưa nó vào Decoder để sử dụng tạo ra Output mới.  10. Kết luận\nTrong bài này, chúng ta đã cùng nhau tìm hiểu khá chi tiết về cơ chế Attention áp dụng cho mô hình Seq2Seq với kiến trúc Encoder-Decoder.\nỞ bài tiếp theo, mình sẽ tiếp tục giới thiệu về Self_Attention và Multi-head Sefl-Attention. Hiểu được 2 lại Attention này là điều kiện tiền để để chúng ta có thể tiếp tục với mô hình Transformer. Mời các bạn đón đọc.\n11. Tham khảo\n Keshav Bhandari Raimi Karim Anusha Lihala floydhub  ","permalink":"https://tiensu.github.io/blog/58_attention/","tags":["RNN","LSTM","Attention"],"title":"Tìm hiểu cơ chế Attention trong mô hình Seq2Seq"},{"categories":["RNN","LSTM"],"contents":"Thực ra, các bài toán về Nartual Language Processing(NLP) không phải là thế mạnh của mình. Từ trước đến giờ, mình chủ yếu làm các bài toán về Computer Vision(CV). Tuy nhiên, trong thời gian gần đây, cộng đồng nói rất nhiều về Attention, Transformer, BERT, \u0026hellip;. Đó là những kỹ thuật tiên tiến mới ra đời, giúp giải quyết rất nhiều tác vụ khó của NLP, đạt đến độ State of Art. Hoạt động trong lĩnh vực AI đã lâu, mình không muốn đứng ngoài dòng chảy công nghệ đó. Mặc dù bên CV, mình còn rất nhiều thứ muốn viết, nhưng trong một số bài viết sắp tới mình muốn \u0026ldquo;đổi gió\u0026rdquo; một chút. Mình sẽ viết về các kỹ thuật sử dụng trong NLP, sau khi đã dành ra kha khá thời gian để tìm hiểu về chúng. Mục đích viết vẫn là để ghi nhớ và chia sẻ với mọi người. Các kỹ thuật mình dự định viết sẽ bao gồm: RNN, LSTM, Transformer, BERT, Attention, Seq2Se2, \u0026hellip;\nBài đầu tiên trong chủ đề NLP, mình sẽ cùng các bạn tìm hiểu về RNN.\nNếu như các bên CV có CNN thì bên NLP có RNN. CNN chuyên xử lý dữ liệu có kiến trúc dạng lưới (grid), (VD: Images, \u0026hellip;), còn RNN chuyên xử lý dữ liệu dạng chuỗi (sequential).\n1. Overview\n1.1 Kiến trúc RNN\nGiả sử:\n $X_t \\in R^{n\\times d}$ là Input tại Time-Step $t$, $h_t \\in R^{n\\times h}$ là Hidden State tại Time-Step $t$, $W_{xh} \\in R^{d\\times h}$ là ma trận trọng số của Hidden State tại Time-Step $t$, $b_h \\in R^{1\\times h}$ là hệ số Bias của Hidden State tại Time-Step $t$, $h_{t-1}$ là Hidden State tại Time-Step $t-1$, $W_{hh} \\in R^{h\\times h}$ là ma trận trọng số của Hidden State tại Time-Step $t-1$, $W_{hq} \\in R^{h\\times q}$ là ma trận trọng số của Output Layer tại Time-Step $t$ $b_q \\in R^{1\\times h}$ là hệ số Bias của Output Layer tại Time-Step $t$, $\\phi$ là hàm kích hoạt.   Khi đó:\n  Trạng thái của Hidden Layer tại Time_Step $t$ sẽ là: $h_t = \\phi(X_tW_{xh} + h_{t-1}W_{hh} + b_h)$\n   Giá trị đầu ra tại Time-Step $t$ sẽ là: $O_t = h_tW_{hq} + b_q$\n   1.2 Ưu điểm, nhược điểm của RNN\nƯu điểm:\n Có khả năng xử lý dữ liệu đầu vào ở bất kỳ độ dài nào. Kích thước mô hình không tăng theo kích thước đầu vào. Việc huấn luyện mô hình có sử dụng thông tin ở Time-Step trước đó. Các hệ số của mô hình (weight và bias) được chia sẻ theo thời gian.  Nhược điểm:\n Việc xử lý, tính toán mất khá nhiều thời gian. Thông tin từ các Time-Step ở xa không được duy trì tốt. Không thể xem xét bất kỳ đầu vào nào trong tương lai cho trạng thái hiện tại.  1.3 Một số loại mô hình RNN\n  Dạng 1: One-to-One    Dạng 2: One-to-Many  Ứng dụng: Music generation\n  Dạng 3: Many-to_One  Ứng dụng: Sentiment classification\n  Dạng 4: Many-to-Many (Input Data và Ouput có chiều dài bằng nhau)  Ứng dụng: Name entity recognition\n  Dạng 4: Many-to-Many (Input Data và Ouput có chiều dài khác nhau)  Ứng dụng: Machine translation\n  1.4 Loss Function\nLoss Function của RNN bằng tổng các Loss tại các Time-Step:  1.5 Backpropagation through time\nBackpropagation được thực hiện tại mỗi Time-Step. Tại Time-Step T, đạo hàm của Loss Function $L$ đối với ma trận trọng số $W$ được cho bởi công thức:\n 2. Xử lý vấn đề Long Term Dependences\nBình thường thì RNN sử dụng 1 trong 3 Activation Function là: Sigmoid, Tanh và ReLU.  Cũng giống như các mô hình khác, hiện tượng Vanishing \u0026amp; Exploring Gradient cũng xảy ra ở trong mô hình RNN. Điều này dẫn đến việc RNN không có khả năng ghi nhớ trạng thái của các Time-Step ở xa. Nguyên nhân của các hiện tượng này là do việc thực hiện phép nhân Gradient theo hàm mũ, làm cho nó tăng/giảm một các đột biến khi số lượng Layers tăng lên.\nSử dụng ReLU chúng ta đã giải quyết được khá tốt vấn đề Vanishing Gradient. Còn đối với Exploring Gradient, chúng ta có thể sử dụng Gradient Clipping.  Ý tưởng của Gradient Clipping cũng tương tự như ReLU, nó đưa ra một giới hạn, và Gradient chỉ có thể nhỏ hơn hoặc bằng giới hạn đó.\n3. Long Short Team Memory (LSTM)\nLong Short Team Memory (LSTM)) là một phiên bản cải tiến của RNN, giúp dễ dàng ghi nhớ dữ liệu quá khứ trong bộ nhớ của nó. Vấn đề Vanishing Gradient của RNN được giải quyết tốt hơn với LSTM. LSTM rất phù hợp giải quyết các bài toán phân loại, xử lý và dự đoán chuỗi thời gian có độ dài không xác định. Mô hình LSTM cũng được huấn luyện bằng cách thuật toán Backpropagation.\nKiến trúc mạng LSTM bao gồm nhiều Layers, mỗi Layers được cấu tạo bởi nhiều đơn vị nhỏ gọi là Cell. Mỗi Cell được đại diện bởi 2 bộ nhớ: Cell State ($C$) và Hidden State ($h$).\n- Hidden State - h,H: Bộ nhớ ngắn hạn (working memory), chỉ lưu thông tin của Cell ngay trước Cell hiện tại. Tồn tại trong cả RNN và LSTM. Hidden State cũng chính là Ouput của RNN/LSTM Cell.\n- Cell State - C: Bộ nhớ dài hạn (long-term memory, memory cell), lưu thông tin của nhiều Cells trong quá khứ. Chỉ tồn tại trên LSTM.\nXét 1 Cell hiện tại ($C_t, h_t$) trong LSTM. Luồng dữ liệu trong Cell này tuần tự đi qua 3 cổng như sau:\n- Forget gate: Cổng này quyết định thông tin nào từ Cell trước đó ($C_{t-1}$) nên được giữ lại hoặc ném đi. Thông tin từ Hidden State trước đó ($h_{t-1}$) và thông tin từ Input hiện tại ($x_t$) được đưa qua hàm Sigmoid, cho ra một giá trị trong khoảng từ 0 đến 1. Giá trị này càng gần 0 nghĩa là thông tin ít quan trọng (*trường hợp = 0 thì có thể bỏ qua - Forget*), giá trị càng gần 1 nghĩa là thông càng tin quan trọng.  $f_t = \\sigma(W_f\\cdot[h_{t-1},x_t] + b_f) = \\sigma(W_{xf}X_t + W_{hf}h_{t-1} + b_f)$\n  Input gate: Cổng này quyết định giá trị nào từ Hidden State trước đó ($h_{t-1}$) và thông tin từ Input hiện tại ($x_t$) sẽ được đi vào Cell hiện tại. Để làm được việc này, nó sử dụng 2 hàm Sigmoid và Tanh. Tương tự như Forget Gate, thông tin từ Hidden State trước đó ($h_{t-1}$) và thông tin từ Input hiện tại ($x_t$) được đưa qua hàm Sigmoid, cho ra một giá trị trong khoảng từ 0 đến 1. Giá trị này càng gần 0 nghĩa là thông tin càng ít quan trọng , giá trị càng gần 1 nghĩa là thông tin càng quan trọng. Hàm Tanh tạo ra một vector ứng cử của Cell State hiện tại ($\\widetilde{C_t}$). Output của 2 hàm này được nhân với nhau tính toán Cell State hiện tại.  $i_t = \\sigma(W_i\\cdot[h_{t-1},x_t] + b_i) = \\sigma(W_{xi}X_t + W_{hi}h_{t-1} + b_i)$\n$\\widetilde{C_t} = tanh(W_c\\cdot[h_{t-1},x_t] + b_c) = tanh(X_tW_{xc} + h_{t-1}W_{hc} + b_c)$\n   Đến đây, ta đã có thể tính được Cell State hiện tại:\n Chú giả về các phép toán: $\\otimes$: Element-wise multiplication\n$\\oplus$: Element-wise addition\n Chúng ta nhân trạng thái của Cell trước đó với $f_t$ rồi cộng với $i_t * \\widetilde{C_t}$ $C_t = f_t * C_{t-1} + i_t * \\widetilde{C_t}$\n - Output gate: Cổng này quyết định thông tin nào sẽ được output ra ngoài ($h_t$). Vẫn vẫn giống như 2 cổng bên trên, thông tin từ Hidden State trước đó ($h_{t-1}$) và thông tin từ Input hiện tại ($x_t$) được đưa qua hàm Sigmoid, cho ra một giá trị trong khoảng từ 0 đến 1. Giá trị này càng gần 0 nghĩa là thông tin càng ít quan trọng , giá trị càng gần 1 nghĩa là thông tin càng quan trọng. Tiếp đó, Cell State được cho qua hàm Tanh để đưa giá trị của Cell State về miền [-1,1].  Output của Cell được tính như sau: $o_t = \\sigma(W_o\\cdot[h_{t-1},x_t] + b_o) = \\sigma(W_{xo}X_t + W_{ho}h_{t-1} + b_o)$\n$h_t = o_t * tanh(C_t)$\n 4. Kết luận\nTrong bài này, chúng ta đã tóm tắt sơ lược về RNN \u0026amp; LSTM. Mặc dù LSTM tiên tiến hơn RNN nhưng nó vẫn còn tồn tại một số nhược điểm.\nTrong bài tiếp theo, chúng ta sẽ cùng tìm hiểu về cơ chế Attention và kiến trúc Transformer. Mời các bạn đón đọc.\n5. Tham khảo\n stanford colah aditi-mittal d2l.ai  ","permalink":"https://tiensu.github.io/blog/57_rnn_summary/","tags":["RNN","LSTM"],"title":"Tóm tắt về RNN \u0026 LSTM"},{"categories":["MLOps"],"contents":"Nếu bạn là người theo nghiệp Code được vài năm, bạn chắc đã từng trải qua cảm giác bực bội, khó chịu khi phải đọc code của một dự án có từ trước đó. Nó được viết một cách cẩu thả, tùy tiện, không theo một quy định hay tổ chức nào cả. Người ta gọi đó là Source Code không có tính Maintainance. Là một Coder có trách nhiệm, có đạo đức, chúng ta nên viết code làm sao cho người đến sau, khi đọc code của bạn có thể nhanh chóng hiểu được vấn đề. \u0026ldquo;Người đến sau\u0026rdquo; ở đây có thể là chính bạn, vì sau một thời gian thì có khi chính bạn cũng quên hết những gì mình từng viết. Việc viết code một cách cẩn thận còn làm giảm rủi ro phát sinh các lỗi tiềm tàng trong quá trình sử dụng về sau này.\nBất kỳ dự án phần mềm nào cũng đều phải có quy định về cách tổ chức Source Code trong dự án, ngay từ khi bắt đầu dự án. Các dự án về AI cũng không ngoại lệ. Trong bài này, chúng ta sẽ cùng xem xét một cách tổ chức Source Code cho các dự án AI. Và nếu bạn thấy phù hợp thì có thể áp dụng cho các dự án của bạn.\n1. Project Structure\nMột dự án được coi là tổ chức tốt khi nó có tính module. Tức là mỗi chức năng của dự án được tách riêng ra thành các thành phần khác nhau. Đó gọi là nguyên lý Separation of Concerns. Theo cách này, chúng ta có thể dễ dàng chỉnh sửa, nâng cấp, bổ sung mỗi chức năng mà không ảnh hưởng quá nhiều đến các chức năng khác. Hơn thế nữa, nó còn làm tăng khả năng tái sử dụng code hiệu quả, hạn chế việc code bị trùng lặp không cần thiết. Từ đó, toàn bộ Source Code của dự án sẽ gọn gàng, sáng sủa, dễ nâng cấp, hạn chế lỗi, \u0026hellip; hơn rất nhiều.\nĐối với các dự án về AI, một cách tổ chức Source Code dự án có thể như sau:  Trước hết, chúng ta cần phân biệt module và package. Module chỉ đơn giản là các file chứa Python code và chúng có thể được Import lẫn nhau. Trong khi đó, Package là một thư mục chứa nhiều Modules hoặc các Sub-packages. Để có thể Import được Package, cần phải có một file init.py trong Package đó. File này có thể rỗng.\nNhư vậy, trong cách tổ chức này, chúng ta có 8 Packages khác nhau:\n configs: Chứa các thông tin cấu hình của hệ thống, có thể thay đổi trong quá trình phát triển và sử dụng. Ví dụ: Các Hyper-parameters, các đường dẫn đến Dataset, Model, các thông số trong kiến trúc của Model, các thông số để huấn luyện Model, \u0026hellip; Một cấu hình ví dụ đơn giản như sau:  CFG = { \u0026#34;data\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;oxford_iiit_pet:3.*.*\u0026#34;, \u0026#34;image_size\u0026#34;: 128, \u0026#34;load_with_info\u0026#34;: True }, \u0026#34;train\u0026#34;: { \u0026#34;batch_size\u0026#34;: 64, \u0026#34;buffer_size\u0026#34;: 1000, \u0026#34;epoches\u0026#34;: 20, \u0026#34;val_subsplits\u0026#34;: 5, \u0026#34;optimizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;adam\u0026#34; }, \u0026#34;metrics\u0026#34;: [\u0026#34;accuracy\u0026#34;] }, \u0026#34;model\u0026#34;: { \u0026#34;input\u0026#34;: [128, 128, 3], \u0026#34;up_stack\u0026#34;: { \u0026#34;layer_1\u0026#34;: 512, \u0026#34;layer_2\u0026#34;: 256, \u0026#34;layer_3\u0026#34;: 128, \u0026#34;layer_4\u0026#34;: 64, \u0026#34;kernels\u0026#34;: 3 }, \u0026#34;output\u0026#34;: 3 } }  dataloader: Chứa code liên quan đến việc data loading và data preprocessing. evaluation: Chứa code thực hiện việc đánh giá hiệu năng và độ chính xác của model. executor: Chứa code (function, script) để huấn luyện model, hoặc sử dụng model để dự đoán. Trong Package này thường có file main.py. model: Chứa code định nghĩa kiến trúc của model. notebooks: Chứa tất cả các Jupyter/Colab Notebook của dự án (nếu có). ops: Chứa code liên quan đến các hoạt động kiểu như algebraic transformations, image manipulation techniques hay graph operations. Package này có thể có hoặc không. utils: Chứa các common functions, có thể sử dụng ở nhiều nơi trong các Packages khác. Những gì mà không nằm trong số các Packages kể trên thì cũng có thể đưa vào Package này.  2. Object Oriented Programming (OOP)\nLập trình hướng đối tượng (OOP) thường không được sử dụng nhiều trong Python giống như Java hay C#. Có lẽ là bởi vì Python là một ngôn ngữ lập trình kiểu Script. Tức là cứ viết là chạy, không cần phải có hàm, phải khai báo, bla bla.\nTuy nhiên, Python cũng hỗ trợ lập trình theo kiểu OOP. Và trong một dự án lớn thì sử dụng OOP cho Python mang lại hiệu quả rất tích cực, giống như Java hay C#.\n2.1 Tính đóng gói (encapsolution)\nVí dụ, ta viết một Class tên là Unet như sau:\nclass UNet(): def __init__(self, config): self.base_model = tf.keras.applications.MobileNetV2( input_shape=self.config.model.input, include_top=False) self.batch_size = self.config.train.batch_size . . . def load_data(self): \u0026#34;\u0026#34;\u0026#34;Loads and Preprocess data \u0026#34;\u0026#34;\u0026#34; self.dataset, self.info = DataLoader().load_data(self.config.data) self._preprocess_data() def _preprocess_data(self): . . . def _set_training_parameters(self): . . . def _normalize(self, input_image, input_mask): . . . def _load_image_train(self, datapoint): . . . def _load_image_test(self, datapoint): . . . def build(self): \u0026#34;\u0026#34;\u0026#34; Builds the Keras model based \u0026#34;\u0026#34;\u0026#34; layer_names = [ \u0026#39;block_1_expand_relu\u0026#39;, # 64x64 \u0026#39;block_3_expand_relu\u0026#39;, # 32x32 \u0026#39;block_6_expand_relu\u0026#39;, # 16x16 \u0026#39;block_13_expand_relu\u0026#39;, # 8x8 \u0026#39;block_16_project\u0026#39;, # 4x4 ] layers = [self.base_model.get_layer(name).output for name in layer_names] . . . self.model = tf.keras.Model(inputs=inputs, outputs=x) def train(self): . . . def evaluate(self): . . . Bạn có thể nhìn thấy rằng mỗi chức năng được đóng gói bên trong một phương thức riêng biệt, và các thuộc tính được khai báo như là các instance variables. Cách viết như thế này rõ ràng là giúp code trở nên sáng sủa, dễ mở rộng và bảo trì. Đây chính là tính đóng gói (encapsolution) của OOP. Có một điều mình nhận thấy là OOP trong Python không thiết lập phạm vi truy cập các thuộc tính và phương thức trong một Class. Tất cả chúng đều là public, tức có thể truy cập từ mọi nơi. TUy nhiên, để bắt chước cho giống với OOP chuẩn, khi code Python, chúng ta thường quy ước như sau:\n Để khai báo phạm vi truy cập thành phần trong Class là public thì tên của chúng phải bắt đầu bằng 1 chữ cái. Để khai báo phạm vi truy cập thành phần trong Class là protected thì tên của chúng phải bắt đầu bằng ký tự \u0026ldquo;_\u0026rdquo;. Để khai báo phạm vi truy cập thành phần trong Class là public thì tên của chúng phải bắt đầu bằng 2 ký tự \u0026ldquo;_\u0026rdquo; (tức là __).  2.2 Tính thừa kế, trừa tượng và đa hình\nNgoài tính Encapsolution, OOP có 3 tính chất quan trọng nưã là tính kế thừa(inheritance), tính trừu tượng(abstraction) và tính đa hình(polymorphism).\nGiả sử, khi bắt đầu viết code cho dự án, chúng ta mới chỉ hình dung sơ bộ là chắc chắn sẽ cần những phương thức này, nhưng cách thực hiện phương thức sẽ khác nhau tùy vào giải pháp lựa chọn. Khi đó, chúng ta sẽ sử dụng tính chất abstraction để viết ra một Class kiểu như sau:\nclass BaseModel(ABC): \u0026#34;\u0026#34;\u0026#34;Abstract Model class that is inherited to all models\u0026#34;\u0026#34;\u0026#34; def __init__(self, cfg): self.config = Config.from_json(cfg) @abstractmethod def load_data(self): pass @abstractmethod def build(self): pass @abstractmethod def train(self): pass @abstractmethod def evaluate(self): pass Tất cả các phương thức trong class này đều chưa được chi tiết các làm việc (không có body), chúng ta mới chỉ khai báo chúng.\nTiếp theo, với mỗi một giải pháp đưa ra để thử nghiệm, chúng ta sẽ viết chúng thành một Class riêng, kế thừa lại abstract class này. Các inheritance class (Child Class) sẽ chi tiết cách làm việc của các phương thức trong abstract class (Parent Class). Tất nhiên, Child Class cũng sẽ có thêm các phương thức của riêng nó.\nVí dụ dưới đây, Class Unet sẽ kế thừa Class BaseModel:\nclass UNet(BaseModel): def __init__(self, config): super().__init__(config) self.base_model = tf.keras.applications.MobileNetV2(input_shape=self.config.model.input, include_top=False) . . . def load_data(self): self.dataset, self.info = DataLoader().load_data(self.config.data ) self._preprocess_data() . . . def build(self): . . . self.model = tf.keras.Model(inputs=inputs, outputs=x) def train(self): self.model.compile(optimizer=self.config.train.optimizer.type, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=self.config.train.metrics) model_history = self.model.fit(self.train_dataset, epochs=self.epoches, steps_per_epoch=self.steps_per_epoch, validation_steps=self.validation_steps, validation_data=self.test_dataset) return model_history.history[\u0026#39;loss\u0026#39;], model_history.history[\u0026#39;val_loss\u0026#39;] def evaluate(self): predictions = [] for image, mask in self.dataset.take(1): predictions.append(self.model.predict(image)) return predictions Chú ý là ở trong Child Class, bên trong hàm tạo init, chúng ta phải gọi lệnh super().init() để thực hiện hàm tạo của Parent Class (super() là đại diện của Parent Class).\nViệc có nhiều Child Classes kế thừa cùng một Parent Class, mỗi Child Class lại thực hiện các phương thức của Parent Class theo các cách khác nhau, chính là tính đa hình của OOP.\n2.3 Instance Mothod, Class Method và Static Method\nInstancemethod, classmethod và staticmethod cũng là các đặc tính thú vị của OOP. Hãy tìm hiểu thêm về nó một chút. Hãy xem xét ví dụ sau để hiểu rõ hơn về các loại phương thức này:\n\u0026gt;\u0026gt;\u0026gt; class Dataset(): ... def im_load_data(self, x): ... print(\u0026#34;executing im_load_data(%s, %s)\u0026#34; % (self, x)) ... @classmethod ... def cm_load_data(cls, x): ... print(\u0026#34;executing cm_load_data(%s, %s)\u0026#34; % (cls, x)) ... @staticmethod ... def sm_load_data(x): ... print(\u0026#34;executing sm_load_data(%s)\u0026#34; % x) ... \u0026gt;\u0026gt;\u0026gt; dataset = Dataset() Dưới đây là cách đơn giản nhất để thực thi một phương thức:\n\u0026gt;\u0026gt;\u0026gt; dataset.im_load_data(\u0026#39;args\u0026#39;) executing im_load_data(\u0026lt;__main__.Dataset object at 0x7ff97a1bd280\u0026gt;, args) Đây là một Instance Method, là phương thức phổ biến nhất. Một đối tượng (instance của class) được ngầm truyền thành tham số thứ nhất (self) của phương thức này. Vì vậy, mặc dù im_load_data cần hai tham số, nhưng dataset.im_load_data chỉ cần một tham số thôi. Và dataset.im_load_data không còn là hàm nguyên gốc ban đầu mà là một phiên bản đã được \u0026ldquo;bind\u0026rdquo; cho dataset:\n\u0026gt;\u0026gt;\u0026gt; dataset.im_load_data \u0026lt;bound method Dataset.im_load_data of \u0026lt;__main__.Dataset object at 0x7ff97a1bd280\u0026gt;\u0026gt; Class Method là phương thức thuộc về cả Class. Khi thực thi, nó không dùng đến bất cứ một Instance nào của Class đó. Thay vào đó, cả Class sẽ được truyền thành tham số thứ nhất (cls) của phương thức này, tương tự như Instance Class.\n\u0026gt;\u0026gt;\u0026gt; Dataset.cm_load_data(\u0026#39;args\u0026#39;) executing cm_load_data(\u0026lt;class \u0026#39;__main__.Dataset\u0026#39;\u0026gt;, args) \u0026gt;\u0026gt;\u0026gt; Dataset.cm_load_data \u0026lt;bound method Dataset.cm_load_data of \u0026lt;class \u0026#39;__main__.Dataset\u0026#39;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; dataset.cm_load_data \u0026lt;bound method Dataset.cm_load_data of \u0026lt;class \u0026#39;__main__.Dataset\u0026#39;\u0026gt;\u0026gt; Một điều thú vị là Class Method cũng có thể gọi từ instance mà không gặp trở ngại gì (nhiều ngôn ngữ vẫn cho làm điều này kèm theo vài warning).\n\u0026gt;\u0026gt;\u0026gt; dataset.cm_load_data(\u0026#39;args\u0026#39;) executing cm_load_data(\u0026lt;class \u0026#39;__main__.Dataset\u0026#39;\u0026gt;, args) Static Method là một phương thức đặc biệt, nó không sử dụng bất cứ thứ gì liên quan đến Class hay Instance của Class đó. Cả Self hay Cls đều không xuất hiện trong tham số của loại phương thức này. Và Static Method hoạt động không khác gì một hàm thông thường.\n\u0026gt;\u0026gt;\u0026gt; dataset.sm_load_data(\u0026#39;args\u0026#39;) executing sm_load_data(args) \u0026gt;\u0026gt;\u0026gt; Dataset.sm_load_data(\u0026#39;args\u0026#39;) executing sm_load_data(args) Đối với Static Method, dù gọi dataset.sm_load_data hay Dataset.sm_load_data thì kết quả trả về vẫn là hàm ban đầu không hề được \u0026ldquo;bind\u0026rdquo; bất cứ một đối tượng nào:\n\u0026gt;\u0026gt;\u0026gt; dataset.sm_load_data \u0026lt;function Dataset.sm_load_data at 0x7ff97a093160\u0026gt; \u0026gt;\u0026gt;\u0026gt; Dataset.sm_load_data \u0026lt;function Dataset.sm_load_data at 0x7ff97a093160\u0026gt; Static method, với sự đặc biệt của nó, được dùng rất hạn chế. Bởi vì nó không giống như instance method hay class method, nó không có bất cứ sự liên quan tới đối tượng gọi nó. Static method không phải là phương thức được sử dụng thường xuyên.\nTuy nhiên, nó vẫn tồn tại là có lý do của nó. Static method thường dùng để nhóm các hàm tiện ích lại với nhau (trong cùng một class). Nó không yêu cầu gì từ class đó, ngoại trừ việc tham chiếu khi được gọi.\nNhưng hàm như vậy không cho vào class nào cũng không vấn đề gì. Nhưng nhóm chúng trong class và gọi chúng thông quan instance hoặc class sẽ giúp chúng ta hiểu hơn về bối cảnh cũng như chức năng của chúng.\n3. Documentation\nHay được hiểu là code phải có comments. Tác dụng của việc Comments code là không phải bàn cãi. Có 2 cách Comment code:\n Comment bằng những lời giải thích tường mình Comment bằng chính cách đặt tên hàm, tên biến, \u0026hellip;  Hãy xem một ví dụ về Code không có Comment:\ndef n(self, ii, im): ii = tf.cast(ii, tf.float32) / 255.0 im -= 1 return ii, im Đọc đoạn code trên, bạn có thấy khó hiểu không?\nBây giờ hãy thêm Comment vào cho nó:\ndef _normalize(self, input_image, input_mask): \u0026#34;\u0026#34;\u0026#34; Normalise input image Args: input_image (tf.image): The input image input_mask (int): The image mask Returns: input_image (tf.image): The normalized input image input_mask (int): The new image mask \u0026#34;\u0026#34;\u0026#34; input_image = tf.cast(input_image, tf.float32) / 255.0 input_mask -= 1 return input_image, input_mask Vẫn cùng 1 đoạn Code, nhưng sau khi thêm Comment, người đọc dễ dàng hiểu được mục đích của nó.\nCách Comment như trên trong Python gọi là docstrings. Chúng ta luôn luôn nên thêm Docstrings trước mỗi File, hàm, Module để chỉ ra mục đích của chúng. Có nhiều cách để định dạng Docstrings. Cách như trong ví dụ này là Style của Google, trong đó:\n Dòng đầu tiên chỉ ra mục đích của đoạn code Tiếp theo là giải thích các tham số truyền vào Cuối cùng là giải thích kết quả trả về là gì?  Hai phần sau có thể bỏ qua nếu chúng không cung cấp nhiều giá trị.\n4. Kết luận\nTrong bài này, chúng ta đã khám phá một vài Best Practices khi Code cho dự án về AI, từ việc tổ chức dự án, sử dụng OOP, đến việc Comment trong Code. Theo cách đó, Code của chúng ta sẽ trở nên ràng mạch, rõ ràng, dễ đọc hiểu, dễ bảo trì, mở rộng và hạn chế phát sinh các lỗi tiềm ẩn trong quá trình vận hành.\nCác bạn có thể tham khảo cấu trúc dự án mẫu tại đây.\nTrong bài tiếp theo, chúng ta sẽ cùng thảo luận về cách thức xây dựng và tổ chức Source Code trong các dự án về AI. Mời các bạn đón đọc.\n5. Tham khảo\n Theaisummer Learnpython Realpython Datacamp  ","permalink":"https://tiensu.github.io/blog/56_ai_code_structure/","tags":["MLOps"],"title":"Tổ chức code trong dự án AI"},{"categories":["Deep Learning","Siamese Network","One-shot Learning"],"contents":"Ngày nay, AI đã len lỏi vào mọi lĩnh vực của đời sống xã hội: y tế, giáo dục, giao thông, \u0026hellip; Có thể nói không ngoa rằng hầu như mọi bài toán AI đều có thể giải quyết được thông qua Neural Network (NN). Tuy nhiên, các NNs luôn đòi hỏi lượng lớn dữ liệu để huấn luyện chúng. Trong thực tế, có một số bài toán mà việc thu thập đủ dữ liệu là một nhiệm vụ bất khả thi, ví dụ như bài toán Face Recognition, Signature Verification, \u0026hellip; Siamese Network (SN) hay Siamese Neural Network (SNN) ra đời để giải quyết tốt hơn những bài toán dạng như thế này.\nSNN chỉ sử dụng một số lượng hình ảnh rất nhỏ (vài ảnh) để có được những dự đoán tốt hơn nhiều so với mạng NN truyền thống. Vì thế nó còn được gọi với 1 số cái tên như One-shot Learning, Few-shot Learning, \u0026hellip; Khả năng học hỏi từ rất ít dữ liệu đã khiến cho SNN trở nên phổ biến hơn trong những năm gần đây. Trong bài viết này, chúng ta sẽ tìm hiểu nó là gì và cách phát triển hệ thống Signature Verification với Pytorch bằng cách sử dụng SNN.\n1. Giới thiệu Siamese Neural Network\nSiamese Neural Network (SNN) là một kiến trúc mạng nơ-ron chứa hai hoặc nhiều mạng con giống hệt nhau. \u0026ldquo;Giống hệt nhau\u0026rdquo; ở đây có nghĩa là, chúng có cùng cấu hình với cùng thông số và trọng số. Việc cập nhật các thông số được phản ánh đồng thời trên cả hai mạng con của nó.\nSNN được sử dụng để tìm sự giống nhau của các dữ liệu đầu (Input Data) vào bằng cách so sánh các vectơ đặc trưng của chúng. Một số ứng dụng phổ biến của SNN có thể kể đến như là: Face Verification, Signature Verification, Image Seaching System, \u0026hellip;\nThông thường, một mạng nơ-ron học cách để dự đoán các lớp của một bài toán. Nếu muốn thêm hay bớt các lớp mới, chúng ta phải cập nhật (huấn luyện) lại mạng nơ-ron trên toàn bộ tập dữ liệu (cả dữ liệu mới và cũ). Ngoài ra, các mạng nơ-ron sâu cần một khối lượng lớn dữ liệu để có thể huấn luyện chúng. SNN, theo một cách khác, học cách tìm ra sự giống nhau giữa các Input Data. Vì vậy, nó cho phép chúng ta phân loại các lớp dữ liệu mới mà không cần huấn luyện lại mạng nơ-ron.  Luồng làm việc của SNN như sau:\n Chọn một cặp Input Data (trong phạm vi bài này là ảnh) được chọn từ dataset. Đưa mỗi ảnh qua mỗi Sub-network của SNN để xử lý. Output của các Sub-networks là một Embedding vector. Tính toán khoảng cách Euclidean giữa 2 Embedding vectors đó. Một Sigmoid Function có thể được áp trên khoảng cách để đưa ra giá trị Score trong đoạn [0,1], thể hiện mức độ giống nhau giữa 2 Embedding vectors. Score càng gần 1 thì 2 vectors càng giống nhau và ngược lại.   2. Ưu điểm của SNN\nSNN có một số ưu điểm nổi bật như sau:\n  Lượng dữ liệu cần thiết để huấn luyện SNN là rất ít. Chỉ cần vài Samples là đủ (1-5 samples) huấn luyện SNN. Phương pháp mà nó sử dụng ở đây là One-Shot Learning hoặc Few-Shot Learning. Chính vì cần ít dữ liệu huấn luyện như vậy nên chúng ta cũng không lo lắng việc dữ liệu bị mất cân bằng (Image Imbalance).\n  Khả năng kết hợp với các bộ phân loại khác cao. Do cơ chế học của SNN khác biệt với các bộ phân lớp thông thường khác, nên chúng ta hoàn toàn có thể kết hợp chúng lại với nhau. Việc làm này thường cho ra kết quả tốt hơn.\n  Học từ sự tương đồng về ngữ nghĩa: SNN tập trung vào việc học các Features ở các lớp sâu hơn, nơi mà các Features giống nhau được đặt gần nhau. Do đó, nó có thể hiểu được phần nào sự tương đồng về ngữ nghĩa của các Input Data.\n  3. Nhược điểm của SNN\nSNN cũng có những nhược điểm sau:\n  Thời gian huấn luyện lâu hơn. SNN học theo từng cặp đôi một với nhau nên khả năng học của nó chậm hơn các NN khác.\n  Không thể hiện xác suất mỗi lớp trong Output. SNN chỉ đưa đưa 1 giá trị Score trong đoạn [0,1], thể hiện sự giống nhau giữa 2 Input Data. Score càng gần 1 thì 2 Input Data càng giống nhau và ngược lại.\n  4. Loss Function của SNN\n Bởi vì, SNN học theo kiểu từng đôi một của Input Data nên Cross Entropy Loss Function thường không được sử dụng. Thay vào đó, 2 Loss Functions là Triple Loss và Contrastive Loss được sử dụng nhiều hơn.\n4.1 Triple Loss function\nÝ tưởng của Triple Loss là sử dụng bộ 3 Input Data bao gồm: Anchor (A), Positive (P) và Nagative (N) mà ở đó, khoảng cách từ A đến P được tối thiểu hóa, trong khi khoảng cách từ A đến N được tối đa hóa trong suốt quá trình huấn luyện model.\n$L(A,P,N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \\alpha,0)$\n Trong công thức trên,\n $\\alpha$ gọi là margin, được sử dụng để nhấn mạnh sự khác biệt hoặc sự tương đồng giữa các cặp Input Data. $f(A), f(P), f(N)$ là các vectors đặc trưng của các Input Data A, P, N, tương ứng.   4.2 Contrastive Loss\nÝ tưởng của Contrastive Loss cũng tương tự như Triplet Loss, sự khác nhau ở chỗ Contrastive Loss chỉ sử dụng 1 cặp Input Data, hoặc là cùng loại, hoặc là khác loại. Nếu cùng loại thì khoảng cách giữa các vectors đặc trưng của chúng sẽ được tối thiểu hóa, còn nếu khác loại thì khoảng cách giữa các vectors đặc trưng của chúng sẽ được tối đa hóa trong suốt quá trình huấn luyện.\nCông thức của Contrastive Loss: $(1 - Y)\\frac{1}{2}(D_w)^2 + (Y)\\frac{1}{2}{max(0,m - D_w)}^2$\n Trong đó, $D_w$ là khoảng cách Euclidean: $\\sqrt{{G_w(X_1) - G_w(X_2)}^2}$\n $G_w$ là Ouput của SNN đối với 1 Input Data.\n Việc lựa chọn sử dụng Loss Function nào còn tùy thuộc vào bài toán cụ thể. Chưa có công bố nào kết luận cái nào tốt hơn cái nào. Bạn nên thử cả 2 loại để tìm ra cái tốt hơn cho bài toán của bạn.\n5. Signature Verification với Siamese Networks\n Trong phần này, chúng ta sẽ xây dựng một SNN model bằng Pytorch để thực hiện nhiệm vụ Signature Verification.\n5.1 Signature dataset\nDataset sử dụng trong bài này là ICDAR 2011. Nó chứa các chữ ký của những người dân ở Hà Lan, cả chữ ký thật và chữ ký giả. Nhãn của dữ liệu nằm trong file CSV tương ứng.  Chúng ta sẽ đoc vào dataset và chuẩn bị cho việc huấn luyện model:\n#preprocessing and loading the dataset class SiameseDataset(): def __init__(self,training_csv=None,training_dir=None,transform=None): # used to prepare the labels and images path self.train_df=pd.read_csv(training_csv) self.train_df.columns =[\u0026#34;image1\u0026#34;,\u0026#34;image2\u0026#34;,\u0026#34;label\u0026#34;] self.train_dir = training_dir self.transform = transform def __getitem__(self,index): # getting the image path image1_path=os.path.join(self.train_dir,self.train_df.iat[index,0]) image2_path=os.path.join(self.train_dir,self.train_df.iat[index,1]) # Loading the image img1 = Image.open(image1_path) img2 = Image.open(image2_path) img1 = img0.convert(\u0026#34;L\u0026#34;) img2 = img1.convert(\u0026#34;L\u0026#34;) # Apply image transformations if self.transform is not None: img1 = self.transform(img1) img2 = self.transform(img2) return img1, img2 , th.from_numpy(np.array([int(self.train_df.iat[index,2])],dtype=np.float32)) def __len__(self): return len(self.train_df) # Load the the dataset from raw image folders siamese_dataset = SiameseDataset(training_csv,training_dir, transform=transforms.Compose([transforms.Resize((105,105)), transforms.ToTensor() ]) ) # Load the dataset as pytorch tensors using dataloader train_dataloader = DataLoader( siamese_dataset, shuffle=True, num_workers=8, batch_size=config.batch_size ) 5.2 SNN model\nChúng ta sẽ tạo SNN model như sau:\n#create a siamese network class SiameseNetwork(nn.Module): def __init__(self): super(SiameseNetwork, self).__init__() # Setting up the Sequential of CNN Layers self.cnn = nn.Sequential( nn.Conv2d(1, 96, kernel_size=11,stride=1), nn.ReLU(inplace=True), nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2), nn.MaxPool2d(3, stride=2), nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2), nn.ReLU(inplace=True), nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2), nn.MaxPool2d(3, stride=2), nn.Dropout2d(p=0.3), nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1), nn.ReLU(inplace=True), nn.Conv2d(384,256 , kernel_size=3,stride=1,padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(3, stride=2), nn.Dropout2d(p=0.3), ) # Defining the fully connected layers self.fc = nn.Sequential( nn.Linear(30976, 1024), nn.ReLU(inplace=True), nn.Dropout2d(p=0.5), nn.Linear(1024, 128), nn.ReLU(inplace=True), nn.Linear(128,2)) def forward_once(self, x): # Forward pass  output = self.cnn(x) output = output.view(output.size()[0], -1) output = self.fc(output) return output def forward(self, input1, input2): # forward pass of input 1 output1 = self.forward_once(input1) # forward pass of input 2 output2 = self.forward_once(input2) return output1, output2 5.3 Loss Function\nTrong bài này, mình sẽ sử dụng Contrastive Loss. Code của nó trong Pytorch như sau:\nclass ContrastiveLoss(torch.nn.Module): \u0026#34;\u0026#34;\u0026#34; Contrastive loss function. \u0026#34;\u0026#34;\u0026#34; def __init__(self, margin=1.0): super(ContrastiveLoss, self).__init__() self.margin = margin def forward(self, x0, x1, y): # euclidian distance diff = x0 - x1 dist_sq = torch.sum(torch.pow(diff, 2), 1) dist = torch.sqrt(dist_sq) mdist = self.margin - dist dist = torch.clamp(mdist, min=0.0) loss = y * dist_sq + (1 - y) * torch.pow(dist, 2) loss = torch.sum(loss) / 2.0 / x0.size()[0] return loss 5.4 Train SNN model\nCác bước tiến hành huấn luyện SNN model như sau:\n Khởi tạo model, Loss Funtion, và Optimizer (bài này sử dụng Adam) Đưa từng cặp Images vào SNN network. Tính toán Loss từ Ouput của mỗi ảnh. Tính toán Gradients của model theo phương pháp Back Propagate. Cập nhật các tham số của model, sử dụng Optimizer. Lưu lại model.  # Declare Siamese Network net = SiameseNetwork().cuda() # Decalre Loss Function criterion = ContrastiveLoss() # Declare Optimizer optimizer = th.optim.Adam(net.parameters(), lr=1e-3, weight_decay=0.0005) #train the model def train(): loss=[] counter=[] iteration_number = 0 for epoch in range(1,config.epochs): for i, data in enumerate(train_dataloader,0): img0, img1 , label = data img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda() optimizer.zero_grad() output1,output2 = net(img0,img1) loss_contrastive = criterion(output1,output2,label) loss_contrastive.backward() optimizer.step() print(\u0026#34;Epoch {}\\nCurrent loss {}\\n\u0026#34;.format(epoch,loss_contrastive.item())) iteration_number += 10 counter.append(iteration_number) loss.append(loss_contrastive.item()) show_plot(counter, loss) return net #set the device to cuda device = torch.device(\u0026#39;cuda\u0026#39; if th.cuda.is_available() else \u0026#39;cpu\u0026#39;) model = train() torch.save(model.state_dict(), \u0026#34;output/model.pt\u0026#34;) print(\u0026#34;Model Saved Successfully\u0026#34;) Kết quả huấn luyện sau 20 epochs:  Giá trị của Loss vẫn còn dao động, có lẽ chúng ta phải Tuning model nhiều hơn. Trong bài này, mình ko đi chi tiết phần đó.\n5.5 Test SNN model\nChúng ta sẽ thực hiện các bước sau để kiểm tra SNN model vừa mới huấn luyện:\n Đọc vào Test dataset, sử dụng lớp Dataloader của Pytorch Đưa cặp Image và nhãn tương ứng đi qua SNN model Tìm khoảng cách Euclidean giữa 2 Output Hiển thị kết quả  # Load the test dataset test_dataset = SiameseDataset(training_csv=testing_csv,training_dir=testing_dir, transform=transforms.Compose([transforms.Resize((105,105)), transforms.ToTensor() ]) ) test_dataloader = DataLoader(test_dataset,num_workers=6,batch_size=1,shuffle=True) #test the network count=0 for i, data in enumerate(test_dataloader,0): x0, x1 , label = data concat = torch.cat((x0,x1),0) output1,output2 = model(x0.to(device),x1.to(device)) eucledian_distance = F.pairwise_distance(output1, output2) if label==torch.FloatTensor([[0]]): label=\u0026#34;Original Pair Of Signature\u0026#34; else: label=\u0026#34;Forged Pair Of Signature\u0026#34; imshow(torchvision.utils.make_grid(concat)) print(\u0026#34;Predicted Eucledian Distance:-\u0026#34;,eucledian_distance.item()) print(\u0026#34;Actual Label:-\u0026#34;,label) count=count+1 if count ==10: break Kết quả:\nPredicted Eucledian Distance:- 1.2930774688720703 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.6725202798843384 Actual Label:- Original Pair Of Signature Predicted Eucledian Distance:- 0.8823959827423096 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.9346675276756287 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.25577670335769653 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.7937518358230591 Actual Label:- Forged Pair Of Signature Predicted Eucledian Distance:- 0.7733522057533264 Actual Label:- Original Pair Of Signature Predicted Eucledian Distance:- 0.7810924649238586 Actual Label:- Original Pair Of Signature Predicted Eucledian Distance:- 1.2326889038085938 Actual Label:- Original Pair Of Signature Predicted Eucledian Distance:- 0.6290231347084045 Actual Label:- Original Pair Of Signature Một số hình ảnh:  6. Kết luận\nTrong bài này, chúng ta đã cùng tìm hiểu về Siamese Neural Network, đồng thời xây dựng một SNN đơn giản để giải quyết bài toán Signature Verification.\nToàn bộ source code của bài này, các bạn có thể tham khảo tại đây\nTrong bài tiếp theo, chúng ta sẽ cùng thảo luận về cách thức xây dựng và tổ chức Source Code trong các dự án về AI. Mời các bạn đón đọc.\n7. Tham khảo\n Towardsdatascience Hackernoon  ","permalink":"https://tiensu.github.io/blog/55_siamese_network/","tags":["Deep Learning","Siamese Network","One-shot Learning"],"title":"Tìm hiểu Siamese Neural Network"},{"categories":["Deep Learning","Face Recognition"],"contents":"1. Nhắc lại bài toán Face Recognition\nFace Recognition là bài toán nhận diện người dựa vào khuôn mặt của họ trong hình ảnh hoặc video. Hai trong số các bài toán của Face Recognition là:\n Face Verification: Ánh xạ 1-1, giữa khuôn mặt của một người đưa vào hệ thống nhận diện với một người đã biết trước. Face Verification trả lời câu hỏi: Đây có phải là anh/chị/ông/bà A không? Face Identification: Ánh xạ 1-nhiều, giữa giữa khuôn mặt của một người đưa vào hệ thống nhận diện với một tập những người đã biết trước trong CSDL. Face Identification trả lời câu hỏi: Đây là ai?  Trong bài này, chúng ta sẽ sử dụng FaceNet model để thực hiện bài toán Face Recognition.\n2. FaceNet model.\nFaceNet là một mô hình nhận dạng khuôn mặt được phát triển bởi Florian Schroff và đồng nghiệp tại Google trong bài báo năm 2015 của họ có tiêu đề FaceNet: A Unified Embedding for Face Recognition and Clustering. Trong mô hình này, hình ảnh của một khuôn mặt sẽ được trích xuất các đặc điểm chất lượng cao và biểu diễn thành vector 128 phần tử (Face Embedding vector 128 chiều).\nFaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity — FaceNet: A Unified Embedding for Face Recognition and Clustering, 2015.\nMô hình là một mạng CNN được đào tạo thông qua hàm Triplet Loss. Nó khuyến khích các Face Embedding vector của cùng một người trở nên giống nhau hơn (khoảng cách nhỏ hơn), trong khi các Face Embedding vectơ của những người khác nhau sẽ trở nên ít giống nhau hơn (khoảng cách lớn hơn). Việc tập trung vào đào tạo một mô hình để tạo ra các Face Embedding vector trực tiếp (thay vì trích xuất chúng từ một lớp trung gian của mô hình) là một đổi mới quan trọng của FaceNet so với VGGFace.\nOur method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. — FaceNet: A Unified Embedding for Face Recognition and Clustering, 2015.\nCác Face Embedding vectors này sau đó được sử dụng như là Input để đào tạo một mô hình phân loại trên bộ dữ liệu tiêu chuẩn về Face Recognition, đạt được kết quả state-of-the-art.\nBài báo cũng đề cập đến các ứng dụng khác của Face Embedding vector, chẳng hạn như phân cụm để nhóm các khuôn mặt giống nhau dựa trên các đặc điểm được trích xuất của chúng. Có thể nó FaceNet là một model rất mạnh mẽ và hiệu quả cho bài toán Face Recognition, và một loạt các ứng dụng khác.\n3. Load a FaceNet Model in Keras\nCó một số dự án cung cấp các công cụ để huấn luyện các mô hình dựa trên FaceNet (sử dụng Pre-trained FaceNet model). Có lẽ nổi bật nhất là dự án OpenFace. Trong dự án náy, các mô hình FaceNet được xây dựng và huấn luyện bằng PyTorch framework. Keras cũng có 1 dự án tương tự, gọi là Keras OpenFace, nhưng tại thời điểm viết bài, các mô hình đó yêu cầu Python 2, điều này hạn chế chúng ta rất nhiều trong việc tiếp cận các tính ưng ưu việt của Python 3. Vì thế mà Keras OpenFace ít được sử dụng.\nMột dự án nổi bật khác là facenet của David Sandberg. Nó cung cấp các mô hình FaceNet được xây dựng và huấn luyện bằng TensorFlow framework. Dự án có vẻ hoàn thiện, mặc dù tại thời điểm viết bài này không cung cấp cài đặt cũng như cách sử dụng API của nó. Một dự án có vẻ hữu ích hơn là Keras FaceNet của Hiroki Taniai. Trong dự án này, tác giả cung cấp một tập lệnh để chuyển đổi mô hình Inception ResNet v1 từ TensorFlow sang Keras. Một Pre-trained model bằng Keras cũng được cung cấp để sẵn sàng để sử dụng. Chúng ta sẽ sử dụng mô hình Keras FaceNet của Hiroki Taniai trong bài viết này. Nó đã được huấn luyện trên tập dữ liệu MS-Celeb-1M. Mô hình có thể được tải xuống từ đây.\n4. Detect Faces\nPhần này, chúng ta cũng sử dụng thư viện MTCNN tương tự như trong bài trước. Code như sau:\n# function for face detection with mtcnn from PIL import Image from numpy import asarray from mtcnn.mtcnn import MTCNN # extract a single face from a given photograph def extract_face(filename, required_size=(160, 160)): # load image from file image = Image.open(filename) # convert to RGB, if needed image = image.convert(\u0026#39;RGB\u0026#39;) # convert to array pixels = asarray(image) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] # bug fix x1, y1 = abs(x1), abs(y1) x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # load the photo and extract the face pixels = extract_face(\u0026#39;...\u0026#39;) Chúng ta sẽ sử dụng hàm này để trích xuất các Face Embedding vector để cung cấp làm đầu vào cho mô hình FaceNet trong phần tiếp theo.\n5. Face Recognition\nTrong phần này, chúng ta sẽ phát triển một hệ thống Face Recongtion để dự đoán danh tính của một khuôn mặt. Model sẽ được huấn luyện và kiểm tra bằng cách sử dụng bộ dữ liệu 5 Celebrity Faces Dataset, bao gồm rất nhiều bức ảnh của năm người nổi tiếng khác nhau. Mô hình MTCNN vẫn được dùng để thực hiện Face Detection, mô hình FaceNet được sử dụng để Face Embedding vector cho mỗi khuôn mặt được phát hiện, sau đó chúng ta sẽ phát triển một mô hình phân loại bằng thuật toán SVM để dự đoán danh tính của khuôn mặt đó.\n5.1 5 Celebrity Faces Dataset\n5 Celebrity Faces Dataset là một bộ dữ liệu nhỏ chứa các bức ảnh của những người nổi tiếng khác nhau. Nó bao gồm các bức ảnh của: Ben Affleck, Elton John, Jerry Seinfeld, Madonna và **. Bộ dữ liệu đã được cung cấp bởi Dan Becker, bạn có thể tải xuống miễn phí từ Kaggle. Lưu ý, cần phải có tài khoản Kaggle để tải xuống tập dữ liệu này.\nTải xuống tập dữ liệu, file data.zip 2.5MB và giải nén nó trong thư mục làm việc trên máy tính của bạn với tên thư mục 5-Celeb-face-dataset. Bây giờ bạn sẽ có một thư mục với cấu trúc sau:\n5-celebrity-faces-dataset ├── train │ ├── ben_afflek │ ├── elton_john │ ├── jerry_seinfeld │ ├── madonna │ └── mindy_kaling └── val ├── ben_afflek ├── elton_john ├── jerry_seinfeld ├── madonna └── mindy_kaling Chúng ta có thể thấy 2 thư mục: thư mục train chứa dữ liệu để huấn luyện model và thư mục val chứa dữ liệu để xác thực hoặc kiểm tra model. Nhìn vào một số bức ảnh trong mỗi thư mục, chúng ta có thể thấy rằng các khuôn mặt có nhiều hướng, ánh sáng và kích thước khác nhau. Điều quan trọng là mỗi bức ảnh chỉ có một khuôn mặt của người đó. Nếu bạn muốn sử dụng dữ liệu của riêng bạn, hãy thu thập và tổ chức dữ liệu tương tự như thế này.\n5.2 Detect Faces\nBước đầu tiên cần làm là phát hiện khuôn mặt trong mỗi bức ảnh. Ta sẽ sử dụng hàm extract_face() trong phần trước để làm việc này.\n# python extract_faces.py --dp \u0026#39;5-celebrity-faces-dataset/train/ben_afflek/\u0026#39; # demonstrate face detection on 5 Celebrity Faces Dataset from os import listdir from PIL import Image from numpy import asarray from matplotlib import pyplot from mtcnn.mtcnn import MTCNN import argparse import tensorflow as tf config_tf = tf.compat.v1.ConfigProto() config_tf.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config_tf) # extract a single face from a given photograph def extract_face(filename, required_size=(160, 160)): # load image from file image = Image.open(filename) # convert to RGB, if needed image = image.convert(\u0026#39;RGB\u0026#39;) # convert to array pixels = asarray(image) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] # bug fix x1, y1 = abs(x1), abs(y1) x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array def main(): ap = argparse.ArgumentParser() ap.add_argument(\u0026#39;-dp\u0026#39;, \u0026#39;--data_path\u0026#39;, required=True) args = vars(ap.parse_args()) i = 1 # enumerate files for filename in listdir(args[\u0026#39;data_path\u0026#39;]): # path path = args[\u0026#39;data_path\u0026#39;] + filename # get face face = extract_face(path) print(i, face.shape) # plot pyplot.subplot(2, 7, i) pyplot.axis(\u0026#39;off\u0026#39;) pyplot.imshow(face) i += 1 pyplot.show() if __name__ == \u0026#39;__main__\u0026#39;: main() Thử thực hiện với những ảnh của Ben Affleck và thể hiện các khuôn mặt được phát hiện lên đồ thị.\n$ python extract_faces.py --data_path \u0026#39;5-celebrity-faces-dataset/train/ben_afflek/\u0026#39; Kết quả:  Tiếp theo, chúng ta sẽ mở rộng ví dụ này để làm việc với toàn bộ ảnh trong các thư mục train và val. Ta viết hàm load_faces() như sau:\n# load images and extract faces for all images in a directory def load_faces(directory): faces = list() # enumerate files for filename in listdir(directory): # path path = directory + filename # get face face = extract_face(path) # store faces.append(face) return faces Tiếp theo là hàm load_dataset():\n# load a dataset that contains one subdir for each class that in turn contains images def load_dataset(directory): X, y = list(), list() # enumerate folders, on per class for subdir in listdir(directory): # path path = directory + subdir + \u0026#39;/\u0026#39; # skip any files that might be in the dir if not isdir(path): continue # load all faces in the subdirectory faces = load_faces(path) # create labels labels = [subdir for _ in range(len(faces))] # summarize progress print(\u0026#39;\u0026gt;loaded %dexamples for class: %s\u0026#39; % (len(faces), subdir)) # store X.extend(faces) y.extend(labels) return asarray(X), asarray(y) Việc còn lại là gọi hàm này với các thư mục train và val để tạo ra các Face Embedding của mỗi khuôn mặt và sử dụng chúng để tạo model phân loại.\nGộp tất cả lại, ta có code đầy đủ như sau:\n# USEAGE python extract_faces_dataset.py --train_data 5-celebrity-faces-dataset/train/ --val_data 5-celebrity-faces-dataset/val/ --save_data 5-celebrity-faces-dataset.npz # face detection for the 5 Celebrity Faces Dataset from os import listdir from os.path import isdir from PIL import Image from numpy import savez_compressed from numpy import asarray from mtcnn.mtcnn import MTCNN import argparse import tensorflow as tf config_tf = tf.compat.v1.ConfigProto() config_tf.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config_tf) # extract a single face from a given photograph def extract_face(filename, required_size=(160, 160)): # load image from file image = Image.open(filename) # convert to RGB, if needed image = image.convert(\u0026#39;RGB\u0026#39;) # convert to array pixels = asarray(image) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] # bug fix x1, y1 = abs(x1), abs(y1) x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # load images and extract faces for all images in a directory def load_faces(directory): faces = list() # enumerate files for filename in listdir(directory): # path path = directory + filename # get face face = extract_face(path) # store faces.append(face) return faces # load a dataset that contains one subdir for each class that in turn contains images def load_dataset(directory): X, y = list(), list() # enumerate folders, on per class for subdir in listdir(directory): # path path = directory + subdir + \u0026#39;/\u0026#39; # skip any files that might be in the dir if not isdir(path): continue # load all faces in the subdirectory faces = load_faces(path) # create labels labels = [subdir for _ in range(len(faces))] # summarize progress print(\u0026#39;\u0026gt;loaded %dexamples for class: %s\u0026#39; % (len(faces), subdir)) # store X.extend(faces) y.extend(labels) return asarray(X), asarray(y) def main(): ap = argparse.ArgumentParser() ap.add_argument(\u0026#39;-td\u0026#39;, \u0026#39;--train_data\u0026#39;, required=True) ap.add_argument(\u0026#39;-vd\u0026#39;, \u0026#39;--val_data\u0026#39;, required=True) ap.add_argument(\u0026#39;-sd\u0026#39;, \u0026#39;--save_data\u0026#39;, required=True) args = vars(ap.parse_args()) # load train dataset trainX, trainy = load_dataset(args[\u0026#39;train_data\u0026#39;]) print(trainX.shape, trainy.shape) # load test dataset testX, testy = load_dataset(args[\u0026#39;val_data\u0026#39;]) # save arrays to one file in compressed format savez_compressed(args[\u0026#39;save_data\u0026#39;], trainX, trainy, testX, testy) if __name__ == \u0026#39;__main__\u0026#39;: main() Chạy code trên:\n$ python extract_faces_dataset.py --train_data 5-celebrity-faces-dataset/train/ --val_data 5-celebrity-faces-dataset/val/ --save_data 5-celebrity-faces-dataset.npz Sẽ mất một chút thời gian để hoàn thành chương trình. Tất cả các khuôn mặt ở 2 tập train và test được lưu vào một tệp mảng NumPy nén có tên là 5-Celeb-face-dataset.npz có dung lượng khoảng 7.5MB và được lưu trữ trong thư mục làm việc hiện tại của chương trình.\n\u0026gt;loaded 17 examples for class: elton_john \u0026gt;loaded 19 examples for class: madonna \u0026gt;loaded 13 examples for class: ben_afflek \u0026gt;loaded 22 examples for class: mindy_kaling \u0026gt;loaded 21 examples for class: jerry_seinfeld (92, 160, 160, 3) (92,) \u0026gt;loaded 5 examples for class: elton_john \u0026gt;loaded 5 examples for class: madonna \u0026gt;loaded 5 examples for class: ben_afflek \u0026gt;loaded 5 examples for class: mindy_kaling \u0026gt;loaded 5 examples for class: jerry_seinfeld 5.3 Tạo Face Embedding vectors\nBước tiếp theo là tạo ra các Face Embedding vectors của mỗi khuôn mặt. Nhớ lại rằng Face Embedding là một vectơ đại diện cho các đặc điểm được trích xuất từ khuôn mặt. Các vectors này sau đó có thể được dùng để tính toán, so sánh với nhau, hoặc tạo ra một bộ phân loại để định danh cho từng khuôn mặt. Ở phần này, chúng ta sẽ dùng mô hình Pre-trained FaceNet để tạo ra các Face Embdding vectors đó.\nĐầu tiên, chúng ta sẽ đọc vào các khuôn mặt đã tạo ở bước trước.\n... # load the face dataset data = load(\u0026#39;5-celebrity-faces-dataset.npz\u0026#39;) trainX, trainy, testX, testy = data[\u0026#39;arr_0\u0026#39;], data[\u0026#39;arr_1\u0026#39;], data[\u0026#39;arr_2\u0026#39;], data[\u0026#39;arr_3\u0026#39;] print(\u0026#39;Loaded: \u0026#39;, trainX.shape, trainy.shape, testX.shape, testy.shape) Tiếp theo, đọc vào Pre-trained FaceNet model:\n... # load the facenet model model = load_model(\u0026#39;facenet_keras.h5\u0026#39;) print(\u0026#39;Loaded Model\u0026#39;) Chuẩn hóa dữ liệu đầu vào theo đúng kỳ vọng của Pre-trained FaceNet:\n... # scale pixel values face_pixels = face_pixels.astype(\u0026#39;float32\u0026#39;) # standardize pixel values across channels (global) mean, std = face_pixels.mean(), face_pixels.std() face_pixels = (face_pixels - mean) / std ... # transform face into one sample samples = expand_dims(face_pixels, axis=0) Tạo Face Embedding vectors:\n... # make prediction to get embedding yhat = model.predict(samples) # get embedding embedding = yhat[0] Viết chung lại thành hàm get_embedding() như bên dưới:\n# get the face embedding for one face def get_embedding(model, face_pixels): # scale pixel values face_pixels = face_pixels.astype(\u0026#39;float32\u0026#39;) # standardize pixel values across channels (global) mean, std = face_pixels.mean(), face_pixels.std() face_pixels = (face_pixels - mean) / std # transform face into one sample samples = expand_dims(face_pixels, axis=0) # make prediction to get embedding yhat = model.predict(samples) return yhat[0] Code đầy đủ từ lúc đọc vào ảnh khuôn mặt, đến khi lưu Face Embedding vector thành 1 file như sau:\n# USEAGE: # python predict_face_embeddings.py --face_dataset 5-celebrity-faces-dataset.npz --facenet_model facenet_keras.h5 --face_embedding 5-celebrity-faces-embeddings.npz # calculate a face embedding for each face in the dataset using facenet from numpy import load from numpy import expand_dims from numpy import asarray from numpy import savez_compressed from tensorflow.keras.models import load_model import argparse import tensorflow as tf tf.get_logger().setLevel(\u0026#39;ERROR\u0026#39;) config_tf = tf.compat.v1.ConfigProto() config_tf.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config_tf) # get the face embedding for one face def get_embedding(model, face_pixels): # scale pixel values face_pixels = face_pixels.astype(\u0026#39;float32\u0026#39;) # standardize pixel values across channels (global) mean, std = face_pixels.mean(), face_pixels.std() face_pixels = (face_pixels - mean) / std # transform face into one sample samples = expand_dims(face_pixels, axis=0) # make prediction to get embedding yhat = model.predict(samples) return yhat[0] def main(): # load the face dataset data = load(args[\u0026#39;face_dataset\u0026#39;]) trainX, trainy, testX, testy = data[\u0026#39;arr_0\u0026#39;], data[\u0026#39;arr_1\u0026#39;], data[\u0026#39;arr_2\u0026#39;], data[\u0026#39;arr_3\u0026#39;] print(\u0026#39;Loaded: \u0026#39;, trainX.shape, trainy.shape, testX.shape, testy.shape) # load the facenet model model = load_model(args[\u0026#39;facenet_model\u0026#39;]) print(\u0026#39;Loaded Model\u0026#39;) # convert each face in the train set to an embedding newTrainX = list() for face_pixels in trainX: embedding = get_embedding(model, face_pixels) newTrainX.append(embedding) newTrainX = asarray(newTrainX) print(newTrainX.shape) # convert each face in the test set to an embedding newTestX = list() for face_pixels in testX: embedding = get_embedding(model, face_pixels) newTestX.append(embedding) newTestX = asarray(newTestX) print(newTestX.shape) # save arrays to one file in compressed format savez_compressed(args[\u0026#39;face_embedding\u0026#39;], newTrainX, trainy, newTestX, testy) if __name__ == \u0026#39;__main__\u0026#39;: ap = argparse.ArgumentParser() ap.add_argument(\u0026#39;-fd\u0026#39;, \u0026#39;--face_dataset\u0026#39;, required=True) ap.add_argument(\u0026#39;-fnm\u0026#39;, \u0026#39;--facenet_model\u0026#39;, required=True) ap.add_argument(\u0026#39;-fe\u0026#39;, \u0026#39;--face_embedding\u0026#39;, required=True) args = vars(ap.parse_args()) main() Chạy code trên:\n$ python predict_face_embeddings.py --face_dataset 5-celebrity-faces-dataset.npz --facenet_model facenet_keras.h5 --face_embedding 5-celebrity-faces-embeddings.npz Kết quả, toàn bộ ảnh khuôn mặt đã được chuyển đổi thành các Face Embedding vectors 128 chiều. Tất cả được lưu lại thành file 5-celebrity-faces-embeddings.npz trong cùng thư mục làm việc.\nLoaded: (93, 160, 160, 3) (93,) (25, 160, 160, 3) (25,) Loaded Model (93, 128) (25, 128) 5.4 Thực hiện Face Classification\nTrong phần này, chúng ta sẽ phát triển một mô hình để phân loại các Face Embedding vectors thành 5 nhãn là tên của những người nổi tiếng trong bộ dữ liệu 5 Celebrity Faces Dataset. Đầu tiên, đọc vào Face Embedding vectors ở phần trước:\n... # load dataset data = load(\u0026#39;5-celebrity-faces-embeddings.npz\u0026#39;) trainX, trainy, testX, testy = data[\u0026#39;arr_0\u0026#39;], data[\u0026#39;arr_1\u0026#39;], data[\u0026#39;arr_2\u0026#39;], data[\u0026#39;arr_3\u0026#39;] print(\u0026#39;Dataset: train=%d, test=%d\u0026#39; % (trainX.shape[0], testX.shape[0])) Để train Classifier model, chúng ta cần chuẩn hóa dữ liệu:\n... # normalize input vectors in_encoder = Normalizer(norm=\u0026#39;l2\u0026#39;) trainX = in_encoder.transform(trainX) testX = in_encoder.transform(testX) .. # label encode targets out_encoder = LabelEncoder() out_encoder.fit(trainy) trainy = out_encoder.transform(trainy) testy = out_encoder.transform(testy) Tiếp theo, chúng ta sẽ tạo và huấn luyện một SVM model. Thuật toán này đã được chứng mình tính hiệu quả trong việc phân loại các Face Embedding vectors.\n... # fit model model = SVC(kernel=\u0026#39;linear\u0026#39;) model.fit(trainX, trainy) ... # predict yhat_train = model.predict(trainX) yhat_test = model.predict(testX) # score score_train = accuracy_score(trainy, yhat_train) score_test = accuracy_score(testy, yhat_test) # summarize print(\u0026#39;Accuracy: train=%.3f, test=%.3f\u0026#39; % (score_train*100, score_test*100)) Sau khi huấn luyện xong model, ta sẽ thử dự đoán một người ngẫu nhiên trong tập Test.\n... # test model on a random example from the test dataset selection = choice([i for i in range(testX.shape[0])]) random_face_pixels = testX_faces[selection] random_face_emb = testX[selection] random_face_class = testy[selection] random_face_name = out_encoder.inverse_transform([random_face_class]) ... # prediction for the face samples = expand_dims(random_face_emb, axis=0) yhat_class = model.predict(samples) yhat_prob = model.predict_proba(samples) ... # get name class_index = yhat_class[0] class_probability = yhat_prob[0,class_index] * 100 predict_names = out_encoder.inverse_transform(yhat_class) ... print(\u0026#39;Predicted: %s(%.3f)\u0026#39; % (predict_names[0], class_probability)) print(\u0026#39;Expected: %s\u0026#39; % random_face_name[0]) ... # plot for fun pyplot.imshow(random_face_pixels) title = \u0026#39;%s(%.3f)\u0026#39; % (predict_names[0], class_probability) pyplot.title(title) pyplot.show() Tổng hơp hết lại, ta có code đầy đủ như sau:\n# USEAGE: # python random_face_identity_classification.py --face_dataset 5-celebrity-faces-dataset.npz --face_embedding 5-celebrity-faces-dataset.npz # develop a classifier for the 5 Celebrity Faces Dataset from random import choice from numpy import load from numpy import expand_dims from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import Normalizer from sklearn.svm import SVC from matplotlib import pyplot import argparse def main(): ap = argparse.ArgumentParser() ap.add_argument(\u0026#39;-fd\u0026#39;, \u0026#39;--face_dataset\u0026#39;, required=True) ap.add_argument(\u0026#39;-fe\u0026#39;, \u0026#39;--face_embedding\u0026#39;, required=True) args = vars(ap.parse_args()) # load faces data = load(args[\u0026#39;face_dataset\u0026#39;]) testX_faces = data[\u0026#39;arr_2\u0026#39;] # load face embeddings data = load(args[\u0026#39;face_embedding\u0026#39;]) trainX, trainy, testX, testy = data[\u0026#39;arr_0\u0026#39;], data[\u0026#39;arr_1\u0026#39;], data[\u0026#39;arr_2\u0026#39;], data[\u0026#39;arr_3\u0026#39;] print(\u0026#39;Dataset: train=%d, test=%d\u0026#39; % (trainX.shape[0], testX.shape[0])) # normalize input vectors in_encoder = Normalizer(norm=\u0026#39;l2\u0026#39;) trainX = in_encoder.transform(trainX) testX = in_encoder.transform(testX) # label encode targets out_encoder = LabelEncoder() out_encoder.fit(trainy) trainy = out_encoder.transform(trainy) testy = out_encoder.transform(testy) # fit model model = SVC(kernel=\u0026#39;linear\u0026#39;, probability=True) model.fit(trainX, trainy) # predict yhat_train = model.predict(trainX) yhat_test = model.predict(testX) # score score_train = accuracy_score(trainy, yhat_train) score_test = accuracy_score(testy, yhat_test) # summarize print(\u0026#39;Accuracy: train=%.3f, test=%.3f\u0026#39; % (score_train*100, score_test*100)) # test model on a random example from the test dataset selection = choice([i for i in range(testX.shape[0])]) random_face_pixels = testX_faces[selection] random_face_emb = testX[selection] random_face_class = testy[selection] random_face_name = out_encoder.inverse_transform([random_face_class]) # prediction for the face samples = expand_dims(random_face_emb, axis=0) yhat_class = model.predict(samples) yhat_prob = model.predict_proba(samples) # get name class_index = yhat_class[0] class_probability = yhat_prob[0,class_index] * 100 predict_names = out_encoder.inverse_transform(yhat_class) print(\u0026#39;Predicted: %s(%.3f)\u0026#39; % (predict_names[0], class_probability)) print(\u0026#39;Expected: %s\u0026#39; % random_face_name[0]) # plot for fun pyplot.imshow(random_face_pixels) title = \u0026#39;%s(%.3f)\u0026#39; % (predict_names[0], class_probability) pyplot.title(title) pyplot.show() if __name__ == \u0026#39;__main__\u0026#39;: main() Kết quả chạy code:\nDataset: train=93, test=25 Accuracy: train=100.000, test=100.000 Predicted: jerry_seinfeld (88.476) Expected: jerry_seinfeld Kèm theo hình ảnh của người được lựa chọn để dự đoán:  6. Kết luận\nTrong bài này, chúng ta đã khám phá cách phát triển hệ thống nhận diện khuôn mặt bằng mô hình FaceNet và bộ phân loại SVM. Cụ thể:\n Giới thiệu về mô hình FaceNet. Cách chuẩn bị dữ liệu để huấn luyện hệ thống. Cách huấn luyện mô hình và sử dụng mô hình đã huấn luyện để tạo dự đoán.  Toàn bộ source code của bài này, các bạn có thể tham khảo tại đây\nTrong bài tiếp theo, chúng ta sẽ cùng nhau tìm hiểu về mô hình Siamese. Mời các bạn đón đọc.\n7. Tham khảo\n Machinelearningmastery  ","permalink":"https://tiensu.github.io/blog/54_face_recognition_facenet/","tags":["Deep Learning","Face Recognition"],"title":"Thực hiện Face Recognition với FaceNet"},{"categories":["Deep Learning","Face Recognition"],"contents":"1. Nhắc lại bài toán Face Recognition\nFace Recognition là bài toán nhận diện người dựa vào khuôn mặt của họ trong hình ảnh hoặc video. Hai trong số các bài toán của Face Recognition là:\n Face Verification: Ánh xạ 1-1, giữa khuôn mặt của một người đưa vào hệ thống nhận diện với một người đã biết trước. Face Verification trả lời câu hỏi: Đây có phải là anh/chị/ông/bà A không? Face Identification: Ánh xạ 1-nhiều, giữa giữa khuôn mặt của một người đưa vào hệ thống nhận diện với một tập những người đã biết trước trong CSDL. Face Identification trả lời câu hỏi: Đây là ai?  Trong bài này, chúng ta sẽ sử dụng VGGFace2 model (đã được Trained trên tập dữ liệu MS-Celeb-1M) để thực hiện bài toán Face Recognition.\n2. VGGFace model.\nVGG model được tạo ra bởi các nhà khoa học trong nhóm Visual Geometry Group (VGG) thuộc trường đại học Oxford. Đến thời điểm hiện tại, nó có 2 phiên bản: VGGFace và VGGFace2.\n2.1 VGGFace\nVGGFace model được công bố vào năm 2015 trong bài báo Deep Face Recognition bởi Omkar Parkhi và đồng nghiệp. Đóng góp lớn nhất của bài báo là miêu tả cách thức thu thập một số lượng lớn dữ liệu để huấn luyện một CNN model (2.6M images, 2.6K people). Tập dữ liệu này sau đó được sử dụng làm cơ sở để phát triển các model CNN cho các nhiệm vụ Face Recognition như Face Identification và Face Verification. Cụ thể, các models được đào tạo trên tập dữ liệu rất lớn, sau đó được định giá trên tập dữ liệu nhận dạng khuôn mặt điểm chuẩn, chứng tỏ rằng mô hình có hiệu quả trong việc tạo ra các đặc điểm tổng quát từ khuôn mặt.\nBài báo cũng mô tả quá trình đào tạo một bộ phân loại khuôn mặt. Trước tiên sử dụng hàm kích hoạt Softmax trong lớp đầu của CNN model ra để phân loại khuôn mặt theo từng người. Lớp này sau đó được loại bỏ để đầu ra của mạng CNN là một biểu diễn đặc trưng Vector của khuôn mặt, được gọi là Face Embedding. Sau đó, model được đào tạo thêm, thông qua tinh chỉnh, để khoảng cách Euclid giữa các Vectos của cùng một người nhỏ nhất có thể và các Vectos của 2 người khác nhau lớn nhất có thể. Điều này đạt được bằng cách sử dụng Triplet Loss.\nMạng CNN ở đây sử dụng theo kiểu kiến trúc của VGG, với các khối lớp có kích thước Kernel nhỏ, hàm kích hoạt ReLU. Tiếp theo là các lớp Max Pooling và cuối cùng là các lớp Fully Connected làm nhiệm vụ phân loại tại phần cuối của mạng. Chính vì sử dụng kiến trúc của họ VGG nên model này có tên là VGGFace.\n2.2 VGGFace 2\nQiong Cao và đồng nghiệp đã cho ra đời VGGFace2 trong bài báo có tiêu đề VGGFace2: A Dataset For Recognizing Faces Across Pose And Age. Vẫn là ý tưởng sử dụng lượng lớn dữ liệu như VGGFace, nhưng kích thước dữ liệu của VGGFace2 lớn hơn rất nhiều. Cụ thể, có tổng số 3.31 triệu ảnh của 9131 người khác nhau, tức trung bình mỗi người có 362.6 ảnh trong bộ dataset của VGGFace2. Phần lớn những ảnh này được thu thập từ Google Image Search với sự đa dạng về giới tính, tuổi tác, màu da, tư thế, sắc tộc, điều kiện môi trường, \u0026hellip; Có một điều khác so với VGGFace, đó là VGGFace2 không sử dụng kiến trúc của họ VGG, thay vào đó, nó sử dụng ResNet-50 hoặc SqueezeNet-ResNet-50. Những models này đều được đánh giá trên tập dữ liệu chuẩn, và đều đạt được state-of-the-art. Tuy nhiên, có lẽ vì chung ý tưởng là sử dụng lượng dữ liệu lớn như VGGFace nên nó vẫn lấy cái tên VGGFace2.\n3. Cài đặt thư viện keras-vggface\nCác tác giả của VGGFace/VGFFace2 cung cấp mã nguồn cho các models của họ, cũng như các Pre-trained models mà chúng ta có thể tải xuống và sử dụng được. Các Pre-trained models này được viết bằng Caffe và PyTorch, không có cho TensorFlow hoặc Keras. Mặc dù vậy, chúng ta hoàn toàn có thể chuyển đổi từ Caffe/Pytorch sang Tensorflow/Keras một cách dễ dàng. Và thực tế là có khá nhiều người đã làm sẵn việc này cho chúng ta. Nổi bật trong số đó là dự án keras-vggface của tác giả Refik Can Malli1. Thư viện này có thể được cài đặt thông qua pip:\n$ pip install git+https://github.com/rcmalli/keras-vggface.git Nếu cài đặt thành công, bạn sẽ nhận được thông báo:\nSuccessfully installed keras-2.4.3 keras-vggface-0.6 pyyaml-5.4.1 Kiểm tra lại bằng cách import vào trong code python:\n# check version of keras_vggface import keras_vggface # print version print(keras_vggface.__version__) Output:\n0.6 4. Detect Faces\nTrước khi có thể thực hiện nhận dạng khuôn mặt, chúng ta cần phát hiện khuôn mặt. Đó là quá trình tự động định vị các khuôn mặt trong một bức ảnh và khoanh vùng chúng bằng cách vẽ một hộp giới hạn xung quanh phạm vi của chúng. Trong bài viêt này, chúng ta sẽ sử dụng Multi-Task Cascaded Convolutional Neural Network, để phát hiện khuôn mặt. Đây là một mô hình học sâu hiện đại để phát hiện khuôn mặt, được mô tả trong bài báo năm 2016 có tiêu đề Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.\nCài đặt MTCNN từ dự án ipazc/mtcnn của Ivan de Paz:\n$ pip install mtcnn Kiểm tra cài đặt:\n# confirm mtcnn was installed correctly import mtcnn # print version print(mtcnn.__version__) Kết quả thực thi:\n0.1.0 Chúng ta sẽ sử dụng thư viện mtcnn để tạo phát hiện khuôn mặt, sau đó trích xuất khuôn mặt bằng VGGFace model trong các phần tiếp theo.\nTrước tiên, mở một hình ảnh dưới dạng một mảng NumPy, sử dụng hàm Matplotlib imread():\n... # load image from file pixels = pyplot.imread(filename) Tiếp theo, gọi thư viện MTCNN và sử dụng nó để phát hiện tất cả các khuôn mặt trong ảnh đã mở:\n... # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) Kết quả là danh sách các hộp giới hạn khuôn mặt, trong đó mỗi hộp xác định góc dưới bên trái của hộp giới hạn, cũng như chiều rộng và chiều cao. Giả sử chỉ có một khuôn mặt trong ảnh cho thí nghiệm lần này, chúng ta có thể xác định tọa độ pixel của hộp giới hạn như sau.\n... # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] x2, y2 = x1 + width, y1 + height Sử dụng kết quả bên trên để cắt ra hình ảnh khuôn mặt:\n... # extract the face face = pixels[y1:y2, x1:x2] VGGFace model yêu cầu mỗi khuôn mặt có kích thước 224x224. Vì thế, chúng ta sẽ resize lại hình ảnh khuôn mặt, sử dụng thư viện PIL:\n... # resize pixels to the model size image = Image.fromarray(face) image = image.resize((224, 224)) face_array = asarray(image) Code đầy đủ, bắt đầu từ việc mở hình ảnh, phát hiện khuôn mặt và hiển thị kết quả như bên dưới (file face_detection.py):\nimport argparse from matplotlib import pyplot from PIL import Image from numpy import asarray from mtcnn.mtcnn import MTCNN import tensorflow as tf from tensorflow.compat.v1.keras.backend import set_session config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True # dynamically grow the memory used on the GPU config.log_device_placement = True # to log device placement (on which device the operation ran) sess = tf.compat.v1.Session(config=config) set_session(sess) # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-i\u0026#34;, \u0026#34;--image\u0026#34;, required=True, help=\u0026#34;path to the image\u0026#34;) args = vars(ap.parse_args()) # extract a single face from a given photograph def extract_face(filename, required_size=(224, 224)): # load image from file pixels = pyplot.imread(filename) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # load the photo and extract the face pixels = extract_face(args[\u0026#39;image\u0026#39;]) # plot the extracted face pyplot.imshow(pixels) # show the plot pyplot.show() Chạy code trên:\n$ python face_detection.py --image sharon_stone1.jpg Kết quả:  5. Face Recognition\nTrong phần này, chúng ta sẽ sử dụng mô hình VGGFace2 để thực hiện Face Recognition với ảnh của những người nổi tiếng từ Wikipedia. Một mô hình VGGFace có thể được tạo ra bằng cách sử dụng hàm tạo VGGFace() và chỉ định loại mô hình cần tạo thông qua đối số mô hình.\n... model = VGGFace(model=✬...✬) Thư viện keras-vggface cung cấp ba Pre-trained VGGModel models, model VGGFace1 sử dụng kiến trúc vgg16 (mặc định) và model VGGFace2 sử dụng kiến trúc resnet50 hoặc senet50. Ví dụ dưới đây tạo VGGFace2 model với kiến trúc resnet50:\n# example of creating a face embedding from keras_vggface.vggface import VGGFace # create a vggface2 model model = VGGFace(model=\u0026#39;resnet50\u0026#39;) # summarize input and output shape print(\u0026#39;Inputs: %s\u0026#39; % model.inputs) print(\u0026#39;Outputs: %s\u0026#39; % model.outputs) Lần đầu tiên khi model được tạo, thư viện sẽ tải xuống các trọng số của mô hình và lưu chúng trong thư mục ./keras/models/vggface/ trong thư mục /home/. Kích thước của weights cho kiểu resnet50 là khoảng 158MB, vì vậy quá trình tải xuống có thể mất vài phút tùy thuộc vào tốc độ kết nối internet của bạn. Chạy ví dụ trên sẽ in ra kích thước của đầu vào và đầu ra của mô hình. Chúng ta có thể thấy rằng mô hình mong đợi đầu vào là hình ảnh của khuôn mặt có kích thước 244 × 244 và kết quả đầu ra sẽ là một dự đoán của lớp là 8.631 người. Điều này là bởi vì mô hình đã được huấn luyện với 8.631 người trong tập dữ liệu MS-Celeb-1M.\nInputs: [\u0026lt;tf.Tensor ✬input_1:0✬ shape=(?, 224, 224, 3) dtype=float32\u0026gt;] Outputs: [\u0026lt;tf.Tensor ✬classifier/Softmax:0✬ shape=(?, 8631) dtype=float32\u0026gt;] Mô hình Keras này có thể được sử dụng trực tiếp để dự đoán xác suất của một khuôn mặt nhất định thuộc về một hoặc nhiều hơn tám nghìn người nổi tiếng được biết đến; ví dụ:\n... # perform prediction yhat = model.predict(samples) Sau khi dự đoán được đưa ra, các chỉ số của các phần tử với xác suất lớn nhất có thể được ánh xạ với tên của những người nổi tiếng và có thể lấy ra năm tên hàng đầu có xác suất cao nhất. Hành vi này được cung cấp bởi hàm decode predictions() trong thư viện keras-vggface:\n... # convert prediction into names results = decode_predictions(yhat) # display most likely results for result in results[0]: print(\u0026#39;%s: %.3f%%\u0026#39; % (result[0], result[1]*100)) Trước khi chúng ta có thể đưa ra dự đoán với một khuôn mặt, các giá trị pixel phải được chia tỷ lệ giống như cách mà dữ liệu đã được chuẩn bị khi mô hình VGGFace được huấn luyện. Điều này có thể đạt được bằng cách sử dụng hàm prerocess_input() được cung cấp trong thư viện keras-vggface và chỉ định version=2 để phù hợp với VGGFace2 model. (version=1 dành cho VGGFace).\n... # convert one face into samples pixels = pixels.astype(✬float32✬) samples = expand_dims(pixels, axis=0) # prepare the face for the model, e.g. center pixels samples = preprocess_input(samples, version=2) Kết hợp tất cả những điều này lại với nhau ta được code đầy đủ:\nimport argparse from numpy import expand_dims from matplotlib import pyplot from PIL import Image from numpy import asarray from mtcnn.mtcnn import MTCNN from keras_vggface.vggface import VGGFace from keras_vggface.utils import preprocess_input from keras_vggface.utils import decode_predictions import tensorflow as tf from tensorflow.compat.v1.keras.backend import set_session config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True # dynamically grow the memory used on the GPU config.log_device_placement = True # to log device placement (on which device the operation ran) sess = tf.compat.v1.Session(config=config) set_session(sess) # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-i\u0026#34;, \u0026#34;--image\u0026#34;, required=True, help=\u0026#34;path to the image\u0026#34;) args = vars(ap.parse_args()) # extract a single face from a given photograph def extract_face(filename, required_size=(224, 224)): # load image from file pixels = pyplot.imread(filename) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # load the photo and extract the face pixels = extract_face(args[\u0026#39;image\u0026#39;]) # convert one face into samples pixels = pixels.astype(\u0026#39;float32\u0026#39;) samples = expand_dims(pixels, axis=0) # prepare the face for the model, e.g. center pixels samples = preprocess_input(samples, version=2) # create a vggface model model = VGGFace(model=\u0026#39;resnet50\u0026#39;) # perform prediction yhat = model.predict(samples) # convert prediction into names results = decode_predictions(yhat) # display most likely results for result in results[0]: print(\u0026#39;%s: %.3f%%\u0026#39; % (result[0][3:-1], result[1]*100)) Chạy code:\n$ python face_recognition.py --image sharon_stone1.jpg Một cách tuần tự, đầu tiên khuôn mặt được phát hiện và trích xuất, sau đó VGGFace2 sẽ dự đoán danh tính của khuôn mặt. Năm cái tên có xác suất cao nhất sẽ được hiển thị. Chúng ta có thể thấy rằng mô hình xác định chính xác khuôn mặt thuộc về Sharon Stone với khả năng là 99,618%:\nSharon_Stone: 99.618% Noelle_Reno: 0.096% Anita_Lipnicka: 0.021% Elisabeth_R\\xc3\\xb6hm: 0.017% Tina_Maze: 0.017% Thử kiểm tra với một người khác, lần này là Channing Tatum:\n$ python face_recognition.py --image Channing_Tatum_by_Gage_Skidmore_3.jpg Kết quả:\nChanning_Tatum: 90.526% Les_Miles: 0.238% Eoghan_Quigg: 0.212% Nico_Rosberg: 0.153% Venke_Knutson: 0.136% Chúng ta có thể thấy rằng mô hình VGGFace2 xác định chính xác khuôn mặt là của Channing Tatum với độ xác suất là 90,526%.\nBạn có thể thử nhận diện với các bức ảnh khác của những người nổi tiếng được lấy từ Wikipedia, bao gồm nhiều giới tính, chủng tộc và độ tuổi khác nhau. Bạn sẽ phát hiện ra rằng mô hình này không hoàn hảo, thi thoảng vẫn có sự nhầm lẫn hoặc xác suất không cao. Bạn cũng có thể thử các phiên bản khác của mô hình, chẳng hạn như vgg16 và senet50, sau đó so sánh kết quả. Ví dụ: mình thấy rằng với một bức ảnh của Oscar Isaac, vgg16 có hiệu quả, nhưng với các kiểu của VGGFace2 thì không. Mô hình còn có thể được sử dụng để xác định các khuôn mặt mới. Một cách tiếp cận sẽ là huấn luyện lại mô hình ở phần phân loại khuôn mặt, với một tập dữ liệu khuôn mặt mới. Chúng ta sẽ áp dụng cách tiếp cận này trong bài viết về FaceNet model.\n6. Face Verification\nMô hình VGGFace2 có thể được sử dụng để thực hiện Face Verification. Điều này liên quan đến việc tính toán và so sánh khoảng cách giữa Face Embedding vector của một khuôn mặt đưa vào với Face Embedding vector của một khuôn mặt đã biết trong hê thống. Nếu 2 vectors có khoảng cách gần nhau thì có thể kết luận 2 khuôn mặt là của cùng 1 người, và ngược lại.\nCác phép đo khoảng cách giữa 2 vectors thường dùng là khoảng cách Euclide và khoảng cách Cosine. Giá trị ngưỡng để xác định thế nào là gần hay xa cần được điều chỉnh cho mỗi tập dữ liệu hoặc ứng dụng cụ thể.\nĐể tạo ra Face Embedding vector, đầu tiên, chúng ta có thể gọi mô hình VGGFace mà không cần bộ phân loại bằng cách đặt đối số include_top=False, chỉ định kích thước của dữ liệu đầu vào, và gán đối số pooling=\u0026lsquo;avg\u0026rsquo; để bộ lọc ánh xạ đầu ra của mô hình được thành một vector, sử dụng global average pooling.\n... # create a vggface model model = VGGFace(model=\u0026#39;resnet50\u0026#39;, include_top=False, input_shape=(224, 224, 3), pooling=\u0026#39;avg\u0026#39;) Mô hình này, sau đó được sử dụng để đưa ra dự đoán, kết quả trả về là một Face Embedding vector của khuôn mặt.\n... # perform prediction yhat = model.predict(samples) Chúng ta sẽ tổng hợp lại những thứ trình bày ở trên thành 1 hàm, tham số truyền vào là danh sách các file ảnh có khuôn mặt. Hàm này sẽ tìm và trích xuất các khuôn mặt từ mỗi ảnh thông qua hàm extract_face() đã sử dụng trong phần trước. Mỗi khuôn mặt cần phải được tiền xử lý trước khi đưa vào mô hình VGGFace2 bằng cách hàm preprocess_input(). Kết quả cuối cùng trả về là một mảng chứa toàn bộ Face Embedding vectors của tất các các khuôn mặt có trong các ảnh truyền vào cho hàm số:\n# extract faces and calculate face embeddings for a list of photo files def get_embeddings(filenames): # extract faces faces = [extract_face(f) for f in filenames] # convert into an array of samples samples = asarray(faces, \u0026#39;float32\u0026#39;) # prepare the face for the model, e.g. center pixels samples = preprocess_input(samples, version=2) # create a vggface model model = VGGFace(model=\u0026#39;resnet50\u0026#39;, include_top=False, input_shape=(224, 224, 3), pooling=\u0026#39;avg\u0026#39;) # perform prediction yhat = model.predict(samples) return yhat Mình sẽ lấy ảnh của Sharon Stone - sharon stone1.jpg (đã được sử dụng trước đây) để làm tiêu chuẩn. Sau đó, mình lấy một ảnh khác cũng của Sharon Stone và một ảnh không phải là Sharon Stone để so sánh:\nFace Verification có thể được thực hiện bằng cách tính toán khoảng cách Cosine giữa Face Embedding vector của ảnh tiêu chuẩn và Face Embedding vector của ảnh cần Verrify. Để làm điều này, ta sẽ sử dụng hàm cosine() trong thư viện SciPy. Khoảng cách lớn nhất giữa 2 vectors là 1.0 (hai vectors trùng nhau hoàn toàn), và khoảng cách tối thiểu là 0.0 (hai vectors khác nhau hoàn toàn). Giá trị khoảng cách giới hạn phổ biến thường được sử dụng cho Face Recognition là từ 0.4 đến 0.6. Ban đầu, sử dụng giá trị 0.5 rồi sau đó dựa trên thực tế để điểu chỉnh dần. Hàm is_match() bên dưới sẽ thực hiện điều này:\n# determine if a candidate face is a match for a known face def is_match(known_embedding, candidate_embedding, thresh=0.5): # calculate distance between embeddings score = cosine(known_embedding, candidate_embedding) if score \u0026lt;= thresh: print(\u0026#39;\u0026gt;face is a Match (%.3f\u0026lt;= %.3f)\u0026#39; % (score, thresh)) else: print(\u0026#39;\u0026gt;face is NOT a Match (%.3f\u0026gt; %.3f)\u0026#39; % (score, thresh)) Code đầy đủ cho chức năng Face Verification như sau:\nfrom matplotlib import pyplot from PIL import Image from numpy import asarray from scipy.spatial.distance import cosine from mtcnn.mtcnn import MTCNN from keras_vggface.vggface import VGGFace from keras_vggface.utils import preprocess_input import tensorflow as tf from tensorflow.compat.v1.keras.backend import set_session config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True # dynamically grow the memory used on the GPU config.log_device_placement = True # to log device placement (on which device the operation ran) sess = tf.compat.v1.Session(config=config) set_session(sess) # extract a single face from a given photograph def extract_face(filename, required_size=(224, 224)): # load image from file pixels = pyplot.imread(filename) # create the detector, using default weights detector = MTCNN() # detect faces in the image results = detector.detect_faces(pixels) # extract the bounding box from the first face x1, y1, width, height = results[0][\u0026#39;box\u0026#39;] x2, y2 = x1 + width, y1 + height # extract the face face = pixels[y1:y2, x1:x2] # resize pixels to the model size image = Image.fromarray(face) image = image.resize(required_size) face_array = asarray(image) return face_array # extract faces and calculate face embeddings for a list of photo files def get_embeddings(filenames): # extract faces faces = [extract_face(f) for f in filenames] # convert into an array of samples samples = asarray(faces, \u0026#39;float32\u0026#39;) # prepare the face for the model, e.g. center pixels samples = preprocess_input(samples, version=2) # create a vggface model model = VGGFace(model=\u0026#39;resnet50\u0026#39;, include_top=False, input_shape=(224, 224, 3), pooling=\u0026#39;avg\u0026#39;) # perform prediction yhat = model.predict(samples) return yhat # determine if a candidate face is a match for a known face def is_match(known_embedding, candidate_embedding, thresh=0.5): # calculate distance between embeddings score = cosine(known_embedding, candidate_embedding) if score \u0026lt;= thresh: print(\u0026#39;\u0026gt;face is a Match (%.3f\u0026lt;= %.3f)\u0026#39; % (score, thresh)) else: print(\u0026#39;\u0026gt;face is NOT a Match (%.3f\u0026gt; %.3f)\u0026#39; % (score, thresh)) # define filenames filenames = [\u0026#39;sharon_stone1.jpg\u0026#39;, \u0026#39;sharon_stone2.jpg\u0026#39;, \u0026#39;sharon_stone3.jpg\u0026#39;, \u0026#39;channing_tatum.jpg\u0026#39;] # get embeddings file filenames embeddings = get_embeddings(filenames) # define sharon stone sharon_id = embeddings[0] # verify known photos of sharon print(\u0026#39;Positive Tests\u0026#39;) is_match(embeddings[0], embeddings[1]) is_match(embeddings[0], embeddings[2]) # verify known photos of other people print(\u0026#39;Negative Tests\u0026#39;) is_match(embeddings[0], embeddings[3]) Chúng ta có thể kiểm tra một số hình ảnh ví dụ bằng cách tải thêm ảnh về Sharon Stone và Channing Tatum từ Wikipedia.\nBức ảnh đầu tiên được lấy làm tiêu bản cho Sharon Stone và những bức ảnh còn lại trong danh sách là để Verify. Chạy ví dụ:\n$ python face_verification.py Kết quả:\nPositive Tests \u0026gt;face is a Match (0.460 \u0026lt;= 0.500) \u0026gt;face is a Match (0.311 \u0026lt;= 0.500) Negative Tests \u0026gt;face is NOT a Match (0.701 \u0026gt; 0.500) Chúng ta có thể thấy rằng hệ thống đã xác minh chính xác hai bức ảnh về Sharon Stone, còn ảnh của Channing Tatum được xác minh chính xác không phải là Sharon Stone.\n7. Kết luận\nTrong bài viết này, chúng ta đã cùng nhau khám phá cách phát triển hệ thống Face Recognition để nhận dạng và xác minh khuôn mặt bằng mô hình VGGFace2. Cụ thể:\n Giới thiệu về VGGFace và VGGFace2 Cách cài đặt thư viện Keras VGGFace để sử dụng các mô hình này bằng Python với Keras. Cách phát triển hệ thống nhận dạng khuôn mặt để dự đoán tên của những người nổi tiếng trong các bức ảnh. Cách phát triển hệ thống xác minh khuôn mặt để xác nhận danh tính của một người được từ bức ảnh khuôn mặt của họ.  Toàn bộ source code của bài này, các bạn có thể tham khảo tại đây.\nTrong bài tiếp theo, chúng ta sẽ khám phá cách thực hiện bài toán Face Recongition bằng mô hình FaceNet. Mời các bạn đón đọc.\n8. Tham khảo\n Machinelearningmastery  ","permalink":"https://tiensu.github.io/blog/53_face_recognition_vggface/","tags":["Deep Learning","Face Recognition"],"title":"Thực hiện Face Identification và Verification với VGGFace2"},{"categories":["Deep Learning","Face Recognition"],"contents":"1. Giới thiệu chung\nFace Recognition là bài toán nhận dạng và xác thực người dựa vào khuôn mặt của họ. Đối với con người thì đó là một nhiệm vụ rất đơn giản, thậm chí là ở trong những điều kiện môi trường khác nhau, tuổi tác thay đổi, đội mũ, đeo kính, \u0026hellip; Tuy nhiên, đối với máy tính thì nó vẫn còn là một thử thách khó khăn trong vài thập kỷ qua cho đến tận ngày nay. Trong thời đại bùng nổ của trí tuệ nhân tạo, tận dụng sức mạnh của các thuật toán DL và lượng dữ liệu vô cùng lớn, chúng ta có thể tạo ra các models hiện đại, cho phép biểu diễn khuôn mặt thành các vectors đặc trưng trong không gian nhiều chiều. Để từ đó, máy tính có thể thực hiện nhận diện ra từng người riêng biệt, mà thậm chí còn vượt qua khả năng của con người trong một số trường hợp.\n2. Phân loại\nFace Recognition có thể chia thành 3 bài toán nhỏ:\n Face Authentication: Hạn chế quyền truy cập của một người đến một nguồn tài nguyên nào đó. Face Verification: Xác nhận một người phù hợp với ID của họ. Face Identification: Gán chính xác tên của người.  Ba bài toán này thực ra chỉ khác nhau ở mục đích sử dụng kết quả nhận diện khuôn mặt vào việc gì, còn về bản chất vẫn là phân loại xem khuôn mặt cần nhận diện có thuộc vào nhóm nào trong bộ dữ liệu cho trước hay không?\nTất cả những bài toán này đều cần phải được giải quyết trong cả 3 trường hợp:\n Người trong ảnh Người trong file video Người thực (stream real-time từ camera)  Tuy nhiên, cũng lại xuất hiện thêm một bài toán con con nữa, đó là đôi khi chúng ta cần phân biệt đâu là người thật, đâu là người giả (người trong video hay ảnh). Vì nếu chúng ta đối xử với cả 3 trường hợp đều như nhau thì rất có thể kẻ gian sẽ lợi dụng để truy cập trái phép vào hệ thống thông qua một bức ảnh, cái mà rất dễ dàng có được.\n3. Luồng xử lý của bài toán Face Recognition\nBài toán Face Recognition bắt buộc phải bao gồm tối thiếu 3 bước sau:\n Bước 1: Face Detection - Xác định vị trí của khuôn mặt trong ảnh (hoặc video frame). Vùng này sẽ được đánh dấu bằng một hình chữ nhật bao quanh. Bước 2: Face Extraction (Face Embedding) - Trích xuất đặc trưng của khuôn mặt thành một vector đặc trưng trong không gian nhiều chiểu (thường là 128 chiều). Bước 3: Face Classification (Face Authentication - Face Verification - Face Identification).  Ngoài 3 bước trên, trong thực tế chúng ta thường bổ sung thêm một số bước để tăng độ chính xác nhận diện:\n Image Preprocessing: Xử lý giảm nhiễu, giảm mờ, giảm kích thước, chuyển sang ảnh xám, chuẩn hóa, \u0026hellip; Face Aligment: Nếu ảnh khuôn mặt bị nghiêng thì căn chỉnh lại sao cho ngay ngắn. Kết hợp nhiều phương pháp khác nhau tại bước 3.   3. Face Detection\nFace Detection là bước đầu tiên trong bài toán Face Recognition, có vai trò rất lớn trong việc nâng cao độ chính xác của toàn bộ hệ thống. Đầu vào của nó là một bức ảnh có chứa mặt người, đầu ra của nó sẽ là các tọa độ của vùng chứa khuôn mặt, thường thể hiện bằng một hình chữ nhật bao quanh khuôn mặt đó.\nCó 2 phương pháp tiếp cận để giải quyết vấn đề ở bước này:\n  Feature-based: Sử dụng các bộ lọc thủ công (hand-crafted filters) để tìm kiếm và định vị vị trí khuôn mặt trong ảnh. Phương pháp này rất nhanh và hiệu quả trong điều kiện gần lý tưởng, nhưng không hiệu quả trong điều kiện phức tạp hơn.\n  Điều kiện gần lý tưởng    Điều kiện phức tạp hơn      Image-based: Sử dụng các thuật toán DL để học và tự động định vị vị trí khuôn mặt dựa trên toàn bộ bức ảnh. Ưu điểm của phương pháp này là độ chính xác cao hơn so với phương pháp Feature-based, nhưng tốc độ thực hiện thì lại chậm hơn. Tùy theo điều kiện cụ thể của từng bài toán mà ta chọn phương pháp phù hợp. VD: chạy trên thiết bị nào (PC hay Embedded Device), có cần Real-time hay không, điều kiện môi trường xung quanh ra sao, \u0026hellip;\n  Dưới đây là bảng tổng hợp các thư viện và thuật toán cho mỗi phương pháp này:\n Nhìn chung, phương pháp Image-based có sử dụng các thuật toán DL nên độ chính xác cao hơn so với phương pháp Feature-based. Nhưng đổi lại, xét về tốc độ thực hiện thì Feature-based lại là kẻ chiến thắng. Tuy nhiên, điều này chỉ biểu hiện rõ rệt nếu chúng ta chạy trên các thiết bị có cấu hình thấp, kiểu như các thiết bị nhúng, còn nếu chạy trên PC hay server thì sự khác biệt về tốc độ thực thi giữa 2 phương pháp là không đáng kể.\n4. Face Embedding\nĐây là bước thứ 2 trong bài toán Face Recognition. Input của nó là bức ảnh khuôn mặt đã tìm ra ở bước 1, còn Output là một Vector nhiều chiều thể hiện đặc trưng của khuôn mặt đó.\nHai thuật toán phổ biến nhất hiện nay để thực hiện Face Embedding là FaceNet và VGGNet.\n FaceNet được tạo ra bởi Florian Schroff và đồng nghiệp tại Google. Họ đã miêu tả nó trong bài báo năm 2015 với tiêu đề FaceNet: A Unified Embedding for Face Recognition and Clustering. Ý tưởng của FaceNet được gọi là Triplet Loss, cho phép hình ảnh được mã hóa hiệu quả dưới dạng vectơ đặc trưng, để từ đó tính toán và đối sánh độ tương đồng nhanh chóng thông qua các phép tính khoảng cách trong không gian. Hệ thống của họ đã đạt được kết quả state-of-the-art.  FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. [\u0026hellip;] Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. — FaceNet: A Unified Embedding for Face Recognition and Clustering, 2015.\n VGGFace được phát triển bởi Omkar Parkhi và đồng nghiệp từ Visual Geometry Group (VGG) tại Oxford. Nó được mô tả trong bài báo năm 2015 của họ có tiêu đề Deep Face Recognition. Trọng tâm chính của họ là thu thập một tập dữ liệu đào tạo rất lớn và sử dụng tập dữ liệu này để đào tạo một mô hình CNN rất sâu về khả năng nhận diện khuôn mặt.  \u0026hellip; we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human in the loop — Deep Face Recognition, 2015.\nCả 2 thuật toán này đều có Pre-trained model. Chúng ta hoàn toàn có thể sử dụng chúng một cách miễn phí trong các dự án của mình. Mình sẽ đi chi tiết hơn về cách dùng mỗi thuật toán này trong các bài tiếp theo.\n5. Face Classification\nNhiệm vụ của bước này là phân loại khuôn mặt vào các nhóm xác định trước trong tập dữ liệu, dựa vào Vector đặc trưng của chúng. Chúng ta có 3 phương pháp:\n Dựa vào khoảng cách: Tính toán và so sánh khoảng cách giữa các Vectors. Khoảng cách càng nhỏ chứng tỏ các Vectors càng giống nhau. Thuật toán kNN là đại diện tiêu biểu cho việc sử dụng khoảng cách để phân loại, ta có thể áp dụng nó. Khoảng cách ở đây có thể sử dụng công thức Cosine hoặc Euclidean. Ưu điểm của phương pháp này là đơn giản, thực thi nhanh nếu số lượng khuôn mặt không nhiều. Nhược điểm là độ chính xác không cao, tốc độ thực thi giảm nếu số lượng khuôn mặt tăng lên. Sử dụng ML: Ta có thể dùng các Vectors đặc trưng của khuôn mặt để huấn luyện một ML model, với các thuật toán như SVM, Decision Tree, \u0026hellip; Thuật toán SVM thường được sử dụng nhiều hơn. Phương pháp này cân bằng giữa tốc độ thực hiện và độ chính xác. Sử dụng DL: Tương tự vậy, ta cũng có thể huấn luyện một DL model đơn giản (3-5 FC layers) từ các Vectors đặc trưng của khuôn mặt. Phương pháp này thường có độ chính xác cao nhất (nếu DL model đủ tốt), nhưng tốc độ thực hiện lại chậm nhất.  Ngoài ra, trong các bài toán thực tế, để tăng độ chính xác lên cao nhất có thể, chúng ta có thể kết hợp phương pháp 1 và 3, hoặc phương pháp 1 và 2.\n6. Kết luận\nTrong bài viết này, mình đã cùng các bạn khám phá bài toán Face Recognition, cụ thể:\n Face Recognition là một vấn đề chung của việc xác định hoặc xác minh người trong ảnh và video. Face Recognition là một quá trình bao gồm Face Detection, Face Embedding và Face Recongition. Các thuật tóan, các mô hình có thể sử dụng tại từng giai đoạn và ưu/nhược điểm của chúng.  Trong bài tiếp theo, mình sẽ hướng dẫn bạn cách thực hiện bài toán Face Identification và Face Verification bằng mô hình VGGFace2. Mời các bạn đón đọc!\n7. Tham khảo\n Machinelearningmastery  ","permalink":"https://tiensu.github.io/blog/52_face_recognition/","tags":["Deep Learning","Face Recognition"],"title":"Bài toán Face Recognition"},{"categories":["MLOps","Scalability"],"contents":"Nếu bạn là người thiết kế giải pháp cho hệ thống phần mềm, (ở công ty mình thường gọi là SA - Solution Architecture) (có sử dụng AI model hoặc không) thì bài viết này dành cho bạn!\nĐối với cá nhân mình, một SA ở công ty, công việc của mình bao gồm:\n Tiếp nhận bài toán của khách hàng (KH). KH có thể là các tổ chức, cá nhân ngoài công ty (outsoursing) hoặc chính là công ty mình nếu công ty phát tự triển sản phẩm (product). Cùng với đội ngũ BA, phân tích làm rõ yêu cầu của KH. Tạo tài liệu \u0026ldquo;đề xuất phát triển\u0026rdquo; (Development Proposal) để thống nhất với KH. Tài liệu này thường bao gồm các nội dung: Tóm tắt yêu cầu bài toán Đưa ra kiến trúc tổng thể giải pháp và các công nghệ sử dụng. Phần này tùy theo đối tượng KH là ai mà mình sẽ trình bày chi tiết hoặc mức tổng quát. Các chức năng mà sản phẩm (phần mềm) cung cấp. Kế hoạch phát triển dự án, từ thời gian nào để thời gian nào, các mốc phát triển chính. Tổ chức dự án: dự án có những ai liên quan (stakeholder), vai trò nhiệm vụ của mỗi người (không cần tên cụ thể, chỉ cần đưa ra các vị trí trong dự án. VD: PM, SM, Developers, QA, \u0026hellip;) Chi phí dự án: liệt kê chi phí cho từng chức năng lớn, bao gồm cả phần quản lý (chi phí Overhead) và phát triển. Cách thức làm việc giữa đội phát triển, quản lý dự án và KH: họp, báo cáo, demo, \u0026hellip; Các điều kiện cần, điều kiện ràng buộc và hạn chế của dự án. Trình bày tài liệu \u0026ldquo;đề xuất phát triển\u0026rdquo; với KH. Đây là bước rất quan trọng để thuyết phục KH. Nếu KH đồng ý với những đề xuất đưa ra thì sẽ chinh thức tiến hành thực hiện dự án. Đưa ra giải pháp chi tiết hơn cho đội phát triển dự án. Theo dõi và hỗ trợ đội phát triển dự án khi có vấn đề khó khăn về kỹ thuật xảy ra. (Viêc quản lý tiến độ phát triển, làm tài liệu dự án hay giao tiếp với KH là trách nhiệm của PM). Nghiên cứu, tìm hiểu những công nghệ mới để hướng dẫn lại cho mọi người hoặc áp dụng vào dự án. Trong trường hợp đội phát triển dự án không đủ người thì SA có thể tham gia làm cùng như một Developer hoặc nếu đội dự án không đủ năng lực kỹ thuật thì SA tổ chức các buổi đào tạo, huấn luyện cho các thành viên trong đội.  Ngoài ra, ở công ty, mình cũng đang quản lý team AI, đào tạo và phân bổ nguồn lực AI cho các dự án. Thi thoảng thì mình cũng có tham gia làm PM cho một vài dự án nếu mình cảm thấy thú vị.\nChi tiết hơn về công việc của một SA cũng như cách thức làm tài liệu \u0026ldquo;đề xuất phát triển\u0026rdquo;, mình sẽ có 1 bài viết chi tiết sau.\nQuay trở lại chủ đề chính của ngày hôm nay, mình sẽ hướng dẫn các bạn cách thiết kế một hệ thống phần mềm (có sử dụng công nghệ AI) để nó có thể hoạt động trơn tru từ lúc bắt đầu phát triển, một vài người dùng đến lúc phục vụ hàng triệu người dùng đồng thời.\nĐầu tiên, chúng ta phải xác định được phạm vi của dự án, của sản phẩm. Nếu sản phẩm của ta chỉ phục vụ một số ít người dùng hoặc làm ra với mục đích demo thì chúng ta không cần phải làm theo những gì được đề cập trong bài viết này. Vì như thế chẳng khác nào \u0026ldquo;mang dao mổ trâu giết gà\u0026rdquo;, vừa tốn công sức lại tốn cả chi phí. Ngược lại, nếu ứng dụng làm ra hướng đến phục vụ số lượng lớn người dùng thì chúng ta phải đặc biệt quan tâm đến thiết kế ngay từ đầu. Nếu không, nó có thể chạy tốt với một vài users, nhưng khi số lượng users tăng lên, nó sẽ không thể đáp ứng được. Lúc đó, có thể chúng ta phải đập đi làm lại từ đầu, rất tốn thời gian, công sức, tiền bạc.\nBài viết này dành cho các dự án đi theo hướng số 2.\n1. On-premise hay Cloud Services\nCái đầu tiên bạn phải nghĩ đến là hạ tầng phát triển và triển khai. Có 2 lựa chọn cho các bạn là: On-premise và Cloud Services.\nMỗi cái đều có ưu, nhược điểm riêng, bạn có thể tham khảo ở đây\nVề cơ bản thì Cloud Services lợi nhiều hơn hại. Nếu như ở local, chúng ta phải tự xây dựng từ đầu thông qua các thư viện/framework như Docker, Nginx, uWSGI, Kubernetes, \u0026hellip; thì trên Cloud hỗ trợ chúng ta rất nhiều trong việc \u0026ldquo;Scalability\u0026rdquo; sản phẩm thông qua các dịch vụ mà nó cung cấp. Chúng ta chỉ cần hiểu rõ chức năng, mục đích của từng service để sử dụng đúng mục đích, tránh lãng phí tiền bạc không cần thiết. Và mình cũng khuyến khích các bạn sử dụng Cloud cho mục đích này.\nMột số nhà cung cấp Cloud Services khá phổ biến hiện này là AWS, GCP, Azure. Mình chọn GCP để làm ví dụ trình bày trong bài này. Các bạn hoàn toàn có thể áp dụng cho các Cloud Services khác một cách tương tự, chỉ cần hiểu rõ các services của chúng là được.\n2. Bước đầu triển khai ứng dụng AI\nGiả sử, chúng ta đã huấn luyện thành công một AI model, độ chính xác như mong muốn. Chúng ta cũng tạo ra một API đơn giản (sử dụng Flask, uWSGI, Nginx, \u0026hellip;) để nhận các yêu cầu dự đoán (inference) gửi đến model.\nTrên GCP, chúng ta tạo ra một VM Instance (nên chon VM Instance dành riêng cho các tác vụ DL, bởi vì nó được tối ưu và bao gồm đầy đủ các thư viện cần thiết như TF, Cuda, \u0026hellip;) và copy toàn bộ dự án của chúng ta lên đó. Cấu hình cho phép HTTP traffic từ bên ngoài kết nối đến VM Instance đó.\n Thử gửi một request đến AI model, kết quả trả về không khác gì khi chạy trên local. Bước đầu, như vậy là thành công.\n3. Cấu hình CI/CD Pipeline\nSau một vài ngày (tuần) sử dụng, chúng ta muốn thay đổi model, thay đổi thư viện sử dụng. Chúng ta bắt đầu nhận ra một số bất cập:\n Việc triển khai yêu cầu rất nhiều thao tác thủ công, mất nhiều thời gian. Không có sự đồng bộ giữa các phiên bản của AI model cũng như các thư viện sử dụng. Khó khăn trong việc debug khi xảy ra lỗi trong quá trình sử dụng.  Để giải quyết 2 khó khăn đầu tiên, chúng ta bổ sung thêm CI/CD Pipeline (CD - Continuous integration, CI - Continuous deployment). Pipeline này sẽ tự động hóa việc tạo, kiểm thử và triển khai sản phẩm. Trên local, một số framework hỗ trợ việc này, bao gồm Jenkins, CircleCI. Trên GCP, Pipeline này được triển khai thành Cloud Build service.\nKhó khăn còn lại có thể vượt qua bằng cách thêm 2 services là Monitoring và Logging. Một khi có 2 services này, chúng ta có thể biết được toàn bộ quá trình hoạt động của hệ thống, và từ đó biết được chính xác nguyên nhân của lỗi (nếu có).\n Để thuận tiện hơn nữa, chúng ta nên đóng gói toàn bộ dự án trong một Docker Container, sau đó mới đặt Container đó trong VM Instance. Mỗi lần thêm mới hay cập nhật thư viện, AI model, chúng ta chỉ cần thay đổi trong file Dockerfile, Rebuild lại Container rồi Redeploy. Mọi thứ trở nên rất đơn giản và dễ dàng. Có rất nhiều Docker Container được xây dựng sẵn cho mục đích phục vụ các bài toán DL. Bạn hoàn toàn có thể tải về và sử dụng chúng miễn phí.\n4. Scaling\nSau một thời gian sử dụng, ứng dụng của chúng ta đã trở nên phổ biến, số lượng người dùng ngày càng tăng lên. Và VM Instance ban đầu bắt đầu bộc lộ những yếu điểm:\n Thời gian đáp ứng lâu hơn Chẳng may VM Instance gặp sự cố thì toàn bộ users không thể sử dụng được ứng dụng.  Lúc này, chúng ta cần đến Scaling. Có 3 loại Scaling:\n Vertical Scaling hay Scaling Up: Tăng cấu hình phần cứng bằng cách thêm CPU, Memory, GPU, Storage vào VM Instance đang sử dụng, làm cho nó có đủ sức mạnh để xử lý đồng thời nhiều yêu cầu đến từ số lượng lớn người dùng. Việc làm này tất nhiên là không thể diễn ra liên tục, nó phải có giới hạn nhất định. Hơn nữa, nếu VM Instance này bị chết thì toàn bộ hệ thống cũng chết theo. Horizontal Scaling hay Scaling Out: Tạo thêm các VM Instance mới bằng cách nhân bản VM Instance hiện tại. Các yêu cầu gửi đến sẽ được phân phối đồng đều trên tất cả các VM Instances. Loại Scaling này thường được sử dụng nhiều hơn. Scaling Down: Ngược lại với 2 loại Scaling trên, tức là giảm cấu hình phần cứng hoặc giảm số VM Instances sử dụng khi số lượng người dùng giảm.   Đối với Scaling Out, làm thế nào để phân phối các yêu cầu từ người dùng đến các VM Instances? câu trả lời là Load Balancing. Load Balancer chịu trách nhiệm điều tiết traffic ngang qua tất cả các VM Instances, làm tăng tính sẵn sàng phục vụ của toàn hệ thống. Nếu một VM Instance bị chết, Load Balancer sẽ chuyển Traffic đến các Instance khác và Scaling Out sẽ giúp tạo ra VM Instance mới để thay thế VM Instance bị chết. Nói chung, ứng dụng của chúng ta vẫn hoạt động bình thường.\nLoad Balancer cũng cung cấp các cơ chế mã hóa, bảo mật, xác thực, và kiểm tra sức khỏe các kết nối (health connections checks). Monitoring và Debugging cũng được tích hợp trong Load Balancer.\n Số lượng VM Instance được thêm mới là không có giới hạn, tùy thuộc vào nhu cầu bài toán. Ngoài ta, các VM Instances có thể được tạo ra ở các khu vực địa lý khác nhau để tối ưu hóa việc gửi nhận và xử lý Traffic.\n Đến lúc này, nhìn chung hệ thống của chúng ta đã có thể chạy ổn định nếu như không có gì bất thường xảy ra. Nhưng nếu như có gì bất thường xảy ra thì sao?\n5. Auto Scaling\nGiả sử tại một số thời điểm, ví dụ như các ngày lễ tết, số lượng Request đến hệ thống tăng lên đột biến. Đây là một common case mà chúng ta thường thấy, chúng được gọi chung với cái tên là sudden spikes in traffic.\nVậy làm thế nào để giải quyết tình huống này?\nBạn có thể nghĩ đến việc Scaling Out hệ thống để đáp ứng yêu cầu này. Nhưng khi nào thì thực hiện Scaling Out và Scaling Out bao nhiêu cho đủ. Chúng ta không thể (không nên) tạo ra các VM Instances một cách tùy ý, vì như vậy sẽ tốn rất nhiều tiền. Mọi thứ sử dụng trên hạ tầng Cloud đều phải trả tiền. Auto Scaling là giải pháp cho những tình huống như thế này. Nó là một phương pháp được sử dụng trong điện toán đám mây để thay đổi số lượng tài nguyên tính toán dựa trên tải. Thông thường, điều này có nghĩa là số lượng phiên bản VM Instances tăng lên hoặc giảm xuống, dựa trên một số chỉ số (metrics).\nCó 2 loại Auto Scaling:\n Schedule Scaling: Sử dụng khi chúng ta biết trước rằng Traffic sẽ thay đổi tại các thời điểm nào đó. Và để đáp ứng, chúng ta sẽ đặt lịch cho hệ thống tự động Scaling Out/Down tại những thời đỉểm đó. Dynamic Scaling: Sử dụng khi chúng ta không biết trước chính xác thời điểm nào lượng Traffic sẽ thay đổi. Vì thế, chúng ta sẽ cấu hình để hệ thống giám sát một số Metrics, khi các Metrics này vượt quá một ngưỡng nào đó thì các VM Instances sẽ tự động được tạo ra để đáp ứng đủ lượng Traffic. Các Metrics ở đây có thể là phần trăm sử dụng CPU, bộ nhớ, hay thời gian đáp ứng yêu cầu, \u0026hellip; Ví dụ, chúng ta cấu hình để hệ thống tạo ra số VM Instances gấp đôi khi phần trăm sử dụng của CPU vượt quá 90%, và khi phần trăm này giảm xuống nhỏ hơn 40% thì số lượng VM Instances cũng được giảm giống như trạng thái bình thường.  6. Caching\nCó một cách khác rất hiệu quả để giảm thời gian đáp ứng yêu cầu, đó là sử dụng cơ chế Caching. Cơ chế này có thể áp dụng được khi hệ thống của bạn có những yêu cầu đến giống hệt nhau. Khi đó, model của chúng ta chỉ phải xử lý yêu cầu lần đầu, kết quả từ model được lưu trong bộ nhớ Cache. Các yêu cầu đến sau mà giống yêu cầu này thì kết quả sẽ được lấy ra từ bộ nhớ Cache đó. Việc lấy kết quả ra từ từ bộ nhớ Cache thường nhanh hơn rất nhiều so với việc xử lý của model. Tuy nhiên, cũng cần phải lưu ý khi sử dụng cơ chế Caching, đó là phải thiết lập thời gian Time Out cho bộ nhớ Cache. Hết thời gian này mà kết quả nào ko được lấy ra để sử dụng thì sẽ tự động bị xóa khỏi bộ nhớ Cache.\n 7. Monitoring, Logging \u0026amp; Alerts\nMonitoring cũng là một phần không thể thiếu được của bất kỳ hệ thống phục vụ số đông người dùng nào. Mặc dù chúng ta có thể tự tin tạo ra một sản phẩm rất tốt, rất hoàn hảo, chạy trơn tru nhưng chúng ta không thể nào dự đoán hết được những lỗi có thể xảy ra trong suốt quá trình hoạt động của ứng dụng. Lỗi có thể đến từ nguyên nhân khách quan: mất mạng, mất điện, \u0026hellip; hoặc chủ quan: code sai, \u0026hellip; Nếu hệ thống chỉ phục vụ một vài người thì thi thoảng xảy ra lỗi cũng không phải vấn đề gì quá lớn lao. Nhưng nếu số lượng Users là hàng nghìn, hàng triệu thì mỗi phút không hoạt động sẽ làm tổn tất của chúng ta rất rất nhiều tiền. Đó là lý do chúng ta cần Monitor, Logging hệ thống và Alert khi xảy ra sự cố, để chúng ta có thể nhanh chóng nhận ra và khắc phục sự cố đó nhanh nhất có thể.\nHầu hết các nền tảng Cloud đều cung cấp cơ chế Monitor, Loging \u0026amp; Alert dưới dạng các Services của họ. Thông thường các bước để cấu hình Monitor, Logging \u0026amp; Alert sẽ là:\n Định nghĩa các Metrics và giám sát chúng liên tục (real-time) trong suốt quá hoạt động của hệ thống. Trực quan hóa các Metrics đó trên Dashboard dưới dạng các biểu đồ để theo dõi sự thay đổi của chúng theo thời gian (real-time). Thông báo đến người quản trị khi có hiện tượng bất thường xảy ra.   Có được cả 3 cơ chế này trong hệ thống, chúng ta có thể yên tâm ngủ ngon mỗi tối, vì mọi hoạt động của hệ thống đều đang được giám sát và điều khiển. Chúng ta chỉ phải quan tâm khi nhận được cảnh báo, báo cáo lỗi xảy ra.\nNhững thứ mà mình đã trình bày từ đầu cho đến bước này có thể áp dụng cho tất cả các loại phần mềm nói chung, trong đó có phần mềm sử dụng AI model. Tuy nhiên, riêng với dạng phần mềm có dính dáng đến AI model, chúng ta sẽ phải làm thêm một số bước khác, đặc thù hơn như dưới đây.\n8. Retraining AI Model\nỞ các bài trước mình đã trình bày về hiện tượng Data Driff và sự cần thiết phải Retraining model. Bạn có thể xem lại tại đây và ở đây.\nĐể có thể Retraining AI model thì chúng ta phải có dữ liệu mới. Dữ liệu này có thể chính là lịch sử hoạt động của model cũ, hoặc đến từ các phản hồi của KH, của người dùng. Do đó, chúng ta cần có một nơi để lưu trữ những dữ liệu như thế này, chúng ta cần một Database.\nXét về loại Database, chúng ta có thể hơi băn khoăn một chút là nên sử dụng loại Database nào? SQL hay NoSQL. Chi tiết so sánh giữa 2 loại này, các bạn có thể xem thêm tại đây. Còn riêng về khía cạnh Scability thì cả 2 lại Database này đều có thể đáp ứng được, mặc dù nói một cách thành thực thì NoSQL có nhiều điểm mạnh hơn SQL trong cá bài toán kiểu như này. Cá nhận mình thì khuyên các bạn sử dụng NoSQL.\nCó đủ data rồi thì chúng ta có thể tiến hành Retraining model theo cách đã trình bày trong bài này và bài [này]](https://tiensu.github.io/blog/40_ai_model_registry/).\nCó 2 điều cần chú ý ở đây:\n Các model hoàn toàn độc lập với nhau vì chúng được lưu trong các Docker Container riêng biệt. Bản thân model được lưu ở Storage service của Cloud Platform, như S3 của AWS hay Storage của GCP.  Một điều nữa cũng rất quan trọng, đó là không phải lúc nào model mới tạo ra cũng tốt hơn model cũ đang sử dụng. Vì thế, chúng ta phải sử dụng song song cả 2 model đồng thời 1 thời gian để đánh giá, so sánh tính hiệu quả của chúng. Đó chính là phương pháp A/B Testing. Để phân phối Traffic, chúng ta có thể cấu hình Load Balancer gửi đến mỗi model tương ứng. Ví dụ, ban đầu chỉ gửi 5% đến model mới, 95% đến model cũ, sau đó tăng dần lượng Traffic lên model mới cho đến khi đủ thông tin để kết luận rằng nó tốt hơn model cũ. Khi đó, chúng ta sẽ thay thế hoàn toàn model cũ bằng model mới.\n9. Offline Inference\nKhông phải bài toán AI nào cũng yêu cầu phải đáp ứng yêu cầu một cách tức thời. Trong chuyên môn, người ta gọi đó là Online Inference. Còn một loại khác mà model sẽ chỉ trả về kết quả tại một số thời điểm nhất định sau khi nhận được yêu cầu. Đó là kiểu Offline Inference hay Batch Inference, dựa trên cơ chế làm việc bất đồng bộ trong hệ thống. Một thằng thì cứ gửi yêu cầu, sau đó làm việc khác mà không phải chờ nhận kết quả. Một thằng thì cứ thong thả, gom đủ một số lượng yêu cầu nhất định mới trả lời một lần, rồi gửi thông báo đến cho thằng gửi yêu cầu đó.\n Để làm được việc này, chúng ta cần đến một kiểu dịch vụ, gọi là Message Queue. Nó là một dạng của dịch vụ không đồng bộ với giao tiếp dịch vụ. Message Queue lưu trữ các thông điệp đến từ một nhà sản xuất (Producer) và đảm bảo rằng mỗi thông báo sẽ chỉ được xử lý một lần bởi một người tiêu dùng (Consumer) duy nhất. Một Message Queue có thể có nhiều Producers và nhiều Consumers.\n 10. Kết luận\nNhư vậy là mình đã trình bày xong một cách tổng quát cách thức xây dựng kiến trúc hệ thống phần mềm có sử dụng AI model để phục vụ số lượng Users lớn. Thực tế thì tùy từng bài toán cụ thể mà các bạn có thể thay đổi, tối ưu hóa cho phù hợp với mục đích của các bạn. Hiện này, Kubernetes đang nổi là như là một xu hướng cho việc thiết kế hệ thống, bởi vì nó tập hơn gần như đầy đủ các thành phần được nhắc đến trong bài này. Mình cũng đã có 1 số bài viết về Kubernetes, các bạn có thể xem lại. Nếu có thời gian, mình sẽ viết thêm một bài về xây dựng 1 hệ thống đầy đủ như này trên GCP.\nHi vọng bài viết này mang lại những kiến thức bổ ích cho mọi người. Hẹn gặp lại các bạn trong những bài viết sau!\n11. Tham khảo\n Theaisummer  ","permalink":"https://tiensu.github.io/blog/51_scalability_in_ai_production/","tags":["MLOps","Scalability"],"title":"Thiết kế hệ thống cho AI model để phục vụ từ 1 đến 1 triệu người dùng"},{"categories":["Deep Learning","Autoencoder"],"contents":"Ở bài trước, chúng ta đã áp dụng Autoencoders model vào bài toán Data Denoising. Trong bài này, chúng ta sẽ tiếp tục cùng nhau tìm hiểu 1 ứng dụng nữa của Autoencoders model trong bài toán Abnormal/Outline Data Detection.\n1. Thế nào là Abnormal/Outline Data?\nĐầu tiên, chúng ta cần hiểu rõ thế nào là Abnormal/Outline Data? Hiểu một cách đơn giản thì Abnormal/Outline Data là những dữ liệu mà khác với đa số phần dữ liệu còn lại. Cái \u0026ldquo;khác\u0026rdquo; có thể được thể hiện một cách trực quan qua đồ thị phân phối dạng Histogram, Scatter, \u0026hellip;  Mặc dù thông thường tỷ lệ Abnormal/Outline Data rất nhỏ (cỡ \u0026lt; 1%) nhưng lại có tác hại rất lớn đến chất lương của model. Vì thế, nếu có thể phát hiện và loại bỏ được chúng thì có thể tạo ra được những model tốt.\nTrong bài toán triển khai AI model vào thực tế, Abnormal/Outline Data còn được gọi là hiện tượng Data Driff. Đó là hiện tượng khi mà model đã hoạt động được một thời gian, đến một thời điểm nào đó, độ chính xác của model giảm xuống mà nguyên nhân là do dữ liệu mới đưa vào model không giống như dữ liệu lúc đầu huấn luyện tạo ra model. Nếu chúng ta phát hiện được sớm hiện tượng này và cập nhật lại model theo dữ liệu mới (chính là Abnormal/Outline Data) thì sẽ tiếp tục duy trì được sự ổn định của model.\nHiện tại, có khá nhiều thuật toán có thể sử dụng để phát hiện ra Abnormal/Outline Data, ví dụ như Isolation Forests, One-class SVMs, Elliptic Envelopes, Local Outlier Factor, hay thậm chí là sử dụng một DL model(CNN).\n2. Sử dụng Autoencoders model cho bài toán Abnormal/Outline Data Detection\nĐể sử dụng Autoencoders model cho bài toàn Abnormal/Outline Data Detection, chúng ta căn cứ vào MSE giữa Output của Decoder và Input Data. Giả sử chúng ta đã huấn luyện được một Autoencoders model khá tốt, đạt được MSE nhỏ trên tập dữ liệu huấn luyện cũng như tập dữ liệu test. Bây giờ, nếu chúng ta đưa vào Encoder một dữ liệu mà khi tính MSE giữa dữ liệu đầu vào đó với Output của Decoder cho ra một giá trị tương đối lớn thì chúng ta có thể kết luận rằng, dữ liệu đầu vào đó là Abnormal/Outline Data.   Bây giờ chúng ta sẽ viết code để thực hiện việc này.\nCấu trúc thư mục làm việc như sau:\n$ tree --dirsfirst . ├── output │ ├── autoencoder.model │ └── images.pickle ├── sunt │ ├── __init__.py │ └── conv_autoencoder.py ├── find_anomalies.py ├── plot.png ├── recon_vis.png └── train_unsupervised_autoencoder.py File conv_autoencoder.py vẫn chứa định nghĩa Autoencoders model như những bài trước.\nFile train_unsupervised_autoencoder.py chứa code để huấn luyện model trên tập MNIST. Cụ thể là ở bài toán này, chúng ta chỉ sử dụng những ảnh chứa ký tự 1, còn những ảnh chứa ký tự khác sẽ được coi như là Abnormal/Outline Data. Kết quả huấn luyện sẽ là:\n autoencoder.model: Autoencoders model đã huấn luyện. images.pickle: Tập các images không có nhãn để tìm Abnormal/Outline Images trong đó. plot.png: Đồ thị Loss trong quá trình huấn luyện. recon_vis.png: So sánh Output của model với ảnh gốc ban đầu.  File find_anomalies.py chứa code để tìm ra Abnormal/Outline Images trong images.pickle.\nCode của file train_unsupervised_autoencoder.py như sau:\n# USAGE # python train_unsupervised_autoencoder.py --dataset output/images.pickle --model output/autoencoder.model # set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use(\u0026#34;Agg\u0026#34;) from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) # import the necessary packages from sunt.conv_autoencoder import ConvAutoencoder from tensorflow.keras.optimizers import Adam from tensorflow.keras.datasets import mnist from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import numpy as np import argparse import random import pickle import cv2 def build_unsupervised_dataset(data, labels, validLabel=1, anomalyLabel=3, contam=0.01, seed=42): # grab all indexes of the supplied class label that are *truly* that particular label, then grab the indexes of the image labels that will serve as our \u0026#34;anomalies\u0026#34; validIdxs = np.where(labels == validLabel)[0] anomalyIdxs = np.where(labels == anomalyLabel)[0] # randomly shuffle both sets of indexes random.shuffle(validIdxs) random.shuffle(anomalyIdxs) # compute the total number of anomaly data points to select\ti = int(len(validIdxs) * contam) anomalyIdxs = anomalyIdxs[:i] # use NumPy array indexing to extract both the valid images and \u0026#34;anomlay\u0026#34; images validImages = data[validIdxs] anomalyImages = data[anomalyIdxs] # stack the valid images and anomaly images together to form a single data matrix and then shuffle the rows images = np.vstack([validImages, anomalyImages]) np.random.seed(seed) np.random.shuffle(images) # return the set of images return images def visualize_predictions(decoded, gt, samples=10): # initialize our list of output images outputs = None # loop over our number of output samples for i in range(0, samples): # grab the original image and reconstructed image original = (gt[i] * 255).astype(\u0026#34;uint8\u0026#34;) recon = (decoded[i] * 255).astype(\u0026#34;uint8\u0026#34;) # stack the original and reconstructed image side-by-side output = np.hstack([original, recon]) # if the outputs array is empty, initialize it as the current side-by-side image display if outputs is None: outputs = output # otherwise, vertically stack the outputs else: outputs = np.vstack([outputs, output]) # return the output images return outputs # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-d\u0026#34;, \u0026#34;--dataset\u0026#34;, type=str, required=True, help=\u0026#34;path to output dataset file\u0026#34;) ap.add_argument(\u0026#34;-m\u0026#34;, \u0026#34;--model\u0026#34;, type=str, required=True, help=\u0026#34;path to output trained autoencoder\u0026#34;) ap.add_argument(\u0026#34;-v\u0026#34;, \u0026#34;--vis\u0026#34;, type=str, default=\u0026#34;recon_vis.png\u0026#34;, help=\u0026#34;path to output reconstruction visualization file\u0026#34;) ap.add_argument(\u0026#34;-p\u0026#34;, \u0026#34;--plot\u0026#34;, type=str, default=\u0026#34;plot.png\u0026#34;, help=\u0026#34;path to output plot file\u0026#34;) args = vars(ap.parse_args()) # initialize the number of epochs to train for, initial learning rate, and batch size EPOCHS = 20 INIT_LR = 1e-3 BS = 32 # load the MNIST dataset print(\u0026#34;[INFO] loading MNIST dataset...\u0026#34;) ((trainX, trainY), (testX, testY)) = mnist.load_data() # build our unsupervised dataset of images with a small amount of contamination (i.e., anomalies) added into it print(\u0026#34;[INFO] creating unsupervised dataset...\u0026#34;) images = build_unsupervised_dataset(trainX, trainY, validLabel=1, anomalyLabel=3, contam=0.01) # add a channel dimension to every image in the dataset, then scale the pixel intensities to the range [0, 1] images = np.expand_dims(images, axis=-1) images = images.astype(\u0026#34;float32\u0026#34;) / 255.0 # construct the training and testing split (trainX, testX) = train_test_split(images, test_size=0.2, random_state=42) # construct our convolutional autoencoder print(\u0026#34;[INFO] building autoencoder...\u0026#34;) (encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1) opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS) autoencoder.compile(loss=\u0026#34;mse\u0026#34;, optimizer=opt) # train the convolutional autoencoder H = autoencoder.fit( trainX, trainX, validation_data=(testX, testX), epochs=EPOCHS, batch_size=BS) # use the convolutional autoencoder to make predictions on the testing images, construct the visualization, and then save it to disk print(\u0026#34;[INFO] making predictions...\u0026#34;) decoded = autoencoder.predict(testX) vis = visualize_predictions(decoded, testX) cv2.imwrite(args[\u0026#34;vis\u0026#34;], vis) # construct a plot that plots and saves the training history N = np.arange(0, EPOCHS) plt.style.use(\u0026#34;ggplot\u0026#34;) plt.figure() plt.plot(N, H.history[\u0026#34;loss\u0026#34;], label=\u0026#34;train_loss\u0026#34;) plt.plot(N, H.history[\u0026#34;val_loss\u0026#34;], label=\u0026#34;val_loss\u0026#34;) plt.title(\u0026#34;Training Loss\u0026#34;) plt.xlabel(\u0026#34;Epoch #\u0026#34;) plt.ylabel(\u0026#34;Loss\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.savefig(args[\u0026#34;plot\u0026#34;]) # serialize the image data to disk print(\u0026#34;[INFO] saving image data...\u0026#34;) f = open(args[\u0026#34;dataset\u0026#34;], \u0026#34;wb\u0026#34;) f.write(pickle.dumps(images)) f.close() # serialize the autoencoder model to disk print(\u0026#34;[INFO] saving autoencoder...\u0026#34;) autoencoder.save(args[\u0026#34;model\u0026#34;], save_format=\u0026#34;h5\u0026#34;) Để cho giống với bài toán thực tế, mình chọn 99% hình ảnh chứa ký tự 1 và 1% hình ảnh chứa ký tự 3. Ký tự 3 coi như là Abnormal/Outline Data.\nTrên Terminal, thực hiện lệnh sau:\n$ python train_unsupervised_autoencoder.py --dataset output/images.pickle --model output/autoencoder.model [INFO] loading MNIST dataset... [INFO] creating unsupervised dataset... [INFO] building autoencoder... Epoch 1/20 2021-03-11 00:42:17.826156: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10 2021-03-11 00:42:17.981520: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7 171/171 [==============================] - 1s 7ms/step - loss: 0.0428 - val_loss: 0.0468 Epoch 2/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0076 - val_loss: 0.0347 Epoch 3/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0039 - val_loss: 0.0129 Epoch 4/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0033 - val_loss: 0.0035 Epoch 5/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0029 - val_loss: 0.0029 Epoch 6/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0027 - val_loss: 0.0028 Epoch 7/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0024 - val_loss: 0.0028 Epoch 8/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0023 - val_loss: 0.0028 Epoch 9/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0022 - val_loss: 0.0025 Epoch 10/20 171/171 [==============================] - 1s 4ms/step - loss: 0.0021 - val_loss: 0.0024 Epoch 11/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0020 - val_loss: 0.0024 Epoch 12/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0020 - val_loss: 0.0023 Epoch 13/20 171/171 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0024 Epoch 14/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0018 - val_loss: 0.0022 Epoch 15/20 171/171 [==============================] - 1s 4ms/step - loss: 0.0019 - val_loss: 0.0023 Epoch 16/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0018 - val_loss: 0.0022 Epoch 17/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0017 - val_loss: 0.0022 Epoch 18/20 171/171 [==============================] - 1s 5ms/step - loss: 0.0017 - val_loss: 0.0022 Epoch 19/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0021 Epoch 20/20 171/171 [==============================] - 1s 6ms/step - loss: 0.0016 - val_loss: 0.0022 [INFO] making predictions... [INFO] saving image data... [INFO] saving autoencoder... Đồ thị Loss trong quá trình huấn luyện:  Loss trên 2 tập Training và Validation khá tương đồng, hầu như không có hiện tượng Overfitting.\nSo sánh Input Data và Output của model:  Sau khi có được Autoencoders model đã huấn luyện, ta thử đi tìm Abnormal/Outline Images. Code cho file find_anomalies.py như sau:\n# USAGE # python find_anomalies.py --dataset output/images.pickle --model output/autoencoder.model # import the necessary packages from tensorflow.keras.models import load_model import numpy as np import argparse import pickle import cv2 from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-d\u0026#34;, \u0026#34;--dataset\u0026#34;, type=str, required=True, help=\u0026#34;path to input image dataset file\u0026#34;) ap.add_argument(\u0026#34;-m\u0026#34;, \u0026#34;--model\u0026#34;, type=str, required=True, help=\u0026#34;path to trained autoencoder\u0026#34;) ap.add_argument(\u0026#34;-q\u0026#34;, \u0026#34;--quantile\u0026#34;, type=float, default=0.999, help=\u0026#34;q-th quantile used to identify outliers\u0026#34;) args = vars(ap.parse_args()) # load the model and image data from disk print(\u0026#34;[INFO] loading autoencoder and image data...\u0026#34;) autoencoder = load_model(args[\u0026#34;model\u0026#34;]) images = pickle.loads(open(args[\u0026#34;dataset\u0026#34;], \u0026#34;rb\u0026#34;).read()) # make predictions on our image data and initialize our list of reconstruction errors decoded = autoencoder.predict(images) errors = [] # loop over all original images and their corresponding reconstructions for (image, recon) in zip(images, decoded): # compute the mean squared error between the ground-truth image and the reconstructed image, then add it to our list of errors mse = np.mean((image - recon) ** 2) errors.append(mse) # compute the q-th quantile of the errors which serves as our threshold to identify anomalies -- any data point that our model reconstructed with \u0026gt; threshold error will be marked as an outlier thresh = np.quantile(errors, args[\u0026#34;quantile\u0026#34;]) idxs = np.where(np.array(errors) \u0026gt;= thresh)[0] print(\u0026#34;[INFO] mse threshold: {}\u0026#34;.format(thresh)) print(\u0026#34;[INFO] {} outliers found\u0026#34;.format(len(idxs))) # initialize the outputs array outputs = None # loop over the indexes of images with a high mean squared error term for i in idxs: # grab the original image and reconstructed image original = (images[i] * 255).astype(\u0026#34;uint8\u0026#34;) recon = (decoded[i] * 255).astype(\u0026#34;uint8\u0026#34;) # stack the original and reconstructed image side-by-side output = np.hstack([original, recon]) # if the outputs array is empty, initialize it as the current side-by-side image display if outputs is None: outputs = output # otherwise, vertically stack the outputs else: outputs = np.vstack([outputs, output]) # show the output visualization cv2.imshow(\u0026#34;Output\u0026#34;, outputs) cv2.waitKey(0) Để tính toán ngưỡng MSE cho việc phân loại dữ liệu là Abnormal/Outline Data hay không, chúng ta sử dụng Quantile với p = 0.999.\nThực hiện lệnh sau trên Terminal để tìm Abnorlmal/Ouline Images:\n$ python find_anomalies.py --dataset output/images.pickle --model output/autoencoder.model [INFO] loading autoencoder and image data... [INFO] mse threshold: 0.03480463531613351 [INFO] 7 outliers found   2 nhận xét:\n Mặc dù Autoencoders model chỉ được huấn luyện với 1% dữ liệu chứa ký tự 3 (*trên tống số *) nhưng nó đã tái hiện lại khá tốt khi đưa hình ảnh chứa số 3 vào. Những hình ảnh chứa ký tự 3 có giá trị MSE cao hơn ngưỡng và được xác định là Abnormal/Outline Data.  3. Kết luận\nTrong bài này, chúng ta đã tìm hiểu về Abnormal/Outline Data và cách xây dựng một Autoencoders model để giải quyết nó.\nTrong thực tế thì không có một phương pháp nào có thể phát hiện hoàn toàn Abnormal/Outline Data. Các bạn có thể tìm hiểu thêm một số phương pháp khác tại đây.\nToàn bộ Source Code của bài này, các bạn có thể xem tại github cá nhân của mình.\n4. Tham khảo\n Pyimagesearch  ","permalink":"https://tiensu.github.io/blog/50_autoencoders_detect_abnormal/","tags":["Deep Learning","Autoencoder"],"title":"Sử dụng Autoencoders model cho bài toán Abnormal/Outline Data Detection"},{"categories":["Deep Learning","Autoencoder"],"contents":"Ở bài trước, chúng ta đã tìm hiểu về Autoencoders model và một số ứng dụng của nó. Trong bài này, mình sẽ cũng các bạn sử dụng Autoencoders model để thực hiện việc giảm nhiễu cho dữ liệu hình ảnh. Việc này có thể áp dụng để tăng độ chính xác của hệ thống OCR bằng cách nâng cao chất lượng của ảnh đầu vào hệ thống.\n1. Denoising Autoencoders\n Về bản chất, Autoencoders model cho bài toán Denoising là sự mở rộng của Autoencoders model ở bài trước. Điểm khác biệt ở chỗ, Output của Decoder không phải là Input Data (có nhiễu) mà là Input Data (không có nhiễu).\nĐể chuẩn bị dữ liệu cho việc huấn luyện Autoencoders model phục vụ mục đích Denoising, ta có thể làm như sau:\n Chọn ra những dữ liệu (gọi là tập B) không có nhiễu từ tập dữ liệu đầy đủ ban đầu (tập A). Thêm nhiễu ngẫu nhiên vào tập B, tạo thành tập C. Huấn luyện Autoencoders model với Input Data là tập C, Output từ Decoder là tập B. Áp dụng Autoencoders model đã trên toàn bộ tập A.  2. Denoising Autoencoders với Keras và TensorFlow\nCấu trúc thư mục làm việc:\n$ tree --dirsfirst . ├── sunt │ ├── __init__.py │ └── conv_autoencoder.py ├── output.png ├── plot.png └── train_denoising_autoencoder.py Module sunt chứa lớp conv_autoencoder.py mà chúng ta đã sử dụng ở bài trước.\nTrọng tâm của bài này nằm ở file train_denoising_autoencoder.py. Sử dụng tập dữ liệu MNIST, chúng ta thêm nhiễu vào rồi huấn luyện Autoencoders model. Ở đây, nhiễu được tạo ra bằng cách sinh ngẫu nhiên dữ liệu theo phân phối chuẩn có điểm trung tâm là 0.5, độ lệch chuẩn là 0.5. Toàn bộ code của file này như sau:\n# USAGE # python train_denoising_autoencoder.py # set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use(\u0026#34;Agg\u0026#34;) from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) # import the necessary packages from sunt.conv_autoencoder import ConvAutoencoder from tensorflow.keras.optimizers import Adam from tensorflow.keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np import argparse import cv2 # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-s\u0026#34;, \u0026#34;--samples\u0026#34;, type=int, default=8, help=\u0026#34;# number of samples to visualize when decoding\u0026#34;) ap.add_argument(\u0026#34;-o\u0026#34;, \u0026#34;--output\u0026#34;, type=str, default=\u0026#34;output.png\u0026#34;, help=\u0026#34;path to output visualization file\u0026#34;) ap.add_argument(\u0026#34;-p\u0026#34;, \u0026#34;--plot\u0026#34;, type=str, default=\u0026#34;plot.png\u0026#34;, help=\u0026#34;path to output plot file\u0026#34;) args = vars(ap.parse_args()) # initialize the number of epochs to train for and batch size EPOCHS = 25 BS = 32 # load the MNIST dataset print(\u0026#34;[INFO] loading MNIST dataset...\u0026#34;) ((trainX, _), (testX, _)) = mnist.load_data() # add a channel dimension to every image in the dataset, then scale the pixel intensities to the range [0, 1] trainX = np.expand_dims(trainX, axis=-1) testX = np.expand_dims(testX, axis=-1) trainX = trainX.astype(\u0026#34;float32\u0026#34;) / 255.0 testX = testX.astype(\u0026#34;float32\u0026#34;) / 255.0 # sample noise from a random normal distribution centered at 0.5 (since our images lie in the range [0, 1]) and a standard deviation of 0.5 trainNoise = np.random.normal(loc=0.5, scale=0.5, size=trainX.shape) testNoise = np.random.normal(loc=0.5, scale=0.5, size=testX.shape) trainXNoisy = np.clip(trainX + trainNoise, 0, 1) testXNoisy = np.clip(testX + testNoise, 0, 1) # construct our convolutional autoencoder print(\u0026#34;[INFO] building autoencoder...\u0026#34;) (encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1) opt = Adam(lr=1e-3) autoencoder.compile(loss=\u0026#34;mse\u0026#34;, optimizer=opt) # train the convolutional autoencoder H = autoencoder.fit( trainXNoisy, trainX, validation_data=(testXNoisy, testX), epochs=EPOCHS, batch_size=BS) # construct a plot that plots and saves the training history N = np.arange(0, EPOCHS) plt.style.use(\u0026#34;ggplot\u0026#34;) plt.figure() plt.plot(N, H.history[\u0026#34;loss\u0026#34;], label=\u0026#34;train_loss\u0026#34;) plt.plot(N, H.history[\u0026#34;val_loss\u0026#34;], label=\u0026#34;val_loss\u0026#34;) plt.title(\u0026#34;Training Loss and Accuracy\u0026#34;) plt.xlabel(\u0026#34;Epoch #\u0026#34;) plt.ylabel(\u0026#34;Loss/Accuracy\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.savefig(args[\u0026#34;plot\u0026#34;]) # use the convolutional autoencoder to make predictions on the testing images, then initialize our list of output images print(\u0026#34;[INFO] making predictions...\u0026#34;) decoded = autoencoder.predict(testXNoisy) outputs = None # loop over our number of output samples for i in range(0, args[\u0026#34;samples\u0026#34;]): # grab the original image and reconstructed image original = (testXNoisy[i] * 255).astype(\u0026#34;uint8\u0026#34;) recon = (decoded[i] * 255).astype(\u0026#34;uint8\u0026#34;) # stack the original and reconstructed image side-by-side output = np.hstack([original, recon]) # if the outputs array is empty, initialize it as the current side-by-side image display if outputs is None: outputs = output # otherwise, vertically stack the outputs else: outputs = np.vstack([outputs, output]) # save the outputs image to disk cv2.imwrite(args[\u0026#34;output\u0026#34;], outputs) Thực hiện lệnh sau trên Terminer:\n$ python train_denoising_autoencoder.py 2021-03-10 23:09:12.350152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4932 MB memory) -\u0026gt; physical GPU (device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5) [INFO] loading MNIST dataset... [INFO] building autoencoder... Epoch 1/25 2021-03-10 23:09:14.814333: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10 2021-03-10 23:09:14.963101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0276 - val_loss: 0.0195 Epoch 2/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0187 - val_loss: 0.0213 Epoch 3/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0177 - val_loss: 0.0182 Epoch 4/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0172 - val_loss: 0.0181 Epoch 5/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0167 - val_loss: 0.0185 Epoch 6/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0164 - val_loss: 0.0167 Epoch 7/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0162 - val_loss: 0.0161 Epoch 8/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0160 - val_loss: 0.0162 Epoch 9/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0158 - val_loss: 0.0159 Epoch 10/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0156 - val_loss: 0.0166 Epoch 11/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0155 - val_loss: 0.0164 Epoch 12/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0154 - val_loss: 0.0157 Epoch 13/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0153 - val_loss: 0.0158 Epoch 14/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0152 - val_loss: 0.0167 Epoch 15/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0151 - val_loss: 0.0157 Epoch 16/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0150 - val_loss: 0.0155 Epoch 17/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0149 - val_loss: 0.0160 Epoch 18/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0149 - val_loss: 0.0174 Epoch 19/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0148 - val_loss: 0.0155 Epoch 20/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0147 - val_loss: 0.0155 Epoch 21/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0147 - val_loss: 0.0157 Epoch 22/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0146 - val_loss: 0.0154 Epoch 23/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0146 - val_loss: 0.0156 Epoch 24/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0145 - val_loss: 0.0156 Epoch 25/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0145 - val_loss: 0.0154 [INFO] making predictions... Đồ thị thể hiện quá trình huấn luyện:  Ta thấy Train Loss và Validation Loss đều giảm dần khi số lượng epochs tăng và không xảy ra hiện tượng Overfitting.\nẢnh mới sinh ra từ Autoencoders model so với ảnh đưa vào:  Bên trái là Input Data, còn bên phải là Output của Autoencoder model. Ta thấy rất rõ, nhiễu đã được khử gần như hoàn toàn.\n3. Kết luận\nTrong bài này, chúng ta đã cùng xây dựng một Autoencoders model để phục vụ mục đích giảm nhiễu của dữ liệu. Đây là một trong những ứng dụng rất hay của Autoencoders model, giúp nâng cao chất lượng dữ liệu trước khi đưa vào huấn luyện các mô hình DL khác, đặc biệt hữu ích trong bài toán OCR.\nToàn bộ Source Code của bài này, các bạn có thể tham khảo tại github cá nhân của mình tại đây.\nHẹn các bạn trong các bài viết tiếp theo.\n4. Tham khảo\n Pyimagesearch  ","permalink":"https://tiensu.github.io/blog/49_autoencoders_denoise/","tags":["Deep Learning","Autoencoder"],"title":"Sử dụng Autoencoders model cho bài toán Denoising Data"},{"categories":["Deep Learning","Autoencoder"],"contents":"Loạt bài tiếp theo mình sẽ viết về kiến trúc Autoencoders và một số ứng dụng của chúng.\nBài đầu tiên, chúng ta sẽ thảo luận Autoencoders là gì, nó có gì khác so với các Generative models khác (ví dụ: GAN), những ứng dụng của chúng, \u0026hellip; Mình cũng sẽ xây dựng một Autoencoders model đơn giản sử dụng Keras và Tensorflow.\n1. Autoencoders là gì?\nAutoencoders là một dạng của Unsupervised Neural Network. Hoạt động của nó được mô tả như sau:\n Nhận một tập dữ liệu đầu vào (input data). Chuyển đổi Input Data sang một dạng biểu diễn khác trong không gian tiềm ẩn (Latent Space). Tái hiện lại Input Data từ Latent Space Representation.  Xét về mặt cấu tạo, một Autoencoders model gồm 2 thành phần (subnetworks):\n Encoder: Nhận Input Data rồi chuyển nó sang dạng khác trong Latent Space. $s = E(x)$\n   Trong đó $x$ là Input Data, $E$ là Encoder, và $s$ là Output trong Latent Space.\n Decoder: Nhận Output của Encoder trong Latent Space, $s$, và tái hiện lại Input Data. $o = D(s)$\n   Trong đó, $o$ là Output của Decoder $D$.\nCông thức chung cho toàn bộ quá trình Autoencoder sẽ là: $o = D(E(x))$\n Kiến trúc tổng quát của Autoencoders được minh họa như sau:  Để huấn luyện Autoencoders model, chúng ta đưa cho nó Input Data, nó sẽ cố gắng tái hiện lại Input Data bằng cách tối thiểu hóa Mean Squared Error giữa Ouput của model và Input Data. Hay nói cách khác Input Data là nhãn của chính nó.\nMột Autoencoders model lý tưởng khi Output của nó giống hệt với Input Data.\nLiệu bạn có thắc mắc là tại sao chúng ta phải tạo ra Autoencoders model, đi một vòng chỉ để tái hiện lại Input Data? Sao ta không copy luôn Input Data ra là xong chuyện???\nMình cũng từng thắc mắc như bạn khi mới bắt đầu tìm hiểu về Autoencoders.\nNhưng bạn nên biết rằng, giá trị thực sự của Autoencoders model nằm ở Output của Encoder trong Latent Space. Hay nói cách khác, chúng ta (thường) chỉ quan tâm đến Encoder và Latent Space mà không quá quan tâm đến Decoder.\nMột ví dụ để bạn dễ hình dung hơn. Giả sử chúng ta có một tập ảnh, mỗi ảnh có kích thước 28x28x1, tức là chúng ta phải sử dụng 28x28x1 = 784 bytes để lưu mỗi ảnh. Sử dụng Autoencoders model, chúng ta chuyển đổi ảnh đó sang một vector nhỏ hơn, chỉ còn 16 bytes trong Latent Space. Sử dụng 16 bytes của vector này, chúng ta sau đó có thể tái hiện lại ảnh ban đầu giống đến 98%. Điều này giúp ta tiết kiệm được rất nhiều không gian lưu trữ, đặc biệt khi phải truyền dữ liệu đó qua môi trường mạng Internet. Đó chính là một trong những ứng dụng của Autoencoders.\n2. Ứng dụng của Autoencoders model\nKiến trúc tổng quát của Autoencoders được minh họa như sau:  Một số ứng dụng của Autoencoders trong lĩnh vực Computer Vision có thể kể đến như:\n Giảm chiểu dữ liệu (Dimensionality Reduction): Ứng dụng này giống như thuật toán PCA nhưng hiệu quả hơn. Giảm nhiễu dữ liệu (Denoising): Giảm nhiễu dữ liệu, một bước trong quá trình tiền xử lý ảnh, giúp nâng cao độ chính xác của hệ thống OCR. Phát hiện bất thường (Anomaly/outlier Detection): Phát hiện những điểm dữ liệu bất thường trong tập dataset, ví dụ như thiết nhãn, lệch ra khỏi phân phối chung của toàn dữ liệu. Một khi đã phát hiện được dữ liệu bất thường, ta có thể loại bỏ chúng hoặc huấn luyện lại model để tăng độ chính xác.  Trong lĩnh vực Natual Language Processing (NLP), Autoencoders model giúp giải quyết các bài toán:\n Tạo ra đoạn text miêu tả nội dung bức ảnh (Image Caption Generation) Tóm tắt nội dung đoạn văn (Text Summarization) Trích xuất đăc trưng (Word Embedding)  3. So sánh Autoencoders và Generative Adversarial Networks (GAN)\nNếu bạn biết về GAN, bạn có thể nhận thấy Autoencoders và GAN có sự tương đồng về cách làm việc. Cả 2 models đề thuộc dạng Generative.\n Autoencoders: Nhận Input Data, chuyển thành một vector có số chiều nhỏ hơn trong Latent Space. Sau đó, cố gắng tái hiện lại Input Data từ các vector trong Latent Space. GAN: Nhận Input Data có số chiều nhỏ, chuyển thành một vector có số chiều lớn hơn. Sau đó, sinh ra một dữ liệu mới từ vector này, đáp ứng một tiêu chí nào đó. Mình sẽ viết một series bài về GAN trong tương lai.  Chi tiết hơn về so sánh giữa Autoencoders và GAN, bạn có thể đọc tại đây.\n4. Xây dựng một Autoencoders model đơn giản với Keras và Tensorflow\nChúng ta sẽ thử cùng nhau huấn luyện một Autoencoders model trên tập dữ liệu MNIST.\nCấu trúc thư mục làm việc như sau:\n$ tree --dirsfirst . ├── sunt │ ├── __init__.py │ └── conv_autoencoder.py ├── output.png ├── plot.png └── train_conv_autoencoder.py  File conv_autoencoder.py: Chứa lớp ConvAutoencoder và phương thúc build để xây dựng kiến trúc mạng của Autoencoders model. File train_conv_autoencoder.py: Huấn luyện Autoencoders model trên tập MINIST.  Mở file conv_autoencoder.py và viết code như sau:\n# import the necessary packages from tensorflow.keras.layers import BatchNormalization from tensorflow.keras.layers import Conv2D from tensorflow.keras.layers import Conv2DTranspose from tensorflow.keras.layers import LeakyReLU from tensorflow.keras.layers import Activation from tensorflow.keras.layers import Flatten from tensorflow.keras.layers import Dense from tensorflow.keras.layers import Reshape from tensorflow.keras.layers import Input from tensorflow.keras.models import Model from tensorflow.keras import backend as K import numpy as np class ConvAutoencoder: @staticmethod def build(width, height, depth, filters=(32, 64), latentDim=16): # initialize the input shape to be \u0026#34;channels last\u0026#34; along with the channels dimension itself channels dimension itself inputShape = (height, width, depth) chanDim = -1 # define the input to the encoder inputs = Input(shape=inputShape) x = inputs # loop over the number of filters for f in filters: # apply a CONV =\u0026gt; RELU =\u0026gt; BN operation x = Conv2D(f, (3, 3), strides=2, padding=\u0026#34;same\u0026#34;)(x) x = LeakyReLU(alpha=0.2)(x) x = BatchNormalization(axis=chanDim)(x) # flatten the network and then construct our latent vector volumeSize = K.int_shape(x) x = Flatten()(x) latent = Dense(latentDim)(x) # build the encoder model encoder = Model(inputs, latent, name=\u0026#34;encoder\u0026#34;) print(encoder.summary()) # start building the decoder model which will accept the output of the encoder as its inputs latentInputs = Input(shape=(latentDim,)) x = Dense(np.prod(volumeSize[1:]))(latentInputs) x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x) # loop over our number of filters again, but this time in reverse order for f in filters[::-1]: # apply a CONV_TRANSPOSE =\u0026gt; RELU =\u0026gt; BN operation x = Conv2DTranspose(f, (3, 3), strides=2, padding=\u0026#34;same\u0026#34;)(x) x = LeakyReLU(alpha=0.2)(x) x = BatchNormalization(axis=chanDim)(x) # apply a single CONV_TRANSPOSE layer used to recover the original depth of the image x = Conv2DTranspose(depth, (3, 3), padding=\u0026#34;same\u0026#34;)(x) outputs = Activation(\u0026#34;sigmoid\u0026#34;)(x) # build the decoder model decoder = Model(latentInputs, outputs, name=\u0026#34;decoder\u0026#34;) print(decoder.summary()) # our autoencoder is the encoder + decoder autoencoder = Model(inputs, decoder(encoder(inputs)), name=\u0026#34;autoencoder\u0026#34;) print(autoencoder.summay()) # return a 3-tuple of the encoder, decoder, and autoencoder return (encoder, decoder, autoencoder) Mở file train_conv_autoencoder.py và viết code như sau:\n# USAGE # python train_conv_autoencoder.py # set the matplotlib backend so figures can be saved in the background import matplotlib matplotlib.use(\u0026#34;Agg\u0026#34;) from tensorflow.compat.v1 import ConfigProto from tensorflow.compat.v1 import InteractiveSession config = ConfigProto() config.gpu_options.allow_growth = True session = InteractiveSession(config=config) # import the necessary packages from sunt.convautoencoder import ConvAutoencoder from tensorflow.keras.optimizers import Adam from tensorflow.keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np import argparse import cv2 # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-s\u0026#34;, \u0026#34;--samples\u0026#34;, type=int, default=8, help=\u0026#34;# number of samples to visualize when decoding\u0026#34;) ap.add_argument(\u0026#34;-o\u0026#34;, \u0026#34;--output\u0026#34;, type=str, default=\u0026#34;output.png\u0026#34;, help=\u0026#34;path to output visualization file\u0026#34;) ap.add_argument(\u0026#34;-p\u0026#34;, \u0026#34;--plot\u0026#34;, type=str, default=\u0026#34;plot.png\u0026#34;, help=\u0026#34;path to output plot file\u0026#34;) args = vars(ap.parse_args()) # initialize the number of epochs to train for and batch size EPOCHS = 25 BS = 32 # load the MNIST dataset print(\u0026#34;[INFO] loading MNIST dataset...\u0026#34;) ((trainX, _), (testX, _)) = mnist.load_data() # add a channel dimension to every image in the dataset, then scale # the pixel intensities to the range [0, 1] trainX = np.expand_dims(trainX, axis=-1) testX = np.expand_dims(testX, axis=-1) trainX = trainX.astype(\u0026#34;float32\u0026#34;) / 255.0 testX = testX.astype(\u0026#34;float32\u0026#34;) / 255.0 # construct our convolutional autoencoder print(\u0026#34;[INFO] building autoencoder...\u0026#34;) (encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1) opt = Adam(lr=1e-3) autoencoder.compile(loss=\u0026#34;mse\u0026#34;, optimizer=opt) # train the convolutional autoencoder H = autoencoder.fit( trainX, trainX, validation_data=(testX, testX), epochs=EPOCHS, batch_size=BS) # construct a plot that plots and saves the training history N = np.arange(0, EPOCHS) plt.style.use(\u0026#34;ggplot\u0026#34;) plt.figure() plt.plot(N, H.history[\u0026#34;loss\u0026#34;], label=\u0026#34;train_loss\u0026#34;) plt.plot(N, H.history[\u0026#34;val_loss\u0026#34;], label=\u0026#34;val_loss\u0026#34;) plt.title(\u0026#34;Training Loss and Accuracy\u0026#34;) plt.xlabel(\u0026#34;Epoch #\u0026#34;) plt.ylabel(\u0026#34;Loss/Accuracy\u0026#34;) plt.legend(loc=\u0026#34;lower left\u0026#34;) plt.savefig(args[\u0026#34;plot\u0026#34;]) # use the convolutional autoencoder to make predictions on the # testing images, then initialize our list of output images print(\u0026#34;[INFO] making predictions...\u0026#34;) decoded = autoencoder.predict(testX) outputs = None # loop over our number of output samples for i in range(0, args[\u0026#34;samples\u0026#34;]): # grab the original image and reconstructed image original = (testX[i] * 255).astype(\u0026#34;uint8\u0026#34;) recon = (decoded[i] * 255).astype(\u0026#34;uint8\u0026#34;) # stack the original and reconstructed image side-by-side output = np.hstack([original, recon]) # if the outputs array is empty, initialize it as the current # side-by-side image display if outputs is None: outputs = output # otherwise, vertically stack the outputs else: outputs = np.vstack([outputs, output]) # save the outputs image to disk cv2.imwrite(args[\u0026#34;output\u0026#34;], outputs) Trong code đã có đầy đủ comments, hi vọng các bạn có thể hiểu được.\nTiến hành chạy code (mình dùng python 3.8, tensorflow 2.3.0 trong môi trường ảo anaconda):\n$ python train_conv_autoencoder.py Output:\n Kiến trúc của Encoder:  Model: \u0026#34;encoder\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param #  ================================================================= input_1 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ conv2d (Conv2D) (None, 14, 14, 32) 320 _________________________________________________________________ leaky_re_lu (LeakyReLU) (None, 14, 14, 32) 0 _________________________________________________________________ batch_normalization (BatchNo (None, 14, 14, 32) 128 _________________________________________________________________ conv2d_1 (Conv2D) (None, 7, 7, 64) 18496 _________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 7, 7, 64) 0 _________________________________________________________________ batch_normalization_1 (Batch (None, 7, 7, 64) 256 _________________________________________________________________ flatten (Flatten) (None, 3136) 0 _________________________________________________________________ dense (Dense) (None, 16) 50192 ================================================================= Total params: 69,392 Trainable params: 69,200 Non-trainable params: 192 Ta thấy từ kiến trúc trên, Input Data ban đầu là 28x28x1 = 784 bytes, sau khi chuyển sang Latent Space, dữ liệu chỉ còn là vector 16 bytes.\n Kiến trúc của Decoder:  Model: \u0026#34;decoder\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param #  ================================================================= input_2 (InputLayer) [(None, 16)] 0 _________________________________________________________________ dense_1 (Dense) (None, 3136) 53312 _________________________________________________________________ reshape (Reshape) (None, 7, 7, 64) 0 _________________________________________________________________ conv2d_transpose (Conv2DTran (None, 14, 14, 64) 36928 _________________________________________________________________ leaky_re_lu_2 (LeakyReLU) (None, 14, 14, 64) 0 _________________________________________________________________ batch_normalization_2 (Batch (None, 14, 14, 64) 256 _________________________________________________________________ conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32) 18464 _________________________________________________________________ leaky_re_lu_3 (LeakyReLU) (None, 28, 28, 32) 0 _________________________________________________________________ batch_normalization_3 (Batch (None, 28, 28, 32) 128 _________________________________________________________________ conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1) 289 _________________________________________________________________ activation (Activation) (None, 28, 28, 1) 0 ================================================================= Total params: 109,377 Trainable params: 109,185 Non-trainable params: 192 Ngược lại với Encoder, từ vector 16 bytes trong Latent Space, Decoder tái hiện lại Input Data với 28x28x1 = 784 bytes.\n Kiến trúc của Autoencoder:  Model: \u0026#34;autoencoder\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param #  ================================================================= input_1 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ encoder (Functional) (None, 16) 69392 _________________________________________________________________ decoder (Functional) (None, 28, 28, 1) 109377 ================================================================= Total params: 178,769 Trainable params: 178,385 Non-trainable params: 384  Log train:  Epoch 1/25 2021-03-10 23:17:48.593945: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10 2021-03-10 23:17:48.741732: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0191 - val_loss: 0.0113 Epoch 2/25 1875/1875 [==============================] - 10s 5ms/step - loss: 0.0104 - val_loss: 0.0097 Epoch 3/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0094 - val_loss: 0.0087 Epoch 4/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0088 - val_loss: 0.0083 Epoch 5/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0084 - val_loss: 0.0081 Epoch 6/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0081 - val_loss: 0.0081 Epoch 7/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0079 - val_loss: 0.0077 Epoch 8/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0077 - val_loss: 0.0076 Epoch 9/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0076 - val_loss: 0.0078 Epoch 10/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0074 - val_loss: 0.0074 Epoch 11/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0073 - val_loss: 0.0073 Epoch 12/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0072 - val_loss: 0.0072 Epoch 13/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0071 - val_loss: 0.0073 Epoch 14/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0070 - val_loss: 0.0071 Epoch 15/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0070 - val_loss: 0.0071 Epoch 16/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0069 - val_loss: 0.0070 Epoch 17/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0068 - val_loss: 0.0069 Epoch 18/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0068 - val_loss: 0.0069 Epoch 19/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0067 - val_loss: 0.0069 Epoch 20/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0067 - val_loss: 0.0070 Epoch 21/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0066 - val_loss: 0.0069 Epoch 22/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0066 - val_loss: 0.0069 Epoch 23/25 1875/1875 [==============================] - 8s 4ms/step - loss: 0.0066 - val_loss: 0.0068 Epoch 24/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0065 - val_loss: 0.0068 Epoch 25/25 1875/1875 [==============================] - 7s 4ms/step - loss: 0.0065 - val_loss: 0.0067 [INFO] making predictions...  Đồ thị quá trình huấn luyện:    Ta thấy Train Loss và Validation Loss đều giảm dần khi số lượng epochs tăng và không xảy ra hiện tượng Overfitting.\n Output code Autoencoder model, so với Input Data:    Bên trái là Input Data, còn bên phải là Output của Autoencoder model. Gần như không có sự khác biệt dữ 2 bên.\n5. Kết luận\nTrong bài này, chúng ta đã cùng tìm hiểu về Autoencoders, cấu tạo, cách hoạt động, ứng dụng, cũng như sự khác nhau của nó so với GAN. Chúng ta cũng đã huấn luyện một Autoencoders đơn giản trên tập MNIST.\nToàn bộ source code của bài này, các bạn có thể tham khảo tại github cá nhân của mình tại đây.\nHẹn các bạn trong các bài viết tiếp theo.\n6. Tham khảo\n Pyimagesearch  ","permalink":"https://tiensu.github.io/blog/48_autoencoders_introduction/","tags":["Deep Learning","Autoencoder"],"title":"Autoencoder với Keras, Tensorflow và Deep Learning"},{"categories":["MLOps","Data Driff"],"contents":"Sau một thời gian nghỉ tết thì hôm nay mình đã trở lại. Trong bài viết mình sẽ cùng các bạn làm một ví dụ nhỏ về Data Driff để các bạn hiểu rõ hơn về nó. Cá nhận mình đánh giá, đây là một trong những vấn đề quan trọng nhất để giữ cho AI model chạy ổn định trong thực tế. Hãy xem lại bài này nếu bạn chưa biết về Data Driff.\n1. Ví dụ Giả sử chúng ta muốn dự đoán chất lượng của rượu tại một cửa hàng chuyên bán rượu, để quyết định xem có nên mua chai rượu đó hay không?\nChúng ta sẽ sử dụng UCI Wine Quality dataset để xây dựng một ML model dự đoán. Mỗi loại rượu có tất cả 12 features: type, fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, và alcohol rate. Nhãn là quality score có giá trị từ 0 đến 10.\nImport các thư viện sử dụng: Output:  Đọc và kiểm tra dữ liệu:  Thống kê dữ liệu:  Kiểm tra xem data có chứa giá trị NULL hay không?  Kiểm tra xem dữ liệu có bị trùng lặp hay không? Nếu có thì xóa bỏ những dữ liệu bị trùng đó.  Kiểm tra sự tương quan (liên hệ) giữa các features đôi một.  Từ đây, ta có thể loại bợt những features mà không có sự liên hệ nhiều đến nhãn (hệ số corr giữa feature đó và nhãn nhỏ).\nĐể đơn giản hóa model, chúng ta sẽ model hóa bài toán thành dạng binary classification. Cụ thể, rượu được coi là ngon khi quality score có giá trị lớn hơn 6 và ngược lại, rượu có quality score nhỏ hơn hoặc bằng 6 được coi là không ngon.  Kiểm tra sự phân phối dữ liệu giữa 2 nhãn.  Ta có thể thấy số lượng dữ liệu phân phối khá đồng đều giữa 2 nhãn. Điều này là cần thiết để tránh việc bias dữ liệu.\nĐể minh họa hiện tượng Data Driff, chúng ta chia tập dữ liệu thành 2 phần:\n Phần 1, chứa tất cả rượu có giá trị của alcohol rate lớn hơn 11%. Phần 2, chứa tất cả rượu có giá trị của alcohol rate nhỏ hơn hoặc bằng 11%.    Với việc phân chia như thế này, rõ ràng là dữ liệu ở phần 2 đã xảy ra hiện tượng Data Driff so với dữ liệu ở phần 1, cụ thể là ở feature alcohol.\nToàn bộ dữ liệu ở phần 1 sẽ được sử dụng để train và test model. Ở đây, mình không thực hiện việc tuning model mà chỉ xây dựng model đơn giản để minh họa ảnh hưởng của Data Driff.\nTách phần 1 thành 2 phần: features và labels. Sau đó lại chia mỗi phần đó thành 2 phần train và test theo tỉ lệ 80:20.  Thử kiểm tra sự phân bố dữ liệu giữa:\n Tập train và test của phần 1:   Tập train và phần 2:    Từ đồ thị phân bố có thể quan sát rõ ràng hiện tượng Data Driff khi mà feature alcohol của tập train và phần 2 nằm về 2 phía của giá trị 11. Tập train và test của phần 1 không có hiện tượng này.\nTiến hành tạo model và huấn luyện trên tập train:  Đánh giá model trên tập test:  Đánh giá model trên phần 2. Chúng ta dự đoán rằng, kết quả test trên 20% của phần 1 sẽ lớn hơn trên toàn bộ phần 2, vì hiện tượng Data Driff xảy ra ở phần 2 so với phần 1.  Ở đây, chúng ta sử dụng 3 metrics để đánh giá: accuracy score, f1 score và confusion matrix. Kết quả đánh giá chỉ ra, giá trị của các metrics trên phần 2 nhỏ hơn rất nhiều so với trên tập test, đúng như dự đoán ban đầu của chúng ta.\n2. Kết luận\nNhư vậy là chúng ta đã cùng nhau làm 1 ví dụ về hiện tượng Data Driff, một trong những vấn đề rất quan trọng của quá trình triển khai AI/ML model trong thực tế. Hi vọng là các bạn có cái nhiều sâu sắc hơn về nó thông qua bài này.\nToàn bộ source code của bài này, các bạn có thể tham khảo tại github cá nhân của mình tại đây.\nHẹn các bạn trong các bài viết tiếp theo.\n","permalink":"https://tiensu.github.io/blog/47_a_example_data_driff/","tags":["MLOps","Data Driff"],"title":"Một ví dụ về hiện tượng Data Driff trong Machine Learning"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Đây là bài viết cuối cùng về Kubernetes trên local. Bài sau (nếu có) thì sẽ là hướng dẫn cấu hình Kubernetes trên cloud.\nTrong bài này, chúng ta sẽ cùng tìm hiểu về Kubernetes Serice và áp dụng chúng vào bài toán AI.\n1. Kubernetes Service là gì?\nỞ bài trước, chúng ta đã biết rằng mặc dù Development rất hiệu quả trong việc giải quyết tác vụ Online Inference, nhưng nó có một nhược điểm là REST API chỉ có tác dụng trong phạm vi Cluster, không thể kết nối ra ngoài. Service chính là giải pháp để giải quyết cho vấn đề đó.\nService cung cấp một Stable Virtual IP (VIP) với mục đích forward dữ liệu đến tới các Pods. Một tiến trình kube-poluxy chịu trách nhiệm ánh xạ giữa VIP và các Pods (vì địa chỉ của các Pods luôn thay đổi).\n2. Làm việc với Kubernetes Service\nChúng ta sẽ sử dụng lại cấu hình của Deployment trong bài trước để tạo REST API cho tác vụ Online Inference trong bài toán AI. Sau đó, sử dụng Service để mở các REST API đó ra bên ngoài.\n2.1 Tạo Kubernetes Deployment\nChạy các lệnh sau để tạo và kiểm tra trạng thái của Deployment, Pods:\n$ kubectl create -f development-online-inference.yaml deployment.apps/online-inference-development created $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-world 5/5 5 5 37m online-inference-development 2/2 2 2 2m11s $ kubectl get pods NAME READY STATUS RESTARTS AGE online-inference-development-5d46c5c7dc-hrkrn 1/1 Running 0 12m online-inference-development-5d46c5c7dc-r4qfz 1/1 Running 0 12m 2.2 Tạo Kubernetes Service\nSử dụng lệnh sau để tạo Service:\n$ kubectl expose deployment online-inference-development --type NodePort --name online-inference-service service/online-inference-service exposed Một số thông tin:\n type: Loại Service. Ở đây sử dụng NodePort để mở REST API thông qua Port của các Worker Node. Chi tiết về các loại Type, tham khảo tại đây. name: Tên của Service đuọc tạo ra.  Kiểm tra Service vừa tạo:\n$ NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 72m online-inference-service NodePort 10.100.53.16 \u0026lt;none\u0026gt; 5000:31412/TCP 3m26s Chú ý đến 2 thông tin: EXTERNAL-IP và PORT(S). Đây là 2 thông tin để cho các yêu cầu đến truy cập vào trong các Pods.\n Giá trị \u0026lt;none\u0026gt; của EXTERNAL-IP được ngầm hiểu là IP của Worker Node, vì chúng ta đã chọn TYPE của Service là NodePort. Giá trị 5000:31412/TCP của PORT(S) có nghĩa là yêu cầu từ bên ngoài Cluster gửi đến Port 31412 của Worker Node sẽ được chuyển tiếp đến Port 5000 của các Pods. TCP là giao thức trao đổi dữ liệu.  Xem đầy đủ thông tin của Service:\n$ kubectl describe services online-inference-service Name: online-inference-service Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=model-api Type: NodePort IP Families: \u0026lt;none\u0026gt; IP: 10.100.53.16 IPs: 10.100.53.16 Port: \u0026lt;unset\u0026gt; 5000/TCP TargetPort: 5000/TCP NodePort: \u0026lt;unset\u0026gt; 31412/TCP Endpoints: 192.168.24.227:5000,192.168.24.228:5000 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; Ta có thể nhìn thấy 2 IP:PORT của 2 Pods là: 192.168.24.227:5000 và 192.168.24.228:5000\n2.3 Thực hiện Online Inference\nĐã cấu hình xong Service, chúng ta thử gửi một yêu cầu dự đoán từ bên ngoài Cluster xem sao:\n$ curl -i -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;CRIM\u0026#34;: 15.02, \u0026#34;ZN\u0026#34;: 0.0, \u0026#34;INDUS\u0026#34;: 18.1, \u0026#34;CHAS\u0026#34;: 0.0, \u0026#34;NOX\u0026#34;: 0.614, \u0026#34;RM\u0026#34;: 5.3, \u0026#34;AGE\u0026#34;: 97.3, \u0026#34;DIS\u0026#34;: 2.1, \u0026#34;RAD\u0026#34;: 24.0, \u0026#34;TAX\u0026#34;: 666.0, \u0026#34;PTRATIO\u0026#34;: 20.2, \u0026#34;B\u0026#34;: 349.48, \u0026#34;LSTAT\u0026#34;: 24.9}\u0026#39; 10.1.30.130:31412/predict HTTP/1.0 200 OK Content-Type: application/json Content-Length: 41 Server: Werkzeug/1.0.1 Python/3.8.6 Date: Tue, 02 Feb 2021 11:06:49 GMT { \u0026#34;prediction\u0026#34;: 12.273424794987877 } Có kết quả trả về, tức là chúng ta đã thành công, :)).\n3. Kết luận\nĐây là bài viết cuối cùng trong năm Canh Tý của mình. Xong bài này mình sẽ về quê đón tết cùng gia đình.\nBài viết đầu tiên trong năm Nhâm Sửu mình sẽ hướng dẫn các bạn cách nhận biết hiện tượng Data Driff, một vấn đề mà theo mình rất quan trong việc giải quyết các bài toán AI thực tế. Mời các bạn đón đọc!\nKính chúc mọi người năm mới AN KHANG THỊNH VƯỢNG!!!\n8. Tham khảo\n Mlinproduction Kubernetes Service  ","permalink":"https://tiensu.github.io/blog/46_kubernetes_services/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 5: Kubernetes Service"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Trong bài toán AI, nếu như Job và CronJob phù hợp nhất cho các tác vụ thực hiện không liên tục, không realtime (VD: Batch Inference, Training, \u0026hellip;) thì Deployment lại là lựa chọn tốt nhất cho các tác vụ cần chạy liên tục, realtime (VD: Online Inference, \u0026hellip;). Trong bài này, hãy cùng tìm hiểu về Deployment và cách sử dụng nó.\n1. Kubernetes Deployment là gì?\nDeployment có thể hiểu là một tập các Pods giống nhau chạy trên một Kubernetes Cluster. Giống như Job, nó cũng quản lý các Pods trong việc thực hiện một nhiệm vụ nào đó. Sự khác nhau giữa Job và Deployment ở tính chất nhiệm vụ mà chúng thực hiện. Đối với Job, các tasks của nó chỉ chạy một lần, sau đó kết thúc luôn. Ngược lại, các tasks của Deployment chạy liên tục từ lúc được khởi tạo và chỉ kết thúc khi có sự can thiệp của người quản trị hoặc một ngoại lệ bất thường.\nMột số đặc điểm trong cách quản lý Pod của Deployment:\n Trong quá trình làm việc, nếu một Pod bị chết, Deployment sẽ tạo ra một Pod khác thay thế. Deployment cũng có khả năng tự động scale up/down số lượng các Pods tùy thuộc vào mức độ nặng/nhẹ của công viêc mà nó thực hiện. Có thể thay đổi cấu hình của Deployment trực tiếp trong file cấu hình mà không phải downtime. Có thể quay lại những thay đổi trước đó trong trường hợp sự thay đổi mới gây ra lỗi.  Chính vì vậy mà Deployment rất phù hợp với nhiệm vụ Online Inference trong bài toán AI. Chúng ta train một model, tạo một REST API để lắng nghe các yêu cầu dự đoán. Sau đó, tạo ra một Deployment để chấp nhận và thực hiện các yêu cầu đó một các realtime. Nếu số lượng các yêu cầu tăng lên cao, Deployment sẽ tự động tạo thêm các Pod để xử lý và ngược lại. Nếu có một phiên bản mới của model, ta có thể dễ dàng đưa luôn vào sử dụng mà không phải downtime. Và nếu model mới đó không hiệu quả bằng model cũ, ta hoàn toàn có thể quay về sử dụng model cũ đó.\n2. Làm việc với Kubernetes Deployment\nChúng ta sẽ thực hiện tạo một Deployment để phục vụ nhiệm vụ Online Inference trong bài toán AI.\nHãy xem cấu trúc thư mục làm việc:\nkubernetes_deployment │ ├── deployment │ │ └── deployment-online-inference.yaml │ └── docker │ ├── api.py │ ├── Dockerfile │ └── train.py 2.1 Train model AI và tạo REST API\nTạo thư mục docker và hai file code python bên trong nó:\n File train.py: Train model AI và lưu file model. File api.py: Tạo API để cho phép yêu cầu dự đoán gửi đến và trả về kết quả.  Nội dung của file train.py như sau:\nimport json import os from joblib import dump import matplotlib.pyplot as plt import numpy as np from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Load data print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() print(\u0026#34;Splitting data...\u0026#34;) X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset]\tX_test, y_test = X[offset:], y[offset:] # ############################################################################# # Fit regression model print(\u0026#34;Fitting model...\u0026#34;) params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) train_mse = mean_squared_error(y_train, clf.predict(X_train)) test_mse = mean_squared_error(y_test, clf.predict(X_test)) metadata = { \u0026#34;train_mean_square_error\u0026#34;: train_mse, \u0026#34;test_mean_square_error\u0026#34;: test_mse } print(\u0026#34;Serializing model to: {}\u0026#34;.format(MODEL_PATH)) dump(clf, MODEL_PATH) print(\u0026#34;Serializing metadata to: {}\u0026#34;.format(METADATA_PATH)) with open(METADATA_PATH, \u0026#39;w\u0026#39;) as outfile: json.dump(metadata, outfile) Nội dung của file api.py như sau:\nimport os from flask import Flask from flask_restful import Resource, Api, reqparse from joblib import load import numpy as np MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) print(\u0026#34;Loading model from: {}\u0026#34;.format(MODEL_PATH)) clf = load(MODEL_PATH) app = Flask(__name__) api = Api(app) class Prediction(Resource): def __init__(self): self._required_features = [\u0026#39;CRIM\u0026#39;, \u0026#39;ZN\u0026#39;, \u0026#39;INDUS\u0026#39;, \u0026#39;CHAS\u0026#39;, \u0026#39;NOX\u0026#39;, \u0026#39;RM\u0026#39;, \u0026#39;AGE\u0026#39;, \u0026#39;DIS\u0026#39;, \u0026#39;RAD\u0026#39;, \u0026#39;TAX\u0026#39;, \u0026#39;PTRATIO\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;LSTAT\u0026#39;] self.reqparse = reqparse.RequestParser() for feature in self._required_features: self.reqparse.add_argument( feature, type = float, required = True, location = \u0026#39;json\u0026#39;, help = \u0026#39;No {} provided\u0026#39;.format(feature)) super(Prediction, self).__init__() def post(self): args = self.reqparse.parse_args() X = np.array([args[f] for f in self._required_features]).reshape(1, -1) y_pred = clf.predict(X) return {\u0026#39;prediction\u0026#39;: y_pred.tolist()[0]} api.add_resource(Prediction, \u0026#39;/predict\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True, host=\u0026#39;0.0.0.0\u0026#39;) Code của 2 files này khá đơn giản nên mình không giải thích gì thêm, hi vọng các bạn có thể tự hiểu được.\n2.2 Chuẩn bị Docker Image\nCũng trong thư mục docker, ta file Dockerfile như sau:\nFROM jupyter/scipy-notebook USER root WORKDIR /docker ADD . /docker RUN pip install flask flask-restful joblib RUN mkdir /docker/model ENV MODEL_DIR=/docker/model ENV MODEL_FILE=clf.joblib ENV METADATA_FILE=metadata.json RUN python3 train.py Sau đó tiến hành build Docker Image:\n$ docker build -t docker-ml-online . Sending build context to Docker daemon 6.656kB Step 1/10 : FROM jupyter/scipy-notebook ---\u0026gt; c1a7c7ef5e27 Step 2/10 : USER root ---\u0026gt; Using cache ---\u0026gt; 0d9f55e9c7e0 Step 3/10 : WORKDIR /docker ---\u0026gt; Using cache ---\u0026gt; 4ed21d81d110 Step 4/10 : ADD . /docker ---\u0026gt; a266bfc5ca35 Step 5/10 : RUN pip install flask flask-restful joblib ---\u0026gt; Running in 97888ed0b989 Collecting flask Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB) Collecting flask-restful Downloading Flask_RESTful-0.3.8-py2.py3-none-any.whl (25 kB) Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (1.0.0) Requirement already satisfied: click\u0026gt;=5.1 in /opt/conda/lib/python3.8/site-packages (from flask) (7.1.2) Collecting Werkzeug\u0026gt;=0.15 Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB) Requirement already satisfied: Jinja2\u0026gt;=2.10.1 in /opt/conda/lib/python3.8/site-packages (from flask) (2.11.2) Collecting itsdangerous\u0026gt;=0.24 Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB) Requirement already satisfied: MarkupSafe\u0026gt;=0.23 in /opt/conda/lib/python3.8/site-packages (from Jinja2\u0026gt;=2.10.1-\u0026gt;flask) (1.1.1) Requirement already satisfied: six\u0026gt;=1.3.0 in /opt/conda/lib/python3.8/site-packages (from flask-restful) (1.15.0) Requirement already satisfied: pytz in /opt/conda/lib/python3.8/site-packages (from flask-restful) (2020.5) Collecting aniso8601\u0026gt;=0.82 Downloading aniso8601-8.1.1-py2.py3-none-any.whl (44 kB) Installing collected packages: Werkzeug, itsdangerous, flask, aniso8601, flask-restful Successfully installed Werkzeug-1.0.1 aniso8601-8.1.1 flask-1.1.2 flask-restful-0.3.8 itsdangerous-1.1.0 Removing intermediate container 97888ed0b989 ---\u0026gt; d9f31d7e7c83 Step 6/10 : RUN mkdir /docker/model ---\u0026gt; Running in 89b237f6427c Removing intermediate container 89b237f6427c ---\u0026gt; b2778ed90f4a Step 7/10 : ENV MODEL_DIR=/docker/model ---\u0026gt; Running in d7a52c9249f9 Removing intermediate container d7a52c9249f9 ---\u0026gt; 5157d919abd5 Step 8/10 : ENV MODEL_FILE=clf.joblib ---\u0026gt; Running in a7f75c6f79e5 Removing intermediate container a7f75c6f79e5 ---\u0026gt; 790a21e54588 Step 9/10 : ENV METADATA_FILE=metadata.json ---\u0026gt; Running in b0b94567182c Removing intermediate container b0b94567182c ---\u0026gt; 92a98ce95a8d Step 10/10 : RUN python3 train.py ---\u0026gt; Running in d8055e4ef00d Loading data... Splitting data... Fitting model... Serializing model to: /docker/model/clf.joblib Serializing metadata to: /docker/model/metadata.json Removing intermediate container d8055e4ef00d ---\u0026gt; b1fb95b775ec Successfully built b1fb95b775ec Successfully tagged docker-ml-online:latest Có Docker Image rồi, tiến hành push nó lên Docker Hub:\n$ docker push tiensu/ml-model-online-infer:latest The push refers to repository [docker.io/tiensu/ml-model-online-infer] f0e40a44cb9c: Pushed a079ef4fd38e: Pushed 76cba4a3a958: Pushed 3451a539eae2: Pushed 66f4cc63b50c: Mounted from tiensu/docker-ml 5f70bf18a086: Mounted from tiensu/docker-ml 6f5a41ae77fd: Mounted from tiensu/docker-ml 5a1b9a3f9355: Mounted from tiensu/docker-ml b1d7816bac14: Mounted from tiensu/docker-ml c91fed2d1998: Mounted from tiensu/docker-ml cc70098d00e3: Mounted from tiensu/docker-ml 88727e93cbac: Mounted from tiensu/docker-ml cadaf24035f3: Mounted from tiensu/docker-ml 8f170f4774e3: Mounted from tiensu/docker-ml 33bd52db887f: Mounted from tiensu/docker-ml 21e5dd010f50: Mounted from tiensu/docker-ml ea370ab22368: Mounted from tiensu/docker-ml 421d1408f872: Mounted from tiensu/docker-ml 18fd1ca0de51: Mounted from tiensu/docker-ml 8f01aab6d756: Mounted from tiensu/docker-ml e18a1c4e1d31: Mounted from tiensu/docker-ml 8552f27c3cd8: Mounted from tiensu/docker-ml 1a4c57efcc23: Mounted from tiensu/docker-ml 94b8fe888eac: Mounted from tiensu/docker-ml 02473afd360b: Mounted from tiensu/docker-ml dbf2c0f42a39: Mounted from tiensu/docker-ml 9f32931c9d28: Mounted from tiensu/docker-ml latest: digest: sha256:67c219ed32f9748c0c3ce64e8c4274932a8dadaf05510402f5d64a038bca2165 size: 6790 2.3 Tạo Kubernetes Deployment\nTrong thư mục deployment, tạo file cấu hình (deployment-online-inference.yaml) của Deployment với nội dung như sau:\napiVersion: apps/v1 kind: Deployment metadata: name: online-inference-deployment spec: replicas: 2 selector: matchLabels: app: model-api template: metadata: labels: app: model-api spec: containers: - name: model-api imagePullPolicy: Always image: tiensu/ml-model-online-infer:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;api.py\u0026#34;] ports: - containerPort: 5000 Một số thông tin cần lưu ý ở đây:\n replicas: Số lượng Pods được tạo ra lúc ban đầu. selector: Định nghĩa tên của Pods/Containers mà nó quản lý.  Chạy các lệnh sau để tạo và kiểm tra Deployment:\n$ kubectl create -f deployment-online-inference.yaml deployment.apps/online-inference-deployment created $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE online-inference-deployment 2/2 2 2 41s Thực ra, Deployment không trực tiếp quản lý các Pods. Thay vào đó, nó sẽ tạo ra các ReplicaSet với mục đích duy trì sự ổn định của các Pods tại bất kì thời điểm nào trong suốt quá trình hoạt động.\nKiểm tra ReplicaSet được Deployment tạo ra:\n$ kubectl get rs NAME DESIRED CURRENT READY AGE online-inference-deployment-59c8579f48 2 2 2 68s Chú ý: Tên của ReplicaSet = Tên của Deployment + chuỗi ngẫu nhiên.\nKiểm tra thử các Pods được quản lý bởi online-inference-deployment-59c8579f48 ReplicaSet:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE online-inference-deployment-59c8579f48-bg2vj 1/1 Running 0 97s online-inference-deployment-59c8579f48-j9fl2 1/1 Running 0 97s Chú ý: Tên của Pod = Tên của ReplicaSet + chuỗi ngẫu nhiên.\nThử debug một Pod xem có gì bất thường không?\n$ kubectl describe pod online-inference-deployment-59c8579f48-bg2vj Name: online-inference-deployment-59c8579f48-bg2vj Namespace: default Priority: 0 Node: duynm-vostro-3670/10.1.30.130 Start Time: Mon, 01 Feb 2021 18:15:27 +0700 Labels: app=model-api pod-template-hash=59c8579f48 Annotations: cni.projectcalico.org/podIP: 192.168.24.197/32 cni.projectcalico.org/podIPs: 192.168.24.197/32 Status: Running IP: 192.168.24.197 IPs: IP: 192.168.24.197 Controlled By: ReplicaSet/online-inference-deployment-59c8579f48 Containers: model-api: Container ID: docker://4cde562c962b48ff4c6bc3c812b140d2555e1984f064108bd8bf607b122cef9a Image: tiensu/ml-model-online-infer Image ID: docker-pullable://tiensu/ml-model-online-infer@sha256:67c219ed32f9748c0c3ce64e8c4274932a8dadaf05510402f5d64a038bca2165 Port: 5000/TCP Host Port: 0/TCP Command: python3 api.py State: Running Started: Mon, 01 Feb 2021 18:15:45 +0700 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-wp4xr (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-wp4xr: Type: Secret (a volume populated by a Secret) SecretName: default-token-wp4xr Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m25s default-scheduler Successfully assigned default/online-inference-deployment-59c8579f48-bg2vj to duynm-vostro-3670 Normal Pulling 2m22s kubelet Pulling image \u0026#34;tiensu/ml-model-online-infer\u0026#34; Normal Pulled 2m8s kubelet Successfully pulled image \u0026#34;tiensu/ml-model-online-infer\u0026#34; in 14.038005704s Normal Created 2m7s kubelet Created container model-api Normal Started 2m7s kubelet Started container model-api OK, mọi thứ đều đang hoạt động đúng như mong muốn.\nTa cũng có thể xem logs của Pod khi chạy:\n$ kubectl logs -f online-inference-development-5d46c5c7dc-bg2vj Loading model from: /docker/model/clf.joblib * Serving Flask app \u0026#34;api\u0026#34; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 263-920-719 10.1.30.130 - - [02/Feb/2021 11:06:49] \u0026#34;POST /predict HTTP/1.1\u0026#34; 200 - Tham số -f dùng để xem log một các realtime.\n2.4 Chạy Online Inference\nBây giờ ta sẽ thử gửi một yêu cầu dự đoán thông qua REST API để xem kết quả trả về. Tuy nhiên, có một chú ý quan trọng là REST API này chỉ mới hoạt động được bên trong phạm vi của Kubernetes Cluster. Để mở rộng nó ra ngoài Internet, chúng ta cần phải sử dụng thêm Service. Service sẽ được trình bày trong bài viết tiếp theo.\nChúng ta sẽ thực hiện Online Inference từ một Pod trong cùng Cluster với Deployment. Sử dụng lệnh sau để chạy và truy cập vào Pod python3:\n$ kubectl run python3 -ti --image=python:3.6 --command=true bash If you don\u0026#39;t see a command prompt, try pressing enter. root@python3:/#  Phần xử lý Inference bây giờ đang nằm trên 2 Pods mà Deployment tạo ra. Ta sẽ gửi yêu cầu dự đoán đến chúng. Xem lại phần debug bên trên của Pod online-inference-deployment-59c8579f48-bg2vj ta thấy Internal IP của nó là 192.168.24.197\nTừ trong Pod python3, thực hiện lệnh sau để gửi yêu cầu dự đoán:\n$ curl -i -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;CRIM\u0026#34;: 15.02, \u0026#34;ZN\u0026#34;: 0.0, \u0026#34;INDUS\u0026#34;: 18.1, \u0026#34;CHAS\u0026#34;: 0.0, \u0026#34;NOX\u0026#34;: 0.614, \u0026#34;RM\u0026#34;: 5.3, \u0026#34;AGE\u0026#34;: 97.3, \u0026#34;DIS\u0026#34;: 2.1, \u0026#34;RAD\u0026#34;: 24.0, \u0026#34;TAX\u0026#34;: 666.0, \u0026#34;PTRATIO\u0026#34;: 20.2, \u0026#34;B\u0026#34;: 349.48, \u0026#34;LSTAT\u0026#34;: 24.9}\u0026#39; 192.168.24.197:5000/predict HTTP/1.0 200 OK Content-Type: application/json Content-Length: 41 Server: Werkzeug/1.0.1 Python/3.8.6 Date: Mon, 01 Feb 2021 11:22:25 GMT { \u0026#34;prediction\u0026#34;: 12.273424794987877 } Như vậy là ta đã nhận được kết quả dự đoán trả về, chứng tỏ Deployment của chúng ta đã hoạt động đúng như ta dự tính.\n2.5 Xóa Deployment khi không sử dụng\nNếu không sử dụng nữa, ta thực hiện lệnh sau để xóa Deployment và các tài nguyên của nó:\n$ kubectl delete deployment online-inference-development deployment.apps \u0026#34;online-inference-development\u0026#34; deleted Kiểm tra lại:\n$ kubectl get rs No resources found. $ kubectl get pods No resources found. 3. Kết luận\nXong, chúng ta đã thực hành thành công với Deployment, và ta cũng biết một thiếu sót của Deployment phải cần đến Service để giải quyết. Đó chính là nội dung của bài tiếp theo. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n8. Tham khảo\n Mlinproduction CronJob  ","permalink":"https://tiensu.github.io/blog/45_kubernetes_deployment/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 4: Kubernetes Deployment"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Ở bài trước, chúng ta đã tìm hiểu và thực hành với Kubernetes Job và thấy được sự phù hợp và hiệu quả của nó đối với các tác vụ trong bài toán AI. Tuy nhiên, có thể dễ dàng nhận thấy một nhược điểm của Job, đó là Job phải được tạo một cách thủ công. Điều này khá là bất tiện, vì chúng ta sẽ phải mất công giám sát hoạt động của hệ thống để can thiệp vào (tạo Job) khi cần. Liệu có cách nào làm cho Job có thể tự động được tạo ra và thực hiện nhiệm vụ của nó tại những thời điểm nhất định, theo chu kỳ? Kubernetes CronJob chính là câu trả lời. Trong bài này, hãy cùng nhau làm việc với CronJob nhé!\n1. Kubernetes CronJob là gì?\nCronJob là một bộ lập lịch, tương tự như Cron Task trong nhân Linux. Nó giúp chúng ta tạo ra một kế hoạch thực hiện một công việc nào đó (bằng cách tạo ra các Jobs), tại những thời điểm trong tương lai theo một chu kỳ mà ta định nghĩa.\nĐối với bài toán AI, CronJob phù hợp với các tác vụ cần chạy định kỳ, ví dụ như là Batch Inference, Feature Extraction, \u0026hellip;\n2. Làm việc với CronJob\nChúng ta sẽ sử dụng lại Docker Image ở bài trước.\nMục đích của mình ở đây là tạo ra một CronJob để thực hiện Batch Inference mỗi phút. File model được lưu trên AWS S3.\nHãy xem cấu trúc thư mục làm việc:\nkubernetes_cronjob │ ├── cronjob │ │ └── cronjob-inference.yaml │ └── docker │ ├── batch_inference.py │ ├── Dockerfile │ └── train.py Giống như Pod và Job, CronJob cũng được tạo thông qua file cấu hình (cronjob-inference.yaml):\napiVersion: batch/v1beta1 kind: CronJob metadata: name: inference-cronjob spec: schedule: \u0026#34;* * * * *\u0026#34; jobTemplate: spec: template: spec: containers: - name: inference-container imagePullPolicy: Always image: tiensu/docker-ml:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;inference.py\u0026#34;] env: - name: AWS_ACCESS_KEY_ID value: \u0026#34;\u0026#34; - name: AWS_SECRET_ACCESS_KEY value: \u0026#34;\u0026#34; restartPolicy: Never backoffLimit: 0 Các thông tin cấu hình khá giống với của Pod, Job. Chỉ có 1 thông tin mới cần lưu ý ở đây:\n schedule: Đây là giá trị chỉ ra chu kỳ chạy của Job, tuân theo các quy tắc định trước. Tham khảo các quy tắc tại đây. Ngoài ra, nếu bạn cảm thấy bối rối khi sử dụng những quy tắc để tạo schedule, bạn có thể sử dụng công cụ này để giúp đỡ bạn. Ở đây, mình đang cấu hình Schedule là 1 phút, tức cứ mỗi phút, CronJob sẽ tạo ra 1 Job để thực hiện lệnh ``python3 inference.py`.  Để tạo CronJob, chạy lệnh sau:\n$ kubectl create -f cronjob-inference.yaml cronjob.batch/inference-cronjob created Xem thông tin của CronJob vừa tạo:\nkubectl get cronjobs NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE inference-cronjob * * * * * False 0 \u0026lt;none\u0026gt; 19s Bởi vì bản chất của CronJob là tạo ra các Jobs, nên ta hãy xem thử các Jobs được tạo ra sau mỗi phút:\n$ kubectl get jobs --watch NAME COMPLETIONS DURATION AGE inference-cronjob-1611919440 1/1 15s 82s inference-cronjob-1611919500 1/1 14s 21s inference-cronjob-1611919560 0/1 0s inference-cronjob-1611919560 0/1 0s 0s inference-cronjob-1611919560 1/1 15s 15s inference-cronjob-1611919380 1/1 16s 3m21s inference-cronjob-1611919620 0/1 0s inference-cronjob-1611919620 0/1 0s 0s Cờ --watch sẽ cho phép lắng nghe sự kiện có bất kỳ sự thay đổi nào trong việc sử dụng tài nguyên của Job, chẳng hạn như tạo, hủy Job.\nChú ý: Tên của Job = Tên của CronJob + chuỗi số ngẫu nhiên.\nĐể xem các Pods tạo ra bởi Job sau mỗi phút, cần kết hợp với Job tại thời điểm đó. Ví dụ xem logs của Job inference-cronjob-1611919380:\nkubectl get pods --selector=job-name=inference-cronjob-1611919380 NAME READY STATUS RESTARTS AGE inference-cronjob-1611919380-cqdtd 0/1 Completed 0 78s Có Pod rồi, ta có thể xem logs tạo ra bởi Pod đó:\n$ kubectl logs inference-cronjob-1611919380-cqdtd Running inference... Loading data... Loading model from: /docker/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.21374322 31.94786177 10.40175849 34.31050209 22.05210667 11.58265489 13.19650094 42.84036647 33.03218733 15.77635169 23.93521876 19.85532224 25.43466604 20.55132127 13.67707622 47.44313586 17.6460682 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.77877263 30.23411048 19.67713185 23.19380271 24.96712102 18.65459129 30.35476911 8.9560549 13.8130382 14.18848318 17.3840622 19.83840166 24.09904134 20.52649052 15.32433651 25.8157052 16.47533793 19.2214524 19.86928427 21.47113681 21.56443118 24.64517965 22.43665872 22.1020877 ] Như ta thấy, Batch Inference đã được thực hiện thành công thông qua CronJob.\nCuối cùng, xóa CronJob khi không sử dụng nữa:\n$ kubectl delete inference-cronjob cronjob.batch/inference-cronjob deleted 3. Kết luận\nXong, mình đã cũng nhau tìm hiểu và sử dụng CronJob để thực hiện nhiệm vụ Batch Inference trong bài toán AI. Mình đặt lịch chạy Job mỗi phút mục đích là để nhanh chóng nhìn thấy kết quả cho lần demo này. Tùy theo yêu cầu thực tế bài toán, các bạn có thể set giá trị phù hợp cho mình.\nBài viết tiếp theo, chúng ta sẽ tìm hiểu và thực hành với Development. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n4. Tham khảo\n Mlinproduction CronJob Crontab  ","permalink":"https://tiensu.github.io/blog/44_kubernetes_cronjob/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 3: Kubernetes CronJob"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Trong bài trước, chúng ta đã tìm hiểu về Pod, cách tương tác với Pod và hạn chế của nó. Bài này, chúng ta sẽ làm việc với một thành phần ở mức high level hơn của Kubernetes, đó là Job. Cụ thể, mình sẽ cùng nhau tạo ra các Job để train model và thực hiện Batch Inference.\n1. Kubernetes Job là gì?\nTheo định nghĩa từ trang chủ của Kubernetes thì:\nA Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.\nHiểu một cách đơn giản thì Jobs chịu trách nhiệm quản lý một hoặc nhiều Pods để thực hiện một công việc nào đó. Trong quá trình làm việc, các Pods có thể chạy song song với nhau, và nếu một Pod bị chết thì Job sẽ tạo ra một Pod khác để thay thể. Job chỉ được coi là hoàn thành thì tất cả các Pod của nó hoàn thành. Khi xóa Job, các Pods được quản lý bởi nó cũng bị xóa theo.\nJob rất phù hợp để chạy các tác vụ kiểu Batch, tức là các tác vụ mà chạy trong một khoảng thời gian nào đó rồi kết thúc. Trong AI, có khá nhiều tác vụ kiểu như vậy, có thể kể ra như Feature Engineering, Cross-Validation, Model Training, Batch Inference. Ví dụ, chúng ta tạo ra một Job để train một model, sau đó lưu model đó vào Storage. Một Job khác sẽ sử dụng model đó để thực hiện Batch Inference.\n2. Sử dụng Job cho các tác vụ AI\nChúng ta sẽ thử tạo 2 Jobs:\n Job thứ nhất để train ML model, lưu model ra file trên AWS S3. Job thứ hai sử dụng model đã trained để thực hiện Batch Inference.  Hãy xem cấu trúc thư mục làm việc:\nkubernetes_job │ ├── docker │ │ ├── batch_inference.py │ │ ├── Dockerfile │ │ └── train.py │ └── job │ ├── job-inference.yaml │ └── job-train.yaml 2.1 Code train \u0026amp; inference model\nTạo thư mục docker và copy 2 file train.py và batch_inference.py đã sử dụng trong các bài trước vào thư mục vừa tạo. Sử a lại nội dung của file train.py như sau:\nimport json import os import boto3 from joblib import dump import matplotlib.pyplot as plt import numpy as np from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] BUCKET_NAME = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Load data print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() print(\u0026#34;Splitting data...\u0026#34;) X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] # ############################################################################# # Fit regression model print(\u0026#34;Fitting model...\u0026#34;) params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) train_mse = mean_squared_error(y_train, clf.predict(X_train)) test_mse = mean_squared_error(y_test, clf.predict(X_test)) metadata = { \u0026#34;train_mean_square_error\u0026#34;: train_mse, \u0026#34;test_mean_square_error\u0026#34;: test_mse } print(\u0026#34;Serializing model to: {}\u0026#34;.format(MODEL_PATH)) dump(clf, MODEL_PATH) print(\u0026#34;Serializing metadata to: {}\u0026#34;.format(METADATA_PATH)) with open(METADATA_PATH, \u0026#39;w\u0026#39;) as outfile: json.dump(metadata, outfile) print(\u0026#34;Moving to S3\u0026#34;) s3 = boto3.client(\u0026#39;s3\u0026#39;) s3.upload_file(MODEL_PATH, BUCKET_NAME, MODEL_FILE) Sửa lại code của file batch_inference.py như sau:\nimport os import boto3 from joblib import load import numpy as np from sklearn import datasets from sklearn.utils import shuffle MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] BUCKET_NAME = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) def load_model(): s3 = boto3.resource(\u0026#39;s3\u0026#39;) try: s3.Bucket(BUCKET_NAME).download_file(MODEL_FILE, MODEL_PATH) except Exception as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] == \u0026#34;404\u0026#34;: print(\u0026#34;The object does not exist.\u0026#34;) else: raise return load(MODEL_PATH) def get_data(): \u0026#34;\u0026#34;\u0026#34; Return data for inference. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] return X_test, y_test print(\u0026#34;Running inference...\u0026#34;) X, y = get_data() # ############################################################################# # Load model print(\u0026#34;Loading model from: {}\u0026#34;.format(MODEL_PATH)) clf = load_model() # ############################################################################# # Run inference print(\u0026#34;Scoring observations...\u0026#34;) y_pred = clf.predict(X) print(y_pred) 2.2 Tạo Docker Images\n Tạo file Dokerfile  Cũng trong cùng thư mục docker, tạo file Dockerfile với nội dung như sau:\nFROM jupyter/scipy-notebook USER root WORKDIR /docker ADD . /docker RUN pip install awscli joblib boto3 RUN mkdir /docker/model # Env variables ENV MODEL_DIR=/docker/model ENV MODEL_FILE=clf.joblib ENV METADATA_FILE=metadata.json ENV BUCKET_NAME=kubernetes-job  Build Docker Image này:  $ docker build -t docker-ml . Sending build context to Docker daemon 6.656kB Step 1/10 : FROM jupyter/scipy-notebook ---\u0026gt; c1a7c7ef5e27 Step 2/10 : USER root ---\u0026gt; Using cache ---\u0026gt; 0c1dbc43bef8 Step 3/10 : WORKDIR /docker ---\u0026gt; Running in fdae735976d0 Removing intermediate container fdae735976d0 ---\u0026gt; b795fe3bbd80 Step 4/10 : ADD . /docker ---\u0026gt; 16082b6c9bda Step 5/10 : RUN pip install awscli joblib boto3 ---\u0026gt; Running in e4f9036ae9fc Collecting awscli Downloading awscli-1.18.221-py2.py3-none-any.whl (3.5 MB) Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (1.0.0) Collecting boto3 Downloading boto3-1.16.61-py2.py3-none-any.whl (130 kB) Collecting s3transfer\u0026lt;0.4.0,\u0026gt;=0.3.0 Downloading s3transfer-0.3.4-py2.py3-none-any.whl (69 kB) Collecting botocore==1.19.61 Downloading botocore-1.19.61-py2.py3-none-any.whl (7.2 MB) Collecting PyYAML\u0026lt;5.4,\u0026gt;=3.10 Downloading PyYAML-5.3.1.tar.gz (269 kB) Collecting colorama\u0026lt;0.4.4,\u0026gt;=0.2.5 Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB) Collecting rsa\u0026lt;=4.5.0,\u0026gt;=3.1.2 Downloading rsa-4.5-py2.py3-none-any.whl (36 kB) Collecting docutils\u0026lt;0.16,\u0026gt;=0.10 Downloading docutils-0.15.2-py3-none-any.whl (547 kB) Collecting jmespath\u0026lt;1.0.0,\u0026gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Requirement already satisfied: urllib3\u0026lt;1.27,\u0026gt;=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore==1.19.61-\u0026gt;awscli) (1.26.3) Requirement already satisfied: python-dateutil\u0026lt;3.0.0,\u0026gt;=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore==1.19.61-\u0026gt;awscli) (2.8.1) Requirement already satisfied: six\u0026gt;=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil\u0026lt;3.0.0,\u0026gt;=2.1-\u0026gt;botocore==1.19.61-\u0026gt;awscli) (1.15.0) Collecting pyasn1\u0026gt;=0.1.3 Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB) Building wheels for collected packages: PyYAML Building wheel for PyYAML (setup.py): started Building wheel for PyYAML (setup.py): finished with status \u0026#39;done\u0026#39; Created wheel for PyYAML: filename=PyYAML-5.3.1-cp38-cp38-linux_x86_64.whl size=44618 sha256=421030371a2f82fdfd722d0b032ce5b0c8d01e02a5ca379c9a0e2eea3a03fd78 Stored in directory: /tmp/pip-ephem-wheel-cache-cs65titp/wheels/13/90/db/290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c Successfully built PyYAML Installing collected packages: jmespath, pyasn1, botocore, s3transfer, rsa, PyYAML, docutils, colorama, boto3, awscli Attempting uninstall: PyYAML Found existing installation: PyYAML 5.4.1 Uninstalling PyYAML-5.4.1: Successfully uninstalled PyYAML-5.4.1 Successfully installed PyYAML-5.3.1 awscli-1.18.221 boto3-1.16.61 botocore-1.19.61 colorama-0.4.3 docutils-0.15.2 jmespath-0.10.0 pyasn1-0.4.8 rsa-4.5 s3transfer-0.3.4 Removing intermediate container e4f9036ae9fc ---\u0026gt; 0f7e7ec1c6a0 Step 6/10 : RUN mkdir /docker/model ---\u0026gt; Running in 7f4c3bd5253a Removing intermediate container 7f4c3bd5253a ---\u0026gt; 0b2c845fbb40 Step 7/10 : ENV MODEL_DIR=/docker/model ---\u0026gt; Running in 643ef25a50eb Removing intermediate container 643ef25a50eb ---\u0026gt; 339c22e0a4f9 Step 8/10 : ENV MODEL_FILE=clf.joblib ---\u0026gt; Running in d81f810dc092 Removing intermediate container d81f810dc092 ---\u0026gt; cd7ecdc2f380 Step 9/10 : ENV METADATA_FILE=metadata.json ---\u0026gt; Running in 701460e9b463 Removing intermediate container 701460e9b463 ---\u0026gt; 7646e477d5a9 Step 10/10 : ENV BUCKET_NAME=kubernetes-job ---\u0026gt; Running in ce92e3cdbc3b Removing intermediate container ce92e3cdbc3b ---\u0026gt; 31d25c8be720 Successfully built 31d25c8be720 Successfully tagged docker-ml:latest  Push Docker Image lên Docker Hub:  Sử dụng các lệnh sau để push Docker Image vừa build lên Docker Hub\n$ docker tag docker-ml:latest tiensu/docker-ml:latest $ docker push tiensu/docker-ml:latest The push refers to repository [docker.io/tiensu/docker-ml] 76fba3826ca9: Pushed 59928edb97b5: Pushed b5e012598fbb: Pushed c4e3257e6eb5: Pushed 5f70bf18a086: Mounted from tiensu/ml-model-batch-infer 6f5a41ae77fd: Mounted from tiensu/ml-model-batch-infer 5a1b9a3f9355: Mounted from tiensu/ml-model-batch-infer b1d7816bac14: Mounted from tiensu/ml-model-batch-infer c91fed2d1998: Mounted from tiensu/ml-model-batch-infer cc70098d00e3: Mounted from tiensu/ml-model-batch-infer 88727e93cbac: Mounted from tiensu/ml-model-batch-infer cadaf24035f3: Mounted from tiensu/ml-model-batch-infer 8f170f4774e3: Mounted from tiensu/ml-model-batch-infer 33bd52db887f: Mounted from tiensu/ml-model-batch-infer 21e5dd010f50: Mounted from tiensu/ml-model-batch-infer ea370ab22368: Mounted from tiensu/ml-model-batch-infer 421d1408f872: Mounted from tiensu/ml-model-batch-infer 18fd1ca0de51: Mounted from tiensu/ml-model-batch-infer 8f01aab6d756: Mounted from tiensu/ml-model-batch-infer e18a1c4e1d31: Mounted from tiensu/ml-model-batch-infer 8552f27c3cd8: Mounted from tiensu/ml-model-batch-infer 1a4c57efcc23: Mounted from tiensu/ml-model-batch-infer 94b8fe888eac: Mounted from tiensu/ml-model-batch-infer 02473afd360b: Mounted from tiensu/ml-model-batch-infer dbf2c0f42a39: Mounted from tiensu/ml-model-batch-infer 9f32931c9d28: Mounted from tiensu/ml-model-batch-infer latest: digest: sha256:40678bdd8d763129322db38be9f83bc70d1278b7836c7c7f4f4ac3ef6af20e5e size: 6582 2.3 Tạo Kubernetes Job để train ML model\nTương tự như tạo Pod, để tạo Job ta cũng cần khai báo các thông tin cần thiết trong file cấu hình job-train.yaml:\napiVersion: batch/v1 kind: Job metadata: name: job-train-ml-model spec: template: spec: containers: - name: train-container imagePullPolicy: Always image: tiensu/docker-ml:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;train.py\u0026#34;] env: - name: AWS_ACCESS_KEY_ID value: \u0026#34;\u0026#34; - name: AWS_SECRET_ACCESS_KEY value: \u0026#34;\u0026#34; restartPolicy: Never backoffLimit: 0 Một số thông tin như sau:\n apiVersion: Phiên bản của Kubernetes API. kind: Loại tài nguyên của Kubernetes cần tạo, ở đây là Job. metadata: Danh sách các nhãn, các thuộc tính tùy ý mà người phát triển có thể gắn cho Job. Thường các thông tin về Metadata của ML model được gắn ở đây. Kubernetes cũng khuyến nghị một số nhãn ở đây. spec.template: Chính là phần cấu hình của Pod mà ta cần khai báo, tương tự như cấu hình của Pod mà ta đã tạo ở bài trước.  imagePullPolicy: Cho phép Kubernetes luôn luôn sử dụng Docker Image từ Docker Hub thay vì Cache Image. env: Danh sách các biến môi trường để Pod sử dụng. Ở đây, chúng ta khai bào 2 biến liên quan đến AWS để làm việc với AWS S3. Mình đã xóa các key mà mình sử dụng. Nếu bạn muốn chạy thử thì hãy thêm key của bạn vào nhé!   restartPolicy: Có khởi động lại Container khi nó bị chết hay không? backoffLimit: Số lần cố gắng thực hiện lại Job khi nó bị thất bị.  Chạy lệnh sau để tạo và kiểm tra trạng thái của Job:\n$ kubectl create -f job-train.yaml job.batch/job-train-ml-model created $ kubectl get jobs NAME COMPLETIONS DURATION AGE job-train-ml-model 1/1 58s 2m19s Kiểm tra xem các pods của Job là gì và trạng thái của chúng:\n$ kubectl get pods --selector=job-name=job-train-ml-model NAME READY STATUS RESTARTS AGE job-train-ml-model-6fkcd 0/1 Completed 0 2m19s Xem logs Job/Pod:\n$ kubectl logs job-train-ml-model-6fkcd Loading data... Splitting data... Fitting model... Serializing model to: /docker/model/clf.joblib Serializing metadata to: /docker/model/metadata.json Moving to S3 Như vậy, có thể thấy là Job đã chạy xong, file model đã được lưu trên S3.\nCuối cùng, ta có thể xóa Job sau khi chúng đã hoàn thành nhiệm vụ của mình:\n$ kubectl delete job job-train-ml-model job.batch \u0026#34;job-train-ml-model\u0026#34; deleted 2.4 Tạo Kubernetes Job để thực hiện Batch Inference\nChúng ta sẽ sử dụng lại Docker Image đã tạo ở trên cho Job này.\nFile cấu hình của Job (job-inference.yaml) như sau:\napiVersion: batch/v1 kind: Job metadata: name: job-inference-ml-model spec: template: spec: containers: - name: inference-container imagePullPolicy: Always image: tiensu/docker-ml:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;batch_inference.py\u0026#34;] env: - name: AWS_ACCESS_KEY_ID value: \u0026#34;\u0026#34; - name: AWS_SECRET_ACCESS_KEY value: \u0026#34;\u0026#34; restartPolicy: Never backoffLimit: 0 So với cấu hình của Job phía trên, chỉ có các thông tin sau thay đổi: Job name, container name, container command.\nĐể tạo và liểm tra trạng thái của Job, chạy lệnh sau:\n$ kubectl create -f job-inference.yaml job.batch/job-inference-ml-model created $ kubectl get jobs NAME COMPLETIONS DURATION AGE job-inference-ml-model 1/1 13s 66s Kiểm tra xem các Pods của Job và trạng thái tương ứng:\n$ kubectl get pods --selector=job-name=job-inference-ml-model NAME READY STATUS RESTARTS AGE job-inference-ml-model-sk2m4 0/1 Completed 0 2m11s Chú ý: Tên của Pod = Tên của Job + chuỗi ngẫu nhiên.\nXem logs của Job/Pod:\n$ kubectl logs job-inference-ml-model-sk2m4 Running inference... Loading data... Loading model from: /docker/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.21374322 31.94786177 10.40175849 34.31050209 22.05210667 11.58265489 13.19650094 42.84036647 33.03218733 15.77635169 23.93521876 19.85532224 25.43466604 20.55132127 13.67707622 47.44313586 17.6460682 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.77877263 30.23411048 19.67713185 23.19380271 24.96712102 18.65459129 30.35476911 8.9560549 13.8130382 14.18848318 17.3840622 19.83840166 24.09904134 20.52649052 15.32433651 25.8157052 16.47533793 19.2214524 19.86928427 21.47113681 21.56443118 24.64517965 22.43665872 22.1020877 ] Như vậy là Job đã thực hiện Batch Inference thành công bằng model nhận được từ S3.\nCuối cùng, xóa Job sau khi nó đã hoàn thành nhiệm vụ để tiết kiệm tài nguyên server:\n$ kubectl delete job job-inference-ml-model job.batch \u0026#34;job-inference-ml-model\u0026#34; deleted 3. Kết luận\nNhư vậy là mình đã cùng các bạn tìm hiểu và sử dụng Kubernetes Job để thực hiện các tác vụ của một bài toán AI. Có một lưu ý dành cho các bạn đó là trong trường hợp việc thực hiện tạo Job thất bại, hãy nhớ sử dụng lệnh kubectl describe pod \u0026lt;pod_name\u0026gt;, trong đó pod_name là tên Pod của Job để xem đầy đủ logs. Dựa vào logs này, các bạn có thể dễ dàng phát hiện ra nguyên nhân lỗi và cách khắc phục chúng.\nbài viết tiếp theo, chúng ta sẽ tìm hiểu và thực hành với CronJob. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n4. Tham khảo\n Mlinproduction Kubernetes Jobs  ","permalink":"https://tiensu.github.io/blog/43_kubernetes_job/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 2: Kubernetes Job"},{"categories":["MLOps","Kubernetes","Docker"],"contents":"Trong các bài viết trước, mình đã giới thiệu về Docker, sử dụng kết hợp với Nginx, uWSGI, Flask để deploy model trong môi trường production. Nhìn chung mà nói, cách kết hợp 4 dịch vụ này đủ để áp ứng cho hầu hết các bài toán AI, ngoại trừ vấn đề cấu hình tương đối phức tạp và khó triển khai trên cloud (thực tế là AWS và GCP đề không hỗ trợ cách này, nếu muốn chúng ta vẫn phải cấu hình bằng tay như dưới local).\nGần đây, Kubernetes nổi lên như là một xu hướng mới, đáp ứng đầy đủ các yêu cầu của việc triển khai model trong môi trường production. Hơn thế nữa, việc cấu hình rất đơn giản và được hỗ trợ bởi các ông lớn cloud (AWS và GCP đều có dịch vụ Kubernetes). Trong loạt bài tiếp theo, mình sẽ cùng mọi người tìm hiểu về hot trend này và cách thức sử dụng nó để deploy các AI model của chúng ta nhé!\n1. Kubernetes là gì?\nTheo định nghĩa từ trang chủ của Kubernetes thì:\nKubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.\nPhần tử hạt nhân của Kubernetes chính là các Containers (Docker Container). Nói theo một cách khác, Kubernetes giúp chúng ta:\n Manage containers: Quản lý đồng thời nhiều docker containers của cùng một ứng dụng hoặc thậm chí là nhiều ứng dụng khác nhau. Manage lifecycle: Quản lý toàn bộ lifecycle của các containers, từ lúc được tạo ra đến khi bị xóa bỏ. Hardware optimization: Tối ưu hóa, tối đa hóa khả năng của phần cứng thiết bị. Schedule: Lập lịch khi nào cần bật/tắt containers. Load balancer: Phân tải xử lý đều cho các containers. Backup: Khi một container chết, sẽ có một container khác đươc tạo để thay thế. Scalling: Dễ dàng scale các containers up/down theo một trong 2 chế độ: tự động hoặc bằng tay. Monitor system: Dễ dàng giám sát hoạt động của toàn bộ hệ thống.  Việc cấu hình cho Kubernetes khá đơn giản, tất cả chỉ thông qua một file cấu hình duy nhất.\nTrong tiếng Hy Lạp, Kubernetes có nghĩa là người chỉ huy hay thuyền trưởng.\nTất cả những đặc điểm trên đều phù hợp với giải pháp mà chúng ta tìm kiếm để đưa AI model vào môi trường production. Tất nhiên là phạm vi ứng dụng của Kubernetes còn rộng lớn hơn rất nhiều, nhưng trong lĩnh vực làm việc và nghiên cứu của mình, mình chỉ tập trung tìm hiểu và sử dụng Kubernetes cho các bài toán về AI.\n2. Kiến trúc và thành phần của Kubernetes\n Kubernetes bao gòm các Nodes, được chia thành 2 loại: Master Node và Worker Nodes. Master Nodes chịu trách nhiệm quản lý các Worker Nodes, trong khi các Worker Nodes làm nhiệm vụ thực hiện các công việc tính toán, \u0026hellip; Mỗi Worker Node lại được chia nhỏ thành các Pods, và trong mỗi Pod chính là các Containers.\nPhần còn lại của bài hôm nay, mình sẽ cùng các bạn tìm hiểu về Pod. Các bài tiếp theo, chúng ta sẽ làm việc với Job, CronJob, Deployment, Service.\n3. Kubernetes Pod\n3.1 Kubernetes Pod là gì?\nTheo định nghĩa, Pod là đối tượng nhỏ nhất có khả năng triển khai trong kiến trúc của Kubernetes, tức là bạn có thể tạo, sử dụng, hay xóa Pod. Có thể coi Pod chính là đại diện của một ứng dụng (instance application) chạy trong Kubernetes.\nNhư đã nói ở phần 2, mỗi Pod chứa một hoặc nhiều Containers để thực hiện một công việc (Job) nào đó. Các Containers trong cùng Pod nằm trong cùng một mạng local và chia sẻ tài nguyên sử dụng với nhau. Chính vì thế mà chúng dễ dàng giao tiếp và làm việc với nhau.\nVì Pod là Single Instance của ứng dụng chạy trong Kebernetes, số lượng Pod được tạo ra hay xóa đi một cách tự động (Load Balancing \u0026amp; Failure Recovery), tùy theo tải mà ứng dụng phải phục vụ.\n3.2 Tạo Pod từ Docker Image có sẵn\nĐể làm việc được với Pod, trước tiên cần phải cài đặt kubectl theo hướng dẫn trên trang chủ của Kubernetes tại đây hoặc tại đây\nCách dễ nhất để tạo và triển khai Kubernetes là sử dụng config file. File này sẽ chỉ định đối tượng được tạo là gì, các metadata gắn với đối tượng đó, tài nguyên cần thiết là bao nhiêu, \u0026hellip;\nDưới đây là template của config file (pod_public.yaml) để tạo một Pod:\napiVersion: v1 kind: Pod metadata: name: python3-pod labels: app: python3 spec: containers: - name: python3-container image: python:3.6 command: [\u0026#39;python3\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;print(\u0026#34;Hello, World!\u0026#34;)\u0026#39;] restartPolicy: Never File config này bao gồm những thông tin sau:\n apiVersion: Phiên bản của Kubernetes API đang sử dụng. kind: Loại tài nguyên (đối tượng) của Kubernetes được tạo ra: Pod, Job, Development, \u0026hellip; Ở đây là Pod object. metadata: Là một tập hợp các labels và các thuộc tính của model mà người phát triển có thể thêm vào tùy ý giống như phiên bản, độ chính xác, thuật toán, \u0026hellip; spec: Bao gồm thông tin của các Containers chạy bên trong Pod: tên, docker image, command. Như trong cấu hình hiện tại thì chỉ có 1 Container. restartPolicy: Cho phép Container có restart hay không khi nó bị lỗi. Giá trị never ở đây tức là không cho phép restart.  Thực hiên lệnh sau để tạo Pod:\n$ kubectl create -f pod_public.yaml pod \u0026#34;python3-pod\u0026#34; created Kiểm tra trạng thái của pod vừa tạo:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE python3-pod 0/1 Completed 0 3s Xem log của pod vừa tạo:\n$ kubectl logs python3-pod Hello, World! Xóa pod vừa tạo:\n$ kubectl delete -f pod_public.yaml pod \u0026#34;python3-pod\u0026#34; deleted 3.3 Tạo Pod từ Docker Image tự tạo\nMình sẽ sử dụng Docker Image đã tạo từ bài này để đưa vào Pod.\nTrước tiên, bạn hãy login vào Docker Hub để tạo một Repository. Giả sử mình tạo Repository tên là ml-model-batch-infer.\nở máy local, thực hiện các bước sau để đưa Docker Image đã tạo lên Repository:\n Login vào Docker Hub  $ docker login -u tiensu Trong đó, tiensu là tên đăng nhập của mình, bạn hãy thay bằng tên đăng nhập của bạn. Nhập mật khẩu khi được hỏi.\n Gán Tag cho Docker Image theo tên mới trên Repository  docker tag docker-model-batch-infer:latest tiensu/ml-model-batch-refer:latest  Push Docker Image đã gắn Tag lên Repository  docker push tiensu/ml-model-batch-infer:latest Output:\nThe push refers to repository [docker.io/tiensu/ml-model-batch-infer] 2a0a8f09fca2: Pushed ea3e588d9e9f: Pushed 2b8e8179f02d: Pushed 254c54a05297: Pushed bdeb303132f3: Pushed 5f70bf18a086: Mounted from jupyter/scipy-notebook 6f5a41ae77fd: Mounted from jupyter/scipy-notebook 5a1b9a3f9355: Mounted from jupyter/scipy-notebook b1d7816bac14: Mounted from jupyter/scipy-notebook c91fed2d1998: Mounted from jupyter/scipy-notebook cc70098d00e3: Mounted from jupyter/scipy-notebook 88727e93cbac: Mounted from jupyter/scipy-notebook cadaf24035f3: Mounted from jupyter/scipy-notebook 8f170f4774e3: Mounted from jupyter/scipy-notebook 33bd52db887f: Mounted from jupyter/scipy-notebook 21e5dd010f50: Mounted from jupyter/scipy-notebook ea370ab22368: Mounted from jupyter/scipy-notebook 421d1408f872: Mounted from jupyter/scipy-notebook 18fd1ca0de51: Mounted from jupyter/scipy-notebook 8f01aab6d756: Mounted from jupyter/scipy-notebook e18a1c4e1d31: Mounted from jupyter/scipy-notebook 8552f27c3cd8: Mounted from jupyter/scipy-notebook 1a4c57efcc23: Mounted from jupyter/scipy-notebook 94b8fe888eac: Mounted from jupyter/scipy-notebook 02473afd360b: Mounted from jupyter/scipy-notebook dbf2c0f42a39: Mounted from jupyter/scipy-notebook 9f32931c9d28: Mounted from jupyter/scipy-notebook latest: digest: sha256:2552cb24c104d9b4fe3a43cc952371a7a1b0cce84e1c95821622b4fe508a6877 size: 6786 Để tạo Pod với Docker Image này, cập nhật lại file config (đổi tên thành pod_custom.yaml) của Pod như sau:\napiVersion: v1 kind: Pod metadata: name: pod-ml-model-batch-infer labels: app: python3 spec: containers: - name: container-ml-model-batch-infer image: tiensu/ml-model-batch-infer:latest command: [\u0026#39;python3\u0026#39;, \u0026#39;batch_inference.py\u0026#39;] restartPolicy: Never Chạy lệnh sau để tạo Pod:\n$ kubectl create -f pod_custom.yaml pod/pod-ml-model-batch-infer created Kiểm tra trạng thái của Pod vừa tạo:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE command-demo 0/1 Completed 0 113m pod-ml-model-batch-infer 0/1 Completed 0 5m28s python3-pod 0/1 Completed 0 90m Chú ý là Docker Image của chúng ta được tải về trên Worker Node. Bạn có thể kiểm tra trên đó bằng lệnh $ docker ps.\nChúng ta có thể xem miêu tả chi tiết quá trình tạo Pod như sau:\n$ kubectl describe pod pod-ml-model-batch-infer Name: pod-ml-model-batch-infer Namespace: default Priority: 0 Node: duynm-vostro-3670/10.1.34.169 Start Time: Wed, 27 Jan 2021 16:44:15 +0700 Labels: app=python3 Annotations: cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs: Status: Succeeded IP: 192.168.24.198 IPs: IP: 192.168.24.198 Containers: container-ml-model-batch-infer: Container ID: docker://535749cae10e6dd605030b6d84ba978cc245bfd44bb6981d3307a3ffa8a5bf94 Image: tiensu/ml-model-batch-infer:latest Image ID: docker-pullable://tiensu/ml-model-batch-infer@sha256:2552cb24c104d9b4fe3a43cc952371a7a1b0cce84e1c95821622b4fe508a6877 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: python3 batch_inference.py State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 27 Jan 2021 16:44:24 +0700 Finished: Wed, 27 Jan 2021 16:44:24 +0700 Ready: False Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-gmswp (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: default-token-gmswp: Type: Secret (a volume populated by a Secret) SecretName: default-token-gmswp Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m18s default-scheduler Successfully assigned default/pod-ml-model-batch-infer to duynm-vostro-3670 Normal Pulling 4m15s kubelet Pulling image \u0026#34;tiensu/ml-model-batch-infer:latest\u0026#34; Normal Pulled 4m11s kubelet Successfully pulled image \u0026#34;tiensu/ml-model-batch-infer:latest\u0026#34; in 4.353859959s Normal Created 4m9s kubelet Created container container-ml-model-batch-infer Normal Started 4m9s kubelet Started container container-ml-model-batch-infer Cuối cùng, hãy xem log tạo ra khi thực hiên Inference:\n$ kubectl logs pod-ml-model-batch-infer Running inference... Loading data... Loading model from: /code/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.20025598 31.94786177 10.42732759 34.12058193 22.05210667 11.58265489 13.1649368 42.84036647 33.03218733 15.77635169 23.93521876 19.91587166 25.43466604 20.55132127 13.65254047 47.47279364 17.58214889 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.65978361 30.18436261 19.67895051 23.22841646 24.94197905 18.65459129 30.19731636 8.9560549 13.8130382 14.23277857 17.3840622 19.83840166 24.91315811 20.44991809 15.32433651 25.8157052 16.47533793 19.2214524 19.87110293 21.47113681 21.56443118 24.64517965 22.43665872 22.18289286] 7. Kết luận\nMặc dù Pod là đối tượng quan trọng, không thể thiếu trong bất kỳ kiến trúc Kubernetes nào nhưng các Best Practice đều không khuyến khích việc sử dung nó một cách trực tiếp, mà nên được triển khai cùng với các đối tượng khác ở mức cao hơn của Kubernetes để quản lý nó, như Job chẳng hạn. Job sẽ tạo ra một hoặc nhiều Pods, và khi một Pod bị chết thì Pod khác sẽ được bật lên để sẵn sàng thay thế cho nó.\nChúng ta sẽ tìm hiểu vấn đề này trong bài viết tiếp theo. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n8. Tham khảo\n Mlinproduction Docker Hub  ","permalink":"https://tiensu.github.io/blog/42_kubernetes_in_ai/","tags":["MLOps","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 1: Kubernetes Pod"},{"categories":["MLOps"],"contents":"Hẳn các bạn đã biết, trong hầu hết các bài toán AI, chúng ta không chỉ train model 1 lần rồi thôi (mình không nói đến việc thử-sai trong quá trình tuning model). Tại thời điểm này, model hoạt động tốt đúng như những gì ta mong đợi, nhưng sau một thời gian, hiệu năng của model có thể giảm xuống. Đó là một trong những dấu hiệu chỉ ra rằng ta phải retrain lại model. Trong bài hôm nay, mình sẽ cùng các bạn tìm hiểu chi tiết hơn về vấn đề này.\n1. Model Drift\nModel Drift là khái niệm mô tả hiện tượng hiệu năng dự đoán của model suy giảm theo thời gian do có sự thay đổi của môi trường làm sai lệch các giả thiết ban đầu của model. Thuật ngữ Model Drift (model chuyển dịch) có thể khiến chúng ta hơi bối rối 1 chút, vì bản chất là model không thay đổi, chỉ có các yếu tố môi trường bên ngoài thay đổi, input data thay đổi.\n2. Làm sao để nhận biệt hiện tượng Model Drift\n2.1 Kiểm tra độ chính xác của model\nBiểu hiên trực tiếp và rõ ràng nhất của Model Drift là độ chính xác dự đoán (độ chính xác ở đây dùng chung cho tất cả các metrics đánh giá model) giảm dần theo thời gian. Nhưng việc giám sát việc này không phải lúc nào cũng đơn giản bởi vì ta phải có cả kết quả dự đoán của model và ground truth, đặc biệt khi model đang chạy trong sản phầm thực tế (môi trường production hay online).\nCó một cách đơn giản hơn để kiểm tra độ chính xác của model có bị suy giảm hay không, đó là offline monitor. Cách này được thực hiện trước khi model triển khai model vào môi trường production. Giả sử ra có dữ liệu từ 01/2019 đến 01/2021. Ta sẽ sử dụng dữ liệu từ 01/2019 đến 06/2020 đê train và đánh giá model, sau đó sử dụng model này để dự đoán trên dữ liệu tháng 07/2020 đến 01/2021. Kết quả dự đoán được lưu lại để đánh giá xem độ chính xác của model có suy giảm hay không, nếu có thì mức độ suy giảm như thế nào? \u0026hellip; Sử dụng cách này cho phép chúng ta ước lượng được tốc độ suy giảm độ chính xác, từ đó lên kế hoạch retrain lại model.\n2.2 Kiểm tra phân bố của dữ liệu\nNếu phân bố của dữ liệu mới có sự sai khác so với dữ liệu huấn luyện model từ ban đầu thì độ chính xác của model cũng sẽ giảm. Vì thế, đây cũng là một dấu hiệu nhận biết sớm của hiện tượng Model Drift.\nĐể đánh giá sự phân bố của dữ liệu, có thể dựa vào các yếu tố sau:\n Phạm vi giá trị của các features Đồ thị histogram của các features Các features có được cho phép nhận giá trị NULL hay không? \u0026hellip;  Facets là một công cụ cho phép chúng ta nhanh chóng nhận ra sự thay đổi trong phân bố dữ liệu dựa trên sự quan sát các đồ thị phân bố trên dashboards. Việc theo dõi này có thể được thực hiện một cách tự động và nó sẽ gửi thống báo cho chúng ta khi sự phân bố dữ liệu thay đổi vượt quá một ngưỡng nào đó.\n2.3 Kiểm tra sự tương quan giữa các features trong dữ liệu\nMối qua hệ giữa các features cũng ảnh hướng đến độ chính xác của model. Vì vậy, kiểm tra sự tương quan giữa các features từng đôi một xem chúng thay đổi ra sao cũng là một cách để nhận biết Model Drift.\n3. Hiểu đúng về Model Retraining\nChúng ta đều hiểu rằng Model Retraining tức là training lại model, tạo ra model mới tốt hơn model cũ. Nhưng nếu chỉ chung chung như thế thì có rất nhiều cách để retraining model:\n Thay đổi hyper-parameters Thay đổi thuật toán ML/DL (model algorithm) Thêm/bớt các features \u0026hellip;  Giữa những cách retraining model kể trên, đâu là cách đúng nhất để loại bỏ hiện tượng Model Drift?\nQuay lại khái niệm của Model Drift, đó là hiện tượng độ chính xác của model suy giảm do có sự thay đổi trong phân phối dữ liệu. Vậy ta chỉ cần train lại model trên tập dữ liệu mới và giữ nguyên tất cả những cái khác: hyper-parameters, thuật toán, features, \u0026hellip; Hiểu một cách đơn giản hơn thì tức là ta sẽ không thay đổi dòng code nào cả, chỉ thay đổi nội dung của file chứa dữ liệu mới để train model.\nNói vậy, không có nghĩa là chúng ta bỏ qua hoàn toàn các cách retraing model khác. Nếu bạn có đủ thời gian, công sức, bạn hay các thành viên trong dự án của bạn hoàn toàn có thể thử nghiệm cách retraining model kể trên. Sau đó sử dụng chiến lược A/B Test để đánh giá các models dựa trên các tiêu chí của bài toán. Model nào cho cho kết quả tốt hơn thì sẽ được sử dụng trong môi trường production.\n4. Tần suất Retrain Model\nMột vấn đề tiếp theo cần quan tâm là tần suất retrain model như thế nào là hợp lý?\nCâu trả lời là không có một quy định, quy tắc cụ thể nào cả. Tùy từng bài toán mà ta có cách xử lý khác nhau.\n Retrain model tại một thời điểm cố định nếu ta biết trước chính xác thời điểm dữ liệu có sự thay đổi lớn. VD: tại các trường đại học, đầu mỗi năm học đều có số lượng lớn sinh viên nhập học và ra trường thì ta nên retrain lại model tại thời điểm đó. Retrain model khi thu thập được đủ một lượng dữ liệu nhất định Retrain model khi các metrics mà ta theo dõi (như đề cập trong mục 2) thay đổi vượt quá một ngưỡng nào đó.  Đối với cách thứ 2\u0026amp;3, cần phải có một hạ tầng độc để giám sát và đưa ra cảnh báo khi sự thay đổi đạt đến mức quy định. Việc chọn ngưỡng cho các metrics cũng cần phải xem xét cẩn thận. Ngưỡng quá thấp sẽ làm cho tần suất retrain model thường xuyên hơn, dẫn đến tốn kém chi phí tính toán (đặc biệt quan trong trường trường hợp sử dụng tài nguyên trên cloud). Ngưỡng quá cao làm cho model không thay đổi kịp với sự thay đổi của môi trường, dẫn đến không tối ưu hóa lợi nhuận, \u0026hellip;\nĐặc biệt trong trường hợp model cần thay đổi realtime mỗi khi có bất cứ dữ liệu mới (VD model dự đoán giao dịch ngân hàng an toàn hay không an toàn) thì nên sử dụng phương pháp học tăng dần, Incremental Learning / Online Learning. Phương pháp này khác các cách Retrain Model đã đề cập ở chỗ model được retrain (cập nhật) chỉ sử dụng dữ liệu mới, không phải retrain trên toàn bộ dữ liệu.\n5. Chạy Retrain Model tự động\nCách cấu hình để Retrain Model tự động liên quan đến tần suất retrain model của bạn.\n  Nếu model được retrained định kỳ, chúng ta có thể sử dụng Kubernetes CronJobs hoặc Jenkins để lập lịch cho model chạy retrain.\n  Nếu model đươc retrained dựa vào trigger khi các metrics thay đổi đến ngưỡng được phát hiện, chúng ta có thể sử dụng Kubernetes Jobs hoặc Jenkins để làm việc này.\n  Cuối cùng, nếu model cần retrain realtime, sử dụng phương pháp Online Learning. River là thư viện lý tưởng cho việc này. Tên cũ của nó là Creme.\n  6. Implement code prototype\n6.1 Query Data by Date Range function\nBởi vì quá trình retraining dựa trên dữ liệu mới, nên chúng ta cần 1 hàm lấy ra những dữ liệu đó, theo 1 khoảng thời gian quy đinh. Dữ liệu mới có thể được lưu ở SQL database, S3, local storage, \u0026hellip;\ndef get_raw_data(end_date, date_window=365): \u0026#39;\u0026#39;\u0026#39; Retrieve all data in date range (end_date - date_window, end_date) \u0026#39;\u0026#39;\u0026#39; Trong đó:\n end_date: ngày cuối cùng trong khoảng thời gian của dữ liệu mới. date_window: số lượng ngày trong khoảng thời gian của dữ liệu mới, tính từ end_date trở lại.  Để nhận dữ liệu mới cho việc retraining model, chúng ta sẽ gọi:\nfrom datetime import date training_data = get_new_data(date.today()) 6.2 Generate a Machine Learning Model function\nHàm này chịu trách nhiệm train AI model: chia dataset thành tập train và tập test, trích xuất vector đặc trừng từ dữ liệu, thực hiện tuning hyper-parameters, huấn luyện model, đánh giá model, \u0026hellip;\nfind_optimal_model(data, ...): \u0026#39;\u0026#39;\u0026#39; Split data, generate features, tune hyper-parameters, train model, ... \u0026#39;\u0026#39;\u0026#39; Tham số data là dữ liệu để huấn luyện mode. Kết quả thực thi của hàm sẽ trả về model đã được trained và các training metrics.\n6.3 Store Trained Model\nMột khi model được trained xong, ta cần lưu nó lại để sử dụng về sau. Cách đơn giản nhất là sử dụng thư viện pickle có sẵn của python. Ngoài ta, bạn cũng có thể sử dụng ONNX hoặc PMML.\n# Serialize and store model on local storage def serialize_model(training_arfifacts): \u0026#39;\u0026#39;\u0026#39; Return a local path to serialized model \u0026#39;\u0026#39;\u0026#39; 6.4 Registry Model\nTham khảo bài Model Registry\n6.5 Model Retraining Enpoint\nTập hợp tất cả các hàm lại trong một script để đơn giản hóa quy trình, retrain.py. Script sẽ chấp nhận một tham số từ command line là end_date, nhận về dữ liệu mới, train mode, store model và registry model.\nfrom datetime import date import sys def retrain(end_date): \u0026#39;\u0026#39;\u0026#39;Model retraining loop.\u0026#39;\u0026#39;\u0026#39; data = get_raw_data(end_date) training_artifacts = find_optimal_model(data, ...) local_path = serialize_model(training_artifacts) model_registry(local_path, training_artifacts) if __name__ == \u0026#39;__main__\u0026#39;: retrain(sys.argv[1]) 6.6 Scheduling the Retraining Procedure\nScript retrain.py lại tiếp tục được đóng gói trong bash script, retrain.sh:\ntoday_date=\u0026#39;date +”%m/%d/%Y”\u0026#39; python retrain.py $today_date Để trigger event gọi đến bash script này, chúng ta có thể lập lịch, sử dụng một trong các công cụ sau:\n Jenkins Airflow Kubernetes Cronjobs Crontab  Các công cụ này đều hỗ trợ đầy đủ việc xử lý ngoại lệ, cơ chế retry, \u0026hellip; Tuy nhiên, viêc thiết lập và cài đặt sẽ tương đối mất thời gian nếu bạn chưa quen thuộc với chúng.\n6.7 Retrieve the Model at Inference Time\nTham khảo bài Model Registry\n7. Kết luận\nBài này, chúng ta đã bàn rất nhiều về Model Retraining. Hi vọng là bạn đã hiểu được phần nào tất cả các khía cạnh của nó để xem xét áp dụng vào dự án của bạn.\nBài viết tiếp theo, chúng ta sẽ cùng tìm hiểu về Kubernetes và áp dụng nó cho các bài toàn AI. Mời các bạn đón đọc!\n8. Tham khảo\n Mlinproduction  ","permalink":"https://tiensu.github.io/blog/41_ai_model_data_driff_and_retraining/","tags":["MLOps","Data Driff","Model Retraining"],"title":"Tìm hiểu về hiện tượng Data Driff và cách cấu hình AI Model Retraining"},{"categories":["MLOps"],"contents":"Khi giải quyết một bài toán AI, rất hiếm khi số lượng model được huấn luyện và đưa vào sử dụng dừng lại ở con số 1. Bởi vì theo thời gian, dữ liệu thay đổi, yêu cầu thay đổi, \u0026hellip; dẫn đến việc chúng ta cần cập nhật lên model mới hơn. Trong trường hợp đó, làm thế nào để quản lý được tất cả các models đó một các hợp lý, đảm bảo sử dụng đúng model mong muốn để thực hiện inference và không làm gián đoạn quá trình inference đang chạy? Đó chính là câu chuyện của Model Registry.\nTrong bài hôm nay, chúng ta sẽ cùng nhau implement một Model Registry đơn giản, sử dụng SQLite. Bạn cũng sử dụng bất kỳ database nào bạn muốn.\n1. Model Registry Database\nĐầu tiên, hãy tạo một database, registry.db, như sau:\nimport sqlite3 conn = sqlite3.connect(\u0026#39;registry.db\u0026#39;) Đối tượng conn tạo ra một kết nối đến registry.db. Chúng ta sẽ sử dụng nó để thực thi các câu lệnh truy vấn sql.\nTiếp theo, tạo bảng model_registry bao gồm các trường thông tin của model.\ncur = conn.cursor() cur.execute(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE model_registry ( id INTEGER PRIMARY KEY ASC, name TEXT UNIQUE NOT NULL, version TEXT NOT NULL, registered_date TEXT DEFAULT CURRENT_TIMESTAMP, metrics TEXT NOT NULL, remote_path TEXT NOT NULL, stage TEXT DEFAULT \u0026#39;DEVELOPMENT\u0026#39; NOT NULL ); \u0026#34;\u0026#34;\u0026#34;) cur.close() 2. Xây dựng Model Registry API\nMục đích của việc xây dựng các API là làm đơn giản hóa quá trình thao tác với database. Tất cả các công việc chung một hành động sẽ được gom vào thành một API.\nChúng ta sẽ xây dựng các API sau:\n Thêm một model mới train vào database. Cập nhật trạng thái của model. Có 2 trạng thái là Development và Production. Lấy tất cả thông tin của model.  Dưới đây là implement các API:\nimport panda as pd class ModelRegistry: def __init__(self, conn, table_name=\u0026#39;model_registry\u0026#39;): self.conn = conn self.table_name = table_name def _insert(self, values): query = \u0026#34;\u0026#34;\u0026#34; INSERT INTO {} (name, version, metrics, remote_path) VALUES (?, ?, ?, ?)\u0026#34;\u0026#34;\u0026#34;.format(self.table_name) self._query(query, values) def _query(self, query, values=None): cur = self.conn.cursor() cur.execute(query, values) cur.close() def publish_model(self, model_name, model_metrics): model_version_query = \u0026#34;\u0026#34;\u0026#34; SELECT version FROM {} WHERE name = \u0026#39;{}\u0026#39; ORDER BY registered_date DESC LIMIT 1 ;\u0026#34;\u0026#34;\u0026#34;.format(self.table_name, model_name) model_version = pd.read_sql_query(model_version_query, conn) if model_version is not None: model_version = int(version.iloc[0][\u0026#39;version\u0026#39;]) model_version = model_version + 1 # Assume that trained models are stored on S3 model_path = \u0026#39;s3://models/{}::v{}\u0026#39;.format(model_name, model_version) self._insert((model_name, model_version, model_metrics, model_path)) def update_stage(self, model_name, model_version, model_stage): query = \u0026#34;\u0026#34;\u0026#34; UPDATE {} SET stage = ? WHERE name = ? AND version = ? ;\u0026#34;\u0026#34;\u0026#34;.format(self.table_name) self._query(query, (model_stage, model_name, model_version)) def get_production_model(self, model_name): query = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM {} WHERE name = \u0026#39;{}\u0026#39; AND stage = \u0026#39;PRODUCTION\u0026#39; ;\u0026#34;\u0026#34;\u0026#34;.format(self.table_name, model_name) return pd.read_sql_query(query, self.conn) Code implement API khá đơn giản, hi vọng bạn có thể hiểu được dễ dàng, :)\n3. Sử dụng Model Registry API\nTrên thực tế , Training và Inference là 2 quá trình cùng chạy đồng thời và Model Registry cung cấp cơ chế trao đổi thông tin giữa 2 quá trình này thông qua database.\nGiả sử rằng chúng ta đã trained xong model thoả mãn yêu cầu đề bài, giờ là lúc ta sử dụng Model Registry API.\nChú ý: Sau mỗi đoạn code ví dụ, ta sẽ sử dụng câu truy vấn sau đây để kiểm tra kết quả:\npd.read_sql_query(\u0026#34;SELECT * FROM model_registry;\u0026#34;, conn) 3.1 Model Training\n Training lần đầu  conn = sqlite3.connect(\u0026#39;registry.db\u0026#39;) model_registry = ModelRegistry(conn=conn) model = None # This would be replaced by the trained model. name = \u0026#39;house_price_prediction\u0026#39; metrics = {\u0026#39;accuracy\u0026#39;: 0.87} model_registry.publish_model(model=model, name=name, metrics=metrics)    id name version registered_data remote_path stage     1 house_price_prediction 1 2021-01-10 12:42:25 s3://models/house_price_prediction::v1 DEVELOPMENT     Training lần thứ 2  model = None # This would be replaced by the trained model. name = \u0026#39;house_price_prediction\u0026#39; metrics = {\u0026#39;accuracy\u0026#39;: 0.89} model_registry.publish_model(model=model, name=name, metrics=metrics)    id name version registered_data remote_path stage     1 house_price_prediction 1 2020-07-12 12:45:27 s3://models/house_price_prediction::v1 DEVELOPMENT   2 house_price_prediction 2 2021-01-10 12:42:25 s3://models/house_price_prediction::v2 DEVELOPMENT    3.2 Chuyển model sang trạng thái sẵn sàng sử dụng cho sản phẩm thực tế\nmodel_registry.update_stage(name=name, version=\u0026#39;2\u0026#39;, stage=\u0026#34;PRODUCTION\u0026#34;)    id name version registered_data remote_path stage     1 house_price_prediction 1 2020-07-12 12:45:27 s3://models/house_price_prediction::v1 DEVELOPMENT   2 house_price_prediction 2 2021-01-10 12:42:25 s3://models/house_price_prediction::v2 PRODUCTION    3.3 Lấy thông tin model\nmodel_registry.get_production_model(name=name)    id name version registered_data remote_path stage     2 house_price_prediction 2 2021-01-10 12:42:25 s3://models/house_price_prediction::v2 PRODUCTION    4. Kết luận\nNhư vậy là chúng ta đã implemented xong Model Register, sử dụng SQLite database. Bạn hoàn toàn có thể áp dụng những gì được trình bày trong bài viết này vào trong dự án của bạn.\nHiện nay cũng có một số open-source giúp bạn thực hiện việc này một cách trực quan hơn. Nổi bật trong số đó là MLflow. Mình sẽ có một bài viết hướng dẫn sử dụng MLflow cho Model Registry trong tương lai.\nBài viết tiếp theo, mình sẽ thảo luận về vấn đề Retraining model. Mời các bạn đón đọc!\n5. Tham khảo\n Mlinproduction MLflow  ","permalink":"https://tiensu.github.io/blog/40_ai_model_registry/","tags":["MLOps"],"title":"Cấu hình AI Model Registry"},{"categories":["MLOps","Docker"],"contents":"Trong bài trước, chúng ta đã tìm hiểu và sử dụng Docker để triển khai AI model theo kiểu online inference. Trong bài này, ta sẽ train một model khác để inference theo kiểu thứ 2, batch inference, sử dụng docker. Mình cũng sẽ thực hiện việc train model bên trong docker luôn (các bài trước đó là train model bên ngoài docker, sau đó chỉ copy model đã train vào trong docker để thực hiện online inference).\n1. Tạo cấu trúc thư mục\nTrước tiên, chúng ta sẽ tạo ra một thư mục để lát nữa khi build docker image, nó sẽ được copy vào trong docker image đó.\ncode ├── Dockerfile ├── batch_inference.py └── train.py Trong đó:\n code: thư mục làm việc chính. Dockerfile: File cấu hình để build docker image. train.py: File code python để thực hiện train model. batch_inference.py: File code python để thực hiện batch inference.  2. Tạo code python để train model và inference data\nLần này, chúng ta sẽ thực hiện train model để dự đoán giá nhà tại Boson, sử dụng tập dữ liệu boson trong thư viện scikit-learn. Đây là kiểu model Leaner Regression.\nTạo file train.py với nội dung như sau:\nimport json import os from joblib import dump import matplotlib.pyplot as plt import numpy as np from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error # ############################################################################# # Load directory paths for persisting model and metadata MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Load and split data print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() print(\u0026#34;Splitting data...\u0026#34;) X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] # ############################################################################# # Fit regression model print(\u0026#34;Fitting model...\u0026#34;) params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) train_mse = mean_squared_error(y_train, clf.predict(X_train)) test_mse = mean_squared_error(y_test, clf.predict(X_test)) metadata = { \u0026#34;train_mean_square_error\u0026#34;: train_mse, \u0026#34;test_mean_square_error\u0026#34;: test_mse } # ############################################################################# # Serialize model and metadata print(\u0026#34;Serializing model to: {}\u0026#34;.format(MODEL_PATH)) dump(clf, MODEL_PATH) print(\u0026#34;Serializing metadata to: {}\u0026#34;.format(METADATA_PATH)) with open(METADATA_PATH, \u0026#39;w\u0026#39;) as outfile: json.dump(metadata, outfile) Tạo file batch_inference.py với nội dung như sau:\nimport os from joblib import load import numpy as np from sklearn import datasets from sklearn.utils import shuffle MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Get a batch of data to inference def get_data(): \u0026#34;\u0026#34;\u0026#34; Return data for inference. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] return X_test, y_test print(\u0026#34;Running inference...\u0026#34;) X, y = get_data() # ############################################################################# # Load model print(\u0026#34;Loading model from: {}\u0026#34;.format(MODEL_PATH)) clf = load(MODEL_PATH) # ############################################################################# # Run inference print(\u0026#34;Scoring observations...\u0026#34;) y_pred = clf.predict(X) print(y_pred) Mình tin chắc các bạn có thể dễ dàng hiểu được đoạn code trên. Ở đây, có một chú ý là mình sử dụng một số biến môi trường MODEL_DIR, MODEL_FILE, \u0026hellip;. Mình không muốn hard-code những biến này vì chúng được sử dụng ở 2 nơi, train và inference model. Thay vào đó, giá trị của chúng sẽ được truyền vào lúc build docker.\n3. Build Docker image để train model\nTạo file Dockerfile với nội dung như sau:\nFROM jupyter/scipy-notebook USER root WORKDIR /code ADD . /code RUN pip install joblib RUN mkdir model # Env variables ENV MODEL_DIR=/code/model ENV MODEL_FILE=clf.joblib ENV METADATA_FILE=metadata.json # COPY train.py ./train.py # COPY batch_inference.py ./batch_inference.py RUN python3 train.py Để build docker image, chạy lệnh sau:\n$ docker build -t docker-model-batch-infer . Output:\nSending build context to Docker daemon 4.608kB Step 1/10 : FROM jupyter/scipy-notebook ---\u0026gt; 069532086d63 Step 2/10 : USER root ---\u0026gt; Running in c580ea3bbd7f Removing intermediate container c580ea3bbd7f ---\u0026gt; efcad69c0b79 Step 3/10 : WORKDIR /code ---\u0026gt; Running in 08ee819c1e52 Removing intermediate container 08ee819c1e52 ---\u0026gt; d05432266229 Step 4/10 : ADD . /code ---\u0026gt; 6b1f81f3a9f6 Step 5/10 : RUN pip install joblib ---\u0026gt; Running in 9a2c41424c59 Removing intermediate container 9a2c41424c59 ---\u0026gt; 11e642103f1f Step 6/10 : RUN mkdir /code/model ---\u0026gt; Running in 5a4068194bfa Removing intermediate container 5a4068194bfa ---\u0026gt; 6ae6727bf9aa Step 7/10 : ENV MODEL_DIR=/code/model ---\u0026gt; Running in f8992466e635 Removing intermediate container f8992466e635 ---\u0026gt; c3491c25966e Step 8/10 : ENV MODEL_FILE=clf.joblib ---\u0026gt; Running in 15f321f925e4 Removing intermediate container 15f321f925e4 ---\u0026gt; a643969fdfd1 Step 9/10 : ENV METADATA_FILE=metadata.json ---\u0026gt; Running in 72c0b8ef67db Removing intermediate container 72c0b8ef67db ---\u0026gt; efce67f3494c Step 10/10 : RUN python3 train.py ---\u0026gt; Running in 10bec25fc25e Loading data... Splitting data... Fitting model... Serializing model to: /code/model/clf.joblib Serializing metadata to: /code/model/metadata.json Removing intermediate container 10bec25fc25e ---\u0026gt; 05d1c3a12437 Successfully built 05d1c3a12437 Successfully tagged docker-model-batch-infer:latest Kiểm tra nội dung file metadata:\n$ docker run docker-model-batch-infer cat /code/model/metadata.json Kết quả:\n{\u0026#34;train_mean_square_error\u0026#34;: 1.7677391462344387, \u0026#34;test_mean_square_error\u0026#34;: 6.588673999729974} Chú ý: Model ở đây chưa được tuning để tối ưu hóa hiệu năng, các thông tin metadata khác như thuật toán, phân phối dữ liệu, phiên bản model, \u0026hellip; cũng không được lưu lại. Nếu bạn sử dụng hướng dẫn này trong thực tế thì cần lưu ý những điểm trên.\n4. Thực hiện Batch Inference\nChúng ta đã có một model đã train, được lưu thành file clf.joblib trong docker image docker-model-batch-infer. Giờ là lúc sử dụng nó để thực hiện inference.\nĐể thực hiện batch inference, ta sẽ khởi động docker image và chạy lệnh thực thi code kèm theo đó:\n$ docker run docker-model-batch-infer python3 batch_inference.py Kết quả:\nRunning inference... Loading data... Loading model from: /code/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.23789723 31.94786177 10.43966955 34.25663827 22.05210667 11.58265489 13.36407623 42.87157933 33.03218733 15.77635169 23.93521876 19.88239305 25.43466604 20.55132127 13.65254047 47.45491473 17.5734174 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.51637481 30.34664466 19.67895051 23.22841646 25.02203256 18.65459129 30.09762517 8.96667041 13.8130382 14.18734797 17.3840622 19.83840166 24.23822033 20.52076144 15.32433651 25.8157052 16.47533793 19.2214524 19.87110293 21.47113681 21.56443118 24.64517965 22.43665872 22.22261406] 5. Kết luận\nNhư vậy là chúng ta đã thực hiên xong việc train một ML model và sử dụng nó để inference dữ liệu bằng docker. Tuy nhiên, để mang nó vào sử dụng trong môi trường production thì chúng ta cần thực hiện thêm một số công việc nữa như là lập lịch đê thực hiện batch inference định kỳ, tuning hyper-parameter, lưu trữ metadata của model, \u0026hellip;\nToàn bộ source code sử dụng trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong các bài viết tiếp theo, mình sẽ tổng hợp lại các thuật toán Deep Learning, sau đó sẽ đi chi tiết vào một số thụât toán với các ứng dụng cụ thể. Mời các bạn đón đọc!\n6. Tham khảo\n Docker Mlinproduction  ","permalink":"https://tiensu.github.io/blog/39_package_ai_model_using_docker_batch_inference/","tags":["MLOps","Docker"],"title":"Đóng gói quá trình Train AI model và Batch Inference sử dụng Docker"},{"categories":["MLOps","Docker"],"contents":"Đã bao giờ bạn gặp tình huống:\nTại sao code trên máy tính của tôi chạy mà mang sang máy tính của bạn lại không chạy?\n99% câu trả lời cho câu hỏi này là do sự khác biệt về môi trường giữa 2 máy tính, 1% còn lại là do các nguyên nhân khác như copy thiếu file, sai đường dẫn, sử dụng câu lệnh không đúng, \u0026hellip;\nTrong bài toán AI, tình trạng này lại càng phổ biến hơn, bởi vì một model AI yêu cầu cơ man nào là thư viện đi kèm, thư viện này liên kết, ràng buộc với thư viện kia. Không những thế, với tốc độ phát triển như vũ bão hiện nay của AI, các thư viện cũng liên tục cập nhật phiên bản mới, và có khi code sử dụng phiên bản cũ lại không chạy được trên phiên bản mới. Rồi thì thư viện A phiên bản 1.x lại chỉ tương thích với thư viện B phiên bản 1.x.x, nếu ta cứ nhắm mắt gõ pip install abc thì mặc định sẽ là phiên bản mới nhất. Rất rất nhiều vấn đề xung đột thư viện xảy ra trong phát triển một bài toán AI trên nhiều máy tính khác nhau hoặc nhiều người cùng làm việc.\nVấn đề đặt ra lúc này là phải làm sao cô lập được môi trường phát triển dành riêng cho 1 bài toán AI cụ thể. Môi trường đó phải tách biệt hoàn toàn với môi trường trên máy tính, và phải dễ dàng di chuyển giữa nhiều máy tính với nhau.\nMột số công cụ đã ra đời để hỗ trợ giải quyết vấn đề này bằng cách tạo ra các môi trường ảo. Có thể kể đến như anaconda, venv, \u0026hellip; Mỗi loại đều có những ưu nhược điểm riêng, và phụ thuộc vào thói quen sử dụng của mỗi người. Cá nhân mình cũng đã từng sử dụng qua các loại kể trên nhưng thấy chúng vẫn chưa thể giải quyết được triệt để vấn đề về xung đột môi trường \u0026hellip;\nCho đến khi mình biết đến Docker, một công cụ rất powerfull, rất tuyệt vời. Có thể nói docker đã giải quyết được tận gốc vấn đề làm đau đầu những nhà phát triể n AI bấy lâu nay.\nTrong bài này, chúng ta sẽ cùng nhau tìm hiểu về docker và cách sử dụng nó trong viêc đóng gói một AI model để thực hiện Inference theo kiểu Online Inference.\n1. Docker là gì?\nTheo định nghĩa chính thức tại trang chủ của docker thì:\nDocker is an open platform for developing, shipping, and running applications.   Hiểu một cách đơn giản thì docker là một nền tảng mã nguồn mở cho việc phát triển, chạy và phân phối các ứng dụng. Nó cho phép chúng ta tách biệt ứng dụng ra khỏi kiến trúc hạ tầng chung của toàn hệ thống và dễ dang mang toàn bộ ứng dụng đó (bao gồm cả môi trường thực thi) sang một máy tính hoàn toàn mới. Điều này giúp các nhà phát triển ứng dụng giảm được thời gian đáng kể ở công đoạn đưa sản phẩm vào sử dụng trong thực tế.\nMột số khái niệm cần biết khi làm việc với docker:\n Docker image: Là một file không thể thay đổi (read-only), chứa toàn bộ source code, thư viện, công cụ, \u0026hellip; cần thiết để một ứng dụng có thể chạy được. Docker container: Là một \u0026ldquo;bản sao\u0026rdquo;, hay một \u0026ldquo;instance\u0026rdquo; của docker image tại thời điểm khởi chạy docker image. Và thực tế là chúng ta chỉ làm việc trên các containers chứ không làm việc với các images. Sau khi kết thúc phiên làm việc thì container sẽ biến mất, và các thay đổi của container đó sẽ không được lưu lại vào docker image sinh ra container đó. Nếu bạn muốn lưu lại các thay đổi bạn đã thực hiện thì có thể sử dụng lệnh \u0026ldquo;docker commit\u0026rdquo;, nhưng nó sẽ tạo ra một docker image mới bao gồm docker image cũ và phần thay đổi. Đối với sự thay đổi trên các file, thư mục, ta có thể lưu lại sự thay đổi để sử dụng ở nơi khác mà không cần tạo docker image mới bằng cách đặt các file cần thay đổi ở thư mục chung, chia sẻ với máy tính bên ngoài (host). Docker hub: Là một kho (repository) chứa các docker images, cho phép bạn chia sẻ các docker images của bạn cho người khác bằng cách upload nó lên docker hub. Khi người nào muốn sử dụng docker image của bạn, họ chỉ cần tải về để sử dụng. Host: Là máy tính cài đặt docker và chạy các docker containers.  2. Cài đặt Docker\nĐể sử dụng docker thì trước tiên cần phải cài đặt docker engine. Các cài đặt khá đơn giản, hãy làm theo hướng dẫn trên trang chủ của docker.\nSau khi cài xong docker, hãy thử chạy lệnh sau:\n$ docker run tensorflow/tensorflow:2.3.0-gpu Nếu thấy output như sau tức là ta đã cài đặt thành công:  Ở đây, tensorflow/tensorflow:2.3.0-gpu là docker image trên docker hub, được cài đặt sẵn tensorflow 2.3.0 và cuda.\nKiểm tra image vừa tải về trong danh sách:\n$ docker images Kết quả: ```python REPOSITORY TAG IMAGE ID CREATED SIZE tensorflow/tensorflow 2.3.0-gpu 3b8d4cbd6723 3 weeks ago 3.18GB Nếu bạn muốn sử dụng GPU (giả sử là NVIDIA) trong docker thì bạn cần thêm 2 điều kiện:\n Máy tính của bạn phải có GPU và đã cài đặt đầy đủ driver, cuda, cudnn (có thể sử dụng GPU bình thường trong các task DL mà không sử dụng docker). Cài thêm NVIDIA Container Tookit theo hướng dẫn ở đây  Như trên máy tính của mình đã có đủ 2 điều kiện trên, mình kiểm tra GPU bên trong docker như sau:\n$ docker run --gpus all --rm tensorflow/tensorflow:2.3.0-gpu nvidia-smi Kết quả:  Hoặc chi tiết hơn:\n$ docker run --gpus all --rm tensorflow/tensorflow:2.3.0-gpu python -c \u0026#34;import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\u0026#34; Kết quả:  Trong đó:\n \u0026ndash;gpus all: Cho phép sử dụng GPU trong docker. \u0026ndash;rm: Xóa docker container sau khi chạy lệnh xong.  Để cho phép mặc định sử dụng GPU trong docker (không cần sử dụng \u0026ndash;gpus all), bạn có thể làm như sau:\n Thêm default-runtime\u0026quot;: \u0026quot;nvidia\u0026quot; vào trong file /etc/docker/daemon.json  # filename: /etc/docker/daemon.json { \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } }  Khởi động lại docker:  $ sudo pkill -SIGHUP dockerd Mình đã tổng hợp lại các lệnh hay sử dụng của docker ở phần phụ lục. Các bạn có thể tham khảo thêm.\n3. Xây dựng uWSGI docker image\nBên cạnh những images được xây dựng sẵn và chia sẻ trên docker hub, docker cũng hỗ trợ bạn tự build các images cho riêng bạn, phù hợp với từng nhu cầu của bạn. Tất cả được thực hiện thông qua 1 file cấu hình, gọi là Dockerfile.\nTrong phần này, ta sẽ cùng nhau xây dựng một DL docker image, bao gồm toàn bộ source code ở bài trước. Ta cũng cấu hình Flask, uWSGI trong docker image để chạy được source code đó.\n 3.1 Bước 1\nTạo một thư mục tên là uwsgi, và copy toàn bộ source code của bài trước (bao gồm cả model, bỏ đi file client.py vì ta sẽ chạy client ở bên ngoài docker) vào thư mục vừa tạo.\n3.2 Bước 2\nTạo file requirements.txt chứa toàn bộ thư viện cần dùng để chạy code (cũng đặt trong thư mục uWSGI). Bạn có thể sinh ra file này tự động bằng lệnh pip freeze \u0026gt; requirements.txt. Tuy nhiên, nếu làm theo cách này thì file requirements.txt sẽ chứa rất nhiều thư viện không cần thiết, bởi vì hầu hết chúng phụ thuộc vào các thư viện khác. Nếu bạn theo dõi từ đầu, bạn chắc chắc biết rằng, ta sẽ chỉ cần những những thư viện sau là đủ: tensorflow, uwsgi, flask, opencv. Trong đó, vì mình dự định không build docker image từ đầu mà kế thừa từ 1 image đã build sẵn (cụ thể là tensorflow/tensorflow:2.3.0-gpu đã tải về ở phần trước) nên tensorflow đã được tích hợp sẵn, không cần cài lại nữa. Cuối cùng, file requirements.txt của chúng ta chỉ như sau:\nFlask==1.1.2 uWSGI==2.0.18 opencv-python==4.4.0.46 3.3 Bước 3\nTạo file cấu hình cho uWSGI. Vì mình muốn kiểm tra riêng sự hoạt động của uWSGI nên file cấu hình sẽ như sau:\n[uwsgi] http = 0.0.0.0:8080 wsgi-file = server.py callable = app die-on-term = true processes = 4 threads = 2 chdir = /uwsgi master = false vacuum = truemodule Trong phần sau, chúng ta sẽ kết hợp thêm Nginx. Khi đó sẽ cần thay đổi lại cấu hình của uWSGI lại một chút.\n3.3 Bước 4\nTạo Dockerfile (trong thư mục uwsgi) chứa các thông tin cấu hình cần thiết để tạo docker image. Nội dung của file này như sau:\nFROM tensorflow/tensorflow:2.3.0-gpu # kế thừa từ image tensorflow/tensorflow:2.3.0-gpu WORKDIR /uwsgi # thư mục làm viêc mặc định bên trong docker ADD . /uwsgi # local folder để copy vào thư mục làm việc của docker, chính là thư mục chúng ta tạo ở bước 1 RUN pip install -r requirements.txt # cài đặt các thư viện cần thiết trong file requirements.txt RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y # sửa lỗi opencv, thử bỏ đi để xem điều gì xảy ra? CMD [\u0026#34;uwsgi\u0026#34;, \u0026#34;app.ini\u0026#34;] # chạy lệnh \u0026#34;uwsgi app.ini\u0026#34; khi khởi chạy docker image Có rất rất nhiều tùy chọn khi viết Dockerfile, tham khảo ở đây nếu bạn cần thêm thông tin.\nThự mục uwsgi lúc này sẽ như sau:\n├── animal_model_classification.h5 ├── app.ini ├── cat.1.jpg ├── Dockerfile ├── dog.1.jpg ├── requirements.txt └── server.py Trong đó, 2 files ảnh là để chúng ta thực hiện việc test về sau.\n4 Build docker image\nOK, mọi thứ cần thiết đã chuẩn bị xong, ta sẽ chạy lệnh sau để buidl docker image:\n$ docker build -t image-classification-production:1.0 . Docker image được sinh ra sẽ có tên là image-classification-production, kèm theo tag 1.0 để phân biệt nó với các phiên bản khác trong tương lai.\nNếu build, thành công, output sẽ như sau:\n---\u0026gt; a2a60f32471b Step 7/7 : CMD [\u0026#34;uwsgi\u0026#34;, \u0026#34;app.ini\u0026#34;] ---\u0026gt; Running in 3776d496ca68 Removing intermediate container 3776d496ca68 ---\u0026gt; 53150d1373a3 Successfully built 53150d1373a3 Successfully tagged image-classification-production:1.0 Kiểm tra docker image trong danh sách:\ndocker images Kết quả:\nREPOSITORY TAG IMAGE ID CREATED SIZE image-classification-production 1.0 53150d1373a3 About a minute ago 5.37GB tensorflow/tensorflow 2.3.0-gpu 3b8d4cbd6723 3 weeks ago 3.18GB 5 Chạy docker image\nĐể chạy docker image vừa tạo, ta sử dụng lệnh sau:\n$ docker run --rm --publish 80:8080 --name dlp image-classification-production:1.0 Có 2 cái mà ta phải chú ý ở đây:\n Tham số --public 80:8080 sẽ \u0026ldquo;expose\u0026rdquo; port 8080 của container tới port 80 của host. Nói cách khác, tất cả các requests đến địa chỉ localhost:80 sẽ được chuyển tiếp đến địa chỉ 0.0.0.0:8080 bên trong container. 8080 được gọi là listening port của uWSGI. Tham số --name dlp sẽ đặt tên cho container là dlp. Ta nên đặt tên cho container để dễ làm việc với nó hơn. Ngược lại, docker sẽ tạo cho nó một ID ngẫu nhiên.  Kết quả:\n2021-01-06 02:52:45.347188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2021-01-06 02:52:45.347580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2021-01-06 02:52:45.347925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4676 MB memory) -\u0026gt; physical GPU (device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5) 2021-01-06 02:52:47.271932: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:47.621364: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:47.918398: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:48.759371: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:49.637126: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. WSGI app 0 (mountpoint=\u0026#39;\u0026#39;) ready in 8 seconds on interpreter 0x55cf948bc720 pid: 1 (default app) uWSGI running as root, you can use --uid/--gid/--chroot options *** WARNING: you are running uWSGI as root !!! (use the --uid flag) *** *** uWSGI is running in multiple interpreter mode *** spawned uWSGI worker 1 (and the only) (pid: 1, cores: 1) Ta quan sá thấy container đã chạy thành công và uWSGI cũng đã được khởi động.\nLưu ý: Trong trường hợp port 80 đã được sử dụng bới ứng dụng khác (nginx trong bài trước chẳng hạn), bạn phải close ứng dụng đó hoặc sử dụng một port khác.\nĐể kiểm tra xem uWSGI có làm viêc đúng hay không, ta có thể sử dụng lại client đã chuẩn bị từ bài trước (nhớ đổi port của ENDPOINT_URL từ 8080 thành 80).\n$ python client.py Kết quả:\nb\u0026#39;cat\u0026#39; Như vậy là uWSGI đã được cài đặt thành công vào docker.\n Lưu ý: Trong quá trình viết bài này, mình gặp lỗi liên quan đến cuda khi chạy test uWSGI. Mình xóa hết các docker images đi và chạy lại từ đầu thì không bị lỗi nữa.\n6. Xây dựng Nginx docker image\nTương tự như việc xây dựng uWSGI docker image, chúng ta sẽ đi build một Nginx docker image, đặt trước uWSGI server thực hiện vai trò như một reverse proxy.\n6.1 Bước 1\nTạo thư mục nginx, cùng cấp với thư mục uwsgi.\n6.2 Bước 2\nTạo file cấu hình Nginx, tên là nginx.conf, đặt trong thư mục nginx, với nội dung như sau:\nserver { listen 80; location / { include uwsgi_params; uwsgi_pass uwsgi:660 ; } Với cấu hình này thì Nginx sẽ lắng nghe trên port 80, chuyển tiếp các requests đến port 660 của uWSGI server thông qua socket (sử dụng giao thức uwsgi).\n6.3 Bước 3\nCập nhật lại cấu hình của uWSGI (file app.ini) để làm việc được với Nginx, như sau:\nmodule = server socket= :660 callable = app die-on-term = true processes = 1 master = false vacuum = true 6.4 Bước 4\nTạo file Dockerfile cho Nginix docker trong thư mục nginx. Nginix docker image được kế thừa từ nginx image trên docker hub, ta chỉ việc thay thế cấu hình mặc định của nó bằng cấu hình mà ta vừa tạo ở bước 3.\nFROM nginx RUN rm /etc/nginx/conf.d/default.conf COPY nginx.conf /etc/nginx/conf.d/ Đến đây, nếu chạy lệnh docker build ... thì ta sẽ có được nginx docker image. Nhưng nếu chỉ chạy một mình image này thì không có tác dụng gì cả. Ta cần phải kết hợp cả 2 docker images uwsgi và nginx. Đó chính là công viêc của docker-compose.\n7. Chạy đồng thời nhiều docker containers với Docker Compose\nLiệu bạn có thắc mắc rằng tại sao ta không build cả Nginx và uWSGI vào chung 1 docker image? Chẳng phải như thế sẽ tiện hơn hay sao?\nCâu trả lời là không nên làm vậy. Theo kiến trúc làm việc kết hợp giữa Nginx và uWSGI thì một Nginx instance có thể kết hợp với nhiề u uWSGI instances. Nếu ta kết hợp chung lại, sẽ không tận dụng được khả năng này. Thêm nữa, dung lượng của docker image sẽ rất lớn nếu ta kết hợp lại.\n Mở rộng ra, nếu một hệ thống của chúng ta bao gồm cả database, backend, front-end, messaging systems, task queue, \u0026hellip; ta không thể chạy tất tần tật mọi thứ trong một docker container được.\nTừ góc độ của nhà phát triển phần mềm, docker-compose chỉ là một file cấu hình, định nghĩa tất cả containers và cách thức mà các containers đó tương tác với nhau.\n7.1 Cài đặt docker-compose\nĐể cài đặt docker-compose. Bạn hãy làm theo hướng dẫn sau trên trang chủ của docker.\n7.2 Định nghĩa cấu hình của docker-compose\nTạo file docker-compose.yml (bên ngoài 2 thư mục uwsgi và nginx), với nội dung như sau:\nversion : \u0026#34;3.7\u0026#34; services: uwsgi: build: ./uwsgi container_name: uwsgi_img_classification restart: always expose: - 660 nginx: build: ./nginx container_name: nginx restart: always ports: - \u0026#34;80:80\u0026#34; Phần chính của cấu hình này là khai báo 2 containers, gọi là 2 services. Hai tham số quan trọng của mỗi services là:\n build: thư mục chứa Dockerfile và các files cần thiết của mỗi container. restart: tự động khởi động lại service nếu xay ra lỗi. expose: uwsgi lắng nghe request đến trên port 660 (chỉ trong phạm vi docker). port: nginx mở port 80 ra bên ngoài (có thể chọn tùy ý) để lắng nghe requests đến, ánh xạ đến port 80 (theo như cấu hình trong file nginx.conf) của container.  Như vậy, có thể tóm tắt lại flow như sau:\n Các requests từ clients đến port 80 của host. Các requests được ánh xạ sang port 80 của nginx container. Các requests tiếp tục được chuyển tiếp đến port 660 của uwsgi container. uwsgi gọi Flask endpoint và thực hiện quá trình nhận diện. uwsgi gửi lại kết quả nhận diện theo hướng ngược lại.  7.3 Build docker-compose\nChạy lệnh sau để build docker-compose với cả 2 containers.\n$ docker-compose build Nếu build thành công, output sẽ như sau:\nStep 7/7 : CMD [\u0026#34;uwsgi\u0026#34;, \u0026#34;app.ini\u0026#34;] ---\u0026gt; Running in 9608e1187e82 Removing intermediate container 9608e1187e82 ---\u0026gt; 357fe8e41768 Successfully built 357fe8e41768 Successfully tagged docker_uwsgi:latest Building nginx Step 1/3 : FROM nginx latest: Pulling from library/nginx 6ec7b7d162b2: Pull complete cb420a90068e: Pull complete 2766c0bf2b07: Pull complete e05167b6a99d: Pull complete 70ac9d795e79: Pull complete Digest: sha256:4cf620a5c81390ee209398ecc18e5fb9dd0f5155cd82adcbae532fec94006fb9 Status: Downloaded newer image for nginx:latest ---\u0026gt; ae2feff98a0c Step 2/3 : RUN rm /etc/nginx/conf.d/default.conf ---\u0026gt; Running in 11140e051282 Removing intermediate container 11140e051282 ---\u0026gt; 1fcc92cfdfc4 Step 3/3 : COPY nginx.conf /etc/nginx/conf.d/ ---\u0026gt; 21bda0089cca Successfully built 21bda0089cca Successfully tagged docker_nginx:latest Kiểm tra thử danh sách images bằng lệnh docker images:\nREPOSITORY TAG IMAGE ID CREATED SIZE docker_nginx latest 21bda0089cca 4 minutes ago 133MB docker_uwsgi latest 357fe8e41768 5 minutes ago 5.37GB image-classification-production 1.0 bd9928abee21 2 hours ago 5.37GB nginx latest ae2feff98a0c 3 weeks ago 133MB tensorflow/tensorflow 2.3.0-gpu 3b8d4cbd6723 3 weeks ago 3.18GB Ta thấy hai containers docker_nginx và docker_uwsgi đã xuất hiện.\n7.4 Kiểm tra hoạt động của hệ thống\nTa sẽ khởi động các containers lên để kiểm tra thử xem hê thống có làm việc chính xác không.\n$ docker-compose up Khởi động thành công:\nStarting nginx ... done Starting uwsgi_img_classification ... done Attaching to nginx, uwsgi_img_classification nginx | /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration nginx | /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ uwsgi_img_classification | [uWSGI] getting INI configuration from app.ini ..... uwsgi_img_classification | 2021-01-06 09:18:12.292414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4662 MB memory) -\u0026gt; physical GPU (device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5) uwsgi_img_classification | 2021-01-06 09:18:14.386546: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:14.737805: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:15.045424: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:16.238951: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:17.246651: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | WSGI app 0 (mountpoint=\u0026#39;\u0026#39;) ready in 7 seconds on interpreter 0x56494b98c680 pid: 1 (default app) uwsgi_img_classification | uWSGI running as root, you can use --uid/--gid/--chroot options uwsgi_img_classification | *** WARNING: you are running uWSGI as root !!! (use the --uid flag) *** uwsgi_img_classification | *** uWSGI is running in multiple interpreter mode *** uwsgi_img_classification | spawned uWSGI worker 1 (and the only) (pid: 1, cores: 1) Chạy client để nhận diện: python client.py.\nKết quả:\nb\u0026#39;cat\u0026#39; 8. Kết luận\nPhù, thật tuyệt vời, mọi thứ đã chạy đúng như mong muốn.\nBài hôm nay khá là dài và khó. Mình đã phải thực hiện cài cắm rất nhiều lần để có thể hoàn thành bài viết này. Hi vọng sẽ có ích cho các bạn trong việc tìm kiếm giải pháp triể n khải AI model vào trong các sản phẩm để đưa đến tay người dùng!\nToàn bộ source code sử dụng trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây. Giống như bài trước, vì model animal_model_classification.h5 có dung lượng khá lớn (\u0026gt; 1.5GB) nên mình không upload lên github được. Các bạn hãy sử dụng model của chính mình để thực hành nhé!\nTrong các bài viết tiếp theo, mình sẽ sử dụng docker để train một model khác và thực hiện batch inference bằng model đó. Mời các bạn đón đọc!\n9. Phụ lục một số lệnh cơ bản của Docker\n1. List docker image $ docker images 2. List container $ docker ps \u0026lt;-a\u0026gt; 3. Run a docker $ docker run -it [image_name] bash 4. Access to running container $ docker exec -it [container_id or container_name] bash 5. Commit change of container to docker image $ docker commit [container_name or container_id] [new_image_name] 6. Stop running container $ docker stop [container_id or container_name] $ docker stop $(docker ps -aq) # Stop all container 7. Start stoped container $ docker start [container_id or container_name] 8. Remove container $ docker rm [container_id or container_name] $ docker rm $(docker ps -aq) # Remove all 9. Export container $ docker export [container_id or container_name] | gzip \u0026gt; file_export.tar.gz 10. Import docker =\u0026gt; images $ zcat file_export.tar.gz | docker [new_name_image] $ docker images # check 11. Remove docker image $ docker rmi [image_name] Loi: docker: Error response from daemon: Unknown runtime specified nvidia. Solution: 1. $ sudo systemctl daemon-reload $ sudo systemctl restart docker 2. $ sudo mkdir -p /etc/systemd/system/docker.service.d $ sudo tee /etc/systemd/system/docker.service.d/override.conf \u0026lt;\u0026lt;EOF [Service] ExecStart= ExecStart=/usr/bin/dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime EOF $ sudo systemctl daemon-reload $ sudo systemctl restart docker ** Move docker image to other computer 1. Save images $ docker save \u0026lt;REPOSITORY\u0026gt; \u0026gt; \u0026lt;images_name\u0026gt;.tar 2. Load images $ docker load \u0026lt; \u0026lt;images_name\u0026gt;.tar 3. Run images $ docker run -it --runtime=nvidia --rm --net=host --privileged \u0026lt;Image ID\u0026gt; 10. Tham khảo\n Docker Nginx uWSGI Flask AI Summer  ","permalink":"https://tiensu.github.io/blog/38_package_ai_model_using_docker_online_inference/","tags":["MLOps","Docker"],"title":"Đóng gói AI model theo kiểu Online Inference sử dụng Docker"},{"categories":["MLOps"],"contents":"Như đã giới thiệu trong bài trước, mặc dù Flask rất dễ để sử dụng nhưng nó không có đầy đủ chức năng để có thể áp dụng vào các sản phẩm trong thực tế. Đó là tính bảo mật, khả năng xử lý đồng thời nhiều kết nối, khả năng mở rộng và nâng cấp model, \u0026hellip; Sử dụng kết hợp bộ ba Flask, uWSGI và Nginx chính là giải pháp hữu hiệu khắc phục những thiếu sót này. Trong bài viết này, chúng ta sẽ cùng tìm hiểu cách cài đặt, cấu hình và sử dụng bộ 3 kể trên để triển khai Animal Classification model dưới dạng server phục vụ các yêu cầu nhận dạng từ các clients.\n1. WSGI, uWSGI, và uwsgi là gì?\nTrước tiên cần hiểu rõ một số thuật ngữ mà ta sử dụng trong bài này.\n WSGI: Viết tắt của Web Server Gateway Interface, là một Interface giữa server và client, được viết bằng python. Hiểu một cách đơn giản, nó quy định các thức để client có thể kết nối và gửi nhận dữ liệu với server. uWSGI: Là một server, sử dụng WSGI để giao tiếp với client (hoặc sử dụng giao thức HTTP trong trường hợp client là web application). uwsgi: Là một giao thức ở tầng thấp hơn, cho phép các servers giao tiếp với nhau.  Bạn có thể xem sơ đồ kiến trúc triển khai sử dụng uWSGI như hình bên dưới đây:\n Phần xử lý nhận diện của ta vẫn được gọi thông qua Flask. Phía trước Flask ta đặt uWSGI rồi đến web application (client).\nViệc đặt uWSGI như vậy mang lại cho ta các lợi ích như sau:\n Quản lý tiến trình: Quản lý việc tạo và duy trì các tiến trình trong quá trình làm việc. Các tiến trình được đồng bộ với nhau trong cùng 1 môi trường và có khả năng scale-up để phục vụ cho nhiều users. Cân bằng tải: Phân phối tải (các requests) đến các tiến trình khác nhau. Giám sát: Giám sát hiệu năng và tài nguyên sử dụng. Hạn chế tài nguyên: Cho phép chỉ định mức tối đa tài nguyên có thể sử dụng.  2. Nginx là gì và tại sao phải sử dụng nó?\nNginx là một webserver với các đặc tính:\n High performance: Hiệu năng cao Highly scalable: Khả năng mở rộng cao Highly available: Tính sẵn sàng cao  Nó hoạt động giống như một bộ cân bằng tải, một reverse proxy cùng với cơ chế caching, cơ chế mã hóa và bảo mật trên các bản tin giao tiếp giữa client và server. Nginx được cho là có thể phục vụ hơn 10,000 kết nối đồng thời.\nNginx được sử dụng khá phổ biến trong các công ty công nghệ lớn, trong nhiều sản phẩm, ứng dụng cần phục vụ số lượng lớn người dùng đồng thời.\nXét về kiến trúc tổng thể, nó thường được sử dụng cùng với uWSGI, đứng trước uWSGI như trong hình sau:\n Mục đích của việc sử dụng đồng thời cả uWSGI và Nginx là để tận dụng những ưu điểm của cả 2. Tất nhiên điều này là không bắt buộc nếu ứng dụng của chúng ta ở mức đơn giản, không cần phải phục vụ số lượng users đồng thời quá lớn. Nhưng dù sao vẫn nên sử dụng kiến trúc này để có thể dễ dàng mở rộng ứng dụng về sau.\n3. Chuẩn bị Flask server\nTa vẫn cần có Flask làm server trực tiếp xử lý request từ client. Có thể hiểu Flask server ở đây là endpoint cũng được. Mình sẽ sử dụng lại file server.py ở bài trước, nhưng đã bỏ đi phần phục vụ web client.\nimport cv2 import os import numpy as np import tensorflow as tf from flask_cors import CORS from tensorflow.keras.models import load_model from flask import Flask, request, render_template, make_response, jsonify config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) app = Flask(__name__) CORS(app) image_width = 300 image_height = 300 classes = [\u0026#39;cat\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;pandas\u0026#39;] APP_ROOT_2 = os.getenv(\u0026#39;APP_ROOT\u0026#39;, \u0026#39;/infer\u0026#39;) model = load_model(\u0026#39;animal_model_classification.h5\u0026#39;) @app.route(APP_ROOT_2, methods=[\u0026#34;POST\u0026#34;]) def infer(): data = request.json img_path = data[\u0026#39;img_path\u0026#39;] return classify_animal(img_path) def classify_animal(img_path): # read image image = cv2.imread(img_path) image = image/255 image = cv2.resize(image, (image_width,image_height)) image = np.reshape(image, [1,image_width,image_height,3]) # pass the image through the network to obtain our predictions preds = model.predict(image) label = classes[np.argmax(preds)] return label if __name__ == \u0026#39;__main__\u0026#39;: app.run() 4. Cài đặt và cấu hình uWSGI\nĐể cài đặt uWSGI, sử dụng lệnh sau:\n$ pip install uwsgi Kiểm tra cài đặt bằng cách chạy câu lệnh sau:\n$ uwsgi --http 0.0.0.0:8080 --wsgi-file server.py --callable app Nếu output như sau thì tức là viêc cài đặt thành công:\n Câu lệnh trên có ý nghĩa là chạy một server tại địa chỉ 0.0.0.0, port 8080, sử dụng ứng dụng đặt trong file server.py.\nuWSGI có rất nhiều tùy chọn cấu hình. Do đó, để thuận tiện, ta thường tạo một file cấu hình, tên là app.ini như sau:\n[uwsgi] http = 0.0.0.0:8080 # địa chỉ server socket = service.sock # socket giao tiếp với Nginx chmod-socket = 660 # cấp quyền truy câp socket wsgi-file = server.py # file chứa code xử lý yêu cầu từ client callable = app # function được gọi khi tạo uWSGI die-on-term = true # cho phép kill server từ terminal processes = 4 # số lượng process threads = 2 # số lượng thread chdir = /media/sunt/DATA/GITHUB/Model_Deployment/uWSGI/ # thư mục dự án virtualenv = /home/sunt/anaconda3/envs/tf2/ # môi trường ảo (nếu có) master = false vacuum = truemodule # định kỳ xóa những file ko cần thiết được sinh ra Để chạy uWSGI, dùng lệnh:\n$ uwsgi app.ini Nếu thành công, output trên terminal sẽ như sau:\n 5. Cài đặt và cấu hình Nginx\nĐể cài đặt Nginx, sử dụng lệnh sau:\n$ sudo apt-get install nginx Tiếp theo, tạo một file cấu hình đặt trong thư mục /etc/nginx/sites-available, tên là service.conf, với nội dụng như sau:\nserver { listen 80; server_name 0.0.0.0; location / { include uwsgi_params; uwsgi_pass unix:/media/sunt/DATA/GITHUB/Model_Deployment/uWSGI/service.sock; } } Theo cấu hình này, Nginx sẽ lắng nghe trên cổng 80 (mặc định) cho tất cả các yêu cầu đến server đặt tại địa chỉ 0.0.0.0. Các yêu cầu sau đó được chuyển đến uWSGI server thông qua socket service.sock (sử dụng giao thức uwsgi).\nTiếp theo, để áp dụng các cấu hình trên cho Nginx, ta cần trỏ liên kết của chúng tới thư mục sites-enabled:\n$ sudo ln -s /etc/nginx/sites-available/service.config /etc/nginx/sites-enabled Kiểm tra lại xem các cấu hình đã đúng hay chưa?:\n$ sudo nginx -t Nếu mọi thứ OK, sẽ có output như sau:\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful Cuối cùng, restart lại Nginx server:\n$ sudo systemctl status nginx   6. Tạo client và kiểm tra kết quả\nSử dụng lại client viết bằng python như ở bài trước:\nimport requests from PIL import Image import numpy as np ENDPOINT_URL = \u0026#39;http://0.0.0.0:8080/infer2\u0026#39; def infer(): data = { \u0026#39;img_path\u0026#39;: \u0026#39;dog.1.jpg\u0026#39; } response = requests.post(ENDPOINT_URL, json = data) response.raise_for_status() print(response.content) if __name__ ==\u0026#34;__main__\u0026#34;: infer() Để kiểm tra hoạt động của client và server, đầu tiên khởi chạy uWSGI (như phần 4.), sau đó chạy client:\n$ python client.py Nếu nhận được kết quả trả vê từ server tức là hệ thống đã hoạt động chính xác:\nb`dog` 7. Kết luận\nNhư vậy là chúng ta đã cùng nhau triển khai thành công DL model sử dụng Nginx, uWSGI và Flask. Nginx thì mặc định được chạy dưới dạng service sau khi cài đặt xong, còn uWSGI thì không. Ta nên cấu hình uWSGI để nó cũng chạy dưới dạng service cho thuận tiện sử dụng. Hi vọng qua bài này, các bạn đã hiểu rõ hơn về cách thức triển khai một AI model trong các sản phẩm thực tế, đáp ứng số lượng lớn user sử dụng đồng thời.\nToàn bộ source code của backend và front-end trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây. Vì model animal_model_classification.h5 có dung lượng khá lớn (\u0026gt; 1.5GB) nên mình không upload lên github được. Các bạn hãy sử dụng model của chính mình để thực hành nhé!\nBài viết tiếp theo, chúng ta sẽ đi nâng cao hơn 1 chút nữa, đó là đóng gói tất cả những phần đã làm hôm nay vào một cái gọi là docker. Docker là gì, và tại sao lại nên dùng nó? Tất cả sẽ được giải đáp trong bài viết đó. Mời các bạn đón đọc!\n8. Tham khảo\n Nginx uWSGI Flask AI Summer  ","permalink":"https://tiensu.github.io/blog/37_deploy_ai_model_with_uwsgi_online_inference/","tags":["MLOps"],"title":"Triển khai AI model sử dụng uWSGI và Nginx"},{"categories":["Text Classification"],"contents":"Bài này mình xin phép đổi chủ đề một chút. Chúng ta sẽ thử làm bài toán phân loại text theo các chủ đề khác nhau. Đây là một trong những bài toán thuộc phạm vi của chủ đề xử lý ngôn ngữ tự nhiên (NLP).\nMình sẽ sử dụng bộ dữ liệu BBC news để thực hành. Bạn hãy download của 2 file Train.csv và Test.csv, sau đó gộp chung chúng lại thành 1 file để làm dữ liệu huấn luyện. Tổng số records là 2225, chia thành 6 chủ đề.\nĐầu tiên, import các thư viện sẽ sử dụng:\nimport csv import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences Khai báo một số tham số:\nvocab_size = 1000 embedding_dim = 16 max_length = 120 trunc_type = \u0026#39;post\u0026#39; padding_type = \u0026#39;post\u0026#39; oov_tok = \u0026#39;\u0026lt;OOV\u0026gt;\u0026#39; training_portion = 0.8 sentences = [] labels = [] stopwords = [ \u0026#34;a\u0026#34;, \u0026#34;about\u0026#34;, \u0026#34;above\u0026#34;, \u0026#34;after\u0026#34;, \u0026#34;again\u0026#34;, \u0026#34;against\u0026#34;, \u0026#34;all\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;and\u0026#34;, \u0026#34;any\u0026#34;, \u0026#34;are\u0026#34;, \u0026#34;as\u0026#34;, \u0026#34;at\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;because\u0026#34;, \u0026#34;been\u0026#34;, \u0026#34;before\u0026#34;, \u0026#34;being\u0026#34;, \u0026#34;below\u0026#34;, \u0026#34;between\u0026#34;, \u0026#34;both\u0026#34;, \u0026#34;but\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;could\u0026#34;, \u0026#34;did\u0026#34;, \u0026#34;do\u0026#34;, \u0026#34;does\u0026#34;, \u0026#34;doing\u0026#34;, \u0026#34;down\u0026#34;, \u0026#34;during\u0026#34;, \u0026#34;each\u0026#34;, \u0026#34;few\u0026#34;, \u0026#34;for\u0026#34;, \u0026#34;from\u0026#34;, \u0026#34;further\u0026#34;, \u0026#34;had\u0026#34;, \u0026#34;has\u0026#34;, \u0026#34;have\u0026#34;, \u0026#34;having\u0026#34;, \u0026#34;he\u0026#34;, \u0026#34;he\u0026#39;d\u0026#34;, \u0026#34;he\u0026#39;ll\u0026#34;, \u0026#34;he\u0026#39;s\u0026#34;, \u0026#34;her\u0026#34;, \u0026#34;here\u0026#34;, \u0026#34;here\u0026#39;s\u0026#34;, \u0026#34;hers\u0026#34;, \u0026#34;herself\u0026#34;, \u0026#34;him\u0026#34;, \u0026#34;himself\u0026#34;, \u0026#34;his\u0026#34;, \u0026#34;how\u0026#34;, \u0026#34;how\u0026#39;s\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;i\u0026#39;d\u0026#34;, \u0026#34;i\u0026#39;ll\u0026#34;, \u0026#34;i\u0026#39;m\u0026#34;, \u0026#34;i\u0026#39;ve\u0026#34;, \u0026#34;if\u0026#34;, \u0026#34;in\u0026#34;, \u0026#34;into\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;it\u0026#34;, \u0026#34;it\u0026#39;s\u0026#34;, \u0026#34;its\u0026#34;, \u0026#34;itself\u0026#34;, \u0026#34;let\u0026#39;s\u0026#34;, \u0026#34;me\u0026#34;, \u0026#34;more\u0026#34;, \u0026#34;most\u0026#34;, \u0026#34;my\u0026#34;, \u0026#34;myself\u0026#34;, \u0026#34;nor\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;on\u0026#34;, \u0026#34;once\u0026#34;, \u0026#34;only\u0026#34;, \u0026#34;or\u0026#34;, \u0026#34;other\u0026#34;, \u0026#34;ought\u0026#34;, \u0026#34;our\u0026#34;, \u0026#34;ours\u0026#34;, \u0026#34;ourselves\u0026#34;, \u0026#34;out\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;own\u0026#34;, \u0026#34;same\u0026#34;, \u0026#34;she\u0026#34;, \u0026#34;she\u0026#39;d\u0026#34;, \u0026#34;she\u0026#39;ll\u0026#34;, \u0026#34;she\u0026#39;s\u0026#34;, \u0026#34;should\u0026#34;, \u0026#34;so\u0026#34;, \u0026#34;some\u0026#34;, \u0026#34;such\u0026#34;, \u0026#34;than\u0026#34;, \u0026#34;that\u0026#34;, \u0026#34;that\u0026#39;s\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;their\u0026#34;, \u0026#34;theirs\u0026#34;, \u0026#34;them\u0026#34;, \u0026#34;themselves\u0026#34;, \u0026#34;then\u0026#34;, \u0026#34;there\u0026#34;, \u0026#34;there\u0026#39;s\u0026#34;, \u0026#34;these\u0026#34;, \u0026#34;they\u0026#34;, \u0026#34;they\u0026#39;d\u0026#34;, \u0026#34;they\u0026#39;ll\u0026#34;, \u0026#34;they\u0026#39;re\u0026#34;, \u0026#34;they\u0026#39;ve\u0026#34;, \u0026#34;this\u0026#34;, \u0026#34;those\u0026#34;, \u0026#34;through\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;too\u0026#34;, \u0026#34;under\u0026#34;, \u0026#34;until\u0026#34;, \u0026#34;up\u0026#34;, \u0026#34;very\u0026#34;, \u0026#34;was\u0026#34;, \u0026#34;we\u0026#34;, \u0026#34;we\u0026#39;d\u0026#34;, \u0026#34;we\u0026#39;ll\u0026#34;, \u0026#34;we\u0026#39;re\u0026#34;, \u0026#34;we\u0026#39;ve\u0026#34;, \u0026#34;were\u0026#34;, \u0026#34;what\u0026#34;, \u0026#34;what\u0026#39;s\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;when\u0026#39;s\u0026#34;, \u0026#34;where\u0026#34;, \u0026#34;where\u0026#39;s\u0026#34;, \u0026#34;which\u0026#34;, \u0026#34;while\u0026#34;, \u0026#34;who\u0026#34;, \u0026#34;who\u0026#39;s\u0026#34;, \u0026#34;whom\u0026#34;, \u0026#34;why\u0026#34;, \u0026#34;why\u0026#39;s\u0026#34;, \u0026#34;with\u0026#34;, \u0026#34;would\u0026#34;, \u0026#34;you\u0026#34;, \u0026#34;you\u0026#39;d\u0026#34;, \u0026#34;you\u0026#39;ll\u0026#34;, \u0026#34;you\u0026#39;re\u0026#34;, \u0026#34;you\u0026#39;ve\u0026#34;, \u0026#34;your\u0026#34;, \u0026#34;yours\u0026#34;, \u0026#34;yourself\u0026#34;, \u0026#34;yourselves\u0026#34; ] Chúng ta có một mảng chứa các stop words, tức là các từ thường hay xuất hiện trong câu nhưng lại không mang nhiều ý nghĩa. Chúng ta sẽ loại bỏ chúng đi trước khi huấn luyện model phân loại.\nBây giờ, ta sẽ đọc dataset và chuẩn bị dữ liệu training:\nwith open(\u0026#39;bbc-text.csv\u0026#39;, \u0026#39;r\u0026#39;) as csvfile: reader = csv.reader(csvfile, delimiter=\u0026#39;,\u0026#39;) next(reader) for row in reader: labels.append(row[0]) sentence = row[1] # remove stop words for word in stopwords: token = \u0026#39; \u0026#39; + word + \u0026#39; \u0026#39; sentence = sentence.replace(token, \u0026#39; \u0026#39;) sentence = sentence.replace(\u0026#39; \u0026#39;, \u0026#39; \u0026#39;) sentences.append(sentence) print(len(sentences)) Chia dataset thành 2 phần: train và validation:\ntrain_size = int(len(sentences) * training_portion) train_sentences = sentences[:train_size] train_labels = labels[:train_size] validation_sentences = sentences[train_size:] validation_labels = labels[train_size:] Để model có thể hiểu được dataset, cần phải chuyển các câu dạng text sang dạng vector:\n# chuyển text sang vector tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok) tokenizer.fit_on_texts(train_sentences) word_index = tokenizer.word_index label_tokenizer = Tokenizer() label_tokenizer.fit_on_texts(labels) training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels)) validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels)) # padding để các câu có cùng chiều dài train_sequences = tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length) validation_sequences = tokenizer.texts_to_sequences(validation_sentences) validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length) Mình sẽ đi chi tiết phần này trong 1 bài viết khác. Hôm nay các bạn chỉ cần hiểu ý tưởng của nó như vậy là được rồi.\nSau khi đã có dữ liệu huấn luyện, giờ là lúc chúng ta định nghĩa kiến trúc model.\nmodel = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), tf.keras.layers.GlobalAveragePooling1D(), tf.keras.layers.Dense(24, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(6, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) model.summary() Ở bài này, mình chỉ sử dụng một model đơn giản gồm các lớp Embedding, GlobalAveragePooling1D, và Dense.\nHàm plot để vẽ đồ thị quá trình training:\ndef plot_graph(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10,6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training \u0026amp; Validation, Accuracy \u0026amp; Loss\u0026#39;) plt.legend(loc=0) plt.show() Bước cuối cùng là chạy train model, ta sẽ train model với 30 epochs:\nnum_epochs = 30 history = model.fit( train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=1 ) Training output:\nEpoch 1/30 56/56 [==============================] - 0s 4ms/step - loss: 1.7737 - acc: 0.2180 - val_loss: 1.7481 - val_acc: 0.2270 Epoch 2/30 56/56 [==============================] - 0s 2ms/step - loss: 1.7163 - acc: 0.2303 - val_loss: 1.6755 - val_acc: 0.2270 Epoch 3/30 56/56 [==============================] - 0s 2ms/step - loss: 1.6299 - acc: 0.2371 - val_loss: 1.5792 - val_acc: 0.2539 .................... Epoch 27/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0580 - acc: 0.9949 - val_loss: 0.2115 - val_acc: 0.9506 Epoch 28/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0521 - acc: 0.9961 - val_loss: 0.2080 - val_acc: 0.9506 Epoch 29/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0467 - acc: 0.9961 - val_loss: 0.2051 - val_acc: 0.9506 Epoch 30/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0420 - acc: 0.9989 - val_loss: 0.2014 - val_acc: 0.9506 Quá trình train diễn ra khá nhanh, mất khoảng 2 phút trên máy tính của mình. Tại epochs cuối cùng, độ chính xác trên tập validation là 95,06%.\nToàn bộ quá trình này được thể hiện trên đồ thị như sau:\nplot_graph(history)   Model hội tụ khá nhanh và cho kết quả tốt, không có hiện tượng overfitting. Có lẽ train thêm một số epochs nữa sẽ cho kết quả tốt hơn. Bạn có thể thử.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong bài viết tiếp theo, mình sẽ vẫn thực hành bài toán phân loại văn bản nhưng sử dụng kỹ thuật Transfer Learning giống như bên Computer Vision. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/ddd_bbc_text_topic_classification-copy/","tags":["Text Classification"],"title":"Phân loại text theo chủ đề  - Transfer Learning"},{"categories":["Text Classification"],"contents":"Bài này mình xin phép đổi chủ đề một chút. Chúng ta sẽ thử làm bài toán phân loại text theo các chủ đề khác nhau. Đây là một trong những bài toán thuộc phạm vi của chủ đề xử lý ngôn ngữ tự nhiên (NLP).\nMình sẽ sử dụng bộ dữ liệu BBC news để thực hành. Bạn hãy download của 2 file Train.csv và Test.csv, sau đó gộp chung chúng lại thành 1 file để làm dữ liệu huấn luyện. Tổng số records là 2225, chia thành 6 chủ đề.\nĐầu tiên, import các thư viện sẽ sử dụng:\nimport csv import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences Khai báo một số tham số:\nvocab_size = 1000 embedding_dim = 16 max_length = 120 trunc_type = \u0026#39;post\u0026#39; padding_type = \u0026#39;post\u0026#39; oov_tok = \u0026#39;\u0026lt;OOV\u0026gt;\u0026#39; training_portion = 0.8 sentences = [] labels = [] stopwords = [ \u0026#34;a\u0026#34;, \u0026#34;about\u0026#34;, \u0026#34;above\u0026#34;, \u0026#34;after\u0026#34;, \u0026#34;again\u0026#34;, \u0026#34;against\u0026#34;, \u0026#34;all\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;and\u0026#34;, \u0026#34;any\u0026#34;, \u0026#34;are\u0026#34;, \u0026#34;as\u0026#34;, \u0026#34;at\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;because\u0026#34;, \u0026#34;been\u0026#34;, \u0026#34;before\u0026#34;, \u0026#34;being\u0026#34;, \u0026#34;below\u0026#34;, \u0026#34;between\u0026#34;, \u0026#34;both\u0026#34;, \u0026#34;but\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;could\u0026#34;, \u0026#34;did\u0026#34;, \u0026#34;do\u0026#34;, \u0026#34;does\u0026#34;, \u0026#34;doing\u0026#34;, \u0026#34;down\u0026#34;, \u0026#34;during\u0026#34;, \u0026#34;each\u0026#34;, \u0026#34;few\u0026#34;, \u0026#34;for\u0026#34;, \u0026#34;from\u0026#34;, \u0026#34;further\u0026#34;, \u0026#34;had\u0026#34;, \u0026#34;has\u0026#34;, \u0026#34;have\u0026#34;, \u0026#34;having\u0026#34;, \u0026#34;he\u0026#34;, \u0026#34;he\u0026#39;d\u0026#34;, \u0026#34;he\u0026#39;ll\u0026#34;, \u0026#34;he\u0026#39;s\u0026#34;, \u0026#34;her\u0026#34;, \u0026#34;here\u0026#34;, \u0026#34;here\u0026#39;s\u0026#34;, \u0026#34;hers\u0026#34;, \u0026#34;herself\u0026#34;, \u0026#34;him\u0026#34;, \u0026#34;himself\u0026#34;, \u0026#34;his\u0026#34;, \u0026#34;how\u0026#34;, \u0026#34;how\u0026#39;s\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;i\u0026#39;d\u0026#34;, \u0026#34;i\u0026#39;ll\u0026#34;, \u0026#34;i\u0026#39;m\u0026#34;, \u0026#34;i\u0026#39;ve\u0026#34;, \u0026#34;if\u0026#34;, \u0026#34;in\u0026#34;, \u0026#34;into\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;it\u0026#34;, \u0026#34;it\u0026#39;s\u0026#34;, \u0026#34;its\u0026#34;, \u0026#34;itself\u0026#34;, \u0026#34;let\u0026#39;s\u0026#34;, \u0026#34;me\u0026#34;, \u0026#34;more\u0026#34;, \u0026#34;most\u0026#34;, \u0026#34;my\u0026#34;, \u0026#34;myself\u0026#34;, \u0026#34;nor\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;on\u0026#34;, \u0026#34;once\u0026#34;, \u0026#34;only\u0026#34;, \u0026#34;or\u0026#34;, \u0026#34;other\u0026#34;, \u0026#34;ought\u0026#34;, \u0026#34;our\u0026#34;, \u0026#34;ours\u0026#34;, \u0026#34;ourselves\u0026#34;, \u0026#34;out\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;own\u0026#34;, \u0026#34;same\u0026#34;, \u0026#34;she\u0026#34;, \u0026#34;she\u0026#39;d\u0026#34;, \u0026#34;she\u0026#39;ll\u0026#34;, \u0026#34;she\u0026#39;s\u0026#34;, \u0026#34;should\u0026#34;, \u0026#34;so\u0026#34;, \u0026#34;some\u0026#34;, \u0026#34;such\u0026#34;, \u0026#34;than\u0026#34;, \u0026#34;that\u0026#34;, \u0026#34;that\u0026#39;s\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;their\u0026#34;, \u0026#34;theirs\u0026#34;, \u0026#34;them\u0026#34;, \u0026#34;themselves\u0026#34;, \u0026#34;then\u0026#34;, \u0026#34;there\u0026#34;, \u0026#34;there\u0026#39;s\u0026#34;, \u0026#34;these\u0026#34;, \u0026#34;they\u0026#34;, \u0026#34;they\u0026#39;d\u0026#34;, \u0026#34;they\u0026#39;ll\u0026#34;, \u0026#34;they\u0026#39;re\u0026#34;, \u0026#34;they\u0026#39;ve\u0026#34;, \u0026#34;this\u0026#34;, \u0026#34;those\u0026#34;, \u0026#34;through\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;too\u0026#34;, \u0026#34;under\u0026#34;, \u0026#34;until\u0026#34;, \u0026#34;up\u0026#34;, \u0026#34;very\u0026#34;, \u0026#34;was\u0026#34;, \u0026#34;we\u0026#34;, \u0026#34;we\u0026#39;d\u0026#34;, \u0026#34;we\u0026#39;ll\u0026#34;, \u0026#34;we\u0026#39;re\u0026#34;, \u0026#34;we\u0026#39;ve\u0026#34;, \u0026#34;were\u0026#34;, \u0026#34;what\u0026#34;, \u0026#34;what\u0026#39;s\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;when\u0026#39;s\u0026#34;, \u0026#34;where\u0026#34;, \u0026#34;where\u0026#39;s\u0026#34;, \u0026#34;which\u0026#34;, \u0026#34;while\u0026#34;, \u0026#34;who\u0026#34;, \u0026#34;who\u0026#39;s\u0026#34;, \u0026#34;whom\u0026#34;, \u0026#34;why\u0026#34;, \u0026#34;why\u0026#39;s\u0026#34;, \u0026#34;with\u0026#34;, \u0026#34;would\u0026#34;, \u0026#34;you\u0026#34;, \u0026#34;you\u0026#39;d\u0026#34;, \u0026#34;you\u0026#39;ll\u0026#34;, \u0026#34;you\u0026#39;re\u0026#34;, \u0026#34;you\u0026#39;ve\u0026#34;, \u0026#34;your\u0026#34;, \u0026#34;yours\u0026#34;, \u0026#34;yourself\u0026#34;, \u0026#34;yourselves\u0026#34; ] Chúng ta có một mảng chứa các stop words, tức là các từ thường hay xuất hiện trong câu nhưng lại không mang nhiều ý nghĩa. Chúng ta sẽ loại bỏ chúng đi trước khi huấn luyện model phân loại.\nBây giờ, ta sẽ đọc dataset và chuẩn bị dữ liệu training:\nwith open(\u0026#39;bbc-text.csv\u0026#39;, \u0026#39;r\u0026#39;) as csvfile: reader = csv.reader(csvfile, delimiter=\u0026#39;,\u0026#39;) next(reader) for row in reader: labels.append(row[0]) sentence = row[1] # remove stop words for word in stopwords: token = \u0026#39; \u0026#39; + word + \u0026#39; \u0026#39; sentence = sentence.replace(token, \u0026#39; \u0026#39;) sentence = sentence.replace(\u0026#39; \u0026#39;, \u0026#39; \u0026#39;) sentences.append(sentence) print(len(sentences)) Chia dataset thành 2 phần: train và validation:\ntrain_size = int(len(sentences) * training_portion) train_sentences = sentences[:train_size] train_labels = labels[:train_size] validation_sentences = sentences[train_size:] validation_labels = labels[train_size:] Để model có thể hiểu được dataset, cần phải chuyển các câu dạng text sang dạng vector:\n# chuyển text sang vector tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok) tokenizer.fit_on_texts(train_sentences) word_index = tokenizer.word_index label_tokenizer = Tokenizer() label_tokenizer.fit_on_texts(labels) training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels)) validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels)) # padding để các câu có cùng chiều dài train_sequences = tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length) validation_sequences = tokenizer.texts_to_sequences(validation_sentences) validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length) Mình sẽ đi chi tiết phần này trong 1 bài viết khác. Hôm nay các bạn chỉ cần hiểu ý tưởng của nó như vậy là được rồi.\nSau khi đã có dữ liệu huấn luyện, giờ là lúc chúng ta định nghĩa kiến trúc model.\nmodel = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), tf.keras.layers.GlobalAveragePooling1D(), tf.keras.layers.Dense(24, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(6, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) model.summary() Ở bài này, mình chỉ sử dụng một model đơn giản gồm các lớp Embedding, GlobalAveragePooling1D, và Dense.\nHàm plot để vẽ đồ thị quá trình training:\ndef plot_graph(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10,6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training \u0026amp; Validation, Accuracy \u0026amp; Loss\u0026#39;) plt.legend(loc=0) plt.show() Bước cuối cùng là chạy train model, ta sẽ train model với 30 epochs:\nnum_epochs = 30 history = model.fit( train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=1 ) Training output:\nEpoch 1/30 56/56 [==============================] - 0s 4ms/step - loss: 1.7737 - acc: 0.2180 - val_loss: 1.7481 - val_acc: 0.2270 Epoch 2/30 56/56 [==============================] - 0s 2ms/step - loss: 1.7163 - acc: 0.2303 - val_loss: 1.6755 - val_acc: 0.2270 Epoch 3/30 56/56 [==============================] - 0s 2ms/step - loss: 1.6299 - acc: 0.2371 - val_loss: 1.5792 - val_acc: 0.2539 .................... Epoch 27/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0580 - acc: 0.9949 - val_loss: 0.2115 - val_acc: 0.9506 Epoch 28/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0521 - acc: 0.9961 - val_loss: 0.2080 - val_acc: 0.9506 Epoch 29/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0467 - acc: 0.9961 - val_loss: 0.2051 - val_acc: 0.9506 Epoch 30/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0420 - acc: 0.9989 - val_loss: 0.2014 - val_acc: 0.9506 Quá trình train diễn ra khá nhanh, mất khoảng 2 phút trên máy tính của mình. Tại epochs cuối cùng, độ chính xác trên tập validation là 95,06%.\nToàn bộ quá trình này được thể hiện trên đồ thị như sau:\nplot_graph(history)   Model hội tụ khá nhanh và cho kết quả tốt, không có hiện tượng overfitting. Có lẽ train thêm một số epochs nữa sẽ cho kết quả tốt hơn. Bạn có thể thử.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong bài viết tiếp theo, mình sẽ vẫn thực hành bài toán phân loại văn bản nhưng sử dụng kỹ thuật Transfer Learning giống như bên Computer Vision. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/ddd_bbc_text_topic_classification/","tags":["Text Classification"],"title":"Phân loại text theo chủ đề "},{"categories":["MLOps"],"contents":"Bạn đã xây dựng thành công một DL model với độ chính xác rất cao, 99%. Xin chúc mừng bạn!\nVấn đề tiếp theo bạn cần nghĩ đến là làm sao đưa model đó vào trong sản phẩm thực tế, để mọi người có thể sử dụng model của bạn một cách đơn giản và dễ dàng. Xây dựng và triển khai model luôn là 2 công đoạn bắt buộc trong một bài toán về AI. Trong các bài tiếp theo, mình sẽ chia sẻ các cách thức mà chúng ta có thể sử dụng để triển khai một DL model vào trong ứng dụng để sử dụng trong thưc tế.\nCó 2 kiểu Inference mà một AI model có thể được sử dụng:\n  Online Inference: Model phải liên tục xử lý và trả về kết quả dự đoán gần như real-time khi nó nhận được yêu cầu. Số lượng yêu cầu đến thường rất lớn, thậm chí là nhiều yêu cầu đến tại cùng 1 thời điểm. Chính vì vậy mà việc triển khai model theo kiểu này thường phức tạp hơn rất nhiều so với kiểu thứ 2.\n  Batch Inference: Model chỉ chạy Inference tại một số thời điểm cố định trong ngày, và mỗi lần Inference sẽ xử lý một tập hợp (batch) các input data nhất định.\n  Mỗi kiểu Inference phù hợp với các bài toán khác nhau, có lẽ mình sẽ viết một bài so sánh chi tiết hơn về 2 kiểu Inference này.\nTrong bài đầu tiên này chúng ta sẽ sử dụng Flask, một web server framework nhỏ nhẹ, dễ dàng trong việc cài đạt và sử dụng, để triển khai model phân loại Cat\u0026amp;Dog\u0026amp;Panda trong bài trước theo kiểu Online Inference.\n1. Giới thiệu về kiến trúc Client-Server và Flask\nClient-server là kiểu kiến trúc xử lý phân tán, gồm 2 thành phần chính là client và server:\n Client gửi các yêu cầu (requests) đến server. Server xử lý các yêu cầu đó và trả lại kết quả cho client. Yêu cầu có thể là truy vấn database, tính toán, so sánh, dự đoán, \u0026hellip;  Dữ liệu trao đổi giữa client và server gọi là các messages.\nGiao thức trao đổi giữa client-server thường là HTTP/HTTPS trong trường hợp client là website, và server khi đó gọi là webserver. Nếu client không phải là website thì giao thức có thể là TCP/UDP hoặc uwsgi. Giao thức sẽ định nghĩa định dạng dữ liệu, cơ chế truyền, truyền lại, cơ chế xác thực dữ liệu, \u0026hellip;. của các bản tin trao đổi giữa 2 bên. Ví dụ, một HTTP request/Response bao gồm 4 thành phần cơ bản:\n URL đích: đường dẫn (path) đến một dịch vụ cụa thể của server mà client cần giao tiếp. Phương pháp giao tiếp (method): có 4 phương pháp là GET, POST, UPDATE, DELETE. Tùy từng yêu cầu cụ thể của bài toán mà ta sử dụng phương pháp phù hợp. Header: là các metadata kiểu như ngày tháng năm (date), tình trạng(status), kiểu dữ liệu (content-type), \u0026hellip; giúp server xử lý và đồng bộ dữ liệu với client. Body: chứa dữ liệu thực tế mà ta cần gửi từ client đến server hoặc ngược lại.  Flask là một framework để tạo ra thành phần server, bao gồm cả web server. Một số ưu điểm của Flask có thể kể đến như sau:\n Nó giúp triển khai DL model dưới dạng web application nếu bạn muốn cung cấp giao diện web cho người dùng. Đơn giản, dễ dàng cài đặt và sử dụng. Hỗ trợ nhiều chức năng thông qua các end-point URL khác nhau.  Tuy nhiên, Flask không hỗ trợ đầy đủ các chức năng cần thiết của 1 server để có thể sử dụng trong môi trường sản phẩm thực tế giống như là tính bảo mật và sự hỗ trợ cùng lúc nhiều client truy cập. Nó chỉ phù hợp cho các ứng dụng mang tính demo, kiểm thử tính năng model. Nếu cần triển khai thực tế, uWSGI là một sự lựa chọn phù hợp (sẽ được đề cập chi tiết trong bài sau).\n2. Xây dựng Server với Flask\nTa sẽ sử dụng Flask để xây dựng một server phục vụ cả 2 loại client: dạng web và dạng code python (dạng thông thường).\nĐầu tiên, tạo file server.py và import Flask và các thư viện cần thiết:\nimport cv2 import os import numpy as np import tensorflow as tf from flask_cors import CORS from tensorflow.keras.models import load_model from flask import Flask, request, render_template, make_response, jsonify Tạo một instance của Flask:\napp = Flask(__name__) Định nghĩa 1 vài hằng số sử dụng:\nimage_width = 300 image_height = 300 classes = [\u0026#39;cat\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;pandas\u0026#39;] APP_ROOT_1 = os.getenv(\u0026#39;APP_ROOT\u0026#39;, \u0026#39;/infer1\u0026#39;) APP_ROOT_1 = os.getenv(\u0026#39;APP_ROOT\u0026#39;, \u0026#39;/infer2\u0026#39;) Hàm render UI mặc định khi truy cập vào địa chỉ server:\n# render default webpage @app.route(\u0026#39;/\u0026#39;) def home(): return render_template(\u0026#39;home.html\u0026#39;) Hàm này chỉ phục vụ client dạng web. File home.html chính là phần front-end mà chúng ta sẽ viết code để tạo giao diện tương tác với người dùng trên web.\nHàm nhận và xử lý request từ client dạng web:\n@app.route(APP_ROOT_1, methods = [\u0026#39;POST\u0026#39;, \u0026#39;GET\u0026#39;]) def classify_image(): if request.method == \u0026#39;POST\u0026#39;: # geting data from html form img_path = request.form[\u0026#34;img_path\u0026#34;] # call funtion to classify image and receive result result = classify_animal(img_path) # return result to client response = {\u0026#39;result\u0026#39;: result, \u0026#39;image\u0026#39;: img_path} return make_response(jsonify(response), 200) Hàm nhận và xử lý request từ client dạng thông thường:\n@app.route(APP_ROOT_2, methods=[\u0026#34;POST\u0026#34;]) def infer(): data = request.json image = data[\u0026#39;image_path\u0026#39;] return classify_animal(img_path) Hãy nhớ lại ở bài trước, sau khi huấn luyện xong model, ta đã lưu nó thành file animal_model_classification.h5 vào ổ cứng. File này có kích thước khá nặng, khoảng 1.7GB. Bây giờ, ta sẽ sử dụng model đó đã nhận diện.\nLoad DL model:\nmodel = load_model(\u0026#39;animal_model_classification.h5\u0026#39;) Hàm dưới đây nhận vào tham số là đường dẫn đến ảnh cần nhận diện và trả về kết quả:\ndef classify_animal(img_path): # read image image = cv2.imread(img_path) image = image/255 image = cv2.resize(image, (image_width,image_height) image = np.reshape(image, [1,image_width,image_height,3]) # pass the image through the network to obtain our predictions preds = model.predict(image) label = classes[np.argmax(preds)] return label Cuối cùng, sử dụng hàm run() để khởi tạo server:\nif __name__ == \u0026#39;__main__\u0026#39;: app.run() Để chạy server, mở cửa sổ terminal và gõ lệnh:\n$ python server.py Nếu mọi thứ Ok thì cửa sổ terminal sẽ xuất hiện như sau:\n Như vậy là đã xong phần backend, tiếp theo ta sẽ viết code cho front-end.\n3. Web client\nWeb client có giao diện đơn giản gồm 1 button cho phép user chọn ảnh cần nhận diện và 1 khu vực để hiển thị ảnh kèm kết quả.\n Vì web không phải là lĩnh vực chuyên sâu của mình nên mình sẽ không đi chi tiết code ở đây. Các bạn có thể tham khảo code web trên github của mình.\nĐể kiểm tra hoạt động, truy cập vào địa chỉ http://localhost:5000, upload một bức ảnh, click Detect button. Kết quả phân loại sẽ được hiển thị.\n 4. Python client\nTạo file client.py và code như sau:\nimport requests import numpy as np ENDPOINT_URL = \u0026#39;http://0.0.0.0:5000/infer2\u0026#39; def infer(): img_path = \u0026#39;dog1.jpg\u0026#39; data = { \u0026#39;image\u0026#39;: image_path } response = requests.post(ENDPOINT_URL, json = data) response.raise_for_status() print(response.content) if __name__ ==\u0026#34;__main__\u0026#34;: infer() Khởi chạy client:\n$ python client.py Kết quả trả về từ server:\nb\u0026#39;dog\u0026#39; Thử lại với một ảnh ảnh con mèo, cat.1.jpg. Kết qủa trả về:\nb\u0026#39;cat\u0026#39; 5. Kết luận\nNhư vậy là chúng ta đã cùng nhau triển khai xong DL model sử dụng Flask. Mặc dù tồn tại nhiều nhược điểm nhưng khi cần nhanh chóng đạt được kết quả để thử nghiệm thì Flask vẫn được tin dùng.\nBài viết tiếp theo, mình sẽ giới thiệu một cách nâng cao hơn để triển khai DL model, thường được áp dụng trong các bài toán thực tế. Mời các bạn đón đọc!\nToàn bộ source code của backend và front-end trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây. Vì model animal_model_classification.h5 có dung lượng khá lớn (\u0026gt; 1.5GB) nên mình không upload lên github được. Các bạn hãy sử dụng model của chính mình để thực hành nhé!\n6. Tham khảo\n Flask AI Summer  ","permalink":"https://tiensu.github.io/blog/36_deploy_ai_model_with_flask_online_inference/","tags":["MLOps"],"title":"Triển khai AI model sử dụng Flask"},{"categories":["CNN","Image Classification"],"contents":"Những bài toán mà chỉ có 2 lớp cần phân biệt gọi là binary classification, còn những bài toán có nhiều hơn 2 lớp được gọi là multiple classification.\nCó một vài sự khác biệt trong cách cài đặt CNN model giữa 2 loại bài toán này. Trong hôm nay chúng ta sẽ cùng tìm hiểu điều đó thông qua thực hiện phân loại 3 classes: Cat, Dog và Panda của bộ dữ liệu animals. Bộ dataset này gồm 3000 ảnh chia đều cho mỗi class.\nTa sẽ bắt tay vào thực hiện code luôn, những điểm khác biệt sẽ được đề cập trong quá trình viết code.\nSử dụng kiến thức đã học từ bài trước, ta sẽ thực hiện bài này theo 2 cách và so sánh kết quả:\n Không sử dụng Transfer Learning Sử dụng Transfer Learning  Đầu tiên, download dataset về thư mục làm việc và dùng thư viện split-folers để chia dữ liệu thành 2 tập train và validation.\nImport các thư viện sẽ sử dụng:\nimport os import random import shutil import tensorflow as tf import matplotlib.pyplot as plt from tensorflow import keras from tensorflow.keras import layers, Model from tensorflow.keras.applications.inception_v3 import InceptionV3 from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping Chuẩn bị dữ liệu training:\ndef data_gen(): train_gen = ImageDataGenerator( rescale=1/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\u0026#39;nearest\u0026#39; ) validation_gen = ImageDataGenerator( rescale=1/255 ) train_datagen = train_gen.flow_from_directory( \u0026#39;Animals/training\u0026#39;, target_size=(300,300), batch_size=32, class_mode=\u0026#39;categorical\u0026#39; ) validation_datagen = validation_gen.flow_from_directory( \u0026#39;Animals/validation\u0026#39;, target_size=(300,300), batch_size=32, class_mode=\u0026#39;categorical\u0026#39; ) return train_datagen, validation_datagen Model tự định nghĩa:\ndef create_own_model(): model = keras.Sequential([ # CONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; POOL =\u0026gt; DO layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, input_shape=(300, 300, 3)), layers.BatchNormalization(), layers.MaxPooling2D(3,3), layers.Dropout(0.25), # (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.MaxPooling2D(2,2), layers.Dropout(0.25), # (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.MaxPooling2D(2,2), layers.Dropout(0.25), # (FC =\u0026gt; RELU =\u0026gt; BN =\u0026gt; DO)*2 =\u0026gt; FC =\u0026gt; SOFTMAX layers.Flatten(), layers.Dense(1024, activation=\u0026#39;relu\u0026#39;), layers.BatchNormalization(), layers.Dropout(0.25), layers.Dense(512, activation=\u0026#39;relu\u0026#39;), layers.BatchNormalization(), layers.Dropout(0.25), layers.Dense(3, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Kiến trúc model:\nCONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; POOL =\u0026gt; DO =\u0026gt; (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO =\u0026gt; (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO =\u0026gt; (FC =\u0026gt; RELU =\u0026gt; BN =\u0026gt; DO)*2 =\u0026gt; FC =\u0026gt; SOFTMAX\nModel sử dụng Transfer Learning:\ndef create_transfer_learning_model(): base_model = InceptionV3( input_shape=(300,300,3), include_top=False, weights=\u0026#39;imagenet\u0026#39; ) for layer in base_model.layers: layer.trainable = False head_model = base_model.output head_model = layers.Flatten()(head_model) head_model = layers.Dense(1024, activation=\u0026#39;relu\u0026#39;)(head_model) head_model = layers.BatchNormalization()(head_model) head_model = layers.Dropout(0.25)(head_model) head_model = layers.Dense(512, activation=\u0026#39;relu\u0026#39;)(head_model) head_model = layers.BatchNormalization()(head_model) head_model = layers.Dropout(0.5)(head_model) head_model = layers.Dense(3, activation=\u0026#39;softmax\u0026#39;)(head_model) model = Model(inputs=base_model.input, outputs=head_model) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Trong phương pháp Transfer Learning, ta sử dụng pre-trained InceptionV3 làm base_model.\nNhư chúng ta thấy, có 2 điểm khác biệt ở đây:\n Hàm activation ở layer cuối Nếu như bài toán binary classification sử dụng hàm sigmoid chỉ có 1 output thì bài toán multiple classification sử dụng hàm softmax, số lượng output bằng số classes cần phân loại. Mình sẽ có bài phân tích chi tiết về các loại hàm này sau. Hàm loss Binary classification sử dụng hàm binary_crossentropy, còn multiple classification sử dụng categorical_crossentropy hoặc sparse_categorical_crossentropy. Mình cũng sẽ viết một bài về các dạng hàm loss trong tương lai.  Định nghĩa callback functions:\ndef create_callbacks(): callback_1 = EarlyStopping(monitor=\u0026#39;val_acc\u0026#39;, patience=4) callback_2 = ModelCheckpoint( \u0026#39;Animals_ModelCheckpoints/model-{epoch:02d}-{val_acc:.2f}.hdf5\u0026#39;, save_best_only=True, save_weights_only=True, monitor=\u0026#39;val_acc\u0026#39;, save_freq=\u0026#39;epoch\u0026#39;, mode=\u0026#39;auto\u0026#39;, verbose=1 ) return [callback_1, callback_2] Hàm vẽ đồ thị:\ndef plot_graph(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epoch = range(len(acc)) plt.plot(epoch, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epoch, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epoch, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epoch, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training \u0026amp; Validation, Accuracy \u0026amp; Loss\u0026#39;) plt.legend(loc=0) plt.show() Train model với kiến trúc tự định nghĩa:\ntraining_datagen, validation_datagen = data_gen() model = create_own_model() history = model.fit( training_datagen, epochs=30, validation_data=validation_datagen, callbacks=create_callbacks(), verbose=1 ) Training output:\nFound 2400 images belonging to 3 classes. Found 600 images belonging to 3 classes. Epoch 1/30 2/75 [..............................] - ETA: 4s - loss: 1.8446 - acc: 0.3906WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0442s vs `on_train_batch_end` time: 0.0907s). Check your callbacks. 75/75 [==============================] - ETA: 0s - loss: 1.2381 - acc: 0.5292 Epoch 00001: val_acc improved from -inf to 0.33333, saving model to Animals_ModelCheckpoints/model-01-0.33.hdf5 75/75 [==============================] - 40s 533ms/step - loss: 1.2381 - acc: 0.5292 - val_loss: 4.0135 - val_acc: 0.3333 Epoch 2/30 75/75 [==============================] - ETA: 0s - loss: 0.8709 - acc: 0.6042 Epoch 00002: val_acc did not improve from 0.33333 75/75 [==============================] - 40s 533ms/step - loss: 0.8709 - acc: 0.6042 - val_loss: 3.9076 - val_acc: 0.3333 Epoch 3/30 75/75 [==============================] - ETA: 0s - loss: 0.8323 - acc: 0.6008 Epoch 00003: val_acc improved from 0.33333 to 0.35000, saving model to Animals_ModelCheckpoints/model-03-0.35.hdf5 75/75 [==============================] - 40s 535ms/step - loss: 0.8323 - acc: 0.6008 - val_loss: 3.0383 - val_acc: 0.3500 ........ Epoch 11/30 75/75 [==============================] - ETA: 0s - loss: 0.6230 - acc: 0.6958 Epoch 00011: val_acc did not improve from 0.72667 75/75 [==============================] - 40s 536ms/step - loss: 0.6230 - acc: 0.6958 - val_loss: 0.9326 - val_acc: 0.5933 Epoch 12/30 75/75 [==============================] - ETA: 0s - loss: 0.5867 - acc: 0.7133 Epoch 00012: val_acc did not improve from 0.72667 75/75 [==============================] - 40s 539ms/step - loss: 0.5867 - acc: 0.7133 - val_loss: 0.6075 - val_acc: 0.7067 Epoch 13/30 75/75 [==============================] - ETA: 0s - loss: 0.5752 - acc: 0.7262 Epoch 00013: val_acc did not improve from 0.72667 75/75 [==============================] - 41s 540ms/step - loss: 0.5752 - acc: 0.7262 - val_loss: 0.5461 - val_acc: 0.7250 Model dừng train sau 13 epochs do giá trị của val_acc không tăng sau 5 epochs liên tiếp từ epoch 9 đến epoch 13. Độ chính xác cao nhất đạt được trên tập validation 72.67% tại epoch thứ 9.\nĐồ thị quá trình training:  Bây giờ, thử sử dụng pre-trained model:\nmodel = create_transfer_learning_model() training_datagen, validation_datagen = data_gen() history = model.fit( training_datagen, validation_data=validation_datagen, epochs=30, callbacks=create_callbacks(), verbose=1 ) Traning output:\nFound 2400 images belonging to 3 classes. Found 600 images belonging to 3 classes. Epoch 1/30 75/75 [==============================] - ETA: 0s - loss: 0.1724 - acc: 0.9525 Epoch 00001: val_acc improved from -inf to 0.95667, saving model to Animals_ModelCheckpoints/model-01-0.96.hdf5 75/75 [==============================] - 41s 548ms/step - loss: 0.1724 - acc: 0.9525 - val_loss: 0.2077 - val_acc: 0.9567 Epoch 2/30 75/75 [==============================] - ETA: 0s - loss: 0.1153 - acc: 0.9679 Epoch 00002: val_acc improved from 0.95667 to 0.99167, saving model to Animals_ModelCheckpoints/model-02-0.99.hdf5 75/75 [==============================] - 42s 559ms/step - loss: 0.1153 - acc: 0.9679 - val_loss: 0.0331 - val_acc: 0.9917 Epoch 3/30 75/75 [==============================] - ETA: 0s - loss: 0.0719 - acc: 0.9771 Epoch 00003: val_acc improved from 0.99167 to 0.99333, saving model to Animals_ModelCheckpoints/model-03-0.99.hdf5 75/75 [==============================] - 42s 557ms/step - loss: 0.0719 - acc: 0.9771 - val_loss: 0.0082 - val_acc: 0.9933 ........ Epoch 8/30 75/75 [==============================] - ETA: 0s - loss: 0.0410 - acc: 0.9833 Epoch 00008: val_acc did not improve from 0.99500 75/75 [==============================] - 43s 569ms/step - loss: 0.0410 - acc: 0.9833 - val_loss: 0.0288 - val_acc: 0.9933 Epoch 9/30 75/75 [==============================] - ETA: 0s - loss: 0.0554 - acc: 0.9842 Epoch 00009: val_acc did not improve from 0.99500 75/75 [==============================] - 44s 588ms/step - loss: 0.0554 - acc: 0.9842 - val_loss: 0.0140 - val_acc: 0.9917 Epoch 10/30 75/75 [==============================] - ETA: 0s - loss: 0.0632 - acc: 0.9804 Epoch 00010: val_acc did not improve from 0.99500 75/75 [==============================] - 43s 572ms/step - loss: 0.0632 - acc: 0.9804 - val_loss: 0.0219 - val_acc: 0.9933 Model dừng train sớm hơn, tại epoch thứ 10 sau 5 epochs liên tiếp không cải thiện về giá trị của val_acc (từ epoch 6 đến epoch 10). Độ chính xác trên tập validation cao nhất đạt được là 99,5% tại epoch thứ 5. Kết quả tốt hơn so với sử dụng model tự định nghĩa rất nhiều.\nĐồ thị quá trình training:  Như vậy, qua bài này ta đã biết được cách thức xây dựng kiến trúc CNN model cho bài toán multiple classification. Đồng thời ta cũng nhận thấy rõ ràng ưu điểm của kỹ thuật Transfer Learning so với cách tự xây dựng CNN model. Có thể nói rằng, sử dụng pre-trained model luôn cho kết quả tốt hơn, trừ khi chúng ta có lý do cụ thể để không sử dụng chúng.\nCuối cùng, ta lưu lại model để sử dụng cho việc dự đoán về sau khi triển khai model vào sản phẩm thực tế:\nmodel.save(\u0026#39;animal_classification_model.h5\u0026#39;) Source code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong các bài viết tiếp theo, mình sẽ viết về một số bài toán NLP và các kỹ thuật cần dùng để giải quyết chúng. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/35_cnn_model_multiple_classification/","tags":["CNN","Image Classification"],"title":"Xây dựng CNN model cho bài toán đa lớp"},{"categories":["CNN","Image Classification"],"contents":"Trong các bài toán thực tế, khi làm việc với bộ dataset lớn và kiến trúc model phức tạp, việc huấn luyện model sẽ mất rất nhiều thời gian. Vài ngày hoặc thậm chí cả tuần mới ra được kết quả. Nếu ta chỉ train một lần thì không có gì đáng nói, nhưng thường thì ta sẽ train đi train lại rất nhiều lần, mỗi lần điều chỉnh hyper-parameter lại phải chạy train lại. Việc này quả thực rât rất mất thời gian và chán nản.\nHoặc khi chỉ có một lượng nhỏ dữ liệu để train model, chắc chắn sẽ cho ra một model không tốt, vì nó không học được kết các khía cạnh của dữ liệu. Khi triển khai thực tế chắc chắn sẽ thất bại.\nKỹ thuật Transfer Learning ra đời để giải quyết khó khăn này. Ý tưởng chính của nó là tận dụng lại những model kinh diển (VGG, Resnet, InceptionNet, \u0026hellip;), đã được train trên những tập dữ liệu lớn (pre-trained models), loại bỏ các layers classification ở gần cuối (thường là các lớp FC), chỉ giữ lại các layers ở đầu làm nhiệm vụ trích xuất đặc trưng của dữ liệu.\nCó 2 loại transfer learning:\n Feature extractor: Sử dụng pre-trained model như là bộ trích xuất đặc trưng của dữ liệu. Các đặc trưng này sau đó sẽ được phân loại sử dụng các thuật toán ML như kNN, SVM, Decision Tree, \u0026hellip; Fine tuning: Loại bỏ các layers cuối làm nhiệm vụ phân loại trong pre-trained model, thêm vào các layers mới dựa theo bộ dữ liệu mà chúng ta có. Sau đó, train lại model tại những layers mà ta mới thêm vào.  Vậy thì khi nào ta nên sử dụng Transfer Learning?\nDựa trên kích thước và độ tương quan giữa CSDL mới và CSDL gốc (chủ yếu là ImageNet) để train các mô hình có sẵn, CS231n đưa ra một vài lời khuyên:\n  CSDL mới là nhỏ và tương tự như CSDL gốc. Vì CSDL mới nhỏ, việc tiếp tục train model dễ dẫn đến hiện tượng overfitting. Cũng vì hai CSDL là tương tự nhau, ta dự đoán rằng các high-level features là tương tự nhau. Vậy nên ta không cần train lại model mà chỉ cần train một classifer dựa trên feature vectors ở đầu ra ở layer gần cuối.\n  CSDL mới là lớn và tương tự như CSDL gốc. Vì CSDL này lớn, overfitting ít có khả năng xảy ra hơn, ta có thể train mô hình thêm một chút nữa (toàn bộ hoặc chỉ một vài layers cuối).\n  CSDL mới là nhỏ và rất khác với CSDL gốc. Vì CSDL này nhỏ, tốt hơn hết là dùng các classifier đơn giản (các linear classifiers) để tránh overfitting). Nếu muốn train thêm, ta cũng chỉ nên train các layer cuối. Hoặc có một kỹ thuật khác là coi đầu ra của một layer xa layer cuối hơn làm các feature vectors.\n  CSDL mới là lớn và rất khác CSDL gốc. Trong trường hợp này, ta vẫn có thể sử dụng mô hình đã train như là điểm khởi tạo cho mô hình mới, không nên train lại từ đầu.\n  Có một điểm đáng chú ý nữa là khi tiếp tục train các mô hình này, ta chỉ nên chọn learning rate nhỏ để các weights mới không đi quá xa so với các weights đã được trained ở các mô hình trước.\nOK, ta sẽ bắt tay vào thực hành kỹ thuật này (theo cách thứ 2) ngay bây giờ!\nYêu cầu bài toán là huấn luyện một CNN model để phân loại ảnh chứa ngựa và người trong bộ dataset horse-or-humand.\nDownload bộ dataset horse-or-humand về máy tính, giải nén và sử dụng thư viện split-folders để chia thành 2 phần train set và validation set.\nĐầu tiên, import các thư viện cần thiết:\nimport os import tensorflow as tf import matplotlib.pyplot as plt from tensorflow.keras import layers from tensorflow.keras import Model from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.applications.inception_v3 import InceptionV3 config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Các pre-trained model phổ biến đã được tích hợp sẵn trong tensorflow. Ở đây ta khai báo lớp InceptionV3 để sử dụng InceptionNet pre-trained model.\nTiếp theo là chuẩn bị dữ liệu huấn luyện:\ndef data_gen(): training_datagen = ImageDataGenerator( rescale=1/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, horizontal_flip=True, fill_mode=\u0026#39;nearest\u0026#39; ) validation_datagen = ImageDataGenerator(rescale=1/255) training_generator = training_datagen.flow_from_directory( \u0026#39;horse-and-humand/train\u0026#39;, target_size=(224,224), batch_size=16, class_mode=\u0026#39;binary\u0026#39; ) validation_generator = validation_datagen.flow_from_directory( \u0026#39;horse-and-humand/validation\u0026#39;, target_size=(224,224), batch_size=16, class_mode=\u0026#39;binary\u0026#39; ) return training_generator, validation_generator Khai báo 2 hàm callback: EarlyStopping và ModelCheckpoint:\ndef create_callbacks(): callback_1 = ModelCheckpoint( \u0026#39;horse-humand_model_checkpoint/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\u0026#39;, monitor=\u0026#39;val_acc\u0026#39;, save_best_only=True, save_weights_only=True, save_freq=\u0026#39;epoch\u0026#39;, mode=\u0026#39;auto\u0026#39;, verbose=1 ) callback_2 = EarlyStopping(monitor=\u0026#39;val_acc\u0026#39;, patience=5) return [callback_1, callback_2] Phần quan trọng nhất trong bài này là định nghĩa model, sử dụng kỹ thuật Transfer Learning:\ndef create_model(): # Load pre-trained model base_model = InceptionV3( input_shape=(224,224,3), # Kích thước ảnh đầu vào include_top=False, # Loại bỏ các FC layers ở cuối weights=\u0026#39;imagenet\u0026#39; # Sử dụng các weights được train trên tập imagenet ) # Đóng băng các layers của pre-trained model, không cho chúng update for layer in base_model.layers: layer.trainable = False # Tạo head_model head_model = base_model.output head_model = layers.Flatten()(head_model) head_model = layers.Dense(1024, activation=\u0026#39;relu\u0026#39;)(head_model) head_model = layers.Dropout(0.2)(head_model) head_model = layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;)(head_model) model = Model(inputs=base_model.input, outputs=head_model) model.compile(optimizer=RMSprop(lr=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Model được tạo thành gồm 2 phần:\n base_model: chính là pre-trained model (đã loại bỏ các FC layers ở cuối). head_model: là các FC layers được thêm vào làm nhiệm vụ phân loại dựa theo tập dữ liệu mới.  Trong bài toán này, ta sử dụng pre-trained model của mạng InceptionNetV3 trên tập dữ liệu imagenet. Head_model bao gồm 2 FC layers xen kẽ DO layer ở giữa.\nHàm vẽ đồ thị:\ndef plot_chart(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10, 6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Traing and Validation, Accuracy and Loss\u0026#39;) plt.legend(loc=0) plt.show() Gộp tất cả lại và tiến hành train model:\nmodel = create_model() training_generator, validation_generator = data_gen() callbacks = create_callbacks() history = model.fit( training_generator, epochs=30, callbacks=[callback_1, callback_2], validation_data=validation_generator, verbose=1 ) Training output:\nEpoch 1/30 65/65 [==============================] - ETA: 0s - loss: 0.1342 - acc: 0.9786 Epoch 00001: val_acc improved from -inf to 0.91797, saving model to horse-humand_model_checkpoint/weights-improvement-01-0.92.hdf5 65/65 [==============================] - 12s 180ms/step - loss: 0.1342 - acc: 0.9786 - val_loss: 0.4456 - val_acc: 0.9180 Epoch 2/30 65/65 [==============================] - ETA: 0s - loss: 0.0992 - acc: 0.9708 Epoch 00002: val_acc did not improve from 0.91797 65/65 [==============================] - 12s 189ms/step - loss: 0.0992 - acc: 0.9708 - val_loss: 0.9824 - val_acc: 0.8594 Epoch 3/30 65/65 [==============================] - ETA: 0s - loss: 0.0501 - acc: 0.9903 Epoch 00003: val_acc improved from 0.91797 to 0.95312, saving model to horse-humand_model_checkpoint/weights-improvement-03-0.95.hdf5 ............ Epoch 8/30 65/65 [==============================] - ETA: 0s - loss: 0.1031 - acc: 0.9834 Epoch 00008: val_acc did not improve from 0.99219 65/65 [==============================] - 13s 201ms/step - loss: 0.1031 - acc: 0.9834 - val_loss: 0.5211 - val_acc: 0.9219 Epoch 9/30 65/65 [==============================] - ETA: 0s - loss: 0.0893 - acc: 0.9786 Epoch 00009: val_acc did not improve from 0.99219 65/65 [==============================] - 13s 203ms/step - loss: 0.0893 - acc: 0.9786 - val_loss: 0.5921 - val_acc: 0.9180 Epoch 10/30 65/65 [==============================] - ETA: 0s - loss: 0.1015 - acc: 0.9815 Epoch 00010: val_acc did not improve from 0.99219 65/65 [==============================] - 13s 204ms/step - loss: 0.1015 - acc: 0.9815 - val_loss: 0.4264 - val_acc: 0.9453 Model dừng train sau 10 epochs, độ chính xác cao nhất đạt được trên tập validation là 94.53%, một kết quả khá cao với số lượng epochs \u0026ldquo;khiêm tốn\u0026rdquo; như vậy.\nQuan sát thư mục horse-humand_model_checkpoint ta cũng thấy model được lưu tại một số điểm checkpoint. Model có độ chính xác cao nhất tại epoch thứ 3.\n├── weights-improvement-01-0.92.hdf5 ├── weights-improvement-03-0.95.hdf5 Kiểm tra quá trình huấn luyện bằng cách thể hiện giá trị loss và accuracy lên đồ thị:\nplot_chart(history)   Mặc dù giá trị của val_loss dao động 1 chút nhưng acc, val_acc và loss đều khá lý tưởng, chứng tỏ sự hiệu quả của kỹ thuật Transfer Learning trong bài toàn này.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nCác bài viết từ trước đến giờ đều chỉ phân loại 2 lớp. Đối với bài toán phân loại nhiều lớp thì sẽ như thế nào? Câu trả lời sẽ có ở bài tiếp theo . Mời các bạn đón đọc!\nTham khảo\n Coursera MachineLearningCoban  ","permalink":"https://tiensu.github.io/blog/34_transfer_learning/","tags":["CNN","Image Classification"],"title":"Sử dụng kỹ thuật Transfer Learning khi huấn luyện CNN model"},{"categories":["CNN","Image Classification"],"contents":"Tiếp theo bài trước, trong bài này chúng ta sẽ áp dụng thêm 2 kỹ thuật mới vào CNN model Cat\u0026amp;Dog classification:\n  Data Augmentation: Đây là kỹ thuật tăng cường dữ liệu huấn luyện cho model. Nó đặc biệt hữu ích khi chúng ta có ít dữ liệu vì từ một ảnh gốc ban đầu, thông qua các phép biến đổi hình thái học (xoay, lật, phóng to, thu nhỏ, thay đổi độ sáng, độ tương phải, \u0026hellip;) ta có thêm được nhiều ảnh mới. Kỹ thuật này không chỉ giới hạn trong các bài toán liên quan đến ảnh, mà các bài toán Data Science và NLP cũng có thể sử dụng được.    Model Checkpoint: Đây thực chất là một hàm callback, được gọi sau mỗi epoch trong quá trình huấn luyện model. Nó sẽ lưu lại model nếu giá trị loss hoặc accuracy được cải thiện sau mỗi epoch.\n  Cùng với EarlyStopping thì 2 kỹ thuật này được cũng được sử dụng rất thường xuyên để hạn chế hiện tượng overfitting của model.\nBây giờ, ta sẽ sử dụng chúng trong bài toán xây dựng CNN model phân loại Cat\u0026amp;Dog.\nImport thư viện:\nimport os import random import tensorflow as tf import shutil import matplotlib.pyplot as plt from tensorflow import keras from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint from tensorflow.keras.preprocessing.image import ImageDataGenerator config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Hai kỹ thuật Data augmentation và Model checkpoint sẽ được sử dụng thông qua lớp ImageDataGenerator và ModelCheckpoint, tương ứng.\nLớp ImageDataGenerator cho phép ta biến đổi ảnh gốc thành nhiều ảnh khác nhau thông quá các tham số truyền vào. Như trong hàm gen_data() dưới đây, ta sinh ra được 6 ảnh mới thông qua các phép biến đổi:\n Quay 40 độ Dịch theo chiều rộng 0.2 pixcel Dịch theo chiều cao 0.2 pixcel Cắt (xén) 0.2 pixcel Phóng to 0.2 pixcel Lật ngang  Chú ý rằng, các ảnh mới sinh ra không được lưu vào ổ cứng máy tính, mà chỉ được sinh ra tại thời điểm huấn luyện model và lưu tạm thời trong RAM. Khi kết thức quá trình training thì các ảnh đó cũng sẽ mất.\nTham số fill_mode='nearest' chỉ ra phương pháp bù lại giá trị cho những pixcel tại các vị trí bị mất mát do quá trình biến đổi. Nearest tức là dựa vào giá trị của các pixcel xung quanh, gần nó nhất (theo một tiêu chuẩn nào đó).\ndef gen_data(): training_datagen = ImageDataGenerator( rescale=1/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\u0026#39;nearest\u0026#39; ) validation_datagen = ImageDataGenerator( rescale=1/255 ) training_generator = training_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/train\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) validation_generator = validation_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/val\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) return training_generator, validation_generator Tiếp theo, ta khai báo một instance của ModelCheckpoint:\ncallback_1 = ModelCheckpoint( \u0026#39;cat_dog_model_checkpoint/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\u0026#39;, # Tên model tại mỗi điểm checkpoint monitor=\u0026#39;val_acc\u0026#39;, # Giá trị cần theo dõi save_best_only=True, # Chỉ lưu những model tốt nhất đến thời điểm checkpoint save_weights_only=True, # Chỉ lưu weights của model (để giảm kích thước) save_freq=\u0026#39;epoch\u0026#39;, # Checkpoint sau mỗi epoch mode=\u0026#39;auto\u0026#39;, # Val_acc phải tăng mới tính là model được cải thiện. Nếu monitor=\u0026#39;loss/val_loss\u0026#39; thì nó phải giảm mới tính là model được cải thiện verbose=1 # Hiển thị thông tin model lúc checkpoint ) Chi tiết từng tham số của ModelCheckpoint instance được giải thích chi tiết theo các comments trong code khai báo.\nTa vẫn sử dụng thêm EarlyStopping để tiết kiệm thời gian training:\ncallback_2 = EarlyStopping(monitor=\u0026#39;val_acc\u0026#39;, patience=5) Kiến trúc CNN model vẫn giữ nguyên như bài trước:\ndef create_model(): model = keras.models.Sequential([ keras.layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150, 150, 3)), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Flatten(), keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=RMSprop(lr=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Hàm plot_chart để thể hiện kết quả training lên đồ thị:\ndef plot_chart(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10, 6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Traing and Validation, Accuracy and Loss\u0026#39;) plt.legend(loc=0) plt.show() Cuối cùng, gộp tất cả lại và train model:\ntraining_generator, validation_generator = gen_data() model = create_model() history = model.fit( training_generator, epochs=30, validation_data=validation_generator, callbacks=[callback_1, callback_2], verbose=1 ) Output:\nFound 20000 images belonging to 2 classes. Found 5000 images belonging to 2 classes. Epoch 1/30 2/625 [..............................] - ETA: 17s - loss: 1.6500 - acc: 0.5469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0223s vs `on_train_batch_end` time: 0.0339s). Check your callbacks. 625/625 [==============================] - ETA: 0s - loss: 0.6804 - acc: 0.5882 Epoch 00001: val_acc improved from -inf to 0.69340, saving model to horse-humand_model_checkpoint/weights-improvement-01-0.69.hdf5 625/625 [==============================] - 115s 184ms/step - loss: 0.6804 - acc: 0.5882 - val_loss: 0.6017 - val_acc: 0.6934 Epoch 2/30 625/625 [==============================] - ETA: 0s - loss: 0.6239 - acc: 0.6591 Epoch 00002: val_acc improved from 0.69340 to 0.73060, saving model to horse-humand_model_checkpoint/weights-improvement-02-0.73.hdf5 625/625 [==============================] - 112s 179ms/step - loss: 0.6239 - acc: 0.6591 - val_loss: 0.5446 - val_acc: 0.7306 Epoch 3/30 625/625 [==============================] - ETA: 0s - loss: 0.5922 - acc: 0.6922 Epoch 00003: val_acc improved from 0.73060 to 0.76340, saving model to horse-humand_model_checkpoint/weights-improvement-03-0.76.hdf5 625/625 [==============================] - 113s 181ms/step - loss: 0.5922 - acc: 0.6922 - val_loss: 0.5106 - val_acc: 0.7634 ............ Epoch 30/30 625/625 [==============================] - ETA: 0s - loss: 0.4392 - acc: 0.8173 Epoch 00030: val_acc did not improve from 0.86600 625/625 [==============================] - 112s 179ms/step - loss: 0.4392 - acc: 0.8173 - val_loss: 0.4723 - val_acc: 0.7768 Model được train đầy đủ 30 epochs, không bị dừng giữa chừng do không thỏa mãn điều kiện của EarlyStopping.\nQuan sát thư mục cat_dog_model_checkpoint ta cũng thấy model được lưu tại một số điểm checkpoint. Model có độ chính xác cao nhất tại epoch thứ 29.\n├── weights-improvement-01-0.69.hdf5 ├── weights-improvement-01-0.98.hdf5 ├── weights-improvement-02-0.73.hdf5 ├── weights-improvement-02-1.00.hdf5 ├── weights-improvement-03-0.76.hdf5 ├── weights-improvement-05-0.80.hdf5 ├── weights-improvement-07-0.81.hdf5 ├── weights-improvement-10-0.83.hdf5 ├── weights-improvement-12-0.83.hdf5 ├── weights-improvement-17-0.83.hdf5 ├── weights-improvement-20-0.85.hdf5 ├── weights-improvement-24-0.86.hdf5 └── weights-improvement-29-0.87.hdf5 Kiểm tra quá trình huấn luyện bằng cách thể hiện giá trị loss và accuracy lên đồ thị:\nplot_chart(history)   Các giá trị loss và accuracy tuy có sự dao động nhưng kết quả cuối cùng vẫn khá tốt. Model không bị overfit quá nhiều, có thể chấp nhận được.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, ta sẽ học thêm một kỹ thuật rất thú vị nữa, giúp chúng ta giảm rất nhiều công sức trong việc huấn luyện model, đó là Tranfer Learning. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/33_dataaugmentation_modelcheckpoint_cnn/","tags":["CNN","Image Classification"],"title":"Sử dụng kỹ thuật Data Augmentation và Model Checkpoint khi huấn luyện CNN model"},{"categories":["CNN","Image Classification"],"contents":"Nếu như bài trước, toàn bộ dữ liệu được đưa vào training, thì bài này, ta sẽ chia tập dữ liệu thành 2 phần:\n Train set: Dùng để huấn luyện model. Validation set: Dùng đễ đánh giá model trong suốt quá trình training.  OK, hãy bắt đầu!\nTrước tiên, download bộ dataset cat-and-dog và giải nén về máy tính của bạn tại thư mục làm việc. Ta chỉ cần download file train.zip.\nBộ dataset này bao gồm 25.000 bức ảnh, chia thành 2 lớp chó và mèo. Mỗi lớp có 12.500 ảnh, kích thước 150x150.\nTa tiếp tục sử dụng lớp ImageDataGenerator để chuẩn bị dữ liệu cho training model. Lớp ImageDataGenerator yêu cầu cấu trúc thư mục dataset có dạng như sau:\n- training - class 1 - image 1 - image 2 - ... - class 2 - image 3 - image 4 - ... - ... - validation - class 1 - image 5 - image 6 - ... - class 2 - image 7 - image 8 Để chuẩn hóa cấu trúc thư mục như yêu cầu, ta có thể tự viết code để copy các ảnh vào đúng thư mục mong muốn. Hoặc có 1 cách đơn giản hơn là sử dụng thư viện split-folders.\nCài đặt thư viện:\npip install split-folders Sử dụng lệnh sau để tạo dữ liệu theo cấu trúc mong muốn:\nsplitfolders cats-and-dogs --ratio .8 .2 --output cat-dog-dataset Ta được thư mục output cat-dog-dataset:\ncat-dog-dataset ├── train │ ├── cats │ └── dogs └── val ├── cats └── dogs Có dữ liệu chuẩn chỉ rồi, giờ ta sẽ bắt tay vào viết code để train model.\nĐầu tiên, như thường lệ vẫn là import các thư viện sử dụng:\nimport os import random import tensorflow as tf import shutil import matplotlib.pyplot as plt from tensorflow import keras from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.preprocessing.image import ImageDataGenerator config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Ở đây, ngoài các thư viện, các lớp quen thuộc, ta có sử dụng thêm hàm EarlyStopping của lớp callback. Hàm này cho phép model dừng training khi nó thoả mãn một tiêu chí về độ chính xác mà người dùng có thể định nghĩa được. So với hàm callback tự viết như trong các bài trước thì việc sử dụng EarlyStopping linh động hơn rất nhiều. Ta sẽ đi chi tiết ở phần sau.\nTiếp theo, ta sử dụng lớp ImageDataGenerator để chuẩn bị dữ liệu cho việc huấn luyện model.\ndef gen_data(): training_datagen = ImageDataGenerator( rescale=1/255 ) validation_datagen = ImageDataGenerator( rescale=1/255 ) training_generator = training_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/train\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) validation_generator = validation_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/val\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) return training_generator, validation_generator Vì dataset đã được chia thành 2 phần, train set và validation set, nên ở đây ta cũng có 2 instances của lớp ImageDataGenerator tương ứng. Một cái dành cho train model, 1 cái dành cho validate model. Hai instances này chỉ khác nhau đường dẫn đến nơi chứa data, còn lại các thông số khác đều giống nhau. Cần lưu ý đến giá trị của tham số batch_size, vì lần này chúng ta sử dụng dữ liệu thật nên nếu bạn set giá trị của nó cao quá có thể dẫn đến hiện tượng out of memory. Thực tế, ban đầu mình set batch_size=64 nhưng bị lỗi nên phải giảm xuống còn 32.\nCNN model được tạo ra giống như bài trước:\ndef create_model(): model = keras.models.Sequential([ keras.layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150, 150, 3)), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Flatten(), keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=RMSprop(lr=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Optimizer vẫn là RMSprop nhưng được learning_rate được khởi tạo với giá trị 0.001 thay vì sử dụng giá trị mặc đinh. Loss Function là binary_crossentroy vì có 2 classes cần phân biệt.\nTa định nghĩa thêm hàm plot_chart để thể hiện kết quả training lên đồ thị:\ndef plot_chart(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10, 6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Traing and Validation, Accuracy and Loss\u0026#39;) plt.legend(loc=0) plt.show() Bên trên ta đã nói về hàm callback sử dụng EarlyStopping. Ta định nghĩa nó như sau:\ncallback = EarlyStopping(monitor=\u0026#39;loss\u0026#39;, patience=5) Mục đích của hàm này là buộc model dừng quá trình training nếu sau 5 epochs liên tiếp mà giá trị của loss không giảm. Bạn cũng có thể thay loss bằng acc, khi đó, model sẽ dừng training nếu giá trị accuracy không tăng sau 5 epochs liên tiếp.\nGộp tất cả lại và train model:\ntraining_generator, validation_generator = gen_data() model = create_model() history = model.fit( training_generator, epochs=30, validation_data=validation_generator, callbacks=[callback], verbose=1 ) Dòng `validation_data=validation_generator` trong hàm fit chỉ ra dữ liệu được dùng để validate model trong suốt quá trình training. Output: ```python Found 20000 images belonging to 2 classes. Found 5000 images belonging to 2 classes. Epoch 1/30 1/625 [..............................] - ETA: 0s - loss: 0.6942 - acc: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0141s vs `on_train_batch_end` time: 0.0326s). Check your callbacks. 625/625 [==============================] - 116s 186ms/step - loss: 0.6851 - acc: 0.5663 - val_loss: 0.6292 - val_acc: 0.6240 Epoch 2/30 625/625 [==============================] - 121s 194ms/step - loss: 0.6250 - acc: 0.6571 - val_loss: 0.5594 - val_acc: 0.7356 Epoch 3/30 625/625 [==============================] - 117s 188ms/step - loss: 0.5928 - acc: 0.6921 - val_loss: 0.5248 - val_acc: 0.7652 ... Epoch 14/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4828 - acc: 0.7799 - val_loss: 0.4186 - val_acc: 0.8050 Epoch 15/30 625/625 [==============================] - 123s 197ms/step - loss: 0.4877 - acc: 0.7753 - val_loss: 0.5091 - val_acc: 0.7568 Epoch 16/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4787 - acc: 0.7796 - val_loss: 0.5068 - val_acc: 0.8126 Epoch 17/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4844 - acc: 0.7826 - val_loss: 0.3928 - val_acc: 0.8374 Epoch 18/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4876 - acc: 0.7832 - val_loss: 0.4588 - val_acc: 0.8388 Ta thấy rằng quá trình training model dừng lại tại epoch thứ 18, vì từ epoch 13 đến epoch 18, giá trị của loss không giảm đi chút nào, thậm chí còn tăng lên.\nThử vẽ đồ thị loss và accurcy:\nplot_chart(history)   Từ đồ thị ta có thể nhận xét rằng model được training khá tốt, có một chút overfitting nhưng không đáng kể.\nCuối cùng, model nên được lưu lại để sử dụng cho việc dự đoán về sau:\nmodel.save(\u0026#39;cats-dogs-model.h5\u0026#39;) Source code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, ta sẽ áp dụng thêm 2 kỹ thuật quan trọng nữa cho bài toán phân loại cat-dog, đó là data augmentation và ModelCheckpoint. Cũng giống như EarlyStopping, mục đích của 2 kỹ thuật này không gì hơn là ngăn chặn hiện tượng Overfitting của model trong quá trình training. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/32_earlystopping_cnn/","tags":["CNN","Image Classification"],"title":"Sử dụng kỹ thuật EarlyStopping khi huấn luyện CNN model"},{"categories":["CNN","Image Classification"],"contents":"Như đã hứa ở bài trước, ở bài này chúng ta sẽ sử dụng một bộ dữ liệu \u0026ldquo;thực tế\u0026rdquo; hơn để huấn luyện một DL model phân loại hình ành, đó là happy-or-sad dataset. Tập dữ liệu này gồm 80 ảnh, được chia thành 2 lớp happy và sad.\nNhư thường lệ, đầu tiên ta sẽ import các thư viện sử dụng:\nimport PIL import pathlib import tensorflow as tf import numpy as np from tensorflow import keras from tensorflow.keras.preprocessing.image import ImageDataGenerator Cũng giống như đã đề cập trong bài trước, ta cần thêm 3 dòng sau để tránh sự xung đột giữa 2 phiên bản của Tensorflow.\nconfig = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Tiếp theo, download và giải nén tập dữ liệu này về máy tính theo đường dẫn bên trên. Ở đây, mình giả sử bạn đặt thư mục happy-or-sad trong cùng thư mục dự án.\nDataset gồm 80 ảnh, chia thành 2 lớp: happy và sad, mỗi lớp có 40 ảnh.\nĐịnh nghĩa hàm callback như các bài trước:\nclass MyCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if logs.get(\u0026#39;acc\u0026#39;) \u0026gt; 0.99: print(\u0026#39;Reach to 99%, stop training!\u0026#39;) self.model.stop_training = True Định nghĩa hàm load dữ liệu huấn luyện:\ndef load_data(): train_datagen = ImageDataGenerator( rescale=1/255 ) train_generator = train_datagen.flow_from_directory( \u0026#39;happy-or-sad\u0026#39;, target_size=(150,150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) return train_generator Trong bài này, vì dataset là ảnh thực tế được lưu trong ổ cứng của máy tính nên ta sử dụng lớp ImageDataGenerator để chuẩn bị dữ liệu cho model training. Bản chất của lớp này là không load toàn bộ dataset một lần (*điều này có thể sẽ không khả thi nếu *) Ngoài chức năng cơ bản đó, nó còn giúp tăng cường dữ liệu (data augmentation), một trong những kỹ thuật hạn chế hiện tượng Overfitting khi huấn luyện model. Chúng ta sẽ sử dụng lớp này thường xuyên trong các bài tiếp theo.\nỞ đây, kích thước ảnh đầu vào là (150,150), các giá trị pixels của ảnh được rescale xuống 255 lần, và sẽ có 32 ảnh được load mỗi lần.\nCNN model được tạo bởi hàm sau:\ndef create_model(): model = keras.models.Sequential([ tf.keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150,150,3)), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(16, (3,3), activation=\u0026#39;relu\u0026#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Model được tạo theo kiến trúc: [CONV =\u0026gt; POOL]x3 =\u0026gt; FCx3. Compiler là rmsprop, Loss Function là binary_crossentropy vì chúng ta chỉ có 2 lớp cần phân biệt.\nCuối cùng, gọi các hàm định nghĩa bên trên để huấn luyện model:\nx_train, y_train = load_data() model = create_model() history = model.fit(x_train, y_train, epochs=100, callbacks=[MyCallback()], verbose=1) Output:\nEpoch 1/100 3/3 [==============================] - 0s 58ms/step - loss: 1.2190 - acc: 0.5500 Epoch 2/100 3/3 [==============================] - 0s 88ms/step - loss: 0.6919 - acc: 0.5000 Epoch 3/100 3/3 [==============================] - 0s 64ms/step - loss: 0.6957 - acc: 0.5375 ................... Epoch 98/100 3/3 [==============================] - 0s 65ms/step - loss: 0.1934 - acc: 0.9000 Epoch 99/100 3/3 [==============================] - 0s 89ms/step - loss: 0.1710 - acc: 0.9250 Epoch 100/100 3/3 [==============================] - 0s 68ms/step - loss: 0.2187 - acc: 0.8875 Như ta quan sát thấy, model không thể đạt đến được độ chính xác 99% như điều kiện dừng ở hàm callback. Độ chính xác cuối cùng là 88,75%.\nQua bài này, ta đã biết cách sử dụng dữ liệu thực tế, lưu trong ổ đĩa cứng để huấn luyện một CNN model, bằng cách sử dụng lớp ImageDataGenerator của thư viện Tensorflow.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, ta sẽ huấn luyện CNN model để phân loại 2 đối tượng trong ảnh: chó và mèo. Khác biệt so với bài này là ta sẽ sử dụng dữ liệu validation trong quá trình huấn luyện model, giúp cho model học tốt hơn.\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/31_happy_and_sad_classification_cnn/","tags":["CNN","Image Classification"],"title":"Xây dựng CNN model với tập dữ liệu Happy\u0026Sad"},{"categories":["CNN","Image Classification"],"contents":"Trong bài trước, chúng ta đã sử dụng các lớp FC để xây dựng model phân loại các sản phẩm trong tập MNIST thành 10 nhóm khác nhau.\nViệc phân loại hình ảnh (trong thực tế)là một nhìệm vụ tương đối phức tạp, nó yêu cầu phải xây dựng các NN model với nhiều lớp. Nếu chỉ sử dụng hoàn toàn FC layer thì sẽ không hiệu quả về cả độ chính xác cũng như hiệu năng của model. Thay vào đó, các lớp CONV, POOL, \u0026hellip; được sử dụng thường xuyên hơn.\nYêu cầu cần giải quyết ở bài này vẫn giống như bài trước, chỉ có điều ta sẽ sử dụng các lớp CONV, POOL, \u0026hellip; trong thư viện tensorflow xây dựng model phân loại các sản phẩm trong tập MNIST. Môi trường thực hành vẫn giống như các bài trước.\nĐầu tiên, như thường lệ ta sẽ import thư viện tensorflow:\nimport tensorflow as tf Tiếp đến là hàm callback:\nclass MyCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if logs.get(\u0026#39;acc\u0026#39;) \u0026gt; 0.99: print(\u0026#39;Reached to 99%, stop training!\u0026#39;) self.model.stop_training = True Nhắc lại là hàm này sẽ được gọi tại thời điểm kết thúc mỗi epoch trong quá trình train model. Nó làm nhiệm vụ kiểm tra độ chính xác của model tại thời điểm đó. Nếu độ chính xác đạt đến 99% thì kết thúc quá trình train.\nHàm load dữ liệu MNIST từ trong tensorflow:\ndef load_data(): (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = x_train.reshape(-1, 28, 28, 1) x_train = x_train/255 return x_train, y_train Giờ đến lúc quan trọng nhất của bài này, đó là tạo CNN model:\ndef create_model(): model = keras.models.Sequential([ keras.layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, input_shape=(28,28,1)), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Flatten(), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model CNN model được tạo thành từ các layers: CONV2D, MaxPooling2D, Flatten, Dense, \u0026hellip; Tùy thuộc mức độ phức tạp của dữ liệu và yêu cầu của bài toán mà ta sử dụng số lượng và cách thức kết hợp các layers này theo những cách khác nhau. Chi tiết về các common pattern, các rules và các tham số sử dụng khi xây dựng mạng CNN, các bạn có thể đọc thêm tại bài viết trước của mình tại đây.\nỞ bài này, chúng tạo một mạng CNN đơn giản theo kiến trúc: [CONV2D =\u0026gt; MaxPooling2D]x2 =\u0026gt; FLATTEN =\u0026gt; DENSEx2.\nCNN model sau đó được compile sử dụng thuật toán RMSprop 9cùng với SGD, Adam và RMSprop là 2 thuật toán tối ưu cũng thường hay được sử dụng khi train DL model. Mình dự định sẽ có các bài viết chi tiết về 2 thuật toán này. Mời các bạn đón đọc), hàm loss là sparse_categorical_crossentropy, và metric là accuracy trên tập train.\nCó model rồi, đã đến lúc tiến hành train model:\nx_train, y_train = load_data() model = create_model() history = model.fit(x_train, y_train, epochs=100, callbacks=[MyCallback()], verbose=1) Model sẽ được train tối đa 100 epochs, hàm callback được truyền vào như 1 tham số để kểm tra điều kiện dừng train của model sau mỗi epoch.\nOutput:\nEpoch 1/100 1875/1875 [==============================] - 14s 7ms/step - loss: 0.1141 - acc: 0.9652 Epoch 2/100 1875/1875 [==============================] - 14s 7ms/step - loss: 0.0406 - acc: 0.9877 Epoch 3/100 1875/1875 [==============================] - ETA: 0s - loss: 0.0318 - acc: 0.9913Reached to 99%, stop training! 1875/1875 [==============================] - 14s 7ms/step - loss: 0.0318 - acc: 0.9913 Rất nhanh, model đạt đến độ chính xác 99% chỉ sau 3 epochs, so với 7 epochs nếu sử dụng hoàn toàn FC layer như bài trước. Với bộ dữ liêu đơn giản như MNIST, 3 epochs hay 7 epochs không có sự khác biệt nhiều về thời gian cũng như độ chính xác. Nhưng nếu dữ liệu rất lớn thì sự khác nhau đó sẽ trở nên rất rõ rệt.\nNếu khi gọi hàm fit() để train model mà gặp lỗi:\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [[node sequential/conv2d/Conv2D (defined at \u0026lt;ipython-input-6-04cbaae553c1\u0026gt;:1) ]] [Op:__inference_train_function_856] thì bạn hãy thêm 3 dòng bên dưới ngay sau khi import tensorflow.\nconfig = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Nguyên nhân lỗi ở đây là do xung đột giữa 2 phiên bản 1.x và 2.x của tensorflow.\nOk, như vậy là chúng ta đã hoàn thành việc xây dựng CNN model để phân loại hình ảnh trong tập dữ liệu MNIST. Tuy đơn giản nhưng đây sẽ là bước đầu để dần dần bạn có thể tự tin tạo ra các CNN model phức tạp hơn, với các tập dữ liệu lớn hơn.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong bài sau, chúng ta sẽ xây dựng một CNN model khác với dữ liệu thực tế hơn. Hãy đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/30_fashion_mnist_classification_advanced/","tags":["CNN","Image Classification"],"title":"Xây dựng CNN model phân loại với tập dữ liệu MNIST"},{"categories":["Neural Network","Image Classification"],"contents":"Bài này, ta sẽ nâng độ khó hơn 1 chút so với bài trước. Yêu cầu đề bài như sau:\n Xây dựng một NN model phân loại các hình ảnh trong tập dữ liệu MNIST Trong quá trình train, khi độ chính xác của model đạt đến 99% thì dừng train.  Mục đích của bài này là giúp ta làm quen với dữ liệu \u0026ldquo;thực tế\u0026rdquo; hơn 1 chút so với bài trước và cách sử dụng hàm callback để điều khiển quá trình train model.\nMôi trường thực hành của bài này giống hệt bài dự đoán giá nhà trước đó.\nOk, hãy cùng bắt đầu!\nĐầu tiên, import tensorflow:\n1 import tensorflow as tf Tiếp theo, ta định nghĩa hàm callback. Hàm này sẽ được gọi mỗi khi model train xong một epoch.\n2 class CustomCallback(keras.callbacks.Callback): 3 def on_epoch_end(self, epoch, logs={}): 4 if logs.get(\u0026#39;acc\u0026#39;) \u0026gt; 0.99: 5 print(\u0026#39;Reached to 99%, stop training!\u0026#39;) 6 self.model.stop_training = True Ở đây, ta sẽ cho model dừng train khi độ chính xác đạt đến 99% như yêu cầu đề bài.\nDataset được load như code của hàm sau:\n7 def load_data(): 8 (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() 9 x_train = x_train/255 10 x_test = x_test/255 11 return x_train, y_train, x_test, y_test MNIST có thể coi là bộ dataset kinh điển mà hầu như bất kỳ ai cũng sử dụng khi mới học AI. Có lẽ vì thế mà nó được tích hợp sẵn trong thư viện tensorflow.\nCó một chú ý ở hàm load_data() là ta cũng scale down giá trị của x_train, x_test bằng cách chia cho 255. Mục đích của việc làm này cũng giống như mình đã trình bày trong bài trước.\nPhần chính của chúng ta là định nghĩa model:\n12 def create_model(): 13 model = tf.keras.models.Sequential([ 14 tf.keras.layers.Flatten(), 15 tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;, input_shape=(28,28)), 16 tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) 17 ]) 18 model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) 19 return model Model của chúng ta hôm nay gồm 2 lớp: 1 lớp input (128 nodes) và 1 lớp output (10 nodes), không có lớp ẩn (hidden layer).\nKích thước của dữ liệu đầu vào là (28,28), bằng với kích thước của mỗi bức ảnh trong tập MNIST.\nSố node của lớp output là 10, bằng với số lớp của tập MNIST mà ta cần phân loại. Hàm kích hoạt Softmax sử dụng ở lớp này sẽ cho ta biết chính xác xác suất của hình ảnh thuộc về mỗi lớp. Lớp nào có xác suất lớn nhất sẽ được lấy làm kết quả cuối cùng.\nModel được compile với thuật toán tối ưu SGD, hàm loss là sparse_categorical_crossentropy, và metric là accuracy trên tập train.\nBây giờ ta sẽ tiến hành train model:\n20 x_train, y_train, x_test, y_test = load_data() 21 model = create_model() 22 23 history = model.fit(x_train, y_train, epochs=100, verbose=1, callbacks=[CustomCallback()]) Model được train tối đa 100 epochs, hàm callback mà ta định nghĩa bên trên được truyền vào như 1 tham số.\nOutput:\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 1s 0us/step Epoch 1/100 1875/1875 - 1s - loss: 0.2573 - acc: 0.9266 Epoch 2/100 1875/1875 - 1s - loss: 0.1128 - acc: 0.9666 Epoch 3/100 1875/1875 - 1s - loss: 0.0771 - acc: 0.9765 Epoch 4/100 1875/1875 - 1s - loss: 0.0573 - acc: 0.9825 Epoch 5/100 1875/1875 - 1s - loss: 0.0445 - acc: 0.9856 Epoch 6/100 1875/1875 - 1s - loss: 0.0345 - acc: 0.9895 Epoch 7/100 Reached to 99%, stop training! 1875/1875 - 1s - loss: 0.0283 - acc: 0.9913 Đầu tiên, tập MNIST sẽ được download về local, sau đó model sẽ được train. Quá trình train dừng lại sau 7 epochs vì độ chính xác đã đạt đến 99% như định nghĩa ở hàm callback.\nNhư vậy là chúng ta đã giải quyết xong yêu cầu đặt ra lúc đầu. Qua bài này ta đã biết:\n Cách load dataset được tích hợp trong tensorflow. Cách xây dựng và sử dụng hàm callback khi train model. Các tạo và train model với 2 lớp NN sử dụng tensorflow.  Source code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, chúng ta sẽ xây dựng model sử dụng lớp CONV trong tensorflow để nâng cao độ chính xác cũng như hiệu năng của model. Mời các bạn đón đọc.\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/29_fashion_mnist_classification/","tags":["Neural Network","Image Classification"],"title":"Xây dựng NN model phân loại với tập dữ liệu MNIST"},{"categories":["Neural Network"],"contents":"Sau một số bài về lý thuyết thì hôm nay chúng ta sẽ bắt tay vào thực hành code một bài toán mẫu giáo, áp dụng những lý thuyết mà ta đã tìm hiểu xem sao nhé!\nThông tin về bài toán và yêu cầu đặt ra như sau:\n Quy tắc tính giá nhà như sau: 50k + 50k/bedroom. Ví dụ: nhà có 1 bedroom thì giá sẽ là 100k, nhà có 2 bedrooms thì giá sẽ là 150k, \u0026hellip; Xây dựng một NN model để dự đoán giá tiền của một căn nhà có 7 phòng ngủ. So sánh giá của căn nhà đó theo 2 cách: tính theo các thông thường và theo kết quả dự đoán của NN model.  Mình giả sử là các bạn đã cài sẵn môi trường trên máy tính của các bạn (hoặc các bạn có thể sử dụng colab cũng được.). Chúng ta sẽ sử dụng tensorflow 2.3.0 và numpy 1.18.5\nĐầu tiên, import và kiểm tra thư viện:\n1 import tensorflow as tf 2 import numpy as np 3 import os 4 from tensorflow import keras 5 6 print(tf.__version__) 7 print(np.__version__) 8 print(tf.config.list_physical_devices(\u0026#39;GPU\u0026#39;)) Kết quả:\n2.3.0 1.18.5 [PhysicalDevice(name=\u0026#39;/physical_device:GPU:0\u0026#39;, device_type=\u0026#39;GPU\u0026#39;)] Máy của các bạn có thể không có GPU cũng không sao, vì bài này rất đơn giản nên cũng không cần đến GPU. Các bài sau thì nên có vì như thế thời gian train model sẽ nhanh hơn rất nhiều.\nTiếp theo, chúng ta sẽ tạo ra dữ liệu huấn luyện dựa theo quy tắc tính giá nhà như trong đề bài:\n9 x_train = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], dtype=float) 10 y_train = np.array([100.0, 150.0, 200.0, 250.0, 300.0, 350.0], dtype=float) Mỗi phần tử của x_train là số phòng ngủ của căn nhà, còn mỗi phần tử của y_train là giá nhà tương ứng với số phòng ngủ đó.\nBây giờ là lúc chúng ta xây dựng NN model:\n11 model = keras.Sequential([ 12 keras.layers.Dense(units=1, input_shape=[1]) 13 ]) 14 model.compile(optimizer=\u0026#39;sgd\u0026#39;, lost=\u0026#39;mse\u0026#39;) Dòng 11-13 định nghĩ một NN model với chỉ một layer, một input có kích thước là 1. Dòng 14, model được compiled, sử thuật toán tối ưu SGD và Lost Function là MSE (Mean Square Error).\nĐến đây, model đã sẵn sàng để train:\n15 model.fit(x_train, y_train/255, epochs=1000) Model được train với dữ liệu train đã tạo ở bên trên, số lượng epoch là 1000. Chú ý rằng ở đây, y_train được chia cho 255. Mục đích của việc scale down này là để model không bị quá bias vào giá trị của y_train khi mà giá trị của y_train lớn hơn giá trị của x_train rất nhiều.\nOutput:\nEpoch 1/1000 1/1 [==============================] - 0s 1ms/step - loss: 37.2759 Epoch 2/1000 1/1 [==============================] - 0s 920us/step - loss: 17.2548 Epoch 3/1000 1/1 [==============================] - 0s 525us/step - loss: 7.9884 .................. Epoch 998/1000 1/1 [==============================] - 0s 535us/step - loss: 3.0594e-06 Epoch 999/1000 1/1 [==============================] - 0s 938us/step - loss: 3.0372e-06 Epoch 1000/1000 1/1 [==============================] - 0s 1ms/step - loss: 3.0151e-06 Ta thấy giá trị loss giảm từ 37.2759 đến 3.0151e-06. Một con số khá thấp.\nCuối cùng, ta sẽ dùng model vừa trained để dự đoán giá của ngôi nhà có 7 phòng ngủ.\n16 prediction = model.predict([7.0]) 17 print(prdiction * 100) Kết quả:\n[[400.25043]] Giá của ngôi nhà nếu tính theo cách thông thường sẽ là 400$. So sánh 2 kết quả ta thấy chúng khá gần nhau. Như vậy là model đã làm việc khá tốt. Bạn có thể tăng số epochs lên và train lại model, sau đó kiểm trả lại kết quả dự đoán xem nó có được cải thiện hay không!\nOK, như vậy là chúng ta đã giải quyết xong yêu cầu của bài toán. Đây chỉ là 1 bài tập trình độ mẫu giáo để chúng ta làm quen với việc sử dụng tensorflow để xây dựng NN model. Trong các bài sau, chúng ta sẽ giải quyết các bài toán phức tạp hơn.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/28_hourse_prices_prediction/","tags":["Neural Network"],"title":"Xây dựng NN model đơn giản dự đoán giá nhà"},{"categories":["Algorithm Optimization"],"contents":"\u0026ldquo;Nearly all of deep learning is powered by one very important algorithm: Stochastic Gradient Descent (SGD)\u0026rdquo; – Goodfellow et al.\nTừ bài trước chúng ta đã biết rằng để model có thể dự đoán đúng thì phải tìm được giá trị phù hợp cho $W$ và $b$. Nếu chúng ta chỉ dựa hoàn toàn vào việc chọn ngẫu nhiên thì gẫn như không bao giờ có thể tìm được giá trị mong muốn. Thay vì thế, chúng ta cần định nghĩa một thuật toán tối ưu (optimization) và sử dụng nó để cải thiện $W$ và $b$. Trong bài này, chúng ta sẽ tìm hiểu một thuật toán tối ưu được sử dụng rất rất phổ biến trong NN and DL model - Gradient Descent (GD) và các biến thể của nó. Ý tưởng chung của họ các thuật toán GD là đánh giá các tham số, tính toán loss, sau đó thực hiện một bước nhỏ theo hướng giảm loss. Cả 3 bước này được thực hiện trong các vòng lặp cho đến khi gặp một điều kiện dừng nào đó.\n1. The Loss Landscape và Optimization Surface\nGradient descent là thuật toán hoạt động theo kiểu tối ưu qua từng vòng lặp thông qua một mặt tối ưu(optimization surface / loss landscape), như minh họa ở hình bên dưới.\n Phía bên trái biểu diễn trong không gian 2 chiều để chúng ta dễ hình dùng, còn bên phải biểu diễn một cách thực tế hơn trong không gian nhiều chiều. Mục đích sử dụng gradient descent là tìm ra điểm global minumum (đáy của cái bát ở bên phải).\nChúng ta có thể thấy, optimization surface có rất nhiều đỉnh (peaks) và thung lũng (valleys*). Mỗi valley có một điểm đáy mà tại đó giá trị loss đạt giá trị cực tiểu, gọi là local minimum. Trong số các điểm local minimum, có 1 điểm mà giá trị loss đạt giá trị nhỏ nhất được gọi là gloabal minimum. Đây chính là điểm mà chính ta muốn tìm trong quá trình training AI model thông qua việc cập nhật các tham số.\nHãy tưởng tượng, việc dò tìm điểm global minimum trên optimization surface giống như việc đặt 1 viên bi (*chính là * $W$) trên mặt đó, nhiệ vụ của viên bi là dò tìm đường để đi đến điểm đích (global minimum).\nNếu chỉ nhìn vào hình trên, mọi người có thể thắc mắc: Nếu muốn đến điểm global minimum, tại sao không nhảy thẳng một phát đến đó?\nNhưng mọi việc không đơn giản như vậy, bởi vì trên thực tế, chúng ta không biết hình dạng của optimization surface như thế nào, chúng ta như một ngươi mù trên đường, không biết phương hướng. Và các thuật toán tối ưu (gradient descent là một trong số đó) chính là cây gậy trong tay, giúp chúng ta dò đường.\nCụ thể hơn 1 chút thì mỗi một điểm trên optimization surface tương ứng với một giá trị loss $L$ - chính là output của loss funtion khi đưa vào cặp giá trị ($W$, $b$). Ý tưởng của thuật toán tối ưu là cố gắng thử sử dụng các cặp giá trị ($W$, $b$) khác nhau, tính toán loss, cập nhật ($W$, $b$) sao cho giá trị loss thấp hơn \u0026hellip; Lý tưởng nhất là chúng ta có thể đạt được giá trị loss nhỏ nhất tại điểm global minimum, nhưng điều này thường khó xảy ra trong thực tế.\n2. Gradient Descent cho hàm 1 biến\nGiả sử Loss Function của chúng ta là hàm bậc 1, $f(x)$. Điểm global minimum là điểm mà tại đó $x = x^*$.\nĐạo hàm của của $f(x)$ là $f'(x)$. Nếu bạn còn nhớ, trong chương trình toán THPT, khi học về đạo hàm ta có các nhận xét:\n Nếu đạo hàm của hàm số tại thời điểm $t$, $f'(x_t) \u0026gt; 0$ thì $x_t$ nằm về phía bên phải so với $x^*$, và ngược lại. $x_t$ càng xa $x^*$ về phía bên phải thì $f'(x_t)$ càng lơn hơn 0, và ngược lại.  Từ nhận xét số 1 có thể suy ra, để điểm tiếp theo $x_{t+1}$ tiến gần về $x^*$ hơn thì cần di chuyển $x_t$ về phía bên trái, tức là phía âm, hay phía ngược dấu với đạo hàm:\n$x_{t+1} = x_t + \\Delta$ ($\\Delta$ là một đại lượng ngược dấu với đạo hàm $f'(x)$)  Từ nhận xét số 2 có thể suy ra lượng di chuyển $\\Delta$ tỉ lệ thuận với $-f'(x)$.\nTổng hợp hai nhận xét trên, ta có công thức cập nhật $x_t$ một cách đơn giản là:\n$x_{t+1} = x_t - \\eta f'(x_t)$ Hoặc viết dưới dạng đơn giản:\n$x = x - \\eta f'(x)$ Trong $\\eta$ là một số \u0026gt; 0, gọi là learning rate. Dấu trừ thể hiện viêc đi ngược chiều với đạo hàm (descent nghĩa là đi ngược).\n3. Gradient Descent cho hàm nhiều biến\nGiả sử Loss Function của chúng ta, $f(\\theta)$ là hàm nhiều biến, trong đó $\\theta$ là tập hợp các vector các tham số của model cần tối ưu. Đạo hàm của $f(\\theta)$ tại thời điểm $\\theta$ là $\\nabla_\\theta f(\\theta)$.\nTương tự hàm 1 biến, quy tắc cập nhật $\\theta$ là:\n$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta f(\\theta_t)$ Hoặc viết dưới dạng đơn giản: $\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta_t)$ Tóm lại, thuật toán GD hoạt động như sau:\n Dự đoán một điểm khởi tạo $\\nabla = \\nabla_0$. Cập nhật $\\nabla$ đến khi đạt được kết quả chấp nhận được (hoặc một điều kiện dừng nào đó). $\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta)$   với $\\nabla_\\theta f(\\theta)$ là đạo hàm của Loss Function tại $\\theta$.\n4. Stochastic Gradient Descent (SGD)\nThuật toán GD nguyên thủy có một nhược điểm to lớn là hội tụ rất chậm và yêu cầu tài nguyên tính toán rất lớn. Nguyên nhân gốc rễ của vấn đề này là do GD tính toán gradient trên toàn bộ training set. Điều này thật khó để chấp nhận nếu áp dụng với một tập dữ liệu lớn.\nMột biến thể của GD, gọi là Stochastic Gradient Descent (SGD) ra đời, khắc phục những hạn chế của GD. Thay vì tính toán và cập nhật weight matrix $W$ trên toàn bộ tập dữ liệu như cách làm của GD (cập nhật theo epoch), SGD chia nhỏ tập training thành các batchs (dữ liệu thường được xáo trộn ngẫu nhiên trước khi chia), tính toán và cập nhật $W$ theo từng batch đó (cập nhật theo batch).\nBiểu diễn theo toán học, công thức cập nhật của SGD như sau:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta;x_i;y_i)$ trong đó, $f(\\theta;x_i;y_i)$ là Loss Function với chỉ 1 cặp điểm dữ liệu (input, label) là ($x_i, y_i$).\nMặc dù ra đời từ rất lâu (1960), SGD vẫn là một thuật toán quan trọng, được sử dụng rộng rãi trong các kiến trúc DL hiện đại. Vì thế, viêc hiểu cặn cẽ về nó là một điều cần thiết khi học AI/ML.\n4.1 Mini-batch SGD\nMột câu hỏi đặt ra khi sử dụng SGD là kích thước của batch (batch_size) là bao nhiêu thì hợp lý? Theo như cách diễn giải bên trên thì có vẻ như batch_size càng nhỏ càng tốt? Và tốt nhất là batch_size = 1?\nTuy nhiên, điều này không đúng. Sử dụng batch_size \u0026gt; 1 mang lại cho chúng ta một số lợi ích nhất định. Nó giúp giảm phương sai khi cập nhật $W$, và đặc biệt hơn, nếu giá trị của batch_size là lũy thừa của 2 thì chúng ta còn hưởng lợi về tốc độ thực thi của các thư viện tối ưu trong đại đố tuyến tính. Trong các bài toán thực tế, batch_size thường nhận các giá trị 32, 64, 128, 256, tùy thuộc vào tài nguyên tính toán của bạn.\nLúc này, công thức cập nhật sẽ trở thành:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta;x_{i:i+n};y_{i:i+n})$ trong đó, $x_{i:i+n}, y_{i:i+n}$ là các cặp điểm dữ liệu (*input, label*) có vị trí từ $i$ đến $i + n -1$.\n4.2 Mở rộng của SGD\nTrong quá trình sử dụng SGD, ta có thể bắt gặp 2 kỹ thuật hỗ trợ tăng tốc độ hội tụ cho SGD. Đố là momentum và nesterov accelerated gradient (NAG).\n4.2.1 Momentum\nMomentum, hiểu theo nghĩa tiếng việt là đà, lấy đà hay quán tính. Mục tiêu của nó là đẩy nhanh tốc độ cập nhật $W$ tại những nơi mà các gradients có cùng hướng, và ngược lại. Quan sát lại hình bên trên, có thể tưởng tượng rằng nếu không có momentum, viên bi của chúng ta rất dễ bị mắc kẹt ở các local minimum, mà không sao thoát ra để tìm đến global minimum được.\nỞ phần trên, ta đã biết công thức cập nhật các tham số như sau:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta_t)$ Thêm vào momentum $V$, với:\n$V_t = \\gamma V_{t-1} - \\eta \\nabla_\\theta f(\\theta)$ ta được:\n$\\theta = \\theta - V_t$ Trong đó, $\\gamma$ là đại lượng thường được chọn giá trị 0.9, hoặc ban đầu chọn là 0.5, sau khi quá trình học diễn ra ổn định thì tăng lên 0.9. Nó hầu như không bao giờ \u0026lt; 0.5. Khi khai báo sử dụng momentum (trong tensorflow chẳng hạn), ta thường truyền vào giá trị của đại lương này.\n4.2.2 Nesterov Accelerated Gradient (NAG)\nMomentum tuy giúp ta vượt qua được các local minimum, nhưng khi tới gần global minimum, do có đà nên viên bi vẫn tiếp tục dao động thêm một khoảng thời gian nữa mới có thể dừng lại đúng điểm cần dừng. NAG ra đời để khắc phục nhược điểm này.\nÝ tưởng chính của NAG là sử dụng gradient ở thời tiếp theo, thay vì gradient ở thời điểm hiện tại khi tính lượng thay đổi của $\\theta$.\nÝ tưởng của Nesterov accelerated gradient.\nNguồn: CS231n Stanford: Convolutional Neural Networks for Visual Recognition  Công thức cập nhật sẽ như sau:\n$V_t = \\gamma V_{t-1} - \\eta \\nabla_\\theta f(\\theta - \\gamma V_{t-1}) \\theta$ $\\theta = \\theta - V_t$ Momentum là một kỹ thuật quan trọng và hiệu quả, gần như luôn luôn được sử dụng cùng với SGD. Còn đối với NAG, chúng ta ít gặp hơn. Trong khi về mặt lý thuyết, nó mang lại hiệu quả hơn momentum, nhưng trong thực tế các kiến trúc nổi tiếng như AlexNet, VGGNet, ResNet, Inception, \u0026hellip; khi train trên tập dữ liệu ImageNet, chỉ sử dụng SGD với momentum. Có lẽ NAG chỉ phù hợp với các tập dữ liệu nhỏ.\n5. Các thuật toán tối ưu khác\nNgoài SGD, hai thuật toán khác cũng rất hay được sử dụng trong các kiến trúc DL hiện đại là Adam và RMSprop. Mình sẽ có bài viết riêng về các thuật toán này. Mời các bạn đón đọc.\nTham khảo\n Pyimagesearch Dive into Deep Learning machinelearningcoban blog  ","permalink":"https://tiensu.github.io/blog/27_optimization_methods_gradient-descent/","tags":["Algorithm Optimization"],"title":"Các phương pháp Optimization - Gradient Descent"},{"categories":["Machine Learning","Deep Learning"],"contents":"Bạn có biết thuật toán kNN - một trong những thuật toán đơn giản nhất của ML? Về bản chất, nó không \u0026ldquo;học\u0026rdquo; bất cứ điều gì từ dữ liệu mà chỉ đơn giản là lưu dữ liệu bên trong model, và tại thời điểm dự đoán, nó so sánh dữ liệu cần dự đoán với dữ liệu trong tập training. Rõ ràng với cách làm việc như vậy thì ưu điểm lớn nhất của kNN là không mất thời gian training model. Không không cần quá quan tâm về độ chính xác thì ưu điểm này chính là lý do mà kNN vẫn còn được sử dụng trong một số trường hợp. Hạn chế của kNN chỉ xuất hiên khi gặp bài toán mà sử dụng lượng dữ liệu lớn. Lúc này thời gian dự đoán của kNN sẽ rất lâu, và đôi khi không thể sử dụng được trong thực tế.\nMột các tiếp cận khác của ML model mà ở đó nó có thể học được các patterns từ dữ liệu trong suốt quá trình training. Sau đó, ở giai đoạn dự đoán, ML model đó có thể thực hiện rất nhanh chóng để đưa ra kết quả. Dạng này của ML được gọi là parameterized learning.\nDưới đây là định nghĩa chính thống từ tác giả:\n“A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at the parametric model, it won’t change its mind about how many parameters it needs.” – Russell and Norvig (2009).\nCó thể nói parameterized learning là nền tằng của các thuật toán ML và DL hiện đại ngày nay. Trong bài này, chúng ta sẽ cùng xem xét chi tiết một vài khía cạnh về nó!\n1. Bốn thành phần cơ bản của Parameterized Learning\nHiểu một cách đơn giản, parameterization là quá trình định nghĩa các tham số cần thiết cho một model. Nó bao gồm 4 thành phần: Data, Score Function, Loss Function, Weight-Bias.\n1.1 Data\nData ở đây chính là input data đưa vào để model học, nó bao gồm Data Points (VD: Các raw pixel của bức anh, các trích xuất đặc trưng của đối tượng, \u0026hellip;) và nhãn kết hợp với Data Points.\nData thường được biểu diễn dưới dạng ma trận (gọi là desing matrix). Mỗi hàng của matrix đại diện cho một Data Point, trong khi mỗi cột của nó thể hiện moojtoj feature.\nVí dụ, xem xét một dataset gồm 1000 bức ảnh màu, mỗi ảnh có kích thước 96x96x3 pixels. Design Matrix cho dataset này sẽ là: $X \\subseteq R^{100 \\times (32 \\times 32 \\times 3)}$, với $X_i$ là ảnh thức $i$ trong $R$. Cùng với Design Matrix, ta cũng định nghĩa vector $y$ mà $y_i$ là nhãn cho ảnh thứ $i$ trong dataset.\n1.2 Scoring Function\nHàm này chấp nhận một input data, sau đó ánh xạ nó sang nhãn tương ứng.\nINPUT_IMAGES =\u0026gt; F(INPUT_IMAGES) =\u0026gt; OUTPUT_CLASS_LABELS\n1.3 Loss Function\nLoss Function đánh giá mức độ phù hợp giữa nhãn dự đoán (predicted label) và nhãn thực tế (ground-truth label). Giá trị của hàm này tỉ lệ nghịch với mức độ phù hợp hay độ chính xác của model. Mục tiêu của chúng ta khi training AI model là tối thiểu hóa loss funtion.\n1.4 Weights and Biases\nWeight Matrix ($W$) và vector bias ($b$) là những cái được điều chỉnh, cập nhật trong quá trình training AI model, dựa trên output của Score Function và Loss Function. Mục đích cuối cùng cũng vẫn là tăng độ chính xác phân loại của AI model.\nTiếp theo, chúng ta sẽ sử dụng 4 thành phần này để xây dựng một linear classification.\n2. Ưu điểm của Parameterized Learning\nCó 2 ưu điểm chính để sử dụng Parameterized Learning:\n Một khi đã hoàn thành viêc training model, ta có thể bỏ qua input data, chỉ giữ lại Weight Matrix $W$ và bias vector $b$. Điều này giúp giảm đáng kể kích thước của model. Tốc độ của việc phân loại sử dụng model đã trained nhanh hơn rất nhiều so với thuật toán kNN. Ta chỉ cần nhân 2 matrix $W$ và $x_i$, rồi cộng với $b$.  3. Linear Classification\nPhần này, chúng ta sẽ đi sâu hơn 1 chút về toán, tìm hiểu cách mà parameterized learning áp dụng vào machine learning.\nGiả sử chúng ta có một bộ dataset gồm $N$ ảnh kích thước ($W_{input}, H_{input}, 3$), ký hiệu là $x_i, i = 1, 2, \u0026hellip;, N$. Mỗi $x_i$ có một nhãn tương ứng $y_j, j = 1, 2, \u0026hellip;, K$. Nói một cách khác, chúng ta có $N$ Data Points, mỗi Data Point có $D = (W_{input} \\times H_{input}\\times3)$ chiều, và được chia thành $K$ nhóm phân biệt.\nTa định nghĩa Score Function thông qua một hàm tuyến tính đơn giản như sau:\n$f(x_i, W, b) = Wx_i + b$ Giả sử mỗi $x_i$ được đại diện bởi một vector cột với kích thước $[D\\times1]$ (đối với images, ta duỗi thẳng theo cả 3 chiều thành $D = (W_{input} \\times H_{input}\\times3)$ giá trị nguyên của các pixcels). Weight Matrix $W$ sẽ có kích thước $[K \\times D]$ và bias $b$ sẽ có kích thước $[K \\times 1]$. Vector bias cho phép ta dịch chuyển Score Function trên toàn bộ trên đồ thị, đóng vai trò quan trọng đối với sự thành công của AI model.\nVí dụ cụ thể đối một dataset có 3000 bức ảnh, mỗi ảnh có kích thước $[32 \\times 32 \\times 3]$ và được phân chia vào 1 trong 3 nhóm. Khi đó mỗi $x_i$ được đại diện bởi D = 32323 = 3072 pixels và có kích thước $[3072 \\times 1]$. Weight Matrix $W$ có kích thước $[3 \\times 3072]$ và bias vector $b$ có kích thước $[3 \\times 1]$.\nHình dưới minh họa linear classification Score Function $f$. Bên trái là ảnh đầu vào kích thuớc $[32 \\times 32 \\times 3]$ được \u0026ldquo;duỗi thẳng\u0026rdquo; thành 3072 pixcels (bằng cách reshape 3D array thành 1D list).\n Weight Matrix $W$ chứa 3 hàng (mỗi hàng tương ứng với 1 nhãn) và 3072 cột (mỗi cột tương ứng với 1 pixel của ảnh). Sau khi nhân 2 matrix $W$ và $x_i$, ta cộng thêm bias vector $b$ sẽ thu được output của Score Function, như bên trái của hình trên. Có 3 giá trị tương ứng với 3 nhãn: Cat, Dog và Panda.\n4. Thực hành Linear Classification với Python\nChúng ta đã có hình dung cơ bản về lý thuyết của Parameterized Learning, giờ ta sẽ bắt tay vào thực hành code để có thể hiểu hơn.\nMục đích của phần này không phải hướng dẫn việc training model, chỉ đơn giản là cài đạt Score Function bằng python mà thôi. Khi training model thì $W$ và $b$ sẽ được khởi tạo và cập nhật dần dần trong quá trình training, còn ở đây, chúng chỉ được khởi tạo 1 lần và sử dụng luôn để tính output của Score Function.\nMục tiêu của ta ở đây là nhận diện xem ảnh dưới đây là con gì trong số 3 con vật: Cat, Dog và Panda.\n Chúng ta sẽ code như sau (xem giải thích trong comment code):\n# import the necessary packages import numpy as np import cv2 # initialize the class labels and set the seed of the pseudorandom number generator so we can reproduce our results labels = [\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;panda\u0026#34;] np.random.seed(1) # randomly initialize our Weight Matrix and bias vector -- in a *real* training and classification task, these parameters would be  # *learned* by our model, but for the sake of this example, let\u0026#39;s use random values W = np.random.randn(3, 3072) b = np.random.randn(3) # load our example image, resize it, and then flatten it into our \u0026#34;feature vector\u0026#34; representation orig = cv2.imread(\u0026#34;dog.png\u0026#34;) image = cv2.resize(orig, (32, 32)).flatten() # compute the output scores by taking the dot product between the Weight Matrix and image pixels, followed by adding in the bias scores = W.dot(image) + b # loop over the scores + labels and display them Chạy code trên ta thu được kết quả trên terminal:\n[INFO] dog: 8058.57 [INFO] cat: -2926.35 [INFO] panda: 3531.41 và ảnh hiển thị:  Nhắc lại lần nữa là khi thực hiện bài toán AI trong thực tế từ đầu, chúng ta cần phải cập nhật $W$ và $b$ thông qua các thuật toán tối ưu. Thuật toán tối ưu kinh điển là Gradient Descent sẽ được tìm hiểu chi tiết trong bài viết tiếp theo.\n5. Loss Function\n5.1 Loss Function là gì?\nLoss Function, tên tiếng việt là hàm mất mát, thể hiện sai số giữa giá trị dự đoán của model và giá trị thực tế. Sai số càng nhỏ thì model dự đoán càng chính xác và ngược lại.\nMục tiêu của việc training model là cập nhật $W$ và $b$ để tối thiểu hóa giá trị của Loss Function, qua đó nâng cao độ chính xác của model.\nMột cách lý tưởng, Loss Function nên giảm dần theo thời gian trong quá trình training như hình bên dưới:\n 5.2 Multi-class SVM Loss\nMulti-class SVM Loss là sự mở rộng của Linear SVM, xuất hiện trong bài toán khi cần phân biệt nhiều labels (\u0026gt; 2). Nó sử dụng Score Function $f$ để ánh xạ mỗi Data Point thành các Scores cho mỗi nhãn.\nScore Function $f$ có dạng như sau:\n$f(x_i, W, b) = Wx_i + b$ Để phán định một model là \u0026ldquo;good\u0026rdquo; hay \u0026ldquo;bad\u0026rdquo;, ta cần thêm một Loss Function.\nTa đã biết khi tạo ra một ML model, chúng ta có Design Matrix $X$, ở đó, mỗi hàng của $X$ là một Data Point ($x_i$) mà chúng ta muốn phân loại (tìm nhãn cho Data Point đó). Ground-truth Label cho $x_i$, ký hiệu $y_i$ là vector mà chúng ta hi vọng Score Function sẽ dự đoán đúng.\nViết lại Score Function như sau:\n$s = f(x_i, W)$ Predicted Score của class j-th tại i-th Data Point sẽ là:\n$s_j = f(x_i, W)_j$ Ta định nghĩa Hinge Loss Function như sau:\n$L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} +1)$ Hàm này tính tổng các sai số giữa các Score của nhãn dự đoán so với nhãn thực tế. Ở đây, sử dụng hàm $max()$ để bỏ qua những giá trị âm của độ lệch. $x_i$ được dự đoán chính xác khi $L_i$ = 0.\nÁp dụng cho toàn bộ tập training, ta lấy trung bình của mỗi $L_i$:\n$L = \\frac{1}{N} \\sum_{i=1}^N L_i$ Một dạng khác của Loss Function mà ta có thể gặp trong một số tài liệu là Squared Hinge Loss:\n$L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} + 1)^2$ Dạng này sẽ \u0026ldquo;trừng phạt\u0026rdquo; (penalize) năng hơn Loss. Việc sử dụng dạng nào còn tùy thuộc vào dataset của bạn. Thực tế thì dạng chuẩn (không bình phương) có vẻ phổ biến hơn 1 chút. Nhưng trong nhiều bài toán, sử dụng dạng bình phương lại cho kết quả tốt hơn. Nói chung, đây sẽ là một hyperparameter mà chúng ta cần phải tuning trong quá trình xây dựng model.\nVí dụ:\nĐể hiểu rõ hơn, hãy cùng xem xét ví dụ sau:\n Chúng ta có 3 training examples cho 3 nhãn: Dogs, Cats và Pandas. Giả sử ta đã biết giá trị của $W$ và $b$, từ đó sẽ tính được giá trị của Score Fcuntion, như trong hình.\nTính các $L_i$ cho từng example:\n$L_i(Image 1) = max(0, 1.33 - 4.26 + 1) + max(0, -1.01 - 4.46 + 1) = 0$ $L_i(Image 2) = max(0, 3.76 - (-1.20) + 1) + max(0, -3.81 - (-1.20) + 1) = 5.96$ $L_i(Image 3) = max(0, -2.37 - (-2.27) + 1) + max(0, 1.03 - (-2.27) + 1) = 5.199$\nTa thấy, $L_i(Image1) = 0$, tức là Image #1 được dự đoán đoán chính xác là Dogs. Điều này cũng hợp lý vì Score của Image #1 đối với nhãn Dogs lớn hơn nhiều so với 2 nhãn còn lại.\nĐối với Image #2 và Image #3, đang được dự đoán là Dogs và Cats, tương ứng. Rõ ràng, đây là dự đoán sai. Nhìn vào $L_i(cats) và L_i(pandas)$ đều \u0026gt; 0, ta thấy khá hợp lý với những dự đoán này.\nTrong bài sau, ta sẽ học các phương pháp tối ưu để tìm ra giá trị của $W$ và $b$ để việc dự đoán sẽ đúng cho cả 3 bức ảnh.\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/26_parameterized_learning/","tags":["Machine Learning","Deep Learning"],"title":"Parameterized Learning"},{"categories":["Ebook"],"contents":"Bạn có là người thích đọc sách nhưng ngân quỹ có giới hạn, không đủ tiền để mua sách \u0026ldquo;xịn\u0026rdquo; trên amazon, hay trên các tạp chí khoa học, \u0026hellip;? Bạn đã từng trải qua cảm giác gặp một cuốn sách/bài báo rất hay. Bạn rất muốn tìm bản \u0026ldquo;full không che\u0026rdquo; để đọc nhưng không thể tìm được bản free trên mạng? (Mình đã từng mất cả ngày seach google, đăng ký tài khoản ở cả những trang web nước ngoài nhưng vẫn không tìm được. Có chăng thì chỉ là bản sample, hoặc không có code đi kèm) Lên amazon hoặc trang bán sách trực tiếp của tác giả xem thì giá bán là XXX đô la. Bạn đắn đo, không biết có nên bỏ ra số tiền như vậy để mua hay không? Nhỡ sách không hay như kỳ vọng thì sao? Rồi bạn lan man nghĩ ngợi, sáng nay bạn còn không còn đủ 10k ăn sáng, \u0026hellip; Cuối cùng là bạn \u0026hellip; thôi, ko mua nữa.\n\u0026ldquo;May mắn\u0026rdquo; là mình tìm được 2 cách lách luật để có thể sở hữu những cuốn sách như vậy. Cũng phải nhấn mạnh với mọi người là mình không phải là người chuyên đọc sách lậu, và cố tình cỗ súy cho việc này. Đối với một số sách mà mình thấy nó thực sự hay và thực sự có ích đối với mình thì mặc dù có thể download được free nhưng mình vẫn trả tiền cho tác giả. Vì mình nghĩ đơn giản, trả tiền cho tác giả thì họ mới có động lực viết ra những quyển sách hay như thế nữa cho mình đọc. Tuy nhiên, có 2 lý do mà mình quyết định viết bài này:\n Tri thức là của chung của mọi người, chia sẻ kiến thức là việc mà mọi người đều nên làm, không chỉ giúp người mà còn giúp chính mình. Những quyển sách đó có thể cũng đã được chia sẻ ở đâu đó rồi. Mình không nói thì có thể bạn cũng sẽ biết được từ một nguồn nào đó ở nơi khác.  Trong thời đại CNTT ngày nay thì mọi thông tin đều có thể tìm kiếm được từ Internet, vấn đề chủ yếu ở đây là bạn hành động như thế nào thôi. Bạn có thể phản đối hoàn toàn việc đọc sách \u0026ldquo;lậu\u0026rdquo;, làm như mình (trả tiền cho một số cuốn) hoặc dùng hoàn toàn hàng free mà không trả đồng nào. Mình không đánh giá bạn qua việc đó, mình chỉ hi vọng các bạn đọc sách một cách hiệu quả, thực sự yêu quý và trân trọng quyển sách đó, chứ đừng download về đầy máy tính rồi cả năm không động vào. Nếu mình là tác giả của những cuốn sách mà được mọi người yêu thích thì mình cũng cảm thấy được an ủi phần nào. (phần còn lại là nhận được tiền từ các bạn, :D).\nOk, lan man vậy thôi, mình sẽ đi vào vấn đề chính luôn.\n1. Download các bài báo khoa học\nKỹ năng đọc hiểu và implement theo các public papers là một kỹ năng cần thiết đối các kỹ sư AI. Thực tế, dự án mà mình vừa trải qua tại VTI, mình cần đọc hiểu bài báo sau Inception Single Shot MultiBox Detector for object detection. Thực ra thì cũng có khá nhiều các bài viết về thuật toán này rồi, nhưng mình muốn đọc paper gốc để hiểu rõ hơn ý định của tác giả nên mình đã vào IEEE để tìm. Nhưng mà trên đó, họ chỉ cho đọc mỗi phần Abstract. Nếu muốn đọc full thì phải đăng ký thành viên và phải trả phí.\n Số tiền tuy không lớn nhưng cũng không nhỏ đối với những người mà tiền trong ví lúc nào cũng chỉ đổ xăng, ăn sáng và uống trà đá như mình, :D.\nTrong trường hợp này, bạn chỉ cần truy cập vào trang web https://sci-hub.st/.\n Tại trang chủ của nó, bạn có thể nhìn thấy ngay slogan của họ: ... to remove all barriers in the way of science. Việc bạn cần làm là dán đường link bài báo mà bạn muốn tải về vào ô textbox rồi click vào nút open.\nVà đây là kết quả:\n Đến đây thì bạn có thể tải về máy tính của bạn một các dễ dàng rồi.\nĐây là cách mà mình được một GS giới thiệu cho khi học môn \u0026ldquo;Văn phong khoa học kỹ thuật\u0026rdquo; ở lớp Master. Thầy cũng nhấn mạnh rằng, đây là cách mà trong giới khoa học ngầm hiểu với nhau chứ không công khai trong bất kì tài liệu chính thức nào. Bởi vì có một thực tế rằng các nhà khoa học khi làm nghiên cứu, họ phải đọc hàng chục, thậm chí hằng trăm bài báo khoa học để tìm ý tưởng, lấy dẫn chứng, \u0026hellip; Giả sử mỗi bài báo có giá 1$ tải về thôi thì cũng là một vấn đề không nhỏ rồi.\nLiệu bạn có thắc mắc, tại sao một trang web như này lại có thể công khai tồn tại một cách hiển nhiên như thế này? Đó cũng là câu hỏi mà mình từng nghĩ đến nhưng vẫn chưa tìm được câu trả lời chính xác. Nhưng có lẽ, nên nhìn nhận vấn đề này giống như cách chúng ta sử dụng hệ điều hành Windows hay bộ công cụ Office của Microsoft. Mặc dù Microsoft biết là có rất nhiều người dùng \u0026ldquo;chùa\u0026rdquo; nhưng họ không chặn, cứ \u0026ldquo;giả ngơ\u0026rdquo; để cho mọi người dùng thoải mái. Không phải Microsoft không thể chặn, mà có lẽ là họ không muốn chặn. Phí để mua bản quyền hệ điều hành Windows khá cao so với mức thu nhập của rất nhiều người, nếu Microsoft ngăn chạn triệt để thì có thể người dùng sẽ quay lưng, chuyển sang một hệ điều hành mở khác (Ubuntu chẳng hạn). Khi người dùng đã trở nên quen với Windows rồi, Microsoft mới bắt đầu tính phí bản quyền của các doanh nghiệp, công ty. Nếu công ty, doanh nghiệp nào không mua bản quyền, thì rất dễ bị kiện bởi Microsoft, hoặc bị chính khách hàng của họ chấm dứt mối quan hệ làm ăn với công ty đó. Nếu tổ chức (trường học, viện nghiên cứu, \u0026hellip;) sử dụng các bài báo không có bản quyền để phục vụ các mục đích thuơng mại, hay công bố quốc tế thì rất có thể cũng sẽ bị kiện bởi tác giả hoặc nhà xuất bản.\nNgoài IEEE, các bài báo ở các nhà xuất bản khác như Sciencedirect, Springer, Researchgate, \u0026hellip; cũng có thể download được theo cách này.\nCó một lưu ý nhỏ là, trang https://sci-hub.st/ này có thể bị chặn, không truy cập được đối với một số mạng nội bộ ở công ty, doanh nghiệp. Khi đó bạn chỉ cần đổi qua một mạng Internet free open nào đó (4G chẳng hạn) là có thể truy cập được.\n2. Download sách\nMột trong những cuốn sách mình rất thích, đó là:\n Trên Amazon, quyển này có giá khá cao, $37.49 cho bản sách điện tử.\n Và đây là cách mà mình đã làm để download cuốn sách này với giá $0.\nVào trang https://b-ok.asia/.\n Gõ tên sách cần tìm ở ô textbox rồi click Search. Kết quả:\n Woo, có đẩy đủ các Edition luôn. Mình chọn 2nd Editon cho cập nhật.\nTrang này mình vô tình biết được trong một lần lang thang lên mạng tìm sách free. Có khá nhiều trang cho download sách, nhưng cá nhân mình thâý trang này dễ dàng và đầy đủ nhất (trong phạm vi nhu cầu của mình).\n3. Đọc các bài viết trên https://medium.com/\nTrang web này có lẽ đã khá quen thuộc với nhiều người. Các bài viết trên đó đều đến từ các chuyên gia, và người có kinh nghiệm trong từng lĩnh vực cụ thể, và được cập nhật hàng ngày. Mình cảm thấy các bài viết trên đó rất chất lượng, rất đáng để đọc. Mình đang cố gắng tập thói quen chuyển bớt thời gian lướt Facebook sang đọc Medium. :D\nTuy nhiên, https://medium.com/ chỉ cho đọc free tối đa 5 bài. Muốn đọc nhiều hơn bạn phải đăng ký thành viên, trả phí 5$/tháng. 5$ thực sự không phải là một con số lớn so với lượng kiến thức hay ho như vậy. Thế nên mình quyết định mua, không xài \u0026ldquo;chùa\u0026rdquo; nữa. :D\nDù vậy, nếu bạn vẫn muốn đọc free thì dưới đây là cách cho bạn:\nGiả sử khi bạn vào https://medium.com/, click vào 1 bài để đọc thì nhận được thông báo:\n Hãy làm theo các bước sau:\nClick vào biểu tượng Cookie trên trình duyệt:\n Chọn Cookies:\n Chọn medium.com rồi click vào nút Remove.\nQuay trở lại trình duyệt, reload lại trang. Bạn sẽ thấy bài viết đã được hiển thị đầy đủ.\nNgoài nội dung kiến thức hay và phong phú, Medium còn có tính năng recommend các bài biết hay, phù hợp với lĩnh vực bạn quan tâm, dựa trên cookies của bạn. Vì thế, nếu bạn chọn cách đọc free thì đôi khi bạn sẽ bỏ lỡ các bài viết hay và mới nhất. Hãy cân nhắc khi quyết định hành động của bạn!\nVậy là mình đã giới thiệu đến mọi người 3 cách để tiếp cận với nguồn tri thức vô tận trên Internet. Nhấn mạnh lại lần nữa là mình không ủng hộ hoàn toàn các cách làm này. Nếu bạn có điều kiện, hãy mua sách, mua tài khoản và trả tiền một cách đầy đủ cho tác giả. Nếu bạn như mình, không quá dư giả nhưng sách thì vẫn muốn đọc thì có thể cân nhắc như cách mình đã làm. Ngoài ra, nếu bạn download một cuốn sách về đọc mà trong lòng cảm thấy \u0026ldquo;áy náy\u0026rdquo; thì hãy làm một việc tốt gì đó bù lại. Chẳng hạn, tối đi làm về trên đường, gặp người ăn xin, bạn có thể cho họ chút tiền lẻ của bạn. Như vậy thì ít nhiều bạn cũng đỡ \u0026ldquo;áy náy\u0026rdquo; hơn phần nào! :)\nCuối cùng, chúc mọi người tìm đuợc những cuốn sách hay, bài báo thú vị và bổ ích cho mình!\n","permalink":"https://tiensu.github.io/blog/25_for_book_lover/","tags":["Ebook"],"title":"Dành cho người yêu sách"},{"categories":["Deep Learning","CNN"],"contents":"Tiếp tục chuỗi các bài viết về CNN, trong bài này mình sẽ chia sẻ với các bạn một số \u0026ldquo;common patterns 7 rules\u0026rdquo; trong việc xây dựng kiến trúc CNN. Nắm rõ những \u0026ldquo;patterns \u0026amp; rules\u0026rdquo; này sẽ giúp các bạn giảm thiếu thời gian và công sức khá nhiều trong các dự án của các bạn!\n3. Common Architectures \u0026amp; Training Patterns\nQua 2 bài viết trước, chúng ta đã biết, CNN được tạo thành từ 4 loại layers chủ yếu, bao gồm: CONV, POOL, RELU, và FC. Sắp xếp các layers này với nhau theo một thứ tự nhất định ta sẽ một CNN (gọi tên đầy đủ là kiến trúc CNN).\n3.1 Layers Patterns\nNói chung, hầu hết các kiến trúc CNN đều có mộ vài lớp CONV và RELU liên tiếp nhau, theo sau bởi lớp POOL. Lặp lại như thế đến khi kích thước của input volumn đủ nhỏ, rồi thêm vào một hoặc nhiều FC layers. Pattern tổng quát như sau:\nINPUT =\u0026gt; [[CONV =\u0026gt; RELU]xM =\u0026gt; POOL?]xN =\u0026gt; [FC =\u0026gt; RELU]xK =\u0026gt; FC\nKý hiệu x ở đây tức là lặp lại 1 hoặc nhiều lần, còn ? nghĩa là tùy chọn, có thể có hoặc không.\nM, N, K thường chọn theo các rules sau:\n 0 \u0026lt;= N \u0026lt;= 3 M \u0026gt;= 0 0 \u0026lt;= K \u0026lt;= 2  Ví dụ một số kiến trúc CNN áp dụng pattern tổng quát bên trên như sau:\n INPUT =\u0026gt; FC INPUT =\u0026gt; [CONV =\u0026gt; RELU =\u0026gt; POOL]x2 =\u0026gt; FC =\u0026gt; RELU =\u0026gt; FC INPUT =\u0026gt; [[CONV =\u0026gt; RELU]x2 =\u0026gt; POOL]x3 =\u0026gt; [FC =\u0026gt; RELU]x2 =\u0026gt; FC  Các kiến trúc CNN kinh điển cũng dựa trên pattern tổng quát này:\n AlexNet: INPUT =\u0026gt; [CONV =\u0026gt; RELU =\u0026gt; POOL]x2 =\u0026gt; [CONV =\u0026gt; RELU]x3 =\u0026gt; POOL =\u0026gt; [FC =\u0026gt; RELU =\u0026gt; DO]x2 =\u0026gt; SOFTMAX VGGNet: INPUT =\u0026gt; [CONV =\u0026gt; RELU]x2 =\u0026gt; POOL =\u0026gt; [CONV =\u0026gt; RELU]x2 =\u0026gt; POOL =\u0026gt; [CONV =\u0026gt; RELU]x3 =\u0026gt; POOL =\u0026gt; [CONV =\u0026gt; RELU]x3 =\u0026gt; POOL =\u0026gt; [FC =\u0026gt; RELU =\u0026gt; DO]x2 =\u0026gt; SOFTMAX  Một cách khái quát, chúng ta sẽ áp dụng các kiến trúc CNN sâu khi gặp bài toán phức tạp, nhiều labels, các đối tượng thay đổi không có quy luật. Sử dụng nhiều CONV layers trước khi áp dụng POOL layer cho phép các CONV layers học được các complex features trước khi áp dụng POOL layer để giảm kích thước của input volumn.\nNhư đã đề cập ở bài trước, một số kiến trúc CNN đã loại bỏ hoàn toàn các POOL layers phía sau CONV layers, chỉ sử dụng CONV layers để giảm kích thước của input volumn. Hơn nữa, các FC layers ở cuối cũng không còn được sử dụng, thay vào đó là average pooling. GoogLeNet, ResNet, SqueezeNet là những kiến trúc sử dụng cách này. Kết quả là giảm số lượng tham số của CNN và thời gian train cũng ngắn hơn.\nĐặc biệt hơn, GoogLeNet còn áp dụng đồng thời 3 loại filters có kích thước khác nhau (1x1, 3x3, 5x5) tại cùng 1 vị trí trong kiến trúc để học multi-level features. Những kiến trúc kiểu như này được coi là công nghệ tiên tiến trong lĩnh vực DL.\n3.2 Quy tắc ngón tay cái\nTrong phần này, chúng ta sẽ cùng xem xét một số rules khi xây dựng CNN model.\n Rule 1  Đầu tiên, images đưa vào CNN nên có chiều rộng và chiều cao bằng nhau (square) ($W_{input} = H_{input}$). Sử dụng squere images cho phép chúng ta tận dụng các lợi ích của các thư viện tối ưu trong đại số tuyến tính. Kích thước thường hay sử dụng là: 32x32, 64x64, 96x96, 224x224, 227x227, 229x229.\n Rule 2  Thứ 2, sau khi đi qua CONV layer đầu tiên, kích thước của images nên có thể chia hết cho 2. Điều này, cho phép POOL layer tiếp sau đó hoạt động theo cách hiệu quả hơn. Để áp dụng rule này, có thể điêu chỉnh kích thước của filters và stride. Nói chung, CONV layers nên có kích thước nhỏ (3x3 hoặc 5x5). Tiny filter (1x1) có thể được sử dụng để học các local features, nhưng chỉ nên áp dụng trong các kiến trúc hiện đại và phức tạp. Kích thước lớn hơn của filters (7x7 hoặc 11x11) cũng có thể xuất hiện ở CONV layer đầu tiên trong kiế trúc để giảm nhanh kích thước không gian của input volumn có kích thước \u0026gt; 200x200 pixels. Nhấn mạnh là chỉ áp dung filers có kích thước lớn ở CONV layer đầu tiên, ngược lại, input volumn sẽ giảm rất nhanh làm mất mát các features quan trọng.\n  Rule 3 Stride của CONV, S = 1 cũng nên được sử dụng cho các CONV layers đối với các input volumns có kích thước trung bình nhỏ (\u0026lt; 200x200 pixels). Sử dụng S = 2 cho các input volumns có kích thước lớn hơn, nhưng cũng chỉ nên áp dụng ở CONV layer đầu tiên. Khi S = 1 thì CONV layers làm nhiệm vụ học các features của images, trong khi POOL layers chịu trách nhiệm giảm kích thước input volumns. Tuy nhiên, nhắc lại lần nữa rằng trong các kiến trúc CNN tiên tiến, POOL layers đang dần dần được thay thể bởi CONV layers với S \u0026gt;= 2.\n  Rule 4 Cá nhân mình thường áp dụng zero-padding trong CONV layer để đảm bảo kich thước của input volumns không đổi khi đi qua CONV layer và sử dụng POOL layer để giảm kích thước input volumn. Thực nghiệm của mình cho thấy classification accuracy thường cao hơn khi sử dụng rule này. Khi làm viêc với Keras framework, bạn có thể làm điều này một cách dễ dàng bằng cách setting padding=same khi tạo CONV layer. Bạn chỉ nên sử thay POOL layer bằng CONV layer khi đã thành thạo ở mức chuyên gia trong việc thiết kế kiến trúc của CNN.\n  Rule 5\n  Đối với POOL layer, kích thước thông thường của nó trong kiến trúc CNN là 2x2, cộng với stride S = 2. Kích thước 3x3 cũng có thể sử dụng ở các layers đầu trong CNN để giảm nhanh kích thước của input volumn. Kích thước \u0026gt; 3x3 chưa từng thấy xuất hiện trong bất cứ mạng CNN nào từ trước đến giờ.\n Rule 6  Về phần Batch Normalization, như trong bài trước đã đề cập, mặc dù nó làm tăng lên đáng kể thời gian training, nhưng chúng ta vẫn nên sử dụng nó trong hầu hết các trường hợp vì những lợi ích mà nó mang lại. BN layer được đặt sau ACT layer như trong các ví dụ sau:\n  INPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; FC\n  INPUT =\u0026gt; [CONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; POOL]x2 =\u0026gt; FC =\u0026gt; RELU =\u0026gt; BN =\u0026gt; FC\n  INPUT =\u0026gt; [[CONV =\u0026gt; RELU =\u0026gt; BN]x2 =\u0026gt; POOL]x3 =\u0026gt; [FC =\u0026gt; RELU =\u0026gt; BN]x2 =\u0026gt; FC\n  Rule 7\n  Droput (DO) được đặt giữa các FC layers với xác suất ngắt kết nối các nodes là 50%. Nó cũng được khuyên sử dụng DO trong mọi kiến trúc CNN của bạn. Cá nhân mình, đôi khi cũng đặt DO ở giữa CONV và POOL layers, và điều này đôi khi cũng tỏ ra hiệu quả trong việc giảm bớt Overfitting. Bạn có thể thử-sai trong các bài toán của bạn.\nOk, đó là 7 rules mình muốn giới thiệu đến các bạn. Bằng viêc ghi nhớ những rules này, bạn sẽ bớt đau đầu hơn khi xây dựng kiến trúc CNN của riêng mình. Một khi bạn đã trở thành chuyên gian xây dựng mạng CNN theo cách truyền thống như thế này, hãy thử bỏ qua max pooling, chỉ sử dụng CONV layer để giảm kích thước không gian của input volumns và sử dụng average pooling thay thế cho FC layer để giảm độ phức tạp tính toán của CNN. Mình sẽ đề cập chi tiết hơn những kỹ thuật advances này trong các bài viết về sau.\n4. Tổng kết\nVậy là mình đã kết thúc 3 bài viết về CNN. Hi vọng với những kiến thức chia sẻ ở đây sẽ giúp ích được cho các bạn, đặc biệt là các patterns và rules ở bài số 3 này. Các bạn có thể áp dụng luôn vào trong bài toán của mình và kiểm tra sự khác biệt.\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/24_convolutional_neural_network_3/","tags":["Deep Learning","CNN"],"title":"Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 3"},{"categories":["Deep Learning","CNN"],"contents":"Vì sử dụng trực tiếp raw pixel của image nên so với CNN, FCN (Fully Connected Network) có 2 nhược điểm kích thước của image tăng lên:\n Hiệu năng giảm mạnh. Kích thước của mạng tăng nhanh. Kết quả thực nghiệm cho thấy, khi áp dụng Fully Connected Network vào bộ dataset CIFAR-10, độ chính xác chỉ đạt được 52%.  CNN, theo một cách khác, sắp xếp các layers theo dạng 3D volume với 3 chiều: Width, Height, Depth. Các neurons trong mỗi layer chỉ kết nối tới 1 small region của layer trước đó - gọi là local connectivity. Điều này giúp giảm bớt rất nhiều kích thước của mạng.\n2. Các loại layers\nCó khá nhiều các dạng layers khác nhau để xây dựng nên CNNs. Các loại dưới đây được sử dụng phổ biến:\n Convolutional (CONV) Activation (ACT, RELU, SOFTMAX) Pooling (POOL) Fully-connected (FC) Batch normalization (BN) Dropout (DO)  Sắp xếp các dạng layers trên liên tiếp nhau theo thứ tự nào đó sẽ cho ta một CNN.\nVí dụ: INPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; FC =\u0026gt; SOFTMAX\nỞ đây, ta định nghĩa một CNN đơn giản, nhận input, áp dụng \u0026ldquo;convolution layer\u0026rdquo;, sau đó là 1 \u0026ldquo;activation layer\u0026rdquo; (RELU), tiếp theo là 1 \u0026ldquo;fully-connected layer\u0026rdquo;. Cuối cùng là một \u0026ldquo;activation layer\u0026rdquo; nữa (SOFTMAX) để đạt được xác suất của output theo các nhãn cần phân loại.\nTrong số các lớp trên, chỉ có CONV và FC là chứa các tham số được cập nhật trong quá trình training. POOL có tác dụng thay đổi kích thước không gian của image khi nó di chuyển qua các lớp của CNN.\nCONV, POOL, RELU và FC là 4 layers quan trọng nhất, gần như không thể thiếu khi xây dựng CNNs.\n2.1 CONV\nCONV chứa một tập K learnable filters (ví dụ: Kernel), mỗi filter có kích thước width x height. Mặc định,width luôn luôn bằng height, trừ khi có lý do đặc biệt. Hai giá trị này có thường nhỏ (1, 3, 5, 7) nhưng K thì có thể rất lớn (4, 8, 16, 32, 64, 128, \u0026hellip;). K cũng được gọi là độ sâu (depth) của CONV layer.\nCùng xem xét forward-pass của một CNN. CONV có K filters, áp dụng vào một input volumn có kích thước WxH. Tưởng tượng rẵng, mỗi filters sẽ trượt ngang qua toàn bộ input volumn, tính toán convolution, sau đó lưu kết quả ra một mảng 2 chiều, gọi là activation map. Xem hình bên dưới:\n Sau khi áp dùng K filters lên input volumn, chúng ta thu được K, 2-dimensional activation maps. Xếp chồng (Stack) những activation maps này theo chiều sâu sẽ thu được kết qủa cuối cùng (output volumn).\n Xét về kích thước của output volumn, có 3 tham số tác động. Chúng là depth, stride và zero-padding size.\n Depth  Như đã nói ở trên, depth chính là số lượng filters trong CONV layer, có giá trị là K.\n Stride  Stride là kích thước của step khi các filters trượt qua input volumn. Giá trị của stride thường là 1 hoặc 2 (S=1 hoặc S=2), tương ứng với step là 1 hoặc 2 pixel. S nhỏ sẽ sinh ra ouput volumn lớn, và có nhiều vùng được bị trùng lặp trong quá trình trượt và tính covolution của các filters. Đối với S lớn, kết quả sẽ ngược lại.\nXem ví dụ sau:\nTrong hình bên dưới, ma trận bên trái là input volumn, ma trận bên phải là filter.\n Sử dụng S = 1 và S = 2 cho convolution, thu được kết quả tương ứng bên trái, phải trong hình sau:\n Cùng với pooling (xem phần bên dưới), stride có thể được sử dụng để giảm kích thước của input volumn.\n Zero-padding  Sử dụng stride làm giảm kích thước của input volumn. Vậy nếu muốn giữ nguyên kích thước của input volumn thì sao? Zero-padding chính là câu trả lời.\nZero-padding tức là gắn thêm (pad) viền (border) cho input volumn. Các giá trị được gắn thêm đều là 0, vì thế mà có tên zero-padding.\nXem ví dụ sau:\n Bên trái là 3x3 ouput khi áp dụng 3x3 convolution tới 5x5 input.\nBên phải là khi áp dụng zero-padding vào input, thu được input mới có kích thước 7x7. Giá trị của zero-padding trong trường hợp này là P = 1.\nBên dưới là 5x5 ouput khi áp dụng 3x3 convolution tới 7x7 input. Ta thấy kích thước 5x5 của input ban đầu được duy trì trong output.\nNếu không sử dụng zero-padding, kích thước của input volumn sẽ giảm rất nhanh, do đó không thể xây dựng CNN nhiều layers.\nCông thức tính kích thước của ouput volumn như sau: O = ((W - F + 2P)/S) + 1\nTrong đó:\n 0: kích thước của output volumn. W: kích thước input volumn F: kích thước của filter P: kích thước zero-padding S: kích thước stride  Nếu O không phải là số nguyên, cần thay đổi lai giá trị của S.\nThử áp dụng công thức này vào ví dụ bên trên:\nO = ((5 - 3 + 2*1)/1) = 5\n2.2 Activation Layers\nActivation layer thường được áp dụng sau mỗi CONV layer trong một mạng CNN. Các activation layer hay dùng là các hàm phi tuyến, giống như: ReLU, ELU, Leaky ReLU. Trong các public paper, ReLU được sử dụng rất phổ biến, gần như là mặc định. Khi viết ACT, ta ngầm hiểu đó là ReLU.\nTrên thực tế, activation layer không được coi là một layer theo đúng nghĩa. Bởi vì nó không có parameters nào được học trong quá trình huấn luyện mô hình. Trong một số diagram kiến trúc của mạng, nó có thể không xuất hiện và được ngầm hiểu rằng nó nằm ngay sau CONV layer.\nVí dụ với kiến trúc sau:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; FC\nCó thể được viết gọn thành:\nINPUT =\u0026gt; CONV =\u0026gt; FC\nMột activation layer nhận input volumn có kích thước là $W_{input}$x$H_{input}$x$D_{input}$, áp dụng activation function theo kiểu element-wise nên kích thước của output volumn đúng bằng kích thước của input volumn: $W_{input}$=$W_{output}$, $H_{input}$=$H_{output}$, $D_{input}$=$D_{output}$.\n2.3 Pooling Layers\nCó 2 phương pháp để giảm kích thước của input volumn: CONV layer (với stride \u0026gt; 1) và POOL layer.\nThông thường, POOL layer được đặt ngay sau ACT layer và trước CONV layer. Trong trường hợp không có ACT layer thì nó nằm giữa 2 CONV layer.\nVí dụ kiến trúc mạng sau:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; POOL =\u0026gt; CONV =\u0026gt; POOL =\u0026gt; FC\nChức năng đầu tiên của POOL layer là giảm kích thước không gian của input volumn một các từ từ. Điều này dẫn đến việc giảm số lượng tham số và sự phức tạp tính toán của mạng. Vì thế mà nó là một trong những phương pháp hiệu của để tránh hiện tượng Overfitting cho mạng DL.\nPOOL layer hoạt động độc lập trên mỗi depth slice của input volumn, sử dụng hàm max hoặc average. Hai cái tên có lẽ cũng đã nói lên cách thức hoạt động của chúng. Max pooling chỉ giữ lại giá trị lớn nhất trong phạm vi kích thước của nó còn average pooling thì lấy giá trị trung bình của các giá trị trong phạm vi kích thước của POOL layer. Về vị trí trong kiến trúc, trong khi max pooling thường được đặt ở giữa của kiến trúc mạng DL để giảm kích thước, còn average pooling lại hay được đặt ở các layers cuối (hoặc gần cuối) (VD: GoogLeNet, SqueezeNet, ResNet) để thay thế cho các FC layers, giúp giảm độ phức tạp cả model.\nKích thước của POOL layer hay được sử dụng là 2x2. Mặc dù vậy, với input volumn có kích thước \u0026gt; 200x200, ta có thể sử dụng kích thước 3x3 của POOL ở những layer đầu.\nBước nhảy (stride, S) của mỗi POOL layer thường là 1 hoặc 2. Hình bên dưới minh họa kết quả hoạt dộng của max pooling với S =1,2, tương ứng.\n Công thức tính kích thước của output volumn sau khi qua POOL layer như sau:\n $W_{output}$ = (($W_{input}$ - $F$)/$S$) + 1 $H_{output}$ = (($H_{input}$ - $F$)/$S$) + 1 $D_{output}$ = $D_{input}$  Trong đó:\n $W_{input}$x$H_{input}$x$D_{input}$: kích thước của input volumn. $W_{output}$x$H_{output}$x$D_{output}$: kích thước của output volumn. F: kích thước của POOL layer (cũng gọi là pool size). S: stride  Trong các bài toán thực tế, có 3 dạng max pooling thường hay được sử dụng.\n Dạng 1: (F = 3, S = 2), gọi là overlapping pooling, thường áp dụng đối với các images/input volumn có kích thước lớn (\u0026gt; 200x200 pixels) Dạng 2: (F = 2, S = 2), gọi là non-overlapping pooling, thường được áp dụng với các images/input volumn có kích thước trung bình (\u0026gt; 64x64 pixels và \u0026lt; 200x200 pixels) Dạng 3: (F = 2, S = 1), gọi là small pooling, áp dụng với các images/input volumn nhỏ (\u0026lt; 64x64 pixels)  Đến đây, có thể các bạn sẽ thắc mắc, khi nào thì dùng CONV layer, khi nào thì dùng POOL layer để giảm kích thước của input volumn?\nSpringenberg et al, trong paper Striving for Simplicity: The All Convolutional Net xuất bản năm 2014 của họ đã đề xuất loại bỏ hoàn toàn POOL layer, chỉ sử dụng CONV layer (với S\u0026gt;1). Họ đã chứng minh tính hiệu của cách tiếp cận này trên một số tập dữ liệu, bao gồm cả CIFAR-10 (small images, low number of class) và ImageNet (large input images, 1000 classes). Xu hướng này cũng xuất hiện trong kiến trúc của mạng Resnet năm 2015 và đang dần dần trở nên phổ biến hơn. Có lẽ trong tương lai không xa, chúng ta sẽ không sử dụng POOL layer (cụ thể là max pooling) trong phần giữa các kiến trúc mạng DL hiện đại nữa mà chỉ sử dụng average pooling tại các layer cuối để thay thế cho FC layer vốn cồng kềnh và phức tạp. Tuy nhiên, trước mắt thì max pooling vẫn chưa thể biến mất hoàn toàn được, nên chúng ta vẫn cần phải học, hiểu và áp dụng chúng trong việc xây dựng kiến trúc mạng DL của riêng mình, cũng như đọc hiểu các kiến trúc mạng DL kinh điển khác.\n2.4 Full-Connected (FC) Layers\nFC Layer chính là Feedforward Neural Network mà chúng ta đã tìm hiểu trong bài \u0026hellip;. Nó luôn được đặt ở cuối trong các kiến trúc mạng DL.\nVí dụ kiến trúc mạng sau,\nINPUT =\u0026gt; (CONV =\u0026gt; RELU =\u0026gt; POOL)x2 =\u0026gt; FC =\u0026gt; FC =\u0026gt; SOFTMAX.\nta đã sử dụng 2 FC layers ở gần cuối mạng, theo sau là ACT layer (SOFTMAX) để phân loại (tính toán xác suất của mỗi classes).\n2.5 Batch Normalization (BN)\nĐược giới thiệu lần đầu vào năm 2015 trong paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift của Ioffe và Szegedy, BN đã nhanh chóng trở nên phổ biến trong các kiến trúc mạng DL. Đúng như tên của nó, BN layer có tác dụng normalize input volumn trước khi đi vào layer tiếp theo.\nGiả sử, ta có input tại thời đểm i (mini-batch i) là $x_i$, sau khi đi qua BN layer, ta thu được giá trị $\\widehat{x_i}$ theo công thức:\n$\\widehat{x_i} = \\frac{x_i - \\mu_ \\beta}{\\sqrt{\\sigma^2_ \\beta + \\varepsilon}}$   Trong đó:   $\\mu_ \\beta = \\frac{1}{M}\\sum_{i=1}^m x_i$    $\\sigma^2_ = \\frac{1}{M}\\sum_{i=1}^m (x_i - \\mu_\\beta)^2$      Giá trị của $\\varepsilon$ được chọn là giá trị dương đủ nhỏ (VD: 1e-7) để tránh việc chia cho 0. Sau khi áp dụng BN, input volumn sẽ có trung bình (mean) xấp xỉ 0 và độ lệch chuẩn (variance) xấp xỉ 1 (còn gọi là zero-centered).\nKhi sử dụng model để test, ta thay thế $\\mu_\\beta$ và $\\sigma_\\beta$ bằng giá trị trung bình của chúng trong suốt quá trình training. Điều này đảm bảo cho ta có thể pass input volumn xuyên qua mạng DL mà không bị biased bởi giá trị $\\mu_\\beta$ và $\\sigma_\\beta$ tại thời điểm cuối cùng ($x_m$).\nBN layer tỏ ra hiệu quả cao trong việc làm cho quá trình training một NN ổn định hơn, giảm số lượng epochs cần thiêt để train model, và quan trọng nhất là hạn chế tình trạng Overfitting. Khi sử dụng BN, việc tuning các tham số khác của model cũng trở nên đơn giản hơn bởi vì BN đã thu hẹp đáng kế phạm vi giá trị của các weights trong mạng.\nHạn chế lớn nhất của BN có lẽ là nó làm tăng thời gian training của bạn do phải tính toán normalization và statistic tại mỗi nơi mà nó xuất hiện trong kiến trúc mạng. Thường là tăng gấp 2 đến 3 so với không sử dụng BN.\nTuy nhiên, có lẽ hạn chế trên không đáng kể so với những ưu điểm mà BN mang lại. Vì vậy, lời khuyên ở đây là nên sử dụng BN thường xuyên trong bài toán của bạn.\nCuối cùng là về vị trí đạt BN layer trong kiến trúc DL. Mặc dù trong paper gốc của tác giả BN layer được đặt trước ACT layer, nhưng điều này lại không hợp lý vê mặt thống kê. Bởi vì output của BN là zero-centered, khi đi qua ACT layer (ReLU), phần giá trị âm sẽ bị triệt tiêu. Điều này vô tình làm mất đi bản chất của BN. Thực nghiệm rất nhiều cũng đã chỉ ra rằng, đặt BN layer ở sau ACT layer cho kết quả tốt hơn (higher accuracy và lower loss) trong hầu hết mọi trường hợp. Vì thế, mặc định, hãy đặt BN layer sau ACT layer, trừ khi bạn có lý do đặc biệt nào khác.\nVí dụ về việc đặt BN layer trong kiến trúc DL:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; BN \u0026hellip;\n2.6 Dropout (DO) layer\nDO thực chất là một dạng của regularization, mục đích là để hạn chế hiện tượng Overfitting. Tại mỗi mini-batch trong quá trình train, DO layer sẽ ngẫu nhiên ngắt kết nối các inputs giữa 2 layer liên tiếp, với xác suất p.\nVí dụ về DO với p = 5 giữa 2 FC layers như hình bên dưới:\n DO chỉ hoạt động theo 1 chiều forwarding, chiều ngược lại (backwarding), các dropped connections sẽ được kết nối lại để tính toán.\nDO giúp giảm Overfitting theo cách như trên bởi vì khi đó, vai trò của các nodes trong mạng sẽ được phân phối đều hơn, không có nodes nào chịu trách nhiệm chính, nhiều hơn các nodes khác. Điều này sẽ giúp model generalize tốt hơn.\nVề vị trí trong kiến trúc mạng DL, DO layer thường được set với p = 0.5 và đặt xen kẽ 2 FC layers ở cuối.\nVí dụ:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; POOL =\u0026gt; FC =\u0026gt; DO =\u0026gt; FC =\u0026gt; DO =\u0026gt; SOFTMAX\nBài thứ 2 về CNN xin được kết thúc tại đây. Trong bài tiếp theo (cũng là bài cuối cùng về CNN), mình sẽ chia sẻ một số patterns và một số rules trong việc xây dựng kiến trúc CNN. Mời các bạn đón đọc!\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/23_convolutional_neural_network_2/","tags":["Deep Learning","CNN"],"title":"Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 2"},{"categories":["Deep Learning","CNN"],"contents":"Sau khi đã tìm hiểu cơ bản về Neural Network, chúng ta sẽ đi tìm hiểu về CNN. CNN là một dạng kiến trúc Neural Network đóng vai trò vô cùng quan trọng trong Deep Learning.\nTrong Feedfoward Neural Network, mỗi neural trong một layer được kết nối đến tất cả các nodes của layer tiếp theo. Ta gọi điều này là Fully Connected (FC) layer. Tuy nhiên, trong CNNs, FC layers chỉ được sử dụng ở 1 vài layers cuối. Các layers còn lại được gọi là convolutional layers.\nMột hàm kích hoạt (activation function) (thường là ReLU) được áp dụng tới output của các convolutional layers. Kết hợp với các dạng layers khác nhau để giảm kích thước của input. Các FC layers ở cuối có nhiệm vụ phân loại output thành các classes khác nhau.\nMỗi layer trong CNN áp dụng một tập các bộ lọc (filters) (có thể lên đến hàng trăm hoặc hàng nghìn), kết hợp kết quả lại, cho qua layer tiếp theo. Trong suốt quá trình training, giá trị của các filters được cập nhật (tương tự như trọng số weight trong Neural Network).\nTrong lĩnh vực xử lý ảnh, CNN có thể học để:\n Phát hiện biên (edges) từ raw pixel data ở layer đầu tiên. Sử dụng edges đã phát hiện để phát hiện hình dạng (shapes) đối tượng ở layer thứ 2. Sử dụng shapes để phát hiện heigher-level features trong các layers tiếp theo.  1. Hiểu rõ về Convolutions\nChúng ta sẽ cùng nhau trả lời một số câu hỏi sau:\n Convolutions là gì? Chúng lamf được những việc gì? Tại sao lại sử dụng chúng? Áp dụng chúng vào xử lý ảnh như thế nào?  Từ convolution, dịch sang tiếng việt là tích chập, nghe có vẻ phức tạp. Bạn chắc chắn đã nghe đến từ này nếu bạn học qua môn Xử lý tín hiệu sô. Tuy nhiên, convolution trong lĩnh vực xử lý lý ảnh hơi khác một chút. Không phải khi có Deep Learning, chúng ta mới sử dụng convolution, các phương pháp xử lý ảnh truyền thống đều sử dụng convolution: Edges detection, Sharpen images, Blurring and Smoothing images, \u0026hellip; Vì thế mới nói, convolution là xương sống của xử lý ảnh. Hiểu rõ convolution là điều kiện tiên quyết để bước chân vào lĩnh vựa xử lý ảnh (theo cả phương pháp truyền thống và sử dụng Deep Learning).\nNghe thì có vẻ \u0026ldquo;đao to búa lớn\u0026rdquo; vậy, nhưng thực sự không phải vậy. Convolution đơn giản chỉ là tổng của các tích đôi một của từng phần tử tron 2 ma trận. Chia nhỏ các bước ra cho dễ hiểu:\n Lấy 2 ma trận có cùng kích thước Nhân 2 ma trận đôi một (element-by-element) (không phải phép nhân ma trận trong đại số tuyến tính). Cộng kết quả của các tích lại.  Yup, đó là convolution.\n1.1 Kernel\nMột image là một ma trận nhiều chiều. Thường là 3 chiều (w, h, c) với width là số cột, height là số hàng và depth là số kênh màu. \u0026ldquo;Image matrix\u0026rdquo; thường được gọi với cái tên \u0026ldquo;big matrix\u0026rdquo;. Một ma trận khác gọi là kernel (hoặc \u0026ldquo;convolution matrix\u0026rdquo;, \u0026ldquo;tiny matrix\u0026rdquo;, filter) đặt bên trên \u0026ldquo;big matrix\u0026rdquo;, trượt từ trái sang phải, từ trên xuống dưới. Trong quá trình di chuyển, các phép toán (convolution, \u0026hellip;) được áp dụng đối với 2 ma trận đó. Sử dụng các kernel khác nhau, ta có thể đạt được các mục đích mong muốn: Blurring (average smoothing, Gaussian smoothing, \u0026hellip;), Edge detection (Laplacian, Sobel, ...), \u0026hellip;\nĐể hiểu rõ hơn, chúng ta sẽ làm thử 1 ví dụ cụ thể.\nGiả sử có \u0026ldquo;image matrix\u0026rdquo; kernel như sau:\n   Theo lý thuyết bên trên, kernel sẽ được trượt qua \u0026ldquo;image matrix\u0026rdquo; từ trái qua phải, từ trên xuống dưới. Số bước trượt thường là 1 hoặc 2. Tại bước, sau khi trượt xong, ta sẽ dừng lại, thực hiện phép convolution giữa kernel và phần \u0026ldquo;image matrix\u0026rdquo; bị che bởi kernel. Giá trị ouput được lưu trong ma trận kết quả tại vị trí trung tâm của kernel tại bước đó.\nChi tiết các bước:\n Chọn tọa độ ($x,y$) từ \u0026ldquo;image matrix\u0026rdquo;. Đặt center của kernel tại ($x,y$). Thực hiện convolution giữa kernel và phần \u0026ldquo;image matrix\u0026rdquo; bị che phủ bởi kernel. Lưu kết quả tại ($x,y$) của ma trận kết quả.  Ví dụ, với $(x,y) = (3,3)$:   Sau khi tính toán xong, ta sẽ gán giá trị 132 cho pixel tại vị trí (3,3) của ma trận kết quả. $O_{i,j}$ = 132.\n1.2 Implement Convolutions bằng python.\nGiờ hãy bắt tay vào code thôi. Việc thực hiện convolution bằng code sẽ giúp bạn hiểu sâu sắc hơn cách áp dụng convolution trong xử lý ảnh.\nTạo file convolutions.py và code như sau:\n# USAGE # python convolutions.py --image mai-ngoc.jpg # import the necessary packages from skimage.exposure import rescale_intensity import numpy as np import argparse import cv2 def convolve(image, K): # grab the spatial dimensions of the image and kernel (iH, iW) = image.shape[:2] (kH, kW) = K.shape[:2] # allocate memory for the output image, taking care to \u0026#34;pad\u0026#34; the orders of the input image so the spatial size (i.e., width and height) are not reduced pad = (kW - 1) // 2 image = cv2.copyMakeBorder(image, pad, pad, pad, pad, cv2.BORDER_REPLICATE) output = np.zeros((iH, iW), dtype=\u0026#34;float\u0026#34;) # loop over the input image, \u0026#34;sliding\u0026#34; the kernel across each (x, y)-coordinate from left-to-right and top-to-bottom for y in np.arange(pad, iH + pad): for x in np.arange(pad, iW + pad): # extract the ROI of the image by extracting the *center* region of the current (x, y)-coordinates dimensions roi = image[y - pad:y + pad + 1, x - pad:x + pad + 1] # perform the actual convolution by taking the element-wise multiplication between the ROI and the kernel, the summing the matrix k = (roi * K).sum() # store the convolved value in the output (x, y)- coordinate of the output image output[y - pad, x - pad] = k # rescale the output image to be in the range [0, 255] output = rescale_intensity(output, in_range=(0, 255)) output = (output * 255).astype(\u0026#34;uint8\u0026#34;) # return the output image return output # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-i\u0026#34;, \u0026#34;--image\u0026#34;, required=True, help=\u0026#34;path to the input image\u0026#34;) args = vars(ap.parse_args()) # construct average blurring kernels used to smooth an image smallBlur = np.ones((7, 7), dtype=\u0026#34;float\u0026#34;) * (1.0 / (7 * 7)) largeBlur = np.ones((21, 21), dtype=\u0026#34;float\u0026#34;) * (1.0 / (21 * 21)) # construct a sharpening filter sharpen = np.array(( [0, -1, 0], [-1, 5, -1], [0, -1, 0]), dtype=\u0026#34;int\u0026#34;) # construct the Laplacian kernel used to detect edge-like regions of an image laplacian = np.array(( [0, 1, 0], [1, -4, 1], [0, 1, 0]), dtype=\u0026#34;int\u0026#34;) # construct the Sobel x-axis kernel sobelX = np.array(( [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]), dtype=\u0026#34;int\u0026#34;) # construct the Sobel y-axis kernel sobelY = np.array(( [-1, -2, -1], [0, 0, 0], [1, 2, 1]), dtype=\u0026#34;int\u0026#34;) # construct an emboss kernel emboss = np.array(( [-2, -1, 0], [-1, 1, 1], [0, 1, 2]), dtype=\u0026#34;int\u0026#34;) # construct the kernel bank, a list of kernels we\u0026#39;re going to apply using both our custom `convole` function and OpenCV\u0026#39;s `filter2D` function kernelBank = ( (\u0026#34;small_blur\u0026#34;, smallBlur), (\u0026#34;large_blur\u0026#34;, largeBlur), (\u0026#34;sharpen\u0026#34;, sharpen), (\u0026#34;laplacian\u0026#34;, laplacian), (\u0026#34;sobel_x\u0026#34;, sobelX), (\u0026#34;sobel_y\u0026#34;, sobelY), (\u0026#34;emboss\u0026#34;, emboss)) # load the input image and convert it to grayscale image = cv2.imread(args[\u0026#34;image\u0026#34;]) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # loop over the kernels for (kernelName, K) in kernelBank: # apply the kernel to the grayscale image using both our custom `convolve` function and OpenCV\u0026#39;s `filter2D` function print(\u0026#34;[INFO] applying {} kernel\u0026#34;.format(kernelName)) convolveOutput = convolve(gray, K) opencvOutput = cv2.filter2D(gray, -1, K) # show the output images cv2.imshow(\u0026#34;Original\u0026#34;, gray) cv2.imshow(\u0026#34;{} - convole\u0026#34;.format(kernelName), convolveOutput) cv2.imshow(\u0026#34;{} - opencv\u0026#34;.format(kernelName), opencvOutput) cv2.waitKey(0) cv2.destroyAllWindows() Kết quả:\nTừ trái sang phải: Ảnh gốc, ảnh áp dụng \"average blur\" sử dụng 7x7 kernel convolution, và ảnh áp dụng \"average blur\" sử dụng OpenCV’s cv2.filter2D.\n 1.3 Vai trò của Convolutions trong Deep Learning\nNhư các bạn đã thấy từ phần trước, chúng ta phải tự định nghĩa (manually hand-define) các kernel cho mỗi nhiệm vụ xử lý ảnh khác nhau.\nLiệu có cách nào \u0026ldquo;tự động hóa\u0026rdquo; việc này?\nCNN chính là câu trả lời. Bằng cách sắp xếp nhiều lớp convolutions, kết hợp với \u0026ldquo;activation function\u0026rdquo;, pooling, backpropagation, \u0026hellip; CNNs có khả năng học để cập nhật giá trị của kernel, từ đó trích xuất được các đặc trưng của đối tượng trong ảnh.\nTrong bài tiếp theo, mình sẽ tìm hiểu kỹ hơn về các dạng layers khác nhau, sau đó sẽ đưa ra một số \u0026ldquo;common layer stacking patterns\u0026rdquo; được sử dụng rộng rãi trong lĩnh vực xử lý ảnh.\nSource code sử dụng trong bài này, các bạn có thể tham khảo tại github cá nhân của mình tại github\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/22_convolutional_neural_network_1/","tags":["Deep Learning","CNN"],"title":"Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 1"},{"categories":["Deep Learning","Neural Network"],"contents":"Trong quá trình tìm hiểu về mạng NN, mình thấy khá là khó hiểu, đặc biệt với các bạn không mạnh về toán. Bài này, mình sẽ diễn giải cách thức làm việc của NN một cách trực quan, dễ hiểu cho các bạn thông qua một ví dụ cụ thể.\n1. Nhắc lại lý thuyết\nGiả sử ta có mạng NN như sau:\n Quá trình training model bao gồm 2 phases:\n1.1 Forward Path\nPhase này tính toán (dự đoán) đầu ra $o_1, o_2$, tính loss.\nGiả sử activation là hàm sigmoid:  Ta sẽ tính lần lượt các đại lượng trung gian:\n $in_{h_1}$: input của $h_1$ $in_{h_2}$: input của $h_2$ $out_{h_1}$: output của $h_1$ $out_{h_2}$: output của $h_2$ $in_{o_1}$: input của $o_1$ $in_{o_2}$: input của $o_2$ $out_{o_1}$: output của $o_1$ $out_{o_2}: output của $o_2$$  Công thức tính của từng đại lượng như sau:\n$in_{h_1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1$\n$in_{h_2} = w_3 * i_1 + w_4 * i_2 + b_1 * 1$\n$out_{h_1} = sigmoid(in_{h_1}) =$$\\frac{1}{1 + e^{-in_{h_1}}}$\n$out_{h_2} = sigmoid(in_{h_2}) =$$\\frac{1}{1 + e^{-in_{h_2}}}$\n$in_{o_1} = w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1$\n$in_{o_2} = w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1$\n$out_{o_1} = sigmoid(in_{o_1}) =$$\\frac{1}{1 + e^{-in_{o_1}}}$\n$out_{o_2} = sigmoid(in_{o_2}) =$$\\frac{1}{1 + e^{-in_{o_2}}}$\n Tiếp theo là tính loss bằng cách so sánh đầu ra của mạng NN với các giá trị thực tế:\n $target_{o_1}$ $target_{o_2}$:  Công thức tính loss như sau:\n$E_{total} = \\sum_{i=1}^2 E_{o_i} = \\sum_{i=1}^2 \\frac{1}{2} (target_{o_i} - out_{o_i})^2 = E_{o_1} + E_{o_2}$\n$E_{o_1} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2$\n$E_{o_2} = \\frac{1}{2} (target_{o_2} - out_{o_2})^2$\n 1.2 Backward Path\nMục đích của phase này là cập nhật trọng số $w$ sao cho tối thiểu hóa loss.\nTa sẽ sử dụng thuật toán tối ưu Stochastic Gradient Descent (SGD) để cập nhật $w$.\nCông thức cập nhật như sau:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta)$ với:\n $\\nabla_\\theta f(\\theta)$ là đạo hàm của Loss Function tại $\\theta$ (đạo hàm từng phần theo $\\nabla$). $\\eta$ là một số \u0026gt; 0, gọi là learning rate. $\\theta$ là tập hợp các vector các tham số của model cần tối ưu. Trong trường hợp này là các trọng số $w$.  Đạo hàm từng phần của các $w$ tại output layer được tính theo quy tắc chain rule như sau:\n$\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_5}$\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_7} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_7}$/p $\\frac{\\partial E_{total}}{\\partial w_8} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_8}$\n Đạo hàm từng phần của các $w$ tại hidden layer được tính như sau:\n$\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_1}$\n$\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_2}$\n$\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_3}$\n$\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_4}$\n Sau khi tính được đạo hàm từng phần của mỗi $w$, ta áp dụng công thức phía trên để cập nhật $w$.\n2. Ví dụ áp dụng\nVẫn với kiến trúc mạng như trên, ta sẽ gán các giá trị khởi tạo cho các tham số như hình bên dưới:\n Ok, bây giờ ta sẽ bắt đầu đi tính toán.\n2.1 Fordward Path\nInput của $h_1$:\n$in_{h_1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1$\n$in_{h_1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1$\n$in_{h_1} = 0.3775$\n Input của $h_2$:\n$in_{h_2} = w_3 * i_1 + w_4 * i_2 + b_1 * 1$\n$in_{h_2} = 0.25 * 0.05 + 0.3 * 0.1 + 0.35 * 1$\n$in_{h_2} = 0.3925$\n Ouput của $h_1$:\n$out_{h_1} = $$\\frac{1}{1 + e^{-in_{h_1}}}$\n$out_{h_1} = \\frac{1}{1 + e^{-0.3775}}$\n$out_{h_1} = 0.593269992$\n Output của $h_2$:\n$out_{h_2} = $$\\frac{1}{1 + e^{-in_{h_2}}}$\n$out_{h_2} = \\frac{1}{1 + e^{-0.3925}}$\n$out_{h_2} = 0.596884378$\n Input của $o_1$:\n$in_{o_1} = w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1$\n$in_{o_1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1$\n$in_{o_1} = 1.105905967$\n Input của $o_2$:\n$in_{o_2} = w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1$\n$in_{o_2} = 0.5 * 0.593269992 + 0.55 * 0.596884378 + 0.6 * 1$\n$in_{o_2} = 1.224921404$\n Output của $o_1$:\n$out_{o_1} = $$\\frac{1}{1 + e^{-in_{o_1}}}$\n$out_{o_1} = \\frac{1}{1 + e^{-1.105905967}}$\n$out_{o_1} = 0.75136507$\n Output cuat $o_2$:\n$out_{o_2} = $$\\frac{1}{1 + e^{-in_{o_2}}}$\n$out_{o_2} = \\frac{1}{1 + e^{-1.224921404}}$\n$out_{o_2} = 0.772928465$\n Tổng lỗi:\n$E_{o_1} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2$\n$E_{o_2} = \\frac{1}{2} (0.01 - 0.75136507)^2$\n$E_{o_1} = 0.274811083$\n $E_{o_2} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2$\n$E_{o_2} = \\frac{1}{2} (0.01 - 0.772928465)^2$\n$E_{o_2} = 0.023560026$\n $E_{total} = \\sum_{i=1}^2 E_{o_i}$\n$E_{total} = 0.274811083 + 0.023560026$\n$E_{total} = 0.298371109$\n 2.2 Backward Path\nTính đạo hàm từng phần của Loss Function theo mỗi $w$.\nCác $w$ của output layer ($w_5, w_6, w_7, w_8$) có cách tính giống nhau:\n $w_5$:  $\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_5}$\n Ta biết:\n$E_{total} = \\sum_{i=1}^2 \\frac{1}{2} (target_{o_i} - out_{o_i})^2$\n$E_{total} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2 + \\frac{1}{2} (target_{o_2} - out_{o_2})^2$\n Nên:\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1})^{2-1} * (-1) + 0$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(target_{o_1} - out_{o_1})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(0.01 - 0.75136507) = 0.74136507$\n Tiếp theo, vì:\n$out_{o_1} = sigmoid(in_{o_1}) =$ $\\frac{1}{1 + e^{-in_{01}}}$\n Nên:\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.186815602$\n Và, $in_{o_1} = w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1$\n Nên:\n$\\frac{\\partial in_{o_1}}{\\partial w_5}$ $ = out_{h_1}$\n$\\frac{\\partial in_{o_1}}{\\partial w_5}$ $ = 0.593269992$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_5}$$\n$\\frac{\\partial E_{total}}{\\partial w_5}$ $ = 0.74136507 * 0.186815602 * 0.593269992$\n$\\frac{\\partial E_{total}}{\\partial w_5}$ $ = 0.082167041$\n  $w_6$:  $\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n $\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1})^{2-1} * (-1) + 0$$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(target_{o_1} - out_{o_1})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(0.01 - 0.75136507) = 0.74136507$\n $\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.186815602$\n $\\frac{\\partial in_{o_1}}{\\partial w_6} $ $= out_{h_2}$\n$\\frac{\\partial in_{o_1}}{\\partial w_6}$ $ = 0.596884378$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_6}$ $ = 0.74136507 * 0.186815602 * 0.596884378$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= 0.082667628$\n  $w_7$:  $\\frac{\\partial E_{total}}{\\partial w_7} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_7}$\n $\\frac{\\partial E_{total}}{\\partial out_{o_2}} = 0 + 2 * \\frac{1}{2} (target_{o_2} - out_{o_2})^{2-1} * (-1)$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}}$ $ = -(target_{o_2} - out_{o_2})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}}$ $ = -(0.99 - 0.772928465) = -0.217071535$\n $\\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$ $ = out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.175510053$\n $\\frac{\\partial in_{o_2}}{\\partial w_7}$ $ = out_{h_1}$\n$\\frac{\\partial in_{o_2}}{\\partial w_7} $ $= 0.593269992$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.217071535 * 0.175510053 * 0.593269992$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.022602541$\n  $w_8$:  $\\frac{\\partial E_{total}}{\\partial w_8} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_8}$\n $\\frac{\\partial E_{total}}{\\partial out_{o_2}} = 0 + 2 * \\frac{1}{2} (target_{o_2} - out_{o_2})^{2-1} * (-1)$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}} $ $= -(target_{o_2} - out_{o_2})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}} $ $= -(0.99 - 0.772928465) = -0.217071535$\n $\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.175510053$\n $\\frac{\\partial in_{o_2}}{\\partial w_8} $ $= out_{h_2}$\n$\\frac{\\partial in_{o_2}}{\\partial w_8} $ $= 0.596884378$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.217071535 * 0.175510053 * 0.596884378$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.022740242$\n Các $w$ của hidden layer ($w_1, w_2, w_3, w_4$) có cách tính giống nhau:\n $w_1$:  $\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_1}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = w_5$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = 0.4$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}}$ $ = 0.138498562 * 0.4$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} $ $= 0.055399425$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$ $ = w_7$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} $ $= 0.5$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.038098237 * 0.5$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.019049118$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_1}} $$= 0.055399425 + (-0.019049118) = 0,036350307$\n ----------------------\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_1}}})}{\\partial in_{h_1}}$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} $ $= out_{h_1}(1 - out_{h_1})$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}}$ $ = 0.59326999(1 - 0.59326999) = 0.241300709$\n ----------------------\n$\\frac{\\partial in_{h_1}}{\\partial w_1} = \\frac{\\partial (w_1 * i_1 + w_2 * i_2 + b_1 * 1)}{\\partial w_1}$\n$\\frac{\\partial in_{h_1}}{\\partial w_1}$ $ = i_1$\n$\\frac{\\partial in_{h_1}}{\\partial w_1} $ $= 0.05$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_1}$\n$\\frac{\\partial E_{total}}{\\partial w_1} $ $= 0.036350306 * 0.241300709 * 0.05$\n$\\frac{\\partial E_{total}}{\\partial w_1} $ $= 0.000438568$\n  $w_2$:  $\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_2}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = w_5$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = 0.4$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}}$ $ = 0.138498562 * 0.4$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} $ $= 0.055399425$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$ $ = w_7$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} $ $= 0.5$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.038098237 * 0.5$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.019049118$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_1}} $$= 0.055399425 + (-0.019049118) = 0,036350307$\n ----------------------\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_1}}})}{\\partial in_{h_1}}$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} $ $= out_{h_1}(1 - out_{h_1})$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}}$ $ = 0.59326999(1 - 0.59326999) = 0.241300709$\n ----------------------\n$\\frac{\\partial in_{h_1}}{\\partial w_2} = \\frac{\\partial (w_1 * i_1 + w_2 * i_2 + b_1 * 1)}{\\partial w_2}$\n$\\frac{\\partial in_{h_1}}{\\partial w_2}$ $ = i_2$\n$\\frac{\\partial in_{h_1}}{\\partial w_2} $ $= 0.1$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_2}$\n$\\frac{\\partial E_{total}}{\\partial w_2} $ $= 0.036350306 * 0.241300709 * 0.1$\n$\\frac{\\partial E_{total}}{\\partial w_2} $ $= 0.000877135$\n  $w_3$:  $\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_3}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = w_6$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = 0.45$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}}$ $ = 0.138498562 * 0.45$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} $ $= 0.062324353$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$ $ = w_8$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} $ $= 0.55$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.038098237 * 0.55$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.02095403$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_2}} $$= 0.062324353 + (-0.02095403) = 0.041370323$\n ----------------------\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_2}}})}{\\partial in_{h_2}}$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} $ $= out_{h_2}(1 - out_{h_2})$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}}$ $ = 0.596884378(1 - 0.596884378) = 0.240613417$\n ----------------------\n$\\frac{\\partial in_{h_2}}{\\partial w_3} = \\frac{\\partial (w_3 * i_1 + w_4 * i_2 + b_1 * 1)}{\\partial w_3}$\n$\\frac{\\partial in_{h_2}}{\\partial w_3}$ $ = i_1$\n$\\frac{\\partial in_{h_2}}{\\partial w_3} $ $= 0.05$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_3}$\n$\\frac{\\partial E_{total}}{\\partial w_3} $ $= 0.041370323 * 0.240613417 * 0.05$\n$\\frac{\\partial E_{total}}{\\partial w_3} $ $= 0.000497713$\n  $w_4$:  $\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_4}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = w_6$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = 0.45$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}}$ $ = 0.138498562 * 0.45$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} $ $= 0.062324353$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$ $ = w_8$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} $ $= 0.55$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.038098237 * 0.55$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.02095403$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_2}} $$= 0.062324353 + (-0.02095403) = 0.041370323$\n ----------------------\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_2}}})}{\\partial in_{h_2}}$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} $ $= out_{h_2}(1 - out_{h_2})$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}}$ $ = 0.596884378(1 - 0.596884378) = 0.240613417$\n ----------------------\n$\\frac{\\partial in_{h_2}}{\\partial w_4} = \\frac{\\partial (w_3 * i_1 + w_4 * i_2 + b_1 * 1)}{\\partial w_3}$\n$\\frac{\\partial in_{h_2}}{\\partial w_4}$ $ = i_2$\n$\\frac{\\partial in_{h_2}}{\\partial w_4} $ $= 0.1$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_4}$\n$\\frac{\\partial E_{total}}{\\partial w_4} $ $= 0.041370323 * 0.240613417 * 0.1$\n$\\frac{\\partial E_{total}}{\\partial w_4} $ $= 0.000995425$\n Đến đây ta đã tính xong các đạo hàm từng phần theo các $w$. Áp dụng SGD để cập nhật các $w$ ta được (chọn $\\eta = 0.9$):\n$w_5^+ = w_5 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_5}$\n$w_5^+ = 0.4 - 0.9 * 0.082167041$\n$w_5^+ = 0.326049663$\n-------------------------\n $w_6^+ = w_6 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_6}$\n$w_6^+ = 0.45 - 0.9 * 0.082667628$\n$w_6^+ = 0.375599135$\n-------------------------\n $w_7^+ = w_7 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_7}$\n$w_7^+ = 0.5 - 0.9 * (-0.022602541)$\n$w_7^+ = 0.520342287$\n-------------------------\n $w_8^+ = w_8 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_8}$\n$w_8^+ = 0.55 - 0.9 * (-0.022740242)$\n$w_8^+ = 0.570466218$\n-------------------------\n $w_1^+ = w_1 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_1}$\n$w_1^+ = 0.15 - 0.9 * 0.000438568$\n$w_1^+ = 0.149605289$\n-------------------------\n $w_2^+ = w_2 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_2}$\n$w_2^+ = 0.2 - 0.9 * 0.0080877135$\n$w_2^+ = 0.192721058$\n-------------------------\n $w_3^+ = w_3 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_3}$\n$w_3^+ = 0.25 - 0.9 * 0.000497713$\n$w_3^+ = 0.249552058$\n-------------------------\n $w_4^+ = w_4 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_4}$\n$w_4^+ = 0.3 - 0.9 * 0.000995425$\n$w_4^+ = 0.299104118$\n-------------------------\n Phù, như vậy là chúng ta đã cập nhật xong giá trị mới cho các trọng số $w$. Đây là những phép toán xảy ra trong mỗi lần cập nhật khi training model. Hi vọng, thông qua ví dụ trong bài này, các bạn đã có thể hiểu rõ hơn bản chất của mạng NN. Hẹn gặp lại các bạn trong các bài tiếp theo!\n3. Tham khảo\n Mattmazur Dive into Deep Learning Wikipedia  ","permalink":"https://tiensu.github.io/blog/21_neural_network_fundamentals_2/","tags":["Deep Learning","Neural Network"],"title":"Neural Network cơ bản (Phần 2)"},{"categories":["Deep Learning","Neural Network"],"contents":"Trong bài này chúng ta sẽ cùng nhau tìm hiểu lys thuyết cơ bản về mạng thần kinh nhân tạo (neural network):\n Cấu trúc của neural network. Thuật toán lan truyền (propagation) và lan truyền ngược (backpropagation).  Những kiến thức trong bài này sẽ là tiền đề để các bạn tiến xa hơn trong thế giới của Deep Learning.\n1. Neural Network là gì?\nNeural Networks là các khối (blocks) để xây dựng lên các hệ thống Deep Learning. Chúng ta sẽ bắt đầu với việc xem xét ở mức \u0026quot;high-level\u0026quot; của Neural Network, bao gồm cả mối liên hệ của chúng với não bộ của con người.\nTrong thực tế, có rất nhiều những công việc rất khó để có thể thực hiện tự động hóa bởi máy móc nhưng lại rất dễ dàng đối với các loài động vật (bao gồm cả con người). Những công việc đó thường liên quan đến việc nhận diện, phân loại đối tượng.\nVí dụ:\n Con chó của gia đình bạn có thể phân biệt được người quen (người trong gia đình bạn) và người lạ (không phải trong gia đình bạn)? Một đứa trẻ có thể nhận biết được sự khác nhau giữa xe oto con và xe oto tải.  Tại sao con chó và đứa trẻ có thể làm được những việc đó?\nCâu trả lời nằm ở cấu tạo bên trong não bộ của chúng. Não bộ của cả 2 đều chứa một mạng thần kinh sinh học kết nối đến hệ thần kinh trung tâm. Mạng này được tạo ra bởi rất nhiều các neurons kết nối với nhau.\nTừ neural là dạng tính từ của neuron, và network ngầm chỉ kiến trúc \u0026quot;graph\u0026quot; của hệ thần kinh. Do vậy, một Artificial Neural Network (ANN) là một hệ thống tính toán, cố gắng mô phỏng (bắt chước) mạng thần kinh sinh học của các loài động vật. ANN là một graph có định hướng, nó bao gồm các nodes và các connections (kết nối giữa các nodes). Mỗi node thực hiện một tính toán đơn giản nào đó, mỗi connection mang một tín hiệu từ node này đến node khác. Những tín hiệu này đi kèm theo một trọng số (weight) chỉ ra mức độ khuyếch đại hoặc giảm bớt cường độ tín hiệu đó. Giá trị weight càng lớn chứng tỏ tín hiệu đi kèm càng quan trọng đối với kết quả đầu ra và ngược lại.\nHình dưới đây là một ANN đơn giản, bao gồm một lớp input ở đầu, 2 lớp ở giữa (hidden layers) và một lớp output ở cuối. Mỗi connection mang theo một tín hiệu xuyên qua hai hidden layers. Kết quả cuối cùng được tính toán tại lớp output.\n 2. Artificial Models\nHãy xem thử một ANN cơ bản như hình bên dưới:\n ANN này thực hiện tính tổng có trọng số ($w_i$) của các input vectors($x_i$). Trong thực tế, các input vectors có thể là pixcel của images, hay các rows của một dataset dạng tabular.\nMỗi $x_i$ kết nối với một neuron thông qua vector trọng số $w_i$.\nDiễn giải bằng công thức toán học thì output của ANN này sẽ là:\n $y = f(w_1x_1 + w_2x_2 + ... + w_nx_n)$ $y = f(\\sum_{i=1}^n w_ix_i)$ $y = f(net)$. Với net = $\\sum_{i=1}^n w_ix_i$  Nói chung, dù diễn đạt theo cách nào đi nữa thì ý tưởng chung vẫn là áp dụng hàm activate (f) vào tổng có trọng số của các input vectors.\n3. Activation Functions\n a) Step function  Hàm activation đơn giản nhất có lẽ là Step function. Hàm này được sử dụng bởi thuật toán Perceptron (sẽ đề cập ở phần sau).\n Hàm này luôn nhận giá trị 1 nếu $\\sum_{i=1}^n$ $w_i$$x_i$ \u0026gt;= 0 và nhận giá trị 0 trong trường hợp còn lại.\n Vấn đề của step function là nó giá trị của nó không có sự khác biệt khi net \u0026gt;=0 hoặc net \u0026lt; 0. Điều này có thể dẫn đến một số vấn đề khi huấn luyện neural network.\n b) Sigmoid function   y = tf.nn.sigmoid(x) d2l.plot(x.numpy(), y.numpy(), \u0026#39;x\u0026#39;, \u0026#39;sigmoid(x)\u0026#39;, figsize=(5, 2.5))  So với step function, sigmoid function có một số ưu điểm sau:\n Giá trị của nó liên tục và phân biệt nhau tại một nơi. Đồ thị của nó đối xứng qua trục y.  Tuy nhiên, có 2 nhược điểm lớn nhất của sigmoid function là:\n Output của nó không tập trung quanh điểm gốc tọa độ. Càng xa gốc tọa độ, giá trị của nó tiệm cận với giá trị bão hòa. Điề u này vô tình triệt tiêu gradient, vì delta của gradient vô cùng nhỏ.  Đạo hàm của sigmoid function như sau:\n with tf.GradientTape() as t: y = tf.nn.sigmoid(x) d2l.plot(x.numpy(), t.gradient(y, x).numpy(), \u0026#39;x\u0026#39;, \u0026#39;grad of sigmoid\u0026#39;, figsize=(5, 2.5))   c) Tanh function  Hàm này giải quyết được nhược điểm thứ nhất của sigmoid function.\n y = tf.nn.tanh(x) d2l.plot(x.numpy(), y.numpy(), \u0026#39;x\u0026#39;, \u0026#39;tanh(x)\u0026#39;, figsize=(5, 2.5))   d) ReLU (Rectified Linear Unit) funtion   Hàm này nhận giá trị 0 khi inputs \u0026lt; 0, nhưng sẽ tăng tuyến tính khi inputs \u0026gt;= 0.\nThực tế chứng minh, ReLU function hoạt động tốt hơn hẳn so với các hàm kể trên. Bắt đầu từ năm 2015, nó được sử dụng thường xuyên trong Deep Learning.\nx = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32) y = tf.nn.relu(x) d2l.plot(x.numpy(), y.numpy(), \u0026#39;x\u0026#39;, \u0026#39;relu(x)\u0026#39;, figsize=(5, 2.5))  ReLU function vẫn có nhược điểm. Khi inputs \u0026lt; 0, nó nhận giá trị 0. Như vậy thì không thể tính được gradient tại những điể m đó. Thực tế thì cũng hiếm khi có trường hợp nào mà inputs lại có giá trị \u0026lt; 0. Tuy nhiên, để giải quyết triệt để vấn đề thì lại sinh ra một hàm mới:\n e) Leaky ReLU function   Đây là một biến thể của ReLU funtion, nó nhận một giá trị khác 0 (thường rất nhỏ) khi inputs \u0026lt; 0. Giá trị của $\\alpha$ rất nhỏ và được cập nhật trong quá trình huấn luyện neural network.\n  f) ELU (Exponential Linear Units) function   Khác với ReLU function, giá trị của $\\alpha$ trong ELU function được cố định từ đầu (lúc xây đựng kiến trúc mạng). Giá trị thông thường của $\\alpha$ là 1.\n Nên sử dụng activation function nào?\nViệc có nhiều hơn 1 activation function đôi khi làm cho bạn bối rối khi lựa chọn sử dụng cái nào, không sử dụng cái nào?\nLời khuyên của mình như sau:\n Bắt đầu với ReLU để đặt được một baseline accuracy. (Hầu hết các public papers đều làm như vậy) Thử chuyển qua sử dụng các biến thể của ReLU: Leaky ReLU, ELU.  Trong các dự án thực tế thì mình thường làm theo các bước:\n Sử dụng ReLU Tune các hyper-parameters khác: architecture, learning rate, regularization strength, ... Ghi lại các giá trị accuracy. Một khi đã tương đối thoả mãn về accuracy, chuyển qua ELU. Độ chính xác thường sẽ tăng khoảng 1-5% tùy thuộc dataset.  Cách này chỉ là kinh nghiệm cá nhân của mình, và không có gì đảm bảo đúng trong mọi trường hợp. Bạn có thể tham khảo hoặc không. Hãy luôn nhớ thử-sai mọi khả năng có thể cho bài toán của bạn.\n4. Feedfoward Network Architecture\nKiến trúc ANN thì có rất nhiều, nhưng phổ biến nhất là dạng feedfoward network.\n Trong kiến trúc này, một connection giữa 2 nodes chỉ được phép đi theo chiều từ layer $i$ tới layer $i+1$ (vì thế mà có tên feedfoward). Không có chiều từ layer $i+1$ đến layer $i$ hoặc bất kỳ chiều nào khác. Khi có thêm chiều từ layer $i+1$ đến layer $i$ (feedback connection) thì ta được kiến trúc RNN (Recurrent Neural Network). Feedfoward network được sử dụng chủ yếu trong các bài toán về Computer Vision (mạng CNN là một ví dụ điển hình) , trong khi feedback network lại được sử dụng chủ yếu trong các bài toán về Natural Language Processing.\nANN được sử dụng cho cả 3 dạng bài toán: supervised, unsupervised, and semi-supervised. Một số ví dụ điển hình là classification, regression, clustering, vector quantization, pattern association, ...\nTrong bài tiếp theo, mình sẽ minh họa cách thức cập nhật trong số của mạng ANN thông qua một ví dụ rất dễ hiểu. Mời các bạn đón đọc.\n3. Tham khảo\n Pyimagesearch Dive into Deep Learning Wikipedia  ","permalink":"https://tiensu.github.io/blog/20_neural_network_fundamentals_1/","tags":["Deep Learning","Neural Network"],"title":"Neural Network cơ bản (Phần 1)"},{"categories":["Deep Learning"],"contents":"Bài viết này nhằm mục đích tổng hợp, tóm tắt lại các thuật toán của Deep Learning, giúp bạn đọc có cái nhìn toàn cảnh và hiểu rõ hơn về Deep Learning.\n1. Deep Learning (DL) là gì?\nTheo Wikipedia: \u0026ldquo;Deep learning (also known as deep structured learning or differential programming) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised\u0026rdquo;.\nHiểu theo một cách khác thì DL là một tập hợp các thuật toán mô phỏng lại cách thức hoạt động của bộ não của con người trong việc phân tích dữ liệu, nhằm tạo ra các models cái mà được sử dụng cho việc đưa ra quyết định dựa trên dữ liệu đầu vào. Về bản chất, DL bao gồm nhiều layers, mỗi layer có thể coi là một mạng Neural Network (NN).\nGiống như não bộ của con người, NN có chứa các neurons. Mỗi neurons nhận các tín hiệu ở đầu vào, nhân chúng với các trọng số (weights), tổng hợp chúng lại, và sau cùng là áp dụng các hàm kích hoạt (thường là non-linear) lên chúng. Các neurons được sắp xếp thành các layers liên tiếp nhau (stack).\nCác kiến trúc mạng DL đều sử dụng thuật toán Backpropagation để tính toán và cập nhật các trọng số của nó thông qua quá trình huấn luyện. Về mặt ý tưởng, dữ liệu được đưa vào mạng DL, sinh ra output, so sánh output với giá trị thực tế (sử dụng loss function) rồi điều chỉnh trọng số theo kết quả so sánh đó.\nViệc điều chỉnh trọng số là một quá trình tối ưu, thường sử dụng thuật toán SGD. Ngoài SGD, một số thuật toán khác cũng hay được sử dụng. Đó là Adam, Radam, SRMprop.\n2. Các thuật toán DL\n 2.1 Convolution Neural Network (CNN)\nCNN sử dụng phép toán tích chập để tạo liên kết giữa các layers trong mạng NN. Mỗi neuron, thay vì kết nối đến tất cả các neurons khác thì sẽ chỉ kết nối đến một vài neurons đại diện. Điều này giúp cho CNN học được các mối liên hệ không gian của dữ liệu tốt hơn.\nĐó là lý do mà vì sao CNN lại được sử dụng rất phổ biến trong các bài toán về Computer Vision như phân loại hình ảnh, phát hiện đối tượng trong ảnh/video, \u0026hellip;\nMột vài kiến trúc mạng CNN kinh điển có thể kể đến như: VGG, ResNet, MobiNet, InceptionNet, Yolo, SSD, \u0026hellip;\n 2.2 Recurrent Neural Network (RNN)\nRNN là thuật toán rất phù hợp với các loại dữ liệu có mối liên hệ về thời gian như time serial forecasting hay trong các bài toán về xử lý ngôn ngữ tự nhiên (NLP), bởi vì một phần output của nó ở thời điể m này được đưa trở lại thành input ở thời điểm tiếp theo. Do cách thức làm việc đặc biệt như vậy mà nó có có năng ghi nhớ được thông tin trong quá khứ và sử dụng những thông tin đó vào việc dự đoán kết quả.\nMột số kiến trúc mạng kinh điển của RNN có thể kể đến như: GRU, LSTM, \u0026hellip;\n 2.3 AutoEncoders\nAutoEncoders là kiến trúc mạng bao gồm 2 thành phần: Encoder và Decoder. Encoder nhận input và mã hóa nó thành các vector trong một không gian có số chiều ít hơn. Decoder sử dụng các vertors đó để giải mã (xây dựng lại) nó thành dữ liệu ban đầu.\nAutoEncoders sử dụng chủ yếu trong các bài toán về giảm chiều dữ liệu, nén dữ liệu, \u0026hellip;\n 2.4 Generative Adversarial Networks (GAN)\nGAN cũng bao gồm 2 thành phần: Generator và Discriminator. Generator sinh ra dữ liệu giả và Discriminator sẽ cố gắng phân biệt dữ liệu mới sinh ra đó là giả hay thật. Hai thành phần này được huấn luyện cùng với nhau, chúng sẽ cạnh tranh nhau để cuối cùng tạo ra được model GAN tốt nhất.\nGAN được dùng để sinh ra dữ liệu mới từ dữ liệu ban đầu, ví dụ: sinh ra các mẫu thời trang mới, các mẫu nhân vật hoạt hình mới, \u0026hellip;\n 2.5 Transformer\nTransformer là kiến trúc mạng mới được phát triển và được sử dụng rộng rãi gần đây trong các bài toán NLP khi nó chứng minh được sự vượt trội của nó so với RNN. Có được này là nhờ vào một cơ chế, gọi là Attention, tức là chỉ tập trung và một số vị trí cụ thể thay vì toàn bộ vị trí trong dữ liệu.\nTransformer cũng bao gồm một số bộ encoders và một số bộ decoders được đặt cạnh nhau (stacked) cùng với các lớp attentions.\n Hiện nay, BERT và GPT-3 là 2 pre-trained model của transformer được sử dụng rất nhiều trong lĩnh vực NLP.\n2.6 Graph Neural Network (GNN)\nDL nói chung thường làm việc với dữ liệu có cấu trúc. Tuy nhiên, trong thực tế có rất nhiều dữ liệu ở dạng phi cấu trúc (unstructed data) và được sắp xếp dưới dạng graph. Ví dụ như là mạng xã hội, cấu tạo phân tử trong hóa học, \u0026hellip;\nGNN chính là mạng NN mô hình hóa graph data. Chúng sẽ nhận diện các mối liên kết của các nodes trong graph và output ra các vector đặc trưng của dữ liệu đó, giống như embedding. Output này được sử dụng làm input cho các NN khác.\n3. Các thuật toán DL trong các bài toán Computer Vision \u0026amp; Natural Language Processing\n3.1 DL trong CV\n  Image Classification  Đây là bài toán nhận diện đối tượng trong ảnh thuộc về class nào. Số lượng class có thể là 2 (binary) hoặc nhiều hơn (multiple).\nCác kiến trúc mạng kinh điển như VGG (VGG16, VGG19), ResNet, InceptionNet, MobiNet, AlexNet \u0026hellip; giải quyết rất tốt bài toán này.\n Object Detection  Đây là bài toán mở rộng của image classification vì không chỉ phân loại mà còn phải định vị được đối tượng trong ảnh.\nCác kiến trúc NN giải quyết vấn đề này bao gồm: Họ R-CNN (R-CNN, Fast R-CNN, Faster R-CNN), Yolo (YOLOv2, YOLOv3, YOLOv4), SSD, EfficientNet, \u0026hellip;\n  Semantic Segmentation  Nếu như Image Classification là bài toán gán nhãn cho toàn bộ bức ảnh thì Semantic Segmentation có thể coi là bài toán gán nhãn cho từng pixcel trong ảnh đó. Tất cả các pixel thuộc cùng một class (hay các objects thuộc cùng một class) được coi như là một thực thể (thể hiện bằng màu sắc giống nhau).\n Một số kiến trúc mạng như Full Connected Networks FCN, UNET, \u0026hellip; giải quyết tốt bài toán này.\n Instance Segmentation  Cũng giống như Semantic Segmentation là gán nhãn cho từng pixel trong ảnh, tuy nhiên Instance Segmentation coi mỗi đối tượng (cùng hoặc khác class) là các thực thể riêng biệt (thể hiện bằng màu sắc khác nhau).\n Kiến trúc Mask_RCNN nổi tiếng nhất cho viêc giải quyết bài toán này.\n Face Recognition  Đây là bài toán nhận diện khuôn mặt của người trong video/ảnh. Nó bao gồm 2 công đoạn: - Đầu tiên là phát hiện vùng chứa khuôn mặt (bài toán Object Detection), - Sau đó nhận diện xem khuôn mặt đó là của ai (bài toán classification)\nMột số thuật toán nổi tiếng giải quyết cho công đoạn 2 là: FaceNet, VGGFace, MTCNN, \u0026hellip; Còn đối với công đoạn 1 có thể sử dụng OpenCV.\n  Optical Character Recogniton (OCR)  Đây là bài toán nhận diện ký tự trong hình ảnh. Tương tự như bài toán Face Detection, nó cũng bao gồm 2 công đoạn: - Phát hiện vùng chứa ký tự trong ảnh (bài toán object detection) - Nhận diện từng ký tự trong vùng chứa đó.\nMột số thuật toán nổi bật là: - Công đoạn 1: Craft, Yolo, \u0026hellip; - Công đoạn 2: CRNN, \u0026hellip;\nMột số thư viện hỗ trợ bài toán OCR: Tesseract, EasyOCR\n  Humand Pose  Humand Pose hay Pose Estimation là bài toán xác định vị trí của các khớp nối của con người trong ảnh/video.\n PostNet là thuật toán nối tiếng cho bài toán này.\n3.2 DL trong NPL\n Các thuật toán Word Embedding  Word Embedding là quá trình biến đổi các từ thành các vector đại diện, làm đầu vào cho các DL model. Việc biến đổi này có tính đến hoàn cảnh và ngữ nghĩa của từ đó trong câu. Một số thuật toán phổ biến:\na. Word2Vec: Nó hoạt động theo theo 1 trong 2 cách, dự đoán từ dựa vào những tù xung quanh của nó (CBOW) hoăc dự đoán các từ xung quanh của 1 từ (Skip-Gram). Các từ được đưa vào Word2Vec theo dạng One-hot-encoding.\n b. Glove: Là một mô hình khác mở rộng ý tưởng của Word2Vec bằng cách kết hợp nó với các kỹ thuật phân tích thừa số ma trận.\n c. Contextual Word Embeddings: Sử dụng 2 layers bi-directional LSTM để embedding các từ, cho phép tận dụng sự phụ thuộc của nó với các từ trước đó.\nd. Transformer: Đây là phương pháp tiên tiến nhất hiện nay, cho phép tập trung vào một vài vị trí cụ thể của từ trong câu thay vì toàn bộ như khi sử dụng LSTM.\n  Sequence Modeling  Mô hình này giải quyết hầu hết các bài toán trong NLP như Machine Translation, Speech Recognition, Autocompletion và Sentiment Classification. Nó có khả năng xử lý cả một chuỗi đầu vào thay vì từng từ một.\n Còn rất nhiều kiến trúc, thuật toán, model nữa mà trong bài này chưa thể kể hết được. Nhưng hi vọng qua đây, các bạn cũng có được cái nhìn tổng quát về các thuật toán, model, kiến trúc phổ biến trong các bài toán DL.\nTrong các bài viết tiếp theo đi chi tiết vào một số thụât toán với các ứng dụng cụ thể. Mời các bạn đón đọc!\n4. Tham khảo\n Dive Into Deep Learning AI Summer  ","permalink":"https://tiensu.github.io/blog/19_deep_learning_algorithms_summary/","tags":["Deep Learning"],"title":"Tổng hợp các thuật toán Deep Learning"},{"categories":["Machine Learning","Data Science"],"contents":"Bạn thường nghe nói Data Scientist là nghê sexy nhất thế kỷ 21, với mức lương cao ngất ngưởng, tạo ra những sản phầm có tầm ảnh hưởng lớn, được mọi người ngưỡng mộ, blabla. Và thế là bạn quyết định chuyển hướng sang học làm data scientist. Bạn lao vào học toán (đại số tuyến tính, xác suất thống kê, đạo hàm, tích phân, \u0026hellip;), học lập trình (python, R, \u0026hellip;), học các thuật toán ML, cách sử dụng các thư viện ML. Thậm chí có bạn còn chơi lớn, học luôn cao học về nghành này vì \u0026ldquo;hình như\u0026rdquo; ngành này yêu cầu phải có trình độ cao học trở lên (bản thân mình chính là 1 ví dụ của trường hợp này, :D). OK, mình không có ý kiến gì về lựa chọn của bạn cả. Trong bài viết này, mình chỉ muốn chia sẻ một số điều mà mình cảm nhận được sau một thời gian làm trong nghành với danh xưng data scientist này. Những điều này là sự khác biệt giữa lý thuyết các bạn học được và thực tế công việc mà các bạn sẽ trải qua. Bạn sẽ tìm được cho mình câu trả lời cho câu hỏi \u0026ldquo;Thế nào là một data scientist giỏi? Và thế nào là một data scientist xuất sắc?\u0026rdquo;.\n1. \u0026ldquo;Tóm lại, câu chuyện bạn muốn kể là gì?\u0026quot;\nMột trong những câu hỏi tôi thường nghe sau mỗi lần kết thúc buổi seminar của CEO (hoặc một người nào đó ở vị trí tương đương) là:\n\u0026ldquo;Tóm lại, câu chuyện bạn muốn kể là gì?\u0026quot;\nThú thực, lần đầu tiên khi nghe câu hỏi này, tôi có phần bối rối. Tôi không hiểu tại sao họ lại nhấn mạnh vào \u0026ldquo;câu chuyện\u0026rdquo;? Tại sao họ không hỏi:\n Những gì tôi đã trình bày trong slide. Kết quả tôi đạt được là gì. Tôi đã làm như thế nào. \u0026hellip;  Trước khi thực sự hiểu được sự quan trọng của kỹ năng kể chuyện (telling story), tôi đã trải qua khá nhiều sai lầm \u0026hellip;\nHoặc là stackeholders (thường là những người non-techical) không hiểu những gì bạn đang nói, đang trình bày, hoặc là nội dung bạn đang truyền tải chưa đủ để thuyết phục họ, thúc đẩy họ đưa ra quyết định hay hành động cụ thể nào (take actions) \u0026hellip;\nVà tôi quyết định cải thiện kỹ năng kể chuyện của mình \u0026hellip;\nKhi tôi đã đạt được sự tiến bộ nhất đinh, rất nhiều thứ đã thay đổi \u0026hellip;\nStackeholders đã bắt đầu hiểu những gì mà tôi đang diễn giải. Và họ đã có những hành động cụ thể \u0026hellip;\nNếu bạn muốn trở thành môt data scientist giỏi, hãy tập trung vào kiến thức kỹ thuật.\nCòn nếu bạn muốn trở thành một data scientist xuất sắc, hãy tập trung vào kỹ năng kể chuyện.\nVậy \u0026hellip; làm thế nào để học kỹ năng kể chuyện? Lời khuyên của tôi là hãy học từ Vox. Họ là những bậc thầy về kỹ năng này, họ luôn luôn có thể giải thích bất kì vấn đề phức tạp nào một cách rất đơn giản và dễ hiểu. Bạn có thể xem thử video bên dưới của họ. Hãy quan sát cách họ giải thích các hiện tượng và vấn đề xã hội theo cách kể chuyện trực quan nhất để ai ai cũng có thể hiểu được.\n  Tóm tắt lại một vài lời khuyên rất hay cho chúng ta để nâng cao kỹ năng kể chuyện. Những điều này không chỉ đúng trong phạm vi data science mà nó còn đúng cho tất cả các lĩnh vực khác.\n𝟏) Lãnh đạo có ít thời gian hơn chúng ta:\nVới vai trò lãnh đạo cấp cao, họ phải xử lý nhiều việc quan trọng hơn.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Hãy đi thẳng vào vấn đề chính Trình bày ngắn gọn, có cấu trúc rõ ràng Trình bày xong, hãy đưa ra lời kiến nghị hành động với họ.  𝟐) Lãnh đạo dễ mất kiên nhẫn\nLãnh đạo cấp cao có quá nhiều việc cần phải xử lý một lúc và vì vậy họ dễ mất kiên nhẫn.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Cho họ thông tin tổng quát là bạn sẽ trình bày trong bao lâu. Nếu bạn trình bày trong 30 phút, thì bạn hãy rút ngắn thời gian nói xuống 15 phút, 15 còn lại để hỏi đáp. Đó là nguyên tắc 50:50.  𝟑) Chào đón sự cắt ngang\nNhững khán giả cấp cao thường xuyên ngắt ngang giữa chừng những bài thuyết trình.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Chuẩn bị bài với cấu trúc rõ ràng để khi “được chen ngang”, bạn nắm rõ là đang nói phần nào.  𝟒) Luôn có các slide phụ lục Nhà lãnh đạo cấp cao cần nghe một bức tranh tổng thể.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Thuyết trình tổng thể về ý tưởng theo một cấu trúc rõ ràng có chủ đích. Chuẩn bị sẵn các số liệu và phần “phụ lục”. Khi trình bày xong, chia sẻ thêm các số liệu này, hoặc mở lên khi có câu hỏi.  2. Data sạch ư? Mơ đi nhé!\nBạn tự hào rằng mình đã từng tham gia rất nhiều cuộc thi trên Kaggle, thâm chí còn leo lên top 10, top 5. Hãy quên điều đó đi, vì các dự án trong thực tế không \u0026ldquo;màu hồng\u0026rdquo; như vậy đâu. Trên Kaggle, bạn đuợc cung cấp sẵn bộ dữ liệu tương đối đầy đủ và sạch sẽ, còn trên thực tế thì hầu như bạn chỉ được đưa cho một mớ bòng bong hỗ độn. Khách hàng bảo, \u0026ldquo;đấy, bọn tao chỉ có như này, mày dùng như nào thì dùng\u0026rdquo;. Trường hợp xấu hơn, chúng ta bắt đầu dự án chả có tý data nào cả. Xấu hơn nữa là chúng ta không biết tìm kiếm data ở đâu, tìm kiếm như thế nào \u0026hellip; Quả thực rất khó khăn.\nThế mới nói, data collection và data integrity là hai trong số những bước quan trọng nhất của bất kỳ dự án data science nào. Garbage in, garbage out (đầu vào là rác thì đầu ra cũng là rác), câu nói này chắc ai làm trong lĩnh vực này đều đã từng nghe. Nhưng có lẽ không phải ai cũng thực sự hiểu được ý nghĩa của nó.\n Data collection: tức là thu thập dữ liệu. Để thu thập được dữ liệu thì cần phải hiểu rõ yêu cầu bài toán là gì, dữ liệu nào đã có, dữ liệu nào còn thiếu, cấu trúc của dữ liệu ra sao, số lượng tối thiểu như thế nào, cần chuẩn bị những công cụ dụng cụ gì để lấy dữ liệu (ví dụ, phải có camera để quay video, chụp ảnh)\u0026hellip; Ngoài ra, không phải lúc nào cũng có thể thu thập đầy đủ dữ liệu ngay từ ban đầu. Dữ liệu có thể đến trong suốt quá trình phát triển dự án hoặc vận hành sản phẩm. Vì vậy, phải có chiển lựợc quản lý dữ liệu hợp lý để dễ dàng cho viêc sử dụng, báo cáo thống kê về sau. Data integrity: tức là tính toàn vẹn của dữ liệu. Bạn cần phải kiểm tra kỹ lưỡng xem dữ liệu thu được có ý nghĩa hay không, có đầy đủ hay không bằng cách tự đặt câu hỏi và trả lời, hoặc nhờ sự trợ giúp từ các bên có liên quan và hiểu biết.  Nếu hai bước này làm không tốt thì cho dù bạn có áp dung bao nhiêu kỹ thuật cao siêu của data clearning, EDA, và tuning model thì kết qủa cũng sẽ không thể tốt đẹp như bạn mong đơi.\n3. Soft skills \u0026gt; Technical skills\nBạn đã từng tự đặt ra câu hỏi:\nNhững kỹ năng gì cần học để có thể làm việc được trong lĩnh vực data science?\nTheo ý kiến cá nhân của tôi, technical skills (lập trình, thống kê, \u0026hellip;) là kỹ năng bắt buộc và nên được ưu tiên học trước tiên khi bắt đầu bước chân vào thế giới data science.\nMột khi đã có đủ tự tin về technical skills của mình, chúng ta nên tập trung vào xây dựng và cải thiện soft skills (giao tiếp, làm việc nhóm, kể chuyện, \u0026hellip;). Khi học, bạn thường học một mình, rất ít khi giao tiếp hoặc làm cùng người khác. Nhưng dự án trong thực tế thường rất lớn, gồm nhiều phần, nhiều công đoạn. Và bạn thường chỉ làm một trong số những công đoạn đó. Nếu bạn là team-leader hay PM, bạn phải giao tiếp với các phòng ban bộ phận khác để lấy thông tin, trình bày kết quả với lãnh đạo, phân chia nhiệm vụ giữa các thành viên, \u0026hellip; Nếu bạn là nhân viên bình thường thì bạn vẫn cần phải trao đổi với các thành viên khác trong dự án, trình bày giải pháp kỹ thuật của mình. Rất may là những soft skills này khá giống với những dự án thuộc lĩnh vực khác, nên nếu bạn là người đã có kinh nghiệm làm việc ở những lĩnh vực khác chuyển sang data science thì bạn không cần quá lo lắng. Ngược lại, đối với các bạn sinh viên mới ra trường thì cần đặc biệt lưu ý hơn về việc phát triển soft skills của mình.\nCó một câu nói khá nổi tiếng của W.Edwards Deming:\n\u0026ldquo;Without data you\u0026rsquo;re just another person with an opinion\u0026rdquo;\nTạm dich: Nếu không có số liệu chứng minh cụ thể, thì tất cả chỉ là ý kiến, suy đoán của bạn mà thôi. (giống như trong các vụ điều tra, phá án. Lập luận, suy đoán có thuyết phục, hợp lý đi bao nhiêu chăng nữa mà không có bằng chứng cụ thể thì cũng vô nghĩa, không thể bắt được hung thủ).\nÁp dụng vào lĩnh vực data science này thì, có data chỉ là bước đầu tiên. Điều quan trọng là làm thế nào để sử dụng data đó để sinh ra những phán quyết, hành động (business decisions) mà mang lại lợi ích cho tổ chức.\nThử thay đổi câu nói trên một chút cho phù hợp với ngữ cảnh:\n\u0026ldquo;Without storytelling skills you\u0026rsquo;re just another persion with data\u0026rdquo;\nBạn có thể phân tích dữ liệu một cách xuất sắc.\nBạn có thể tạo ra một model tốt nhất trên thế giới.\nBạn thuộc làu làu cuốn Code Complete và viết code đẹp như trong phim.\nNhưng nếu bạn không thể sử dụng những kết quả đó để thuyết phục stackeholders để đưa ra business decisions, thì những nỗ lực của bạn vẫn chỉ nằm trên những slides PowerPoints mà thôi.\nHơi buồn, nhưng đó là sự thật!\n4. \u0026ldquo;What pattern observed behind?\u0026quot;\nĐối với hầu hết các bài toán, trừ khi bạn đang làm việc trong một công ty lớn về công nghệ (cutting-edge technology company), những models phức tạp thường không phải lựa chọn đầu tiên để phân tích và dự đoán.\nBạn cần phải thực sự hiểu những gì diễn ra đằng sau model và kết quả của chúng ta để thuyết phục các stackeholders, bởi vì họ có thể hỏi bạn:\n Tại sao lại không phát hiện ra lỗi này? Tại sao lỗi này lại bị nhầm sang lỗi kia? Tại sao độ chính xác lại thấp như vậy? Nếu xảy ra trường hợp này thì phải làm như thế nào? \u0026hellip;  Stackeholders không thể \u0026ldquo;nhắm mắt\u0026rdquo; cho sử dụng một model trong các sản phẩm thực tế khi mà họ chưa hiểu rõ (blackbox model), vì như thế nguy cơ thất thoát tiền bạc là rất lớn.\nĐây có lẽ cũng là một trong những lý do mà các models đơn giản như decision tree, logistic regression hay kNN vần còn còn được sử dụng khá nhiều trong các bài toán công nghiệp.\n5. Luôn luôn nhìn bức tranh tổng quát\nĐây có lẽ là một trong những lỗi phổ biến nhất khi bắt đầu với data science. Mọi người thường quá chú trọng vào việc làm sao train được model tốt, viết code cho đẹp mà không để ý đến việc model sẽ chạy trong thực tế như thế nào, tốc độ đáp ứng ra sao, \u0026hellip;\nNói chung ngay từ lúc bắt đầu dự án data science, bạn nên nhìn một cách tổng quát bài toán, từ việc hiểu rõ yêu cầu (cả function và non-function), chuẩn bị dữ liệu, huấn luyện model, triển khai model trong thực tế và cập nhật model. Có được cái nhìn rõ ràng về bức tranh lớn, mạch suy nghĩ và làm việc của bạn sẽ được xuyên suốt từ đầu đến cuối và không bị \u0026ldquo;khớp\u0026rdquo; khi chuyển tiếp giữa các giai đoạn với nhau.\n 6. Kết luận\nTrong bất kỳ lình vực nào, lý thuyết và thực tế luôn luôn có sự khác biệt. Sự khác biệt này càng rõ hơn trong phạm vi data science nói riêng. Hiễu được những điều này chính là bạn đã có được cái nhìn về bức tranh lớn của data science.\nHi vọng những điều chia sẽ trong bài viết này sẽ giúp ích cho các bạn trên con đường chinh phục sexy job. Hẹn gặp lại trong những bài viết sau!\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/18_data_scientist_theory_and_real/","tags":["Machine Learning","Data Science"],"title":"Nghề Data Scientis - Lý thuyết và thực tế - Sự khác biêt"},{"categories":["Machine Learning","XGBoost"],"contents":"Trong quá trình training, XGBoost thường xuyên phải thực hiện công việc chọn lựa ngẫu nhiên tập dữ liệu con (subsamples) từ tập dữ liệu gốc ban đầu. Các kỹ thuật để làm việc này được gọi bằng cái tên Stochastic Gradient Boosting (SGB).\nTrong bài này chúng ta sẽ cùng tìm hiểu về SGB và tuning SGB để tìm ra kỹ thuật phù hợp với bài toán.\n1. Tuning Row Subsampling\nRow subsampling liên quan đến việc chọn ngẫu nhiên các samples từ tập train set. Trong XGBoost, giá trị của row subsampling được chỉ ra bởi tham số subsample. Giá trị mặc định là 1, nghĩa là sử dụng toàn bộ tập train set, không subsampling.\nTiếp tục sử dụng tập Otto dataset, chúng ta sẽ grid-search tham số subsample với các giá trị như sau: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0].\nCó 9 giá trị của subsample, mỗi model sẽ được đánh giá sử dụng 10-fold cross-validation. Như vậy, có 9x10=90 models cần phải trained.\nCode đầy đủ như sau:\n# XGBoost on Otto dataset, tune subsample from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] param_grid = dict(subsample=subsample) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(subsample, means, yerr=stds) pyplot.title(\u0026#34;XGBoost subsample vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;subsample\u0026#39;) pyplot.ylabel(\u0026#39;Accuracy\u0026#39;) pyplot.savefig(\u0026#39;subsample.png\u0026#39;) Kết quả chạy:\nFitting 10 folds for each of 9 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 3.3min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 12.3min finished Best: 0.999919 using {\u0026#39;subsample\u0026#39;: 0.2} 0.999887 (0.000126) with: {\u0026#39;subsample\u0026#39;: 0.1} 0.999919 (0.000081) with: {\u0026#39;subsample\u0026#39;: 0.2} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.3} 0.999919 (0.000108) with: {\u0026#39;subsample\u0026#39;: 0.4} 0.999919 (0.000108) with: {\u0026#39;subsample\u0026#39;: 0.5} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.6} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.7} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.8} 0.999887 (0.000103) with: {\u0026#39;subsample\u0026#39;: 1.0} Độ chính xác của model đạt được bằng 0.999919 tại điểm subsample = 0.2, hay subset của mỗi model = 30% train set.\nĐồ thị bên dưới thể hiện mối quan hệ giữa subsample và accuracy.\n 2. Tuning Column Subsampling by Tree\nChúng ta cũng có thể tạo ra một tập ngẫu nhiên các input features để sử dụng cho mỗi decision tree. Trong XGBoost, điều này được cấu hình thông qua tham số colsample_tree. Giá trị mặc định của nó là 1, tức là tất cả các input features đều được sử dụng cho mỗi tree.\nTa sẽ thử tune tham số này với tập giá trị như sau: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\nCode đầy đủ:\n# XGBoost on Otto dataset, tune colsample_bytree from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] param_grid = dict(colsample_bytree=colsample_bytree) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(colsample_bytree, means, yerr=stds) pyplot.title(\u0026#34;XGBoost colsample_bytree vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;colsample_bytree\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;colsample_bytree.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 9 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 3.0min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 10.6min finished Best: 0.999887 using {\u0026#39;colsample_bytree\u0026#39;: 1.0} 0.998578 (0.000484) with: {\u0026#39;colsample_bytree\u0026#39;: 0.1} 0.999855 (0.000152) with: {\u0026#39;colsample_bytree\u0026#39;: 0.2} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.3} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.4} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.5} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.6} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.7} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.8} 0.999887 (0.000103) with: {\u0026#39;colsample_bytree\u0026#39;: 1.0} Độ chính xác của XGBoost đạt được là 0.999887 tại colsample_bytree = 1.0. Điều này có nghĩa rằng trong trường hợp này, subsampling column không mang lại giá trị nào.\nĐồ thị thể hiện mối quan hệ giữa subsampling column và accuracy.\n 3. Tuning Column Subsampling by Split\nThay vì subsampling column cho mỗi tree, ta có thể subsampling column ở mức node (hay Split). Tức là tại mỗi node, ta sẽ subsampling column để tìm ra 1 tập ngẫu nhiên các input features để quyết định hướng đi tiếp theo. Đây chính là điểm khác biệt giữa Random Forest và Bagging meta-data mà ta đã đề cập đến trong bài 2 của chuỗi các bài viết về XGBoost.\nSubsampling column ở mức node được cấu hình thông qua tham số colsample_bylevel. Ta sẽ tiến hành tune tham số này với giá trị thay đổi từ 10% đến giá trị mặc định ban đầu của nó (100%).\nCode đầy đủ như bên dưới:\n# XGBoost on Otto dataset, tune colsample_bylevel from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() colsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] param_grid = dict(colsample_bylevel=colsample_bylevel) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(colsample_bylevel, means, yerr=stds) pyplot.title(\u0026#34;XGBoost colsample_bylevel vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;colsample_bylevel\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;colsample_bylevel.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 9 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 2.6min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 9.3min finished Best: 0.999919 using {\u0026#39;colsample_bylevel\u0026#39;: 0.3} 0.999903 (0.000129) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.1} 0.999903 (0.000079) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.2} 0.999919 (0.000081) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.3} 0.999903 (0.000107) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.4} 0.999887 (0.000103) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.5} 0.999903 (0.000107) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.6} 0.999903 (0.000107) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.7} 0.999871 (0.000121) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.8} 0.999887 (0.000103) with: {\u0026#39;colsample_bylevel\u0026#39;: 1.0} colsample_bylevel = 0.3 cho ta model với độ chính xác cao nhất, 0.999919.\nĐồ thị thể hiện mối quan hệ giữa colsample_bylevel và accuracy.\n 6. Kết luận\nNhư vậy là chúng ta đã biêt cách tuning các kỹ thuật subsample hay stochastic gradient boosting của XGBoost. Nếu có phần cứng đủ mạnh, các bạn có thể tune tất cả các tham số đồng thời với nhau. Khi đó, độ chính xác của model có thể tăng lên 1 chút. Nhưng cái giá phải trả là thời gian train sẽ rất lâu. :D\nĐây cũng là bài cuối cùng trong loạt bài viết về XGBoost model. Hi vọng qua những bài viết của mình, các bạn có thể hiểu hơn về XGBoost và tự tin hơn khi làm viêc với nó. Hẹn mọi người ở những chủ để tiếp theo! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/17_tuning_subsampling/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 14: Tuning Subsample"},{"categories":["Machine Learning","XGBoost"],"contents":"Một vấn đề còn tồn tại của XGBoost là khả năng học trên tập dữ liệu huấn luyện một cách rất nhanh chóng. Điều này đôi khi dễ dẫn đến hiện tượng Overfitting, mặc dù XGBoost đã sử dụng regularization. Một cách hiệu quả để điều khiển quá trình học của XGBoost là sử dụng learning_rate (hay eta).\nTrong bài này, chúng ta sẽ cùng nhau tune learning_rate, learning_rate kết hợp với số lượng trees để tìm ra giá trị tối ưu của hai tham số này.\n1. Tuning Learning_Rate\nChúng ta tiếp tục sử dụng Otto dataset trong bài này. Sử dụng giá trị mặc định của số lượng trees là 100, ta sẽ đánh giá sự phù hợp của mỗi giá trị learning_rate trong tập sau: [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nCó 6 giá trị của learning_rate, kết hợp với 10-fold cross-validation \u0026ndash;\u0026gt; Có 60 models được trained.\nCode tuning như sau:\n# XGBoost on Otto dataset, Tune learning_rate from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] param_grid = dict(learning_rate=learning_rate) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(learning_rate, means, yerr=stds) pyplot.title(\u0026#34;XGBoost learning_rate vs Log Loss\u0026#34;) pyplot.xlabel(\u0026#39;learning_rate\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;learning_rate.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 6 candidates, totalling 60 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 26 tasks | elapsed: 9.1min [Parallel(n_jobs=-1)]: Done 60 out of 60 | elapsed: 13.8min finished Best: 0.999887 using {\u0026#39;learning_rate\u0026#39;: 0.001} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.2} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.3} Giá trị learning_rate tối ưu tìm được là 0.001.\nĐồ thị bên dưới thể hiện mối qua hệ giữa learning_rate và độ chính xác của model.\n 2. Tuning Learning_Rate và số lượng decision tree\nNói chung, khi có nhiều trees được thêm vào XGBoost, những trees thêm vào sau nên sử dụng giá trị learning_rate nhỏ. Ta sẽ kiểm tra nhận định này thông qua quá trình tuning như sau:\n Số lượng trees (n_estimators) = [100, 200, 300, 400, 500] learning_rate = [0.0001, 0.001, 0.01, 0.1]  Có 5 giá trị của n_estimators và 4 giá trị của learning_rate, kết hợp với 10-fold cross-validation ta có 200 models cần train.\nCode đầy đủ như dưới đây:\n# XGBoost on Otto dataset, Tune learning_rate and n_estimators from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot import numpy # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() n_estimators = [100, 200, 300, 400, 500] learning_rate = [0.0001, 0.001, 0.01, 0.1] param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot results scores = numpy.array(means).reshape(len(learning_rate), len(n_estimators)) for i, value in enumerate(learning_rate): pyplot.plot(n_estimators, scores[i], label=\u0026#39;learning_rate: \u0026#39; + str(value)) pyplot.legend() pyplot.xlabel(\u0026#39;n_estimators\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;n_estimators_vs_learning_rate.png\u0026#39;) Sau khoảng 2 tiếng chờ đơi thì chúng ta cũng thu được kết quả:\nFitting 10 folds for each of 20 candidates, totalling 200 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 6.2min [Parallel(n_jobs=-1)]: Done 168 tasks | elapsed: 58.2min [Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 67.6min finished Best: 0.999887 using {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 100} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 100} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 200} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 300} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 400} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 500} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 100} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 200} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 300} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 400} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 500} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 100} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 200} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 300} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 400} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 500} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 100} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 200} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 300} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 400} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 500} Ta có thể thấy, kết quả tốt nhất của model đạt được tại learning_rate=0.001 và n_estimators=100. Tuy nhiên, kết quả này cũng không có sự khác biệt đáng kể so với những trường hợp khác. Bạn có thể thử nghiệm với các metrics đánh giá khác (F1-score, precition, recall, log_loss) để nhìn thấy sự khác biệt rõ hơn.\nBên dưới là đồ thị thể hiện mối quan hệ của mỗi learning_rate với các giá trị khác nhau của n_estimators.\n 3. Kết luận\nỞ bài viết này, chúng ta đã tiến hành tuning XGBoost model với 2 hyper-parameters là learning_rate và n_estimators.\nBài viết tiếp theo chúng ta sẽ tiếp tục tune thêm một tham số khác là subsample. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/16_tuning_learning_rate_and_number_decition_tree/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 13: Tuning Learning_Rate và số lượng của Decision Tree"},{"categories":["Machine Learning","XGBoost"],"contents":"Ý tưởng cơ bản của thuật toán Gradient Boosting là lần lượt thêm các decision trees nối tiếp nhau. Tree thêm vào sau sẽ cố gắng giải quyết những sai sót của tree trước đó. Câu hỏi đặt ra là bao nhiêu trees (weak learner hay estimators) là đủ?\nTrong bài nãy, hãy cùng nhau tìm hiều cách lựa chọn số lượng và kích thước của các trees phù hợp với từng bài toán của các bạn.\n1. Tune số lượng của decision tree\nThông thường khi sử dụng GBM, ta thường chọn số lượng trees tương đối nhỏ. Có thể là vài chục, vài trăm, hoặc vài nghìn. Nguyên nhân có lẽ là vì tăng số lượng trees lên nhiều hơn, hiệu năng của model cũng không tăng, thậm chí còn giảm đi so với khi sử dụng số lượng trees ít hơn.\nMình sẽ sử dụng Otto dataset để minh họa việc tuning số lượng trees. Ở đây mình sử dụng 10-fold cross-validation, số lượng trees trong khoảng [50, 400, 50] -\u0026gt; Có 80 models được train.\nSố lượng của trees được chỉ ra bởi giá trị của tham số n_estimators.\n# XGBoost on Otto dataset, Tune n_estimators from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() n_estimators = range(50, 400, 50) param_grid = dict(n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(n_estimators, means, yerr=stds) pyplot.title(\u0026#34;XGBoost n_estimators vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;n_estimators\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;_estimators.png\u0026#39;) Kết quả:\nBest: -0.001155 using {\u0026#39;n_estimators\u0026#39;: 100} -0.001160 (0.001059) with: {\u0026#39;n_estimators\u0026#39;: 50} -0.001155 (0.001053) with: {\u0026#39;n_estimators\u0026#39;: 100} -0.001156 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 150} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 200} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 250} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 300} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 350} Số lượng trees phù hợp nhất là 100, neg_log_loss đạt được tại đó là -0.001055. Hiệu năng của model không được cải thiện khi tăng số lượng trees từ 100 lên 350.\nĐồ thị bên dưới thể hiện mối quan hệ giữa số lượng trees và inverted logarihmic:\n 2. Tune kích thước của decision tree\nKích thước của tree hay còn gọi là số lớp (layers) hay độ sâu (depth) của tree đó. Nếu tree quá nông (shallow) sẽ dẫn đến underfitting vì model chỉ học được rất ít chi tiết từ dữ liệu. Ngược lại, tree quá sâu (deep) thì model lại học quá nhiều chi tiết từ dữ liệu -\u0026gt; overfitting.\nKích thước của tree được chỉ ra bởi giá trị của tham số max_depth. Ta sẽ thử grid-seach tham số này theo phạm vi [1, 11, 2].\nCode đầy đủ như bê dưới:\n# XGBoost on Otto dataset, Tune max_depth from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(✬Agg✬) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() max_depth = range(1, 11, 2) print(max_depth) param_grid = dict(max_depth=max_depth) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(max_depth, means, yerr=stds) pyplot.title(\u0026#34;XGBoost max_depth vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;max_depth\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;max_depth.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 5 candidates, totalling 50 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 26 tasks | elapsed: 5.0min [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 7.8min finished Best: -0.001136 using {\u0026#39;max_depth\u0026#39;: 5} -0.001319 (0.001100) with: {\u0026#39;max_depth\u0026#39;: 1} -0.001153 (0.001066) with: {\u0026#39;max_depth\u0026#39;: 3} -0.001136 (0.001077) with: {\u0026#39;max_depth\u0026#39;: 5} -0.001150 (0.001063) with: {\u0026#39;max_depth\u0026#39;: 7} -0.001150 (0.001063) with: {\u0026#39;max_depth\u0026#39;: 9} Quan sát ouput, ta thấy rằng max_depth = 5 cho kết quả tốt nhất. Tăng giá trị này lên 7 hoặc 9, hiệu năng của model không những không được cải thiện mà còn kém đi.\nĐồ thị thể hiện mối quan hệ của kích thước tree và neg_log_loss.\n 3. Tune đồng thời số lượng và kích thước của decision tree\nCó một mối liên hệ giữa số lượng và kích thước của mỗi tree. Nhiều tree hơn thì kích thước của mỗi tree sẽ nhỏ hơn. Ngược lại, ít tree hơn thì kích thước của mỗi tree sẽ lớn hơn.\nĐể tìm ra cặp giá trị (n_estimators, max_depth) phù hợp, ta sẽ thử grid-search như sau:\n n_estimators: (50, 100, 150, 200) max_depth: (2, 4, 6, 8) 10-fold cross-validation -\u0026gt; 4x4x10 = 160 models  Code đầy đủ bên dưới:\n# XGBoost on Otto dataset, Tune n_estimators and max_depth from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot import numpy # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() n_estimators = [50, 100, 150, 200] max_depth = [2, 4, 6, 8] print(max_depth) param_grid = dict(max_depth=max_depth, n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot results scores = numpy.array(means).reshape(len(max_depth), len(n_estimators)) for i, value in enumerate(max_depth): pyplot.plot(n_estimators, scores[i], label=\u0026#39;depth: \u0026#39; + str(value)) pyplot.legend() pyplot.xlabel(\u0026#39;n_estimators\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;n_estimators_vs_max_depth.png\u0026#39;) Kết quả:\nBest: -0.001131 using {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 100} -0.001266 (0.001112) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 50} -0.001249 (0.001101) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 100} -0.001248 (0.001101) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 150} -0.001247 (0.001100) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 200} -0.001141 (0.001094) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 50} -0.001131 (0.001088) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 100} -0.001132 (0.001089) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 150} -0.001132 (0.001089) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 200} -0.001160 (0.001059) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 50} -0.001155 (0.001053) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 100} -0.001156 (0.001054) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 150} -0.001155 (0.001054) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 200} -0.001155 (0.001068) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 50} -0.001150 (0.001063) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 100} -0.001150 (0.001064) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 150} -0.001150 (0.001064) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 200} Từ kết quả ta thấy kết quả tốt nhất đạt được tại max_depth=4 và n_estimators=100, tương tự như kết quả của 2 lần tuning riêng rẽ 2 tham số ở bên trên.\nĐồ thị thể hiện mối quan hê của mỗi max_depth với các giá trị của n_estimators.\n Kết quả thể hiện trên đồ thị cũng minh họa cho nhận định về mối quan hệ giữa số lượng và kích thước của tree mà ta đã nói bên trên.\n6. Kết luận\nQua bài viết này, chúng ta đã biết cách tuning XGBoost model, sử dụng phương pháp grid-search (hỗ trợ bởi thư viện scikit-learn) để tìm được số lượng và kích thước của trees phù hợp với bài toán đặt ra ban đầu. Ngoài phương pháp này, còn có 1 phương pháp khác cũng rất hiệu quả là bayes (sử dụng định luật bayes). Phương pháp này thước được các ông lớn AWS, Google, \u0026hellip; sử dụng trong các dịch vụ về AI của họ.\nBài viết tiếp theo chúng ta sẽ tiếp tục tune learning_rate đồng thời với số lượng của tree trong XGboost model. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/15_tuning_number_and_size_decision_tree/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 12: Tuning số lượng và kích thước của Decision Tree"},{"categories":["Machine Learning","XGBoost"],"contents":"Thư viện XGBoost được thiết kế để tận dụng tối đa sức mạnh của phần cứng hệ thống, bao gồm tất cả CPU cores và bộ nhớ. Trong bài viết này, ta sẽ cùng nhau tìm hiểu cách thiết lập một server trên AWS để train XGBoost model, sao cho vừa nhanh, vừa rẻ! :D\nBài viết gồm 4 phần:\n Tạo tài khoản AWS Chạy AWS EC2 Instance Kết nối đến EC2 Instance và chạy code train XGBoost model Đóng AWS EC2 Instance  Chú ý quan trọng: Sẽ mất khoảng 1-2$ chi phí để sử dụng các dịch vụ của AWS trong bài viết này.\n1. Tạo tài khoản AWS\n(Nếu bạn đã có tài khoản AWS, hãy bỏ qua bước này!)\n Truy cập vào màn hình console của AWS. Tại đây ta có thể đăng nếu đã có tài khoản hoặc đăng ký tài khoản mới nếu chưa có.    Bạn cần cung cấp một số thông tin cần thiết, và đặc biệt là phải có một thẻ credit còn hiệu lực để có thể tiến hành tạo tài khoản. Các công đoạn khác, hãy làm theo chỉ dần trên màn hình.  2. Chạy AWS EC2 Instance\nChúng ta sẽ sử dụng dịch vụ EC2 để chạy XGBoost.\n Đăng nhập vào AWS console. Sau khi đăng nhập thành công, danh sách các dịch vụ của AWS sẽ hiển thị. Chọn EC2.    Click vào nút Launch EC2 Instance. Click vào Community AMIs    Nhập ami-1c40bf7d vào Search community AMIs và chọn Select.    Chọn EC2 Instance type là r4.8xlarge (32 cores CPU). Click Review and Launch. Click Launch. Chọn Create a new key pair, điền tên của key là xgboost-key và click Download Key Pair`.   Lưu file key vào máy tính ở local, sau đó click Launch EC2 Instances.\n Click View EC2 Instances. Chờ khoảng 3 phút và kiểm tra trạng thái của EC2 Instance.   Nếu trạng thái là Running thì tức là đã tạo EC2 Instance thành công. Ta cũng để ý thấy địa chỉ public IP của EC2 Instance là: 54.92.106.10. Tiếp theo ta sẽ sử dụng địa chỉ này để kết nối đến EC2 Instance từ localhost thông qua giao thức SSH.\n3. Kết nối đến EC2 Instance và chạy code\n3.1 Kết nối đến EC2 Instance qua giao thức ssh\n Trên máy tính local (mình dùng Ubuntu), mở cửa sổ Terminal và gõ lệnh:  $ cd Documents #Thư mục chứa key file $ chmod 600 xgboost-key.pem $ ssh -i xgboost.pem fedora@54.92.106.10 Nếu đây là lần đầu kết nối đến EC2 Instance, sẽ có 1 cảnh báo xuất hiện. Gõ yes.\nNếu kết nối thành công, màn hình Terminal sẽ xuất hiện như sau:\n  Kiểm ra số lượng CPU cores  $cat /proc/cpuinfo | grep processor | wc -l Kết quả:\n32 3.2 Cài đặt các thư viện cần thiết\n Cài đặt GCC, Python và SciPy  sudo dnf install gcc gcc-c++ make git unzip python python3-numpy python3-scipy python3-scikit-learn python3-pandas python3-matplotlib  Cài đặt Cmake  XGBoost yêu cầu cmake \u0026gt;= 3.13. Nếu bạn cài bằng bằng lệnh dnf install cmake thì phiên bản của make la 3.9. Để cài cmake \u0026gt;= 3.13, bạn phải cài build từ source.\n$ wget https://github.com/Kitware/CMake/releases/download/v3.15.2/cmake-3.15.2.tar.gz $ tar -zxvf cmake-3.15.2.tar.gz $ cd cmake-3.15.2 $ ./bootstrap $ make $ sudo make install Kiểm tra GCC:\n$gcc --version Kết quả:\n[fedora@ip-172-31-37-253 ~]$ gcc --version gcc (GCC) 6.3.1 20161221 (Red Hat 6.3.1-1) Kiểm tra Python:\n$ python3 --version Kết quả:\nPython 3.5.1 Kiểm tra SciPy:\n$ python3 -c \u0026quot;import scipy;print(scipy.__version__)\u0026quot; $ python3 -c \u0026quot;import numpy;print(numpy.__version__)\u0026quot; $ python3 -c \u0026quot;import pandas;print(pandas.__version__)\u0026quot; $ python3 -c \u0026quot;import sklearn;print(sklearn.__version__)\u0026quot; Kết quả:\n0.16.1 1.11.0 0.18.0 0.17.1 Kiểm tra Cmake\n$ cmake --version Kết quả:\n3.15.1 3.3. Cài đặt thư viện XGBoost\n$ pip3 install xgboost==1.1 Tại thời điểm viết bài, phiên bản mới nhất của xgboost là 1.2. Nhưng vì phiên bản của python=3.5 nên bạn chỉ có thể sử dụng được phiên bản 1.1 của xgboost.\nKiểm tra:\n$ python3 -c \u0026quot;import xgboost;print(xgboost.__version__)\u0026quot; Kết quả:\n1.1.0 4. Train XGBoost model\nTương tự như ở bài 9, chúng ta cũng sẽ sử dụng Otto dataset để kiểm tra khả năng của XGBoost model theo số lượng cores của CPU.\n Tạo thư mục xgboost trên máy local, tạo file check_num_threads.py với code như sau:  # Otto multi-core test from pandas import read_csv from xgboost import XGBClassifier from sklearn.preprocessing import LabelEncoder from time import time # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # evaluate the effect of the number of threads results = [] num_threads = [1, 16, 32] for n in num_threads: start = time() model = XGBClassifier(nthread=n) model.fit(X, label_encoded_y) elapsed = time() - start print(n, elapsed) results.append(elapsed) Copy file train.csv ở bài trước vào thư mục xgboost.\n Tại cửa sổ Terminal của máy local, copy thư mục xgboost lên EC2 Instance:  $ scp -i xgboost-key.pem -r xgboost fedora@54.92.106.10:/home/fedora  Tại của sổ Terminal kết nôi tới EC2 Instance, tiến hành chạy code:  $ cd xgboost python3 check_num_threads.py Kết quả thu được:\n1 70.75178146362305 16 6.106862545013428 32 5.045598745346069 Sử dụng 32 cores, mất 5s để train XGBoost model với tập dữ liệu tương đối lớn. Đây quả là một kết quả ấn tượng. :D\nBonus:: Trong trường hợp việc train model mất nhiều thời gian hơn mà chẳng may bạn bị mất kết nối đến EC2 Instance giữa chừng thì thế nào? Bạn phải chạy train lại từ đầu ư? Quả là mất thời gian phải không? Giải pháp để ngăn chặn tình huống này, hoặc là bạn chạy lệnh train ở chế độ background process và ghi kết quả ra một file như lệnh sau:\n$ nohup python script.py \u0026gt;script.py.out 2\u0026gt;\u0026amp;1 \u0026amp; hoặc bạn cũng có thể sử dụng Tmux.\n5. Tắt EC2 EC2 Instance\nTiết kiệm là quốc sách hàng đầu, hãy luôn nhớ tắt EC2 EC2 Instance mỗi khi sử dụng xong. Bản thân mình đã từng một lần quên không tắt trong vài ngày. Kết quả là con số trên hóa đơn AWS tháng đó làm mình buồn mất cả tuần, :).\nĐể tắt EC2 EC2 Instance, đơn giản là làm theo như hình vẽ sau:\n  Chọn 1-\u0026gt;2-\u0026gt;3 nếu bạn muốn tắt tạm thời (khi nào muốn dùng thì khởi động lên). Chọn 1-\u0026gt;2-\u0026gt;4 nếu bạn muốn xóa hẳn EC2 EC2 Instance này.  6. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách cài đặt vào cấu hình EC2 Instance để train XGBoost model trên AWS.\nBài viết tiếp theo sẽ thiên về lý thuyết một chút, chúng ta sẽ tìm hiểu cách cấu hình hyper-parameters cho gradient boosting model. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/14_train_xgboost_models_on_aws/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 11: Train XGBoost model trên AWS"},{"categories":["Machine Learning","XGBoost"],"contents":"Thư viện XGBoost được thiết kế để làm việc h iệu quả vớicơ chế xử lý song song trên nhiều core (multithreading) của phần cứng, cả trong quá trình train và dự đoán. Hãy cùng nhau tìm hiểu cơ chế đó thông qua bài viết này.\n1. Chuẩn bị dataset\nChúng ta sẽ sử dụng Otto Group Product Classification Challenge dataset để minh họa cơ chế multithreading của thư viện XGBoost. Để download dataset này, bạn cần đăng nhập vào Kaggle. có 2 file là train.csv và test.csv. Vì chỉ có file train.csv là có nhãn nên ta sẽ sử dụng file này. Download file train.csv về máy tính (dạng train.csv.zip). Giải nén nó ra và đặt trong thư mục làm việc của bạn.\nDataset này bao gồm khoảng 94.000 sản phẩm và 93 input featuresđược chia thành 10 nhóm (ví dụ: thời trang, điện tử, \u0026hellip;). Mục tiêu là xây dựng một model để phân loại một sản phẩm mới vào các nhóm này. Cuộc thi này đã kết thúc vào 05/2015 và người chiến thắng cũng sử dụng XGBoost để tạo model.\n2. Ảnh hưởng của số lượng threads đến thời gian train model\nXGBoost được viết bằng C++, sử dụng OpenMP API để phát triển cơ chế xử lý song song. Trong một số trường hợp, bạn có thể phải compile mại XGBoost mới có thể sử dụng cơ chế song song này. Chi tiết về việc cài đặt XGBoost, có thể xem tại đây.\nHai lớp XGBClassifier và XGBRegressor trong thư viện XGBoost cung cấp tham số nthread để chỉ ra số lượng threads mà XGBoost sử dụng trong quá trình train. Mặc định thì giá trị của tham số này là -1, tức là sử dụng tất cả các core của hệ thống.\nmodel = XGBClassifier(nthread=-1) Bây giờ chúng ta sẽ xây dựng một số XGBoost models khác nhau theo số lượng threads sử dụng và đo đặc thời gian train của mỗi model.\nCode snippet:\n# evaluate the effect of the number of threads results = [] num_threads = [1, 2, 3, 4, 5, 6, 7, 8] for n in num_threads: start = time.time() model = XGBClassifier(nthread=n) model.fit(X_train, y_train) elapsed = time.time() - start print(n, elapsed) results.append(elapsed) Áp dụng vào bộ dữ liệu Otto (bạn có thể thay đổi giá trị của mảng num_threads theo máy tính của bạn):\nfrom matplotlib import pyplot # load data data = read_csv(\u0026#39;test.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # evaluate the effect of the number of threads results = [] num_threads = [1, 2, 3, 4] for n in num_threads: start = time() model = XGBClassifier(nthread=n) model.fit(X, label_encoded_y) elapsed = time() - start print(n, elapsed) results.append(elapsed) # plot results pyplot.plot(num_threads, results) pyplot.ylabel(\u0026#39;Speed (seconds)\u0026#39;) pyplot.xlabel(\u0026#39;Number of Threads\u0026#39;) pyplot.title(\u0026#39;XGBoost Training Speed vs Number of Threads\u0026#39;) pyplot.show() Chạy code trên thu được kết quả:\n1 50.13007640838623 2 26.999273538589478 3 18.629448890686035 4 15.561982154846191 5 13.940500497817993 6 12.550707578659058 7 13.075348854064941 8 12.36113166809082 và đồ thị thể hiện mối quan hệ giữa số lượng threads và thời gian train model.\n Có thể thấy rõ xu hướng giảm của thời gian train model khi số lượng threads tăng lên. Nếu kết quả chạy trên máy của bạn không giống như vậy, bạn cần phải xem xét lại cách enable cơ chế song song của XGBoost như link tham khảo mình đề cập bên trên.\n3. Ảnh hưởng của số lượng threads đến thời gian cross-validation\nk-fold cross-validation hỗ trợ bởi thư viện scikit-learn cũng có cơ chế xử lý song song tương tự như XGBoost. Tham số n_jobs của hàm cross_val_crore() chỉ ra số lượng threads sử dụng. Mặc định, n_jobs=1 tức là chỉ sử dụng 1 core (1 thread). Ta có thể gán giá trị -1 cho nó để sử dụng tất cả cores của hệ thống.\nresults = cross_val_score(model, X, label_encoded_y, cv=kfold,scoring=\u0026#39;neg_log_loss\u0026#39;, n_jobs=-1, verbose=1) Đến đây lại xuất hiện một câu hỏi, chúng ta nên chọn phương án nào trong 3 phương án sau:\n Disable multithreading của XGBoost, enable multithreading của cross-validation. Enable multithreading của XGBoost, disable multithreading của cross-validation. Enable multithreading của XGBoost, enable multithreading của cross-validation.  Để trả lời câu hỏi này, không gì chính xác hơn là chúng ta sẽ code cho cả 3 cách và so sánh thời gian thực thi của mỗi cách:\n# Otto, parallel cross validation from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import cross_val_score from sklearn.preprocessing import LabelEncoder import time # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # prepare cross validation kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) # Single Thread XGBoost, Parallel Thread CV start = time.time() model = XGBClassifier(nthread=1) results = cross_val_score(model, X, label_encoded_y, cv=kfold, coring=\u0026#39;neg_log_loss\u0026#39;, n_jobs=-1) elapsed = time.time() - start print(\u0026#34;Single Thread XGBoost, Parallel Thread CV: %f\u0026#34; % (elapsed)) # Parallel Thread XGBoost, Single Thread CV start = time.time() model = XGBClassifier(nthread=-1) results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=\u0026#39;neg_log_loss, n_jobs=1) elapsed = time.time() - start print(\u0026#34;Parallel Thread XGBoost, Single Thread CV: %f\u0026#34; % (elapsed)) # Parallel Thread XGBoost and CV start = time.time() model = XGBClassifier(nthread=-1) results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=\u0026#39;neg_log_loss\u0026#39;, n_jobs=-1) elapsed = time.time() - start print(\u0026#34;Parallel Thread XGBoost and CV: %f\u0026#34; % (elapsed)) Kết quả cuối cùng:\nSingle Thread XGBoost, Parallel Thread CV: 101.820478 Parallel Thread XGBoost, Single Thread CV: 455.847770 Parallel Thread XGBoost and CV: 98.794466 Rõ ràng, phương án thứ 3 sử dụng thời gian ít nhất. Bây giờ bạn đã biết câu trả lời rồi phải không? :D\n4. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách cấu hình cơ chế multithreading của XGBoost model. Chúng ta cũng nhận thức được ảnh hưởng của số lượng threads (số lượng cores) đến thời gian train model, từ đó biết cách kiểm tra xem hệ thống có hỗ trợ cơ chế xử lý song song của XGBoost hay không? Cuối cùng, cách cấu hình tốt nhất cho cả XGBoost và cross-validation để giảm thời gian thực thi cũng đã được tìm ra.\nỞ bài viết tiếp theo chúng ta sẽ tìm hiểu cách scale-up XGBoost model để sử dụng nhiều cores của hệ thống hơn trên AWS cloud. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/13_multithreading-xgboost/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 10: Cấu hình Multithreading cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Overfitting vẫn luôn là một vấn đề làm đau đầu những kỹ sư AI. Trong bài viết này chúng ta sẽ cùng tìm hiểu cách thức monitor (giám sát) performance (hiệu năng) của XGBoost model trong suốt quá trình train. Từ đó cấu hình early stopping để quyết định khi nào thì nên dừng lại quá trình này để tránh hiện tượng overfitting. Bài viết gồm 2 phần:\n Monitor hiệu năng của XGBoost model thông qua learning curve (đường cong học tập). Cấu hình early stopping.  1. Giám sát hiệu năng của XGBoost model\nĐể monitor porformance của XGBoost model, ta cần cung cấp cả train set, test set và một metric (chỉ tiêu đánh giá) khi train model (gọi hàm model.fit()). Ví dụ, để tính toán error metric trên tập test set, sử dụng code snippet sau:\neval_set = [(X_test, y_test)] model.fit(X_train, y_train, eval_metric=\u0026#34;error\u0026#34;, eval_set=eval_set, verbose=True) XGBoost model hỗ trợ một số metric như sau:\n rmse: root mean squared error. mae: mean absolute error. logloss: binary logarithmic loss. mlogloss: multiclass log loss (cross entropy). error: classification error. auc: area under ROC curve. Danh sách đầy đủ các metrics, các bạn có thể xem tại đây.  Code dưới đây minh hoạ việc monitor performance trong quá trình train một XGBoost model trên tập dữ liệu Pima Indians onset of diabetes.\n# monitor training performance from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on training data model = XGBClassifier() eval_set = [(X_test, y_test)] model.fit(X_train, y_train, eval_metric=\u0026#34;error\u0026#34;, eval_set=eval_set, verbose=True) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Ở đây, ta sử dụng 67% dữ liệu cho việc train, 33% còn lại cho viêc đánh giá model. Error metric được tính toán tại cuối mỗi vòng lặp (sau khi mỗi boosted tree được thêm vào model). Cuối cùng, độ chính xác của model được in ra.\nKết quả hiển thị trên màn hình như sau:\n[0]\tvalidation_0-error:0.28347 [1]\tvalidation_0-error:0.25984 [2]\tvalidation_0-error:0.25591 [3]\tvalidation_0-error:0.24803 [4]\tvalidation_0-error:0.24409 [5]\tvalidation_0-error:0.24803 [6]\tvalidation_0-error:0.25591 [7]\tvalidation_0-error:0.24803 [8]\tvalidation_0-error:0.25591 [9]\tvalidation_0-error:0.24409 ... [89]\tvalidation_0-error:0.26378 [90]\tvalidation_0-error:0.27165 [91]\tvalidation_0-error:0.26772 [92]\tvalidation_0-error:0.27165 [93]\tvalidation_0-error:0.26378 [94]\tvalidation_0-error:0.27165 [95]\tvalidation_0-error:0.26378 [96]\tvalidation_0-error:0.25984 [97]\tvalidation_0-error:0.26378 [98]\tvalidation_0-error:0.25984 [99]\tvalidation_0-error:0.25984 Accuracy: 74.02% Quan sát kết quả ta thấy performance của model không thay dổi quá nhiều trong suốt quá trình train. Thậm chí đến cuối quá trình, performance còn kém hơn so với nửa đầu.\nĐể có cái nhìn tường minh hơn, hãy thể hiện performance của model trên đồ thị. Ta sẽ monitor performace của model trên cả train set và test set:\neval_set = [(X_train, y_train), (X_test, y_test)] model.fit(X_train, y_train, eval_metric=\u0026#34;error\u0026#34;, eval_set=eval_set, verbose=True) Performance của model trên mỗi tập evaluation set được lưu bởi model sau khi train kết thúc. Để truy cập giá trị performace này, sử dụng hàm model.evals_result():\nresults = model.evals_result() print(results) Kết quả in ra sẽ giống như sau:\n{ \u0026#39;validation_0\u0026#39;: {\u0026#39;error\u0026#39;: [0.259843, 0.26378, 0.26378, ...]}, \u0026#39;validation_1\u0026#39;: {\u0026#39;error\u0026#39;: [0.22179, 0.202335, 0.196498, ...]} } validation_0 và validation_1 theo thứ tự tương ứng với hai tập validation set mà ta đã định nghĩa trong tham số eval_set khi gọi hàm fit().\nError metric được truy cập như sau:\nresults[\u0026#39;validation_0\u0026#39;][\u0026#39;error\u0026#39;] results[\u0026#39;validation_1\u0026#39;][\u0026#39;error\u0026#39;] Thêm nữa, bạn có thể lựa chọn nhiều metrics để đánh giá model bằng cách cung cấp một mảng các giá trị metric tới tham số eval_metric của hàm fit(). Giá trị của các metric thu được sau đó đươc thể hiện trên đồ thị, gọi là learning curve.\nCode đầy đủ dưới đây minh họa việc thu thập giá tị của các metrics và thể hiện trên learning curve:\n# plot learning curve from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on training data model = XGBClassifier() eval_set = [(X_train, y_train), (X_test, y_test)] model.fit(X_train, y_train, eval_metric=[\u0026#34;error\u0026#34;, \u0026#34;logloss\u0026#34;], eval_set=eval_set, verbose=True) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) # retrieve performance metrics results = model.evals_result() epochs = len(results[\u0026#39;validation_0\u0026#39;][\u0026#39;error\u0026#39;]) x_axis = range(0, epochs) # plot log loss fig, ax = pyplot.subplots() ax.plot(x_axis, results[\u0026#39;validation_0\u0026#39;][\u0026#39;logloss\u0026#39;], label=\u0026#39;Train\u0026#39;) ax.plot(x_axis, results[\u0026#39;validation_1\u0026#39;][\u0026#39;logloss\u0026#39;], label=\u0026#39;Test\u0026#39;) ax.legend() pyplot.ylabel(\u0026#39;Log Loss\u0026#39;) pyplot.title(\u0026#39;XGBoost Log Loss\u0026#39;) pyplot.show() # plot classification error fig, ax = pyplot.subplots() ax.plot(x_axis, results[\u0026#39;validation_0\u0026#39;][\u0026#39;error\u0026#39;], label=\u0026#39;Train\u0026#39;) ax.plot(x_axis, results[\u0026#39;validation_1\u0026#39;][\u0026#39;error\u0026#39;], label=\u0026#39;Test\u0026#39;) ax.legend() pyplot.ylabel(\u0026#39;Classification Error\u0026#39;) pyplot.title(\u0026#39;XGBoost Classification Error\u0026#39;) pyplot.show() Chạy code trên, error và logloss metric trên cả 2 tập train set và test set được in ra. Ta có thể bỏ qua điều này bằng cách truyền giá trị False (giá trị mặc định) cho tham sô verbose khi gọi hàm fit(). Hai đồ thị được tạo ra. Đồ thị đầu tiên thể hiện logloss của XGBoost model đối với mỗi epoch (iteration) trong quá trình train.\n Đồ thị thứ 2 hiển thị error metric của mỗi epoch.\n Quan sát cả 2 đồ thị trên ta có một nhận xét rằng, nếu dừng train sớm hơn tại epoch \u0026lt; 100 thì performace của model sẽ tốt hơn. Đây chính là tiền đề của kỹ thuật early stopping mà chúng ta sẽ tìm hiểu ngay sau đây.\n2. Cấu hình early stopping cho XGBoost model\nEarly stopping là một kỹ thuật khá phổ biến áp dụng cho các ML model phức tạp để tránh hiện tượng overfitting. Nó làm việc bằng cách monitor performance của model trên tập test set trong suốt quá trình train và buộc quá trình này dừng lại một khi performance của model không được cải thiện sau một số epochs nhất định.\nTrên đồ thị learning curve, điểm bắt đầu overfitting là điể mà tại đó performace của model trên tập test set bắt đầu giảm trong khi performance của model trên tập train set vẫn tiếp tục tăng.\nĐể cấu hình early stopping cho XGBoost model, cần cung cấp thêm giá trị cho tham số early_stopping_rounds khi gọi hàm fit(). Ý nghĩa của nó là chỉ ra số lượng epochs mà quá trình húân luyện trải qua mà performance không có sự cải thiện nào.\nVí dụ:\neval_set = [(X_test, y_test)] model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\u0026#34;logloss\u0026#34;, eval_set=eval_set, verbose=True) Nếu có nhiều evaluation sets hoặc nhiều metrics được cung cấp, early stopping sẽ sử dụng cái cuối cùng trong danh sách.\nCode đầy đủ cấu hình early stopping như bên dưới:\n# early stopping from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() eval_set = [(X_test, y_test)] model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\u0026#34;logloss\u0026#34;, eval_set=eval_set, verbose=True) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Kết quả chạy code:\n[0]\tvalidation_0-logloss:0.60491 Will train until validation_0-logloss hasn\u0026#39;t improved in 10 rounds. [1]\tvalidation_0-logloss:0.55934 [2]\tvalidation_0-logloss:0.53068 [3]\tvalidation_0-logloss:0.51795 [4]\tvalidation_0-logloss:0.51153 [5]\tvalidation_0-logloss:0.50935 [6]\tvalidation_0-logloss:0.50818 [7]\tvalidation_0-logloss:0.51097 [8]\tvalidation_0-logloss:0.51760 [9]\tvalidation_0-logloss:0.51912 [10]\tvalidation_0-logloss:0.52503 [11]\tvalidation_0-logloss:0.52697 [12]\tvalidation_0-logloss:0.53335 [13]\tvalidation_0-logloss:0.53905 [14]\tvalidation_0-logloss:0.54546 [15]\tvalidation_0-logloss:0.54613 [16]\tvalidation_0-logloss:0.54982 Stopping. Best iteration: [6]\tvalidation_0-logloss:0.50818 Accuracy: 74.41% Quá trình train model dừng lại ở epoch 16 (gần với những gì mà chúng ta phán đoán dựa trên đồ thị learning curve) và model đạt được metric thấp nhất tại epoch 6. Việc chọn giá trị của tham số early_stopping_rounds thường dựa vào quan sát trên đồ thị learning curve. Nếu bạn không biết thì có thể chọn giá trị mặc định là 10.\n3. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách monitor performance của XGBoost model trong quá trình train và cấu hình early stopping để hạn chế hiện tượng overfitting của model.\nỞ bài viết tiếp theo chúng ta sẽ tìm hiểu cách cấu hình XGBoost model để tận dụng hết tài nguyên của phần cứng khi train model và khi sử model để dự đoán. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/12_early_stopping/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 9: Cấu hình Early_Stopping cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Feature selection hay lựa chọn features là một bước tương đối quan trọng trước khi train XGBoost model. Lựa chọn đúng các features sẽ giúp model khái quát hóa vấn đề tốt hơn (low variance) -\u0026gt; đạt độ chính xác cao hơn.\nTrong bài viết này, hãy cùng xem xét về cách dùng thư viện XGBoost để tính importance scores và thể hiện nó trên đồ thị, sau đó lựa chọn các features để train XGBoost model dựa trên importance scores đó.\n1. Tính và hiển thị importance score trên đồ thị\n1.1 Cách 1\nModel XGBoost đã train sẽ tự động tính toán mức độ quan trọng của các features. Các giá trị này được lưu trong biến feature_importances_ của model đã train. Kiểm tra bằng cách:\nprint(model.feature_importances_) Thể hiện các features importance lên đồ thị:\n# plot pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_) pyplot.show() Code dưới đây minh họa đầy đủ việc train XGBoost model trên tập dữ liệu Pima Indians onset of diabetes và hiển thị các features importances lên đồ thị:\n# plot feature importance manually from numpy import loadtxt from XGBoost import XGBClassifier from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] y = dataset[:,8] # fit model on training data model = XGBClassifier() model.fit(X, y) # feature importance print(model.feature_importances_) # plot pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_) pyplot.show() Chạy code trên, importance score được in ra:\n[0.10621197 0.2424023 0.08803366 0.07818192 0.10381887 0.1486732 0.10059207 0.13208601] và đồ thị:\n 1.2 Cách 2\nNhược điểm của cách này là các importance scores được sắp xếp theo thứ tự của các features trong tập dataset. Điều này làm cho chúng ta khó quan sát trong trường hợp số lượng features lớn. Liệu có thể sắp thứ tự các importance scores này theo giá trị của chúng được hay không? Câu trả lời là có thể. Thư viện XGBoost có một hàm gọi là plot_importance() giúp chúng ta thực hiện việc này.\n# plot feature importance plot_importance(model) pyplot.show() Code dưới đây minh họa đầy đủ việc train XGBoost model trên tập dữ liệu Pima Indians onset of diabetes và hiển thị các features importances lên đồ thị:\n# plot feature importance using built-in function from numpy import loadtxt from XGBoost import XGBClassifier from XGBoost import plot_importance from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] y = dataset[:,8] # fit model on training data model = XGBClassifier() model.fit(X, y) # plot feature importance plot_importance(model) pyplot.show() Chạy code ví dụ bên trên thu được kết quả:\n Quan sát đồ thị ta thấy, các features được tự động đặt tên từ f0 đến f7 theo thứ tự của chúng trong mảng dữ liệu input X. Từ đồ thị có thể kết lụân rằng:\n f6 có importance score cao nhất - 333 f4 có importance score thấp nhất - 124  Nếu có bảng mô tả dữ liệu, ta có thể ánh xạ f4, f6 thành tên các features tương ứng.\n2. Lựa chọn features (feature selection) theo importance scores\nThư viện scikit-learn cung cấp lớp SelectFromModel cho phép lựa chọn các features để train model. Lớp này yêu cầu 2 tham số bắt buộc:\n model: model đã được train trên toàn bộ dataset. threshold: ngưỡng để lựa chọn features. Chỉ những features có importance score không nhỏ hơn ngưỡng mới được lựa chọn. Sau khi gọi hàm transform() thì lớp SelectFromModel sẽ chuyển đổi tập dữ liệu ban đầu thành tập dữ liệu nhỏ hơn chỉ bao gồm các features được chọn.  # select features using threshold selection = SelectFromModel(model, threshold=thresh, prefit=True) select_X_train = selection.transform(X_train) Sau khi có tập dữ liệu mới, ta tiến hành train và đánh giá model mới tạo ra như bình thường.\n# train model selection_model = XGBClassifier() selection_model.fit(select_X_train, y_train) # eval model select_X_test = selection.transform(X_test) y_pred = selection_model.predict(select_X_test) Trong các bài toán thực tế, ta thường không biết chính xác giá trị nào của threshold là phù hợp. Vì vậy mà ta sẽ tuning giá trị này bằng phương pháp grid-seach (mình sẽ có 1 bài viết riêng giải thích chi tiết về các phương pháp tuning hyper-parameters. Ở đây, bạn chỉ cần hiểu một cách đơn giản là kiểm tra với nhiều giá trị của threshold để chọn ra giá trị tốt nhất). Chúng ta sẽ bắt đầu kiểm tra với tất cả features, kết thúc với feature quan trọng nhất.\nCode hoàn chỉnh như bên dưới:\n# use feature importance for feature selection from numpy import loadtxt from numpy import sort from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.feature_selection import SelectFromModel # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on all training data model = XGBClassifier() model.fit(X_train, y_train) # make predictions for test data and evaluate predictions = model.predict(X_test) accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) # Fit model using each importance as a threshold thresholds = sort(model.feature_importances_) for thresh in thresholds: # select features using threshold selection = SelectFromModel(model, threshold=thresh, prefit=True) select_X_train = selection.transform(X_train) # train model selection_model = XGBClassifier() selection_model.fit(select_X_train, y_train) # eval model select_X_test = selection.transform(X_test) predictions = selection_model.predict(select_X_test) accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Thresh=%.3f, n=%d, Accuracy: %.2f%%\u0026#34; % (thresh, select_X_train.shape[1], accuracy*100.0)) Chạy code trên thu được kết quả như sau:\nAccuracy: 74.02% Thresh=0.088, n=8, Accuracy: 74.02% Thresh=0.089, n=7, Accuracy: 71.65% Thresh=0.098, n=6, Accuracy: 71.26% Thresh=0.098, n=5, Accuracy: 74.41% Thresh=0.100, n=4, Accuracy: 74.80% Thresh=0.136, n=3, Accuracy: 71.26% Thresh=0.152, n=2, Accuracy: 71.26% Thresh=0.240, n=1, Accuracy: 67.32% Có thể thấy rằng độ chính xác của model cao nhất trên tập dữ liệu gồm 4 features quan trọng nhất và thấp nhất trên tập dữ liệu chỉ gồm một feature.\nTuning theo kiểu grid-seach như này đặc biệt hiệu quả trong trường hợp bộ dữ liệu lớn.\n3. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách thể hiện importance score của các features trên đồ thị và sử dụng importance score để lựa chọn các features sao cho model đạt được độ chính xác cao nhất.\nBài viết tiếp theo ta sẽ tìm hiểu cách giám sát (monitor) hiệu năng của model trong quá trình train và cấu hình early stop (dừng train khi model đáp ứng một tiêu chí nào đó). Hai kỹ thuật này rất cần thiết để train một XGBoost model tốt. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/11_feature-selection/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 8: Lựa chọn features cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Giả sử bạn đã train xong một XGBoost model đạt được độ chính xác rất cao. Câu hỏi đặt ra là làm sao lưu lại model đó để sử dụng về sau (không phải mất công train lại model mỗi khi cần sử dụng)?\nTrong bài viết này, chúng ta hãy cùng tìm hiểu cách thức lưu một XGBoost model thành 1 file sử dụng Python pickle API. Nội dung bài viết gồm 2 phần chính:\n Lưu và sử dụng XGBoost model bằng thư viện pickle. Lưu và sử dụng XGBoost model bằng thư viện joblib.  1. Lưu và sử dụng XGBoost model bằng thư viện pickle.\nPickle là một cách chuẩn chỉ để lưu một dối tượng trong Python thành một file. Cách sử dụng tương đối đơn giản.\n Lưu model thành file  # save model to file pickle.dump(model, open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;wb\u0026#34;))  Gọi model đã lưu để sử dụng  # load model from file loaded_model = pickle.load(open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;rb\u0026#34;)) Ví dụ dưới đây mình họa việc train một XGBoost model trên tập dữ liệu Pima Indians onset of diabetes, lưu model thành file và gọi model đã lưu để dự đoán.\n# Train XGBoost model, save to file using pickle, load and make predictions from numpy import loadtxt from XGBoost import XGBClassifier import pickle from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) # save model to file pickle.dump(model, open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;wb\u0026#34;)) print(\u0026#34;Saved model to: pima.pickle.dat\u0026#34;) # some time later... # load model from file loaded_model = pickle.load(open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;rb\u0026#34;)) print(\u0026#34;Loaded model from: pima.pickle.dat\u0026#34;) # make predictions for test data predictions = loaded_model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Kết quả:\nSaved model to: pima.pickle.dat Loaded model from: pima.pickle.dat Accuracy: 74.02% 2. Lưu và sử dụng XGBoost model bằng thư viện joblib.\nJoblib là một phần của hệ sinh thái SciPy, nó cũng hỗ trợ việc lưu ML model thành file rât dễ dàng, sử dụng cấu trúc dữ liệu của NumPy. Ưu điểm của viêc sử dụng joblib so với pickle là nó hoạt động khá nhanh, đặc biệt với những model có kích thước lớn. Cách sử dụng:\n Lưu model thành file  # save model to file joblib.dump(model, \u0026#34;pima.joblib.dat\u0026#34;)  Sử dụng model đã lưu  # load model from file loaded_model = joblib.load(\u0026#34;pima.joblib.dat\u0026#34;) Ví dụ dưới đây mình họa việc train một XGBoost model trên tập dữ liệu Pima Indians onset of diabetes, lưu model thành file và gọi model đã lưu để dự đoán.\n# Train XGBoost model, save to file using joblib, load and make predictions from numpy import loadtxt from XGBoost import XGBClassifier from joblib import dump from joblib import load from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) # save model to file dump(model, \u0026#34;pima.joblib.dat\u0026#34;) print(\u0026#34;Saved model to: pima.joblib.dat\u0026#34;) # some time later... # load model from file loaded_model = load(\u0026#34;pima.joblib.dat\u0026#34;) print(\u0026#34;Loaded model from: pima.joblib.dat\u0026#34;) # make predictions for test data predictions = loaded_model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Output của đoạn code trên:\nSaved model to: pima.joblib.dat Loaded model from: pima.joblib.dat Accuracy: 74.02% 3. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách thức lưu XGBoost model thành file sử dụng pickle và joblib, sau đó gọi lại model đã lưu từ file để dự đoán.\nBài viết tiếp theo sẽ tìm hiểu cách tính toán và lựa chọn các features tốt nhất cho việc train XGBoost model.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/10_save-load-xgboost-model/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 7: Lưu và sử dụng XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Ta đã biết, XGBoost thực chất là tập hợp gồm nhiều decision tree. Việc thể hiện mỗi decision tree đó trên đồ thì sẽ giúp chúng ta hiểu sâu sắc hơn quá trình boosting khi đưa vào một tập dữ liệu. Trong bài này, hãy cùng tìm hiểu cách thức thể hiện đó từ một XGBoost model đã được train.\n1. Vẽ một decision tree đơn lẻ\nXGBoost Python API cung cấp một hàm cho việc vẽ các decision tree của một XGBoost model đã train, đó là plot_tree(). Hàm này nhận một tham số đầu tiên chính là model cần thể hiện.\nplot_tree(model) Đồ thị vẽ ra bởi hàm này có thể được lưu dưới dạng file hoặc hiển thị trên màn hình bằng cách sử dụng hàm pyplot.show() của thư viện matplotlib. Yêu cầu là thư viện graphviz đã được cài đặt.\nĐể minh họa cho việc này, hãy cùng tạo một một XGBoost model và train nó trên tập dữ liệu Pima Indians onset of diabetes dataset. Code đầy đủ như bên dưới:\n# plot decision tree from numpy import loadtxt from XGBoost import XGBClassifier from XGBoost import plot_tree from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] y = dataset[:,8] # fit model on training data model = XGBClassifier() model.fit(X, y) # plot single tree plot_tree(model) pyplot.show() Đoạn code bên trên sẽ tạo ra một đồ thị của decision tree đầu tiên trong model (index 0). Các feature và feature value được thể hiện trên đồ thị.\n Một vài quan sát:\n Các features được đặt tên tự động từ f1 đến f5 tương ứng với các feature indices trong dataset. Trong mỗi node, hai hướng trái phải được phân biệt bằng màu sắc. Bên trái là màu xanh, trong khi bên phải là màu đỏ.  2. Một số tùy chọn\nNgoài tham số model cần vẽ là bắt buộc, hàm plot_tree() còn nhận vào một vài tham số tùy chọn khác:\n num_trees: Chỉ số tree muốn vẽ. Giá trị mặc định là 0. Ví dụ:  plot_tree(model, num_trees=4) sẽ vẽ boosted tree thứ 5.\n rankdir: Hướng của đồ thị. Ví dụ: LR là left-to-right. Mặc định là UT - top-to-bottom.  Ví dụ:\nplot_tree(model, num_trees=0, rankdir=\u0026#39;LR\u0026#39;) sẽ cho kết quả như sau:\n 3. Kết luận\nTrong bài này, chúng ta đã tìm hiểu cách vẽ các decision tree của một XGBoost model đã train. Đây là cách rất hay giúp chúng ta có cái nhiều sâu hơn vào bên trong của model, hiểu rõ hơn cách thức mà model hoạt động.\nTrong bài tiếp theo, chúng ta sẽ tìm hiểu cách lưu lại XGBoost model để train và sử dụng model đã lưu để dự đoán trên một mẫu data mới.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/09_visualize-xgboost-model/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 6: Trực quan hóa XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Mục đích của việc phát triển mô hình dự đoán là tạo ra một mô hình có độ chính xác cao khi kiểm tra trên bộ dữ liệu độc lập với dữ liệu train (gọi là unseen data). Trong bài viết này, chúng ta cùng tìm hiểu hai phương pháp đánh giá một XGBoost model:\n Sử dụng train và test dataset. Sử dụng k-fold cross-validation. Bạn hoàn toàn có thể áp dụng những phương pháp trong bài này cho những ML models khác. Tại vì dạo này mình đang tìm hiểu vê XGBoost model nên mình lấy XGBoost model làm ví dụ thôi.  1. Phương pháp 1: Sử dụng train-test set\nĐây là phương pháp đơn giản nhất để đánh giá một ML model. Từ tập dữ liệu ban đầu, ta chia thành 2 phần, gọi là train set và test set theo tỉ lệ nhất định (thường là 7:3, 8:2 hoặc thậm chí 9:1 tùy theo kích thước của tập và đặc trưng của tập dữ liệu). Sau đó, tiến hành train model trên train set rồi sử dụng model đã train đó để dự đoán trên tập test set. Dựa trên kết quả của dự đoán để đưa ra đánh giá chất lượng của model.\nƯu điểm của phương pháp này là nhanh. Nó sẽ phù hợp để áp dụng khi bài toán của bạn đáp ứng ít nhất 1 trong 2 tiêu chí sau:\n Tập dữ liệu có kích thước lớn (hàng triệu mẫu) và có cơ sở để tin rằng cả 2 phần dữ liệu đều đại diện đầy đủ cho tất cả các khía cạnh của vấn đề cần dự đoán (để chắc chắn hơn về điều này, ta có thể xáo trộn ngẫu nhiên tập dữ liệu trước khi chia) Thuật toán train của model rất lâu để hội tụ.  Nếu điều kiện thứ 2 không thỏa mãn mà ta vẫn sử dụng phương pháp này thì sẽ gặp phải vấn đề high variance. Tức là khi 2 tập train set và test set chứa những đại diện khác nhau của vấn đề cần dự đoán thì kết quả đánh giá trên tập test set không thể hiện đúng chất lượng của model.\nThư viện scikit-learn cung cấp hàm train_test_split() giúp chúng ta thực hiện việc chia dữ liệu:\n# split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) Source code dưới đây sử dụng Pima Indians onset of diabetes dataset để train XGBoost model và đánh giá model theo phương pháp này:\n# train-test split evaluation of XGBoost model from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Chạy code trên thu được kết quả:\nAccuracy: 74.02% 2. Phương pháp 2: k-fold cross-validation\nCross-validation là phương pháp mở rộng của phương pháp bên trên để hạn chế được vấn đề high variance. Các bước tiến hành của nó như sau:\n Xáo trộn dữ liệu một cách ngẫu nhiên. Chia tập dữ liệu ban đầu thành k phần (k=5,10,\u0026hellip;), mỗi phần gọi là một fold. - train model trên k-1 fold và đánh giá trên fold còn lại. Lặp lại k lần bước bên trên để mỗi fold trong tập dữ liệu đều có cơ hội trở thành test set. Sau khi toàn bộ quá trình kết thúc ta sẽ có k kết quả đánh giá khác nhau, kêt quả cuối cùng sẽ được tổng hợp dựa vào trung bình (mean) và độ lệch chuẩn (standard deviation) của k kết quả đó.  Phương pháp này cho kết quả đánh giá tin cậy hơn so với phương pháp sử dụng train-test set bởi vì nó được train và đánh giá nhiều lần trên các tập dữ liệu khác nhau. Việc lựa chọn k cũng cần phải xem xét sao cho kích thước của mỗi fold đủ lớn để dữ liệu trong mỗi fold mang tính đại diện cao về mặt thống kê của toàn bộ dữ liệu. Thực nghiệm cho thấy k=5 hoặc k=10 là lựa chọn tốt nhất cho hầu hết các trường hợp. Hãy sử dụng 2 giá trị này trước khi thử nghiệm với các giá trị khác.\nThư viện scikit-learn cung cấp lớp KFold để sử dụng phương pháp này. Đầu tiên, khai báo đối tượng KFold và chỉ ra giá trị của k. Sau đó sử dụng hàm cross_val_score() để bắt đầu đánh giá model.\nkfold = KFold(n_splits=10, random_state=7) results = cross_val_score(model, X, Y, cv=kfold) Code đầy đủ như bên dưới:\n# k-fold cross validation evaluation of XGBoost model from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # CV model model = XGBClassifier() kfold = KFold(n_splits=10, random_state=7) results = cross_val_score(model, X, Y, cv=kfold) print(\u0026#34;Accuracy: %.2f%%(%.2f%%)\u0026#34; % (results.mean()*100, results.std()*100)) Chạy code trên thu được kết quả:\nAccuracy: 73.96% (4.77%) Nếu bài toán là multi-classification hoặc dữ liệu bị mất cân bằng (imbalanced) giữa các lớp (số lượng mẫu giữa các lớp chênh lệch nhau lớn) thì ta có thể sử dụng lớp  StratifiedKFold thay vì KFold của thư viện scikit-learn. Việc làm này có tác dụng làm cho sự phân phối dữ liệu trong mỗi fold giống nhau hơn, nâng cao hiệu quả của model.\n# stratified k-fold cross validation evaluation of XGBoost model from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import cross_val_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # CV model model = XGBClassifier() kfold = StratifiedKFold(n_splits=10, random_state=7) results = cross_val_score(model, X, Y, cv=kfold) print(\u0026#34;Accuracy: %.2f%%(%.2f%%)\u0026#34; % (results.mean()*100, results.std()*100)) Chạy code trên được output là:\nAccuracy: 73.57% (4.39%) Có thể thấy ràng variance có sự giảm nhẹ trong kết quả.\n3. Lựa chọn phương pháp đánh giá nào?\n Nói chung, k-fold cross-validation là phương pháp tốt nhất cho việc đánh giá hiệu năng của một ML model trong hầu hết mọi trường hợp. Sử dụng stratified cross-validation để đảm bảo sự thống nhất về mặt phân phối dữ liệu khi dữ liệu có nhiều lớp cần dự đoán và bị mất cân bằng giữa các lớp. Sử dụng train-test set trong trường hợp thuật toán train mất nhiều thời gian để hội tụ và số lượng mẫu của dữ liệu rất lớn.  Lời khuyên hợp lý nhất là hãy thử nghiệm nhiều lần và tìm ra phương pháp phù hợp với bài toán của bạn sao cho nhanh nhất có thể. Phương pháp được coi là phù hợp khi nó kết quả đánh giá của nó đáp ứng đúng (hoặc gần đúng) yêu cầu bài toán đặt ra ban đầu. Lời khuyên cuối cùng (từ kinh nghiệm thực tế của tác giả):\n Sử dụng 10-fold cross-validation cho bài toán regression. Sử dụng stratified 10-fold-validation cho bài toán classification.  4. Kết luận\nTrong bài viết này, chúng ta đã cùng tìm 2 phương pháp phổ biến đánh gía hiệu quả của một ML model nói chung, XGBoost mode nói riêng:\n Sử dụng train-test set Sử dụng k-fold cross-validation Ngoài ra, mình cũng đưa vài lời khuyên cho các bạn trong viêc lựa chọn phương pháp nào để áp dụng trong bài toán của các bạn!  Trong bài tiếp theo, chúng ta sẽ tìm hiểu cách thức visualize XGBoost model để hiểu sâu hơn bản chất của nó.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/08_evaluate-xgbosst-models/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 5: Đánh giá hiệu năng của XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"XGBoost là một thuật toán thuộc họ Gradient Boosting. Những ưu điểm vượt trội của nó đã được chứng minh qua các cuộc thi trên kaggle. Dữ liệu đầu vào cho XGBoost model phải ở dạng số. Nếu dữ liệu không ở dạng số thì phải được chuyển qua dạng số (numeric) trước khi đưa vào XGBoost model để train. Có một vài phương pháp để thực hiện việc này, hãy cùng nhau điểm qua trong phần còn lại của bài viết.\nSau khi xem hết bài viết này, bạn sẽ biết:\n Làm thế nào để mã hóa chuỗi (string) đầu ra cho việc phân loại? Làm thế nào để mã hóa dữ liệu đầu vào kiểu \u0026ldquo;categorical\u0026rdquo; sử dụng mã hóa one-hot (one hot encoding). Làm thế nào để tự động xử lý vấn đề thiếu dữ liệu trong dataset (missing data)?  1. Mã hóa chuỗi đầu ra\nChúng ta sẽ sử dụng bài toán phân loại hoa Iris để minh họa cho vấn đề này. Đưa vào các số liệu đo đặc các thành phần của hoa Iris, cần dự đoán hoa Iris đó thuộc loại nào (đầu ra dự đoán của model là các chuỗi ký tự).\nHãy xem ví dụ của dataset:\n5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa XGBoost sẽ không thể mô hình hóa bài toán này bởi vì nó yêu cầu dữ liệu đầu vào phải ở dạng số. Chúng ta có thể dễ dang chuyển đổi từ kiểủ chuổi (string) sang kiểu số (integer) sử dụng lớp LabelEncoder của thư viện sklearn. Ba giá trị đầu ra kiểu string (Iris-setosa, Iris-versicolor, Iris-virginica) sẽ được ánh xạ thành các giá trị số tương ứng (0, 1, 2):\n# encode string class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) Chúng ta cần lưu lại bộ mã hóa này để chuyển đổi ngược lại từ số sang chuỗi trong quá trình dự đoán.\nDưới đây là toàn bộ code minh họa việc việc đọc dataset, mã hóa dữ liệu đầu ra, train XGBoost model và đánh giá độ chính xác của model đó:\n# multiclass classification from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder # load data data = read_csv(\u0026#39;iris.csv\u0026#39;, header=None) dataset = data.values # split data into X and y X = dataset[:,0:4] Y = dataset[:,4] # encode string class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y,test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Chạy đoạn code trên, được kết quả như sau:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, objective='multi:softprob', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 92.00% 2. Mã hóa one-code dữ liệu kiểu Categorical\nRất nhiều bộ dataset chứa các dữ liệu kiểu categorical. Trong phần này, chúng ta sử dụng bộ dataset breast cancer để làm việc. Bộ dataset này miêu tả thông tin của y tế của các bệnh nhân, nhãn của nó chỉ ra bệnh nhân đó có bị ung thư hay không.\nVí dụ của bộ dữ liệu này như bên dưới:\n'40-49','premeno','15-19','0-2','yes','3','right','left_up','no','recurrence-events' '50-59','ge40','15-19','0-2','no','1','right','central','no','no-recurrence-events' '50-59','ge40','35-39','0-2','no','2','left','left_low','no','recurrence-events' '40-49','premeno','35-39','0-2','yes','3','right','left_low','yes','no-recurrence-events' '40-49','premeno','30-34','3-5','yes','2','left','right_up','no','recurrence-events' Chúng ta nhìn thấy rằng tất cả 9 giá trị input đều ở là kiểu categorical và được thể hiện ở dạng string. Đây cũng là bài toán phân lớp nhị phân và nhãn cần dự đoán cũng đang ở dạng string. Vì vậy, ta có thể sử dụng lại cách tiếp cận ở phần trước, chuyển các giá trị dạng string sang integer sử dụng LabelEncoder.\n# encode string input values as integers features = [] for i in range(0, X.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(X[:,i]) features.append(feature) encoded_x = numpy.array(features) encoded_x = encoded_x.reshape(X.shape[0], X.shape[1]) Khi sử dụng LabelEncoder, XGBoost có thể hiểu rằng các giá trị encoded integer của mỗi input feature có mối quan hệ thứ tự. Ví dụ, đối với input feature breast-quad, giá trị left-up được mã hóa là 0, left-low được mã hóa là 1. Điều này thực tế là không đúng trong trường hợp này. Để tránh điều này, ta phải ánh xạ các giá trị integer thành 1 giá trị kiểu binary. Ví dụ, các giá trị của biến đầu vào breast-quad là:\nleft-up left-low right-up right-low central được ánh xạ thành 5 giá trị binary tương ứng:\n1,0,0,0,0 0,1,0,0,0 0,0,1,0,0 0,0,0,1,0 0,0,0,0,1 Điều này được gọi là OneHotEncoder. Ta có thể mã hóa OneHot tất cả các input features kiểu categorical sử dụng lớp OneHotEncoder của thư viện scikit-leaen:\nonehot_encoder = OneHotEncoder(sparse=False, categories=✬auto✬) feature = onehot_encoder.fit_transform(feature) OneHot Encoder cho tất cả input features của dữ liệu:\n# encode string input values as integers columns = [] for i in range(0, X.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(X[:,i]) feature = feature.reshape(X.shape[0], 1) onehot_encoder = OneHotEncoder(sparse=False, categories=✬auto✬) feature = onehot_encoder.fit_transform(feature) columns.append(feature) # collapse columns into array encoded_x = numpy.column_stack(columns) Nếu biết chắc chắn răng một input feature nào đó có mối quan hệ về thứ tự, có thể bỏ qua OneHot Encoder mà chỉ cần LabelEncoder cho feature đó.\nToàn bộ source code của phần này:\n# binary classification, breast cancer dataset, label and one hot encoded from numpy import column_stack from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder # load data data = read_csv(✬datasets-uci-breast-cancer.csv✬, header=None) dataset = data.values # split data into X and y X = dataset[:,0:9] X = X.astype(str) Y = dataset[:,9] # encode string input values as integers columns = [] for i in range(0, X.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(X[:,i]) feature = feature.reshape(X.shape[0], 1) onehot_encoder = OneHotEncoder(sparse=False, categories=✬auto✬) feature = onehot_encoder.fit_transform(feature) columns.append(feature) # collapse columns into array encoded_x = column_stack(columns) print(\u0026#34;X shape: : \u0026#34;, encoded_x.shape) # encode string class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(encoded_x, label_encoded_y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Chạy code trên ta được kết quả:\n('X shape: : ', (286, 43)) XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 68.42% 3. Giải quyết vấn đề missing data\nXGBoost có thể tự động học cách để đưa ra cách giải quyết tốt nhất cho vấn đề missing data. Trên thực tế, XGBoost được thiết kế để làm việc với sparse data, giống như one hot encoded data ở phần trước. Chi tiết hơn về các kỹ thuật xử lý missing data của XGBoost, có thể tham khảo Section 3.4 Sparsity-aware Split Finding trong bài báo XGBoost: A Scalable Tree Boosting System.\nTrong phần này, ta sẽ sử dụng Horse Colic dataset để minh họa khả năng xử lý missing data của XGBoost. Trong dataset này, tỷ lệ mising data tương dối lớn, rơi vào khoảng 30%.\nDễ dàng đọc được dataset này với thư viện Pandas:\ndataframe = read_csv(\u0026#34;horse-colic.csv\u0026#34;, delim_whitespace=True, header=None) dataframe.head() Ta có thể quan sát thấy rằng, missing data được đánh dấu bằng dấu ?.\n 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 0 2 1 530101 38.50 66 28 3 3 ? 2 5 4 4 ? ? ? 3 5 45.00 8.40 ? ? 2 2 11300 0 0 2 1 1 1 534817 39.2 88 20 ? ? 4 1 3 4 2 ? ? ? 4 2 50 85 2 2 3 2 2208 0 0 2 2 2 1 530334 38.30 40 24 1 1 3 1 3 3 1 ? ? ? 1 1 33.00 6.70 ? ? 1 2 0 0 0 1 3 1 9 5290409 39.10 164 84 4 1 6 2 2 4 4 1 2 5.00 3 ? 48.00 7.20 3 5.30 2 1 2208 0 0 1 4 2 1 530255 37.30 104 35 ? ? 6 2 ? ? ? ? ? ? ? ? 74.00 7.40 ? ? 2 2 4300 0 0 2 Thay ? bởi giá trị 0:\n# set missing values to 0 X[X == \u0026#39;?\u0026#39;] = 0 # convert to numeric X = X.astype(\u0026#39;float32\u0026#39;) Bài toán đối với dataset này là Binary Classification, nhãn bao gồm 2 giá trị là 1 và 2. Ta dễ dàng chuyển sang 0 và 1 sử dụng LabelEncoder:\n# encode Y class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) Code đầy đủ:\n# binary classification, missing data from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder # load data dataframe = read_csv(\u0026#34;horse-colic.csv\u0026#34;, delim_whitespace=True, header=None) dataset = dataframe.values # split data into X and y X = dataset[:,0:27] Y = dataset[:,27] # set missing values to 0 X[X == \u0026#39;?\u0026#39;] = 0 # convert to numeric X = X.astype(\u0026#39;float32\u0026#39;) # encode Y class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Output:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 82.83% Hãy kiểm tra khả năng của XGBoost bằng cách thử thay missing value với các giá trị khác nhau:\n Thay missing value bởi 1  X[X == \u0026#39;?`] = 1 Kết quả:\nAccuracy: 81.82%  Thay missing value bởi NaN  X[X == \u0026#39;?`] = 1 Kết quả:\nAccuracy: 83.84%  Thay thế missing value bằng giá trị trung bình (mean) của toàn bộ feature đó  # impute missing values as the mean imputer = SimpleImputer() imputed_x = imputer.fit_transform(X) Source đầy đủ:\n# binary classification, missing data, impute with mean import numpy from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import Imputer # load data dataframe = read_csv(\u0026#34;horse-colic.csv\u0026#34;, delim_whitespace=True, header=None) dataset = dataframe.values # split data into X and y X = dataset[:,0:27] Y = dataset[:,27] # set missing values to NaN X[X == \u0026#39;?\u0026#39;] = numpy.nan # convert to numeric X = X.astype(✬float32✬) # impute missing values as the mean. imputer = Imputer() imputed_x = imputer.fit_transform(X) # encode Y class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(imputed_x, label_encoded_y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Kết quả:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 81.82% Có thể thấý rằng, đối với bài toán này, giải pháp thay thế missing value bằng NaN mang lại kết quả tốt nhất. Trong các bài toán thực tế, chúng ta cũng cần phải thử-sai nhiều cách khác nhau để chọn được phương pháp tối ưu cho bài toán đó.\n4. Kết luận\nTrong bài viết này chúng ta đã cùng nhau tìm hiểu các cách để chuẩn bị dữ liệu cho việc train XGBoost model. Cụ thể:\n Mã hóa dữ liệu kiểu string bằng LabelEncoder Mã hóa dữ liệu kiểu categorical bằng OneHot Encoder Xử lý missing data  Trong bài tiếp theo, chúng ta sẽ tìm hiểu các phương pháp đánh giá hiệu năng của XGBoost model.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/07_data-preparation-for-gradient-boosting/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 4: Chuẩn bị dữ liệu cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"XGBoost là một thuật toán rất mạnh mẽ, tối ưu hóa về tốc độ và hiệu năng cho việc xây dựng các mô hình dự đoán. Một thống kê chỉ ra rằng, hầu hết những người chiến thắng trong các cuộc thi trên Kaggle đều sử dụng thuật toán này. Trong bài viết này, hãy cùng nhau xây dựng một mô hình XGBoost đơn giản để có thể hiểu được cách thức làm việc của nó.\nNội dung bài viết chia thành các phần:\n Cài đặt thư viện XGBoost Chuẩn bị dữ liệu Train XGBoost model Đánh giá XGBoost model Nguồn tham khảo  1. Cài đặt thư viện XGBoost\nCó 2 cách để cài đặt thư viện XGBoost. Sử dụng pip hoặc biên dịch từ mã nguồn:\n1.1 Sử dụng pip để cài đặt:\npip install XGBoost Để cập nhật thư viện, sử dụng lệnh sau:\npip install --upgrade XGBoost 1.2 Biên dịch từ mã nguồn\nSử dụng cách này nếu muốn cài đặt phiên bản mới nhất của XGBoost.\ngit clone --recursive https://github.com/dmlc/XGBoost cd XGBoost cp make/minimum.mk ./config.mk make -j8 cd python-package sudo python setup.py install Tại thời điểm viết bài, phiên bản của XGBoost là 1.2\nChuẩn bị dữ liệu  Trong bài viết này, chúng ta sẽ sử dụng dataset về bênh tiểu đường của Ấn Độ. Dataset bao gồm 8 features, miêu tả chi tiết tình trạng của mỗi bệnh nhân và một feature tương ứng chỉ ra bênh nhân có bị tiểu đường hay không. Chi tiết về dataset này, bạn có thể tham khảo trên UCI Machine Learning Repository website\nĐây là một dataset khá đơn giản bởi vì tất cả các features của nó đều đã ở dạng số và vấn đề chỉ là \u0026ldquo;binary classification\u0026rdquo;.\n6,148,72,35,0,33.6,0.627,50,1 1,85,66,29,0,26.6,0.351,31,0 8,183,64,0,0,23.3,0.672,32,1 1,89,66,23,94,28.1,0.167,21,0 0,137,40,35,168,43.1,2.288,33,1 Tải dataset và đặt nó trong thư mục làm việc hiện tại của bạn với tên là pima-indians-diabetes.csv.\nTiếp theo, load dataset từ file vừa tải về để chuẩn bị cho training và evaluating XGBoost model.\n Import các thư viện sử dụng:  from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score  Load csv file  dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;)  Chia dataset thành dữ liệu input (X) và output (Y)  X = dataset[:, 0:8] y = dataset[:, 8]  Chia X và y thành data training và data testing  Training data được sử dụng để train XGBoost model, trong khi testing data được sử dụng để đánh giá độ chính xác của model đó. Để làm điều này, ta có thể sử dụng hàm train_test_split() trong thư viện scikit-learn.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_seed=42) Đến đây, dữ liệu đã được chuẩn bị sẵn sàng cho việc train XGBoost model.\n2. train XGBoost model\nThư viện XGBoost cung cấp một \u0026ldquo;Wrapper class\u0026rdquo; cho phép sử dụng XGBoost model tương tự như như làm việc với thư viện scikit-learn. XGBoost model trong thư viện XGBoost là XGBClassifier.\nTạo XGBoost model và thực hiện train:\nmodel = XGBClassifier() model.fit(X_train, y_train) Ở đây, chúng ta đang sử dụng giá trị mặc định của các tham số. Mình sẽ có các bài việc về việc *tuning papameters\u0026quot; cho XGBoost model, mời các bạn đón đọc.\nBạn có thể quan sát các tham số sử dụng trong model bằng lệnh sau:\nprint(model) 3. Đánh giá XGBoost model\nĐể sử dụng model đã train để dự đoán, sử dụng hàm model.predict():\npredictions = model.predict(X_test) Ta có thể đánh giá độ chính xác của model bằng cách so sánh kết quả dự đoán của model với kêt quả thực tế. Hàm accuracy_score() giúp chúng ta thực hiện việc này:\naccuracy = accuracy_score(y_test, predictions) print(\u0026#39;Accuracy: %.2f%%\u0026#39; % (accuracy*100)) Kết quả cuối cùng:\nAccuracy: 77.95% Kết quả khá tốt đối với bài toán này.\n5. Tổng kết\nTrong bài viết này, chúng ta đã xây dựng XGBoost model sử dụng thư viện XGBoost. Cụ thể, chúng ta đã học:\n Cách cài đặt thư viện XGBoost Chuẩn bị dữ liệu train model Đánh giá model  Trong bài tiếp theo, chúng ta sẽ bàn luận về một số phương pháp chuẩn bị dữ liệu train cho XGBoost model.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/06_build-xgboost-model/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 3: Xây dựng XGBoost model"},{"categories":["Machine Learning","Ensemble Learning","XGBoost"],"contents":"Tiếp tục phần 2 của loạt bài tìm hiểu toàn cảnh về Ensemble Learning, trong phần này ta sẽ đi qua một số thuât toán thuộc nhóm Bagging và Boosting.\n Các thuật toán thuộc nhóm Bagging bao gồm:  Bagging meta-estimator Random forest   Các thuật toán thuộc họ Boosting bao gồm:  AdaBoost Gradient Boosting (GBM) XGBoost (XGBM) Light GBM CatBoost    Để minh họa cho các thuật toán kể trên, mình sẽ sử dụng bộ dữ liệu Loan Prediction Problem.\n1. Bagging techniques\n1.1 Bagging meta-estimator\nBagging meta-estimator là thuật toán sử dụng cho cả 2 loại bài toán classification (BaggingClassifier) và regression (BaggingRegressor).\nCác bước thực hiện của thuật toán như sau:\n Bước 1: Tạo ngẫu nhiên các N bags từ tập train set. Bước 2: Tạo N objects của lớp BaggingClassifier và train trên mỗi bag, độc lập với nhau. Bước 3: Sử dụng các objects đã trained để dự đoán trên tập test set.  Code cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.ensemble import BaggingClassifier from sklearn import tree from sklearn.preprocessing import LabelEncoder #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1)) model.fit(x_train, y_train) accuracy = model.score(x_test,y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Kết quả:\nAccuracy: 77.83% Đối với bài toán regression, thay BaggingClassifier bằng BaggingRegressor.\nMột số tham số:\n base_estimator: Định nghĩa thuật toán mà base model sử dụng. Mặc định là decision tree. n_estimators: Định nghĩa số lượng base models. Mặc định là 10. max_samples: Định nghĩa số lượng mẫu data tối đa trong mỗi bag. Mặc định là 1. max_features: Định nghĩa số lượng features tối đa sử dụng trong mỗi bag. Mặc định là 1. n_jobs: Số lượng jobs chạy song song cho cả quá trình train và predict. Mặc định là 1. Nếu giá trị bằng -1 thì số jobs bằng số cores của hệ thống. random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi BaggingClassifier thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh các models với nhau.  1.2 Random Forest\nCác thức hoạt động của Random Forest gần giống Bagging meta-estimator, chỉ khác một điều duy nhất là tại mỗi node của tree trong Decision Tree, nó tạo ra một tập ngẫu nhiên các features và sử dụng tập này đê chọn hướng đi tiếp theo. Trong khi đó, Bagging meta-estimator sử dụng tất cả features để chọn đường.\nCode ví dụ:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = RandomForestClassifier() model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Output:\nAccuracy: 79.86% Một số tham số:\n n_estimators: Số lượng decition trees (base models). Mặc định là 100 (đối với phiên bản scikit-learn từ 0.22) và 10 (đối với phiên bản \u0026lt; 0.22). criterion: Chỉ ra hàm được sử dụng để quyết định hướng đi tại mỗi node của tree. Tham số này có thể nhận 1 trong 2 giá trị {\u0026ldquo;gini\u0026rdquo;, \u0026ldquo;entropy\u0026rdquo;}. Giá trị mặc định là \u0026ldquo;gini\u0026rdquo;. max_features: Số lượng features được sử dụng tại mỗi node để tìm đường đi tiếp theo. Một số giá trị thường được sử dụng là:  auto/sqrt: max_features = sqrt(n_features). Đây là giá trị mặc định. log2: max_features = log2(n_features). None: max_features = n_features.   max_depth: Độ sâu của mỗi tree. Mặc định, các nodes sẽ được mở rộng tận khi tất cả các leaves chứa ít hơn min_samples_split mẫu (samples). min_sample_split: Số lượng mẫu tối thiểu tại mỗi leaf node để có thể tiếp tục mở rộng tree. Giá trị mặc định là 2. min_samples_leaf: Số lượng mẫu tối thiểu tại mỗi leaf node. Mặc định là 1. max_leaf_nodes: Số lượng leaf node tối đa của mỗi tree. Giá trị mặc định là không có giới hạn số lượng. n_jobs: Số lượng jobs chạy song song. Mặc định là 1. Gán giá trị -1 để sử dụng tất cả các cores của hệ thống. random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi RandomForestClassifier thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh các models với nhau.  2. Boosting techniques\n2.1 AdaBoost\nAdaBoost là thuật toán đơn giản nhất trong họ Boosting, nó cũng thường sử dụng decision tree để làm base model.\nThuật toán thực hiện như sau:\n Bước 1: Ban đầu, tất cả các mẫu dữ liệu được gán cho cùng một giá trị trọng số (weight). Bước 2: Lựa chọn ngẫu nhiên một tập dữ liệu con (tập S) từ tập dữ liệu ban đầu (tập D) và train decition tree model trên tập dữ liệu con này. Bước 3: Sử dụng model đã trained, tiến hành dự đoán trên toàn tập D. Bước 4: Tính toán lỗi (error) bằng cách so sánh giá trị dự đoán và giá trị thực tế. Bước 5: Gán giá trị weight cao hơn cho những mẫu dữ liệu có error cao hơn. Bước 6: Lặp lại bước 2,3,4,5 đến khi error không đổi hoặc số lượng tốí đa của weak learner đạt được.  Code mẫu cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn import tree from sklearn.ensemble import AdaBoostClassifier from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = AdaBoostClassifier(random_state=1) model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Kết quả:\nAccuracy: 72.22% Đối với bài toán regression, thay AdaBoostClassifier bằng AdaBoostRegressor.\nMột vài tham số quan trọng:\n base_estimator: Chỉ ra weak learner là gì. Mặc định sử dụng decition tree. n_estimators: Số lượng của weak learners. Mặc định là 50. learning_rate: Điều chỉnh mức độ đóng góp của mỗi weak learner đến kết quả cuối cùng. random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi AdaBoostClassifier thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh các models với nhau.  2.2 Gradient Boosting (GBM)\nĐể giúp mọi người dễ hình dung, mình sẽ trình bày ý tưởng của GBM thông qua ví dụ sau:\nCho bảng dữ liệu bên dưới:\n   ID Married Gender City Monthly Income Age (target)     1 Y F Hanoi 51.000 35   2 N M HCM 25.000 24   3 Y F Hanoi 70.000 38   4 Y M HCM 53.000 30   5 N M Hanoi 47.000 33    Bài toán đạt ra là cần dự đoán tuổi dựa trên các input features: Tình trạng hôn nhân, giới tính, thành phố sinh sống, thu nhập hàng tháng.\n Bước 1: Train decition tree model thứ nhất trên tập dữ liệu bên trên. Bước 2: Tính toán lỗi dựa theo sai số giữa giá trị thưc tế và giá trị dự đoán.     ID Married Gender City Monthly Income Age (target) Age (prediction 1) Error 1     1 Y F Hanoi 51.000 35 32 3   2 N M HCM 25.000 24 32 -8   3 Y F Hanoi 70.000 38 32 6   4 Y M HCM 53.000 30 32 -2   5 N M Hanoi 47.000 33 32 1     Bước 3: Một decition tree model thứ 2 được tạo, sử dụng cùng input features với model trước đó, nhưng target là Error 1. Bước 4: Giá trị dự đoán của model thứ 2 được cộng với giá trị dự đoán của model thứ nhất.     ID Age (target) Age (prediction 1) Error 1 (new target) Prediction 2 Combine (Pred1+Pred2)     1 35 32 3 3 35   2 24 32 -8 -5 27   3 38 32 6 3 35   4 30 32 -2 -5 27   5 33 32 1 3 35     Bước 5: Giá trị kết hợp bở bước 3 coi như là giá trị dự đoán mới. Ta tính lỗi (Error 2) dựa trên sai số giữa giá trị này và giá trị thực teses.     ID Age (target) Age (prediction 1) Error 1 (new target) Prediction 2 Combine (Pred1+Pred2) Error 2     1 35 32 3 3 35 0   2 24 32 -8 -5 27 -3   3 38 32 6 3 35 3   4 30 32 -2 -5 27 3   5 33 32 1 3 35 -3     Bước 6: Lặp lại bước 2-5 ho đến khi số lượng weak learner đạt được hoặc giá trị lỗi không đổi.  Code ví dụ cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree from sklearn.ensemble import GradientBoostingClassifier from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = GradientBoostingClassifier(learning_rate=0.01,random_state=1) model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Output:\nAccuracy: 78.47% Đối với bài toán regression, thay GradientBoostingClassifier thành GradientBoostingRegressor.\nMột số tham số quan trọng:\n min_sample_split: Số lượng mẫu tối thiểu tại mỗi leaf node để có thể tiếp tục mở rộng tree. Giá trị mặc định là 2. min_samples_leaf: Số lượng mẫu tối thiểu tại mỗi leaf node. Mặc định là 1. max_depth: Độ sâu của mỗi tree. Nên xem xét tham số này khi tuning model. Giá trị mặc định là 3. max_features: Số lượng tối đa features xem xét khi tìm đường mở rộng tree. Những features này được chọn ngẫu nhiên.  2.3 XGBoost\nXGBoost (extreme Gradient Boosting) là phiên bản cải tiến của Gradient Boosting. Ưu điểm vượt trội của nó được chứng minh ở các khía cạnh:\n  Tốc độ xử lý\n XGBoost thực hiện tinh toán song song nên tốc độ xử lý có thể tăng gấp 10 lần so với GBM. Ngoài ra, XGboost còn hỗ trợ tính toán trên Hadoop.    Overfitting\n XGBoost áp dụng cơ chế Regularization nên hạn chế đáng kể hiệ tượng Overfitting (GBM không có regularization).    Sự linh hoạt\n XGboost cho phép người dùng sử dụng hàm tối ưu và chỉ tiêu đánh giá của riêng họ, không hạn chế ở những hàm cung cấp sẵn.    Xử lý missing value\n XGBoost bao gồm cơ chế tự động xử lý missing value bên trong nó. Vì thế, có thể bỏ qua bước này khi chuẩn bị dữ liệu cho XGBoost.    Tự động cắt tỉa\n Tính năng tree pruning hộ trợ việc tự động bỏ qua những leaves, nodes không mang giá trị tích cực trong quá trình mở rộng tree.    Chính vì những ưu điểm đó mà hiệu năng của XGBoost tăng lên đáng kể so với các thuật toán ensemble learning khác. Nó được sử dụng ở hầu hết các cuộc thi trên Kaggle cũng như Hackathons.\nCode ví dụ cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree import xgboost as xgb from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = xgb.XGBClassifier(random_state=1, eta=0.01) model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Kết quả:\nAccuracy: 82% Đối với vài toán regression, sử dụng XGBRegressor thay vì XGBClassifier.\nMột số tham số quan trọng:\n n_thread: Số lượng cores của hê thống được sử dụng để chạy model. Giá trị mặc định là -1, XGBoost sẽ tự động phát hiện và sử dụng tất cả các cores. eta: Tương tự learning_rate trong GBM. Giá trị mặc định là 0.3. max_depth: Độ sâu tối đa của decision tree. Giá trị mặc định là 6. colsample_bytree: Tương tự max_features của GBM. lambda: L2 regularization. Giá trị mặc định là 1. alpha: L1 regularization. Giá trị mặc định là 0.  2.4 Light GBM\nTại sao chúng ta vẫn cần thuật toán này khi mà ta đã có XGBoost rất mạnh mẽ rồi?\nSự khác nhau nằm ở kích thước của dữ liệu huấn luyện. Light GBM đánh bại tất cả các thuật toán khác khi tập dataset có kích thước cực lớn. Thực tế chứng minh, nó cần ít thời gian đê xử lý hơn trên tập dữ liệu này (Có lẽ vì thế mà có chứ light - ánh sáng). Nguyên nhân sâu xa của sự khác biệt này nằm ở cơ chế làm viêc của Light GBM. Trong khi các thuật toán khác sử dụng cơ chế level-wise thì nó lại sử dụng leaf-wise.\nHình dưới đây minh họa sự khác nhau giữa 2 cơ chế level-wise và leaf-wise:\n   Như chúng ta thấy, leaf-wise chỉ mở rộng tree theo 1 trong 2 hướng so với cả 2 hướng của level-wise, tức là số lượng tính toán của Light GBM chỉ bằng 1/2 so với XGBoost.\nCode ví dụ cho bài toán classifier:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree import lightgbm as lgb from sklearn.preprocessing import LabelEncoder #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = lgb.LGBMClassifier() model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Trong trường hợp regression, sử dụng LGBMRegressor thay cho LGBMClassifier.\nMột số tham số quan trọng:\n num_leaves: Số lượng leaves tối đa trên mỗi node. Giá trị mặc định là 31 max_depth: Độ sâu tối đa của mỗi tree. Mặc định là không có giới hạn. learing_rate: learning rate của mỗi tree. Mặc định là 0.1. n_estimators: Số lượng weak learners. Mặc định là 100. n_jobs: Số lượng cores của hê thống được sử dụng để chạy model. Giá trị mặc định là -1, XGBoost sẽ tự động phát hiện và sử dụng tất cả các cores.  2.5 CatBoost\nKhi làm việc với tập dữ liệu mà có số lượng lớn input features kiểu categorical, nếu chúng ta áp dụng one-hot encoding thì số chiều dữ liệu sẽ tăng lên rất nhanh (theo hàm mũ e).\nCatBoost ra đời chính là để gánh vác sứ mệnh giải quyết những bài toán như vậy (CatBoost = Categories + Boosting). Khi làm việc với CatBoost, chúng ta không cần thực hiện one-hot encoding.\nCode ví dụ cho classification:\n# importing required libraries import pandas as pd import numpy as np from catboost import CatBoostClassifier from sklearn.metrics import accuracy_score # read the train and test dataset train_data = pd.read_csv(\u0026#39;train-data.csv\u0026#39;) test_data = pd.read_csv(\u0026#39;test-data.csv\u0026#39;) # Now, we have used a dataset which has more categorical variables # hr-employee attrition data where target variable is Attrition  # seperate the independent and target variable on training data train_x = train_data.drop(columns=[\u0026#39;Attrition\u0026#39;],axis=1) train_y = train_data[\u0026#39;Attrition\u0026#39;] # seperate the independent and target variable on testing data test_x = test_data.drop(columns=[\u0026#39;Attrition\u0026#39;],axis=1) test_y = test_data[\u0026#39;Attrition\u0026#39;] # find out the indices of categorical variables categorical_var = np.where(train_x.dtypes != np.float)[0] model = CatBoostClassifier(iterations=50) # fit the model with the training data model.fit(train_x,train_y,cat_features = categorical_var,plot=False) # predict the target on the train dataset predict_train = model.predict(train_x) # Accuray Score on train dataset accuracy_train = accuracy_score(train_y,predict_train) print(\u0026#39;\\naccuracy_score on train dataset : {:.2f}%\u0026#39;.format(accuracy_train*100)) # predict the target on the test dataset predict_test = model.predict(test_x) # Accuracy Score on test dataset accuracy_test = accuracy_score(test_y,predict_test) print(\u0026#39;\\naccuracy_score on test dataset : {:.2f}%\u0026#39;.format(accuracy_test*100)) Kết quả:\naccuracy_score on train dataset : 91.41% accuracy_score on test dataset : 86.05% Thay CatBoostRegressor cho CatBoostClassifier trong bài toán regression.\nMột số tham số quan trọng:\n loss_function: Định nghĩa loss_function sử dụng để training model. iterations: Số lượng weak learner. learning_rate: Learning rate của mỗi tree. depth: Độ sâu của mỗi tree.  3. Kết luận\nChúng ta đã cùng nhau đi qua 2 phần khá dài để tìm hiểu về Ensemble Learning. Rất nhiều khía cạnh đã được bàn bạc và kèm theo code ví dụ. Hi vọng các bạn đã có cái nhìn rõ hơn về Ensemble Learning. Trong các bài tiếp theo, mình sẽ đi sâu hơn về XGBoost, một thuật toán mạnh mẽ, chiến thắng trong hầu như mọi cuộc thi Kaggle. Hãy đón đọc!\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/05_comprehensive_guide_to_ensemble_model_2/","tags":["Machine Learning","Ensemble Learning","XGBoost"],"title":"XGBoost - Bài 2: Toàn cảnh về Ensemble Learning - Phần 2"},{"categories":["Machine Learning","Ensemble Learning","XGBoost"],"contents":"1. Giới thiệu về Ensemble Learning\nGiả sử chúng ta có một bài toán phân loại sản phẩm sử dụng ML. Team của bạn chia thành 3 nhóm, mỗi nhóm sử dụng một thuật toán khác nhau và đánh giá độ chính xác trên tập validation set:\n Nhóm 1: Sử dụng thuật toán Linear Regression. Nhóm 2: Sử dụng thuật toán k-Nearest Neighbour. Nhóm 3: Sử dụng thuật toán Decision Tree. Độ chính xác của mỗi nhóm lần lượt là 70%, 67% và 76%. Điều này hoàn toàn dễ hiểu bởi vì 3 models làm việc theo những các khác nhau. Ví dụ, Linear Regression cố gắng tìm ra mối quan hệ tuyến tính giữa các điểm dữ liệu, trong khi Decision Tree thì lại dựa vào mỗi quan hệ phi tuyến để liên kết dữ liệu.  Có cách nào kết hợp kết quả cả 3 models để tạo ra kết quả cuối cùng hay không?\n Câu hỏi này là tiền đề cho một phương pháp, một họ các thuật toán hoạt động rất hiệu quả trong các bài toán ML. Đó là Ensemble Learning hay Ensemble Models.\nHình dưới đây thể hiện bức tranh tổng quát về Ensemble Learning.\n 2. Basic Ensemble Techniques\nỞ mức độ cơ bản, có 3 kỹ thuật là:\n Max Voting Averaging Weighted Averaging Mặc dù đơn giản nhưng những kỹ thuật này lại tỏ ra hiệu quả trong một số trường hợp nhất định. Hãy cùng tìm hiểu kỹ hơn về chúng.  2.1 Max Voting\nKỹ thuật này hay được sử dụng cho bài toán phân lớp, ở đó, nhiều models được sử dụng để dự đoán cho mỗi mẫu dữ liệu. Kết quả dự đoán của mỗi model được xem như là một vote. Cái nào có số vote cao nhất thì sẽ là kết quả dự đoán cuối cùng. Nói cách khác, đây là kiểu bầu chọn theo số đông, được áp dụng rất nhiều trong cuộc sống, chính trị, \u0026hellip;\nLấy ví dụ, đợt vừa rồi, công ty của bạn tổ chức khám sức khỏe cho nhân viên tại bệnh viện X. Sau khi khám xong, phòng tổ chức nhân sự (TCNS) lấy ý kiến mọi người về chất lượng khám bệnh để xem năm sau có tiếp tục khám ở bênh viên X đó nữa không. Bảng dưới là ý kiến của 5 người được chọn ngẫu nhiên trong số toàn bộ nhân viên.\n   Người 1 Người 2 Người 3 Người 4 Người 5     Có Không Không Có Có    Có 3 ý kiến muốn tiêp tục khám ở bệnh viện X vào năm sau, và 2 ý kiến muốn đổi bênh viện khác. Căn cứ theo max voting thì phòng TCNS sẽ tiếp tục chọn bệnh viên Xlà nơi khám bệnh cho nhân viên cho năm tiếp theo.\nCode minh họa:\nx_train, y_train, x_test, y_test = get_data() model_1 = DecisionTreeClassifier() model_2 = KNeighborsClassifier() model_3= LogisticRegression() model_1.fit(x_train,y_train) model_2.fit(x_train,y_train) model_3.fit(x_train,y_train) pred_1=model_1.predict(x_test) pred_2=model_2.predict(x_test) pred_3=model_3.predict(x_test) final_pred = np.array([]) for i in range(0,len(x_test)): final_pred = np.append(final_pred, mode([pred_1[i], pred_2[i], pred_3[i]])) Thư viện scikit-learn có module VotingClassifier giúp chúng ta đơn giản hóa việc này:\nfrom sklearn.ensemble import VotingClassifier x_train, y_train, x_test, y_test = get_data() model_1 = LogisticRegression(random_state=1) model_2 = DecisionTreeClassifier(random_state=1) model = VotingClassifier(estimators=[(\u0026#39;lr\u0026#39;, model_1), (\u0026#39;dt\u0026#39;, model_2)], voting=\u0026#39;hard\u0026#39;) model.fit(x_train,y_train) model.score(x_test,y_test) 2.2 Averaging\nTương tự như kỹ thuật Voting, Averaging cũng sử dụng kết quả dự đoán của nhiều models. Tuy nhiên, ở bước quyết định kết quả cuối cùng, giá trị trung bình của tất cả kêt quả của các models được lựa chọn.\n Tiếp tục với ví dụ ở trên, một đề nghị khác của phòng TCNS là yêu cầu nhân viên chấm điểm chất lượng khám bệnh của bênh viện X, theo thang điểm từ 1 đến 5.\nBảng kết quả trả lời của 5 người ngẫu nhiên:\n   Người 1 Người 2 Người 3 Người 4 Người 5     2 4 3 5 4    Điểm đánh giá cuối cùng sẽ là: (2+4+3+5+4)/5 = 3.6\nCode ví dụ:\nx_train, y_train, x_test, y_test = get_data() model_1 = tree.DecisionTreeClassifier() model_2 = KNeighborsClassifier() model_3 = LogisticRegression() model_1.fit(x_train,y_train) model_2.fit(x_train,y_train) model_3.fit(x_train,y_train) pred_1 = model1.predict_proba(x_test) pred_2 = model2.predict_proba(x_test) pred_3 = model3.predict_proba(x_test) final_pred =(pred1+pred2+pred3)/3 2.3 Weighted Average\nĐây là kỹ thuật mở rộng của averaging. Mỗi model được gắn kèm với một trọng số tỷ lệ với mức độ quan trọng của model đó. Kết quả cuối cùng là trung bình có trọng số của tất cả kết quả của các models.\n Vẫn với ví dụ ở mục 2.2, nhưng trong số 5 người được hỏi thì người thứ nhất có vợ là bác sĩ, người thứ 2 có mẹ là y tá, người thứ 3 có người yêu là sinh viên trường y. Vì vậy, ý kiến của 3 người này rõ ràng có giá trị hơn so với 2 người còn lại. Ta đánh trọng số cho mỗi người như bảng dưới (hàng thứ 2 là trọng số, hàng thứ 3 là điểm đánh giá):\n   Người 1 Người 2 Người 3 Người 4 Người 5     1 0.8 0.5 0.3 0.3   2 4 3 5 4    Điểm đánh giá cuối cùng sẽ là: (21 + 40.8 + 30.5 + 50.3 + 4*0.3)/5 = 1.88\nCode minh họa:\nx_train, y_train, x_test, y_test = get_data() model_1 = DecisionTreeClassifier() model_2 = KNeighborsClassifier() model_3 = LogisticRegression() model_1.fit(x_train,y_train) model_2.fit(x_train,y_train) model_3.fit(x_train,y_train) pred_1 = model1.predict_proba(x_test) pred_2 = model2.predict_proba(x_test) pred_3 = model3.predict_proba(x_test) final_pred=(pred_1*0.3 + pred_2*0.3 + pred_3*0.4) 3. Advanced Ensemble techniques\nĐã có basic thì chắc chắn phải có advanced, phải không mọi người. :D\nCó 4 kỹ thuật của Ensemble Learning được xếp vào nhóm advanced:\n Stacking Blending Bagging Boosting  Chúng ta tiếp tục đi qua lần lượt từng kỹ thuật này:\n3.1 Stacking\nHãy xem các bước thực hiện của kỹ thuật này:\n Bước 1: Train model A (base model) theo kiểu cross-validation với k=10. Bước 2: Tiếp tuc train model A trên toàn bộ train set. Bước 3: Sử dụng model A để dự đoán trên test set. Bước 4: Lặp lại bước 1,2,3 cho các base model khác. Bước 5:  Kết quả dự đoán trên train set của các base models được sử dụng như là input features (ensemble train set) để train stacking model. Kết quả dự đoán trên test set của các base models được sử dụng như là test set (ensemble test set) của stacking model.   Bước 6: Train và đánh giá stacking model sử dụng ensemble train set và ensemble test set.  Code minh họa ý tưởng:\n# We first define a function to make predictions on n-folds of train and test dataset. This function returns the predictions for train and test for each model. def Stacking(model,train,y,test,n_fold): folds=StratifiedKFold(n_splits=n_fold,random_state=1) test_pred = np.empty((test.shape[0],1),float) train_pred = np.empty((0,1),float) for train_indices,val_indices in folds.split(train,y.values): x_train,x_val = train.iloc[train_indices],train.iloc[val_indices] y_train,y_val = y.iloc[train_indices],y.iloc[val_indices] model.fit(X=x_train,y=y_train) train_pred = np.append(train_pred,model.predict(x_val)) test_pred = np.append(test_pred,model.predict(test)) return test_pred.reshape(-1,1),train_pred # Now we’ll create two base models – decision tree and knn. model_1 = DecisionTreeClassifier(random_state=1) test_pred_1 ,train_pred_1 = Stacking(model=model_1, n_fold=10, train=x_train, test=x_test, y=y_train) train_pred_1 = pd.DataFrame(train_pred_1) test_pred_1 = pd.DataFrame(test_pred_1) model_2 = KNeighborsClassifier() test_pred_2, train_pred_2 = Stacking(model=model_2, n_fold=10, train=x_train,test=x_test, y=y_train) train_pred_2 = pd.DataFrame(train_pred_2) test_pred_2 = pd.DataFrame(test_pred_2) # Create a final model, logistic regression, on the predictions of the decision tree and knn models. df = pd.concat([train_pred_1, train_pred_2], axis=1) df_test = pd.concat([test_pred_1, test_pred_2], axis=1) model = LogisticRegression(random_state=1) model.fit(df,y_train) model.score(df_test, y_test) Đoạn code trên chỉ minh họa stack model với 2 levels. Decision Tree và kNN là level 0, còn Logistic Regression là level 1. Bạn hoàn toàn có thể thử nghiệm với nhiều levels hơn.\n3.2 Blending\nCác bước thực hiện phương pháp này như sau:\n Buớc 1: Chia dataset thành train set, validation set và test set. Bước 2: Base model được train trên train set. Bước 3: Sử dụng base model để dự đoán trên validation set và test set. Bước 4: Lặp lại bước 2,3 cho các base models khác. Bước 5:  Validation set và các kết quả dự đoán trên validation set của các base models được sử dụng như là input features (ensemble train set) của blending model. Test set và các kết quả dự đoán trên test set của các base models được sử dụng như là test set (ensemble test set) của blending model.   Bước 6: Train và đánh giá blending model sử dụng ensemble train set và ensemble test set.  Code minh họa ý tưởng:\n# build two models, decision tree and knn, on the train set in order to make predictions on the validation set. model_1 = DecisionTreeClassifier() model_1.fit(x_train, y_train) val_pred_1 = model1.predict(x_val) test_pred_1 = model1.predict(x_test) val_pred_1 = pd.DataFrame(val_pred_1) test_pred_1 = pd.DataFrame(test_pred_1) model_2 = KNeighborsClassifier() model_2.fit(x_train, y_train) val_pred_2 = model_2.predict(x_val) test_pred_2 = model2.predict(x_test) val_pred_2 = pd.DataFrame(val_pred_2) test_pred_2 = pd.DataFrame(test_pred_2) # Combining the meta-features and the validation set, a logistic regression model is built to make predictions on the test set. df_val = pd.concat([x_val, val_pred_1,val_pred_2], axis=1) df_test = pd.concat([x_test, test_pred1,test_pred_2], axis=1) model = LogisticRegression() model.fit(df_val, y_val) model.score(df_test, y_test) 3.3 Bagging\nBagging (Bootstrap Aggregating) khác với hai kỹ thuật trên ở chỗ, nó sử dụng chung 1 thuật toán cho tất cả các base models. Tập dataset sẽ được chia thành các phần khác nhau (bags) và mỗi base model sẽ được train trên mỗi bag đó.\nCác bước thực hiện của bagging như sau:\n Bước 1: Chia tập dữ liệu ban đầu thành nhiều phần khác nhau (bags). Bước 2: Tạo các base models (weak learner) và train chúng trên các bags. Các base model được train song song và độc lập với nhau. Bước 3: Kết quả dự đoán cuối cùng được quyết định bằng cách kết hợp kết quả từ các base models.   3.4 Boosting\nNếu như các base models được train độc lập với nhau trong phương pháp bagging, thì ở phương pháp boosting, chúng lại được train một cách tuần tự. Base model sau được train dựa theo kết quả của base model trước đó để cố gắng sửa những lỗi sai tồn tại ở model này.\nCác bước tiến hành như sau:\n Bước 1: Tạo một tập dữ liệu con (tập A) từ tập dữ liệu ban đầu (tập D). Bước 2: Gán cho mỗi điểm dữ liệu trong tập A một trọng số w có giá trị giống nhau. Bước 3: Tạo một base model X và train trên tập A. Bước 4: Sử dụng model X để dự đoán trên toàn bộ tập D. Bước 5: Tính toán sai số dự đoán dựa vào kết quả dự đoán và kết quả thực tế. Bước 6: Gán giá trị w cao hơn cho những điểm dữ liệu bị dự đoán sai. Bước 7: Lặp lại bước 1,2,3,4,5,6 đối với base model mới, Y. Bước 8: Model cuối cùng (boosting model) sẽ là trung bình có trọng số của tất cả các base models.  Mỗi base model được gọi là một weak learner. Chúng sẽ không hoạt động tốt trên toàn bộ tập D, nhưng khi kết hợp nhiều weak learners ta được một strong learner. Strong learner này chắc chắn sẽ hiệu quả trên tập D. Ta nói, các weak learners đã boost performance cho strong learner.\nBagging và Boosting là 2 kỹ thuật quan trọng, hiệu quả. Có một số thuật toán đã được phát triển dựa trên nền tảng của chúng. Đặc biệt là thuật toán XGBoost. Trong bài tiếp theo, chúng ta sẽ đi chi tiết hơn về các thuật toán này.\nMời các bạn đón đọc!\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/04_comprehensive_guide_to_ensemble_model_1/","tags":["Machine Learning","Ensemble Learning","XGBoost"],"title":"XGBoost - Bài 1: Toàn cảnh về Ensemble Learning - Phần 1"},{"categories":["Machine Learning","Ensemble Learning","XGBoost"],"contents":"XGBoost là một thuật toán rất được quan tâm gần đây vì những ưu điểm vượt trội của nó so với các thuật toán khác. Vì vậy, mình quyết định sẽ viết một chuỗi các bài về chủ để này.\nNội dung các bài viết sẽ chủ yếu tập trung vào code thực hành, sẽ có (ít) lý thuyết toán để các bạn đỡ bị đau đầu. :D\nDanh sách các bài viết (sẽ cập nhật dần dần):\n Bài 1: Toàn cảnh về Ensemble models - Phần 1 Bài 2: Toàn cảnh về Ensemble models - Phần 2 Bài 3: Xây dựng model XGBoost đầu tiên Bài 4: Chuẩn bị dữ liệu cho XGBoost model Bài 5: Các phương pháp đánh giá độ chính xác của XGBoost model Bài 6: Trực quan hóa XGBoost model Bài 7: Lưu và sử dụng XGBoost model Bài 8: Lựa chọn Features cho XGBoost model Bài 9: Cấu hình Early_Stopping cho XGBoost model Bài 10: Cấu hình Multi-Threading cho XGBoost model Bài 11: Train XGBoost model trên AWS Bài 12: Tuning số lượng và kích thước của Decision Tree Bài 13: Tuning Learning_Rate và số lượng của Decision Tree Bài 14: Tuning Subsample  ","permalink":"https://tiensu.github.io/blog/03_xgboost-model-serial-introduction/","tags":["Machine Learning","Ensemble Learning","XGBoost"],"title":"XGBoost - Giới thiệu chuỗi bài viết về thuật toán XGBoost"},{"categories":["Machine Learning"],"contents":"Bài viết này nhằm mục đích tổng hợp, tóm tắt lại các thuật toán của Machine Learning, giúp bạn đọc có cái nhìn toàn cảnh và hiểu rõ hơn về Deep Learning.\nCác thuật toán ML, nhìn chung có thể phân loại theo một trong 2 cách:\n Theo cách thức \u0026ldquo;học\u0026rdquo; của thuật toán Theo cách thức làm việc của thuật toán  Cả 2 cách phân loại đều hợp lý, bạn có thể chọn tùy ý. Trong bài bài này, mình sẽ đi sâu hơn theo cách thứ 2. Cũng phải nói thêm rằng, cho dù phân loại theo cách nào thì cũng đều mang tính chất tương đối, vì một thuật toán có thể thuộc nhiều nhóm khác nhau, tùy thuộc vào dữ liệu đưa vào huấn luyện model.\n 1. Phân loại theo cách \u0026ldquo;học\u0026rdquo; (Learning Style)\n1.1 Học có giám sát (Supervised Learning)\nTrong cách học này, dữ liệu đưa vào huấn luyện model, gọi là input data, đi kèm với một nhãn đã biết trước (input data đã được dánh nhãn). Ví dụ như là spam/not-spam, giá cổ phiếu tại 1 thời điểm, \u0026hellip;\nTrong quá trình training, output của model được so sánh với nhãn. Nếu có sự sai khác, model sẽ cố gắng cập nhật các trọng số của nó để giảm sự sai khác đó đến một mức nào đó thỏa mãn yêu cầu bài toán.\nCác vấn đề có thể giải quyết theo cách này: Phân lớn, hồi quy.\nMột số thuật toán thuộc loại này: Logistic Regression, Backpropagation, \u0026hellip;\n 1.2 Học không giám sát (Unsupervised Learning)\nInput data không được đánh nhãn theo cách học này. Model được huấn luyện bằng cách giảm cấu trúc phức tạp của dữ liệu, tìm ra các đặc trưng, các mối liên hệ tương quan trong dữ liệu.\nCác vấn đề có thể giải quyết theo cách này: phân cụm, giảm chiều dữ liệu.\nMột số thuật toán thuộc loại này: Apriori, K-Means, \u0026hellip;\n 1.3 Học bán giám sát (Semi-supervised)\nInput data bao gồm cả 2 loại: đã đánh nhãn và không đánh nhãn.\nModel sẽ sử dụng kết hợp cả 2 cách học giám sát và không giám sát trong quá trình huấn luyện. Dựa vào kết quả dự đoán của model trên dữ liệu chưa đánh nhãn, nhà phát triển sẽ tốn ít công sức hơn trong việc đánh nhãn cho những dữ liệu đó. Độ chính xác của model sẽ được cải thiện dần dần khi có nhiều dữ liệu được đánh nhãn hơn.\nThực tế, tất cả các thuật toán đều có thể thuộc thể loại này vì không phải lúc nào cũng có đầy đủ dữ liệu được đánh nhãn ngay từ đầu.\n 2. Phân loại theo cách làm việc\n2.1 Regression Algorithms\nCác thùât toán được xếp vào nhóm này khi nhãn của dữ liệu là các giá trị liên tục. Ví dụ: nhiệt độ, giá tiền, diện tích, \u0026hellip;\nMột số thuật toán:\n Ordinary Least Squares Regression (OLSR) Linear Regression Logistic Regression Stepwise Regression Multivariate Adaptive Regression Splines (MARS) Locally Estimated Scatterplot Smoothing (LOESS)   2.2 Classification Algorithms\nCác thuật toán thụộc nhóm này khi nhãn của dữ liệu chỉ bao gồm một số lượng hữu hạn các giá trị. Ví dụ: Spam/not-spam, hình dạng (tròn, vuông, tam giác), \u0026hellip;\nMột số thuật toán:\n Linear Classifier Support Vector Machine (SVM) Kernel SVM Sparse Representation-based classification (SRC)   2.3 Instance-based Algorithms\nCác thuật toán thuộc nhóm này không \u0026ldquo;học\u0026rdquo; gì từ dữ liệu. Khi nào cần dự đoán nhãn cho dữ liệu mới, chúng sẽ quét toàn bộ dữ liệu ban đầu và tính toán tương quan với dữ liệu mới để quyết định nhãn.\nMột số thuật toán:\n k-Nearest Neighbor (kNN) Learning Vector Quantization (LVQ) Self-Organizing Map (SOM) Locally Weighted Learning (LWL)   2.4 Regularization Algorithms\nCác thuật toán có thể được mở rộng theo cách \u0026ldquo;trừng phạt\u0026rdquo; model dựa trên độ phức tạp của chúng, làm cho model trở nên đơn giản hơn, kết quả là \u0026ldquo;học\u0026rdquo; tốt hơn.\nMột số thuật toán:\n Ridge Regression Least Absolute Shrinkage and Selection Operator (LASSO) Elastic Net Least-Angle Regression (LARS)   2.5 Decision Tree\nĐây là phương pháp xây dựng model dựa vào trực tiếp giá trị thực tế của input data. Tùy theo các điều kiện cụ thể áp dụng vào input data mà model sẽ đưa ra các quyết định khác nhau. Trong ML, các thuật toán thuộc nhóm này được sử dụng khá phổ biến.\nMột số thuật toán: Classification and Regression Tree (CART)\n Iterative Dichotomiser 3 (ID3) C4.5 and C5.0 (different versions of a powerful approach) Chi-squared Automatic Interaction Detection (CHAID) Decision Stump M5 Conditional Decision Trees   2.6 Bayesian Algorithms\nĐây là họ các thụât toán áp dụng định luật Bayes trong xác suất thống kê.\nMột số thuật toán:\n Naive Bayes Gaussian Naive Bayes Multinomial Naive Bayes Averaged One-Dependence Estimators (AODE) Bayesian Belief Network (BBN) Bayesian Network (BN)   2.7 Clustering Algorithms\nDựa trên số lượng cụm (nhóm, lớp) cho trước, các thuật toán clusering sẽ phân bổ các điểm dữ liệu về từng lớp, dựa trên sự tương quan giữa các điểm dữ liệu đó với nhau.\nMột số thuật toán:\n k-Means k-Medians Expectation Maximisation (EM) Hierarchical Clustering   2.8 Association Rule Learning Algorithms\nCác thuật toán này tập trung vào việc tìm ra các quy tắc kết hợp giữa các điểm dữ liệu để sinh ra dữ liệu mới, hoặc dữ liệu tồn tại trong tập ban đầu.\nMột số thuật toán:\n Apriori algorithm Eclat algorithm   2.9 Artificial Neural Network Algorithms (ANN)\nĐược truyền cảm hứng từ cấu tạo não bộ của các loài động vật, các thuật toán này mô phỏng lại cách làm viêc của các bộ não đó. Chúng được cấu tạo gồm các layers và các nerurons liên kết với nhau.\nMột số thuật toán:\n Perceptron Multilayer Perceptrons (MLP) Back-Propagation Stochastic Gradient Descent Hopfield Network Radial Basis Function Network (RBFN)   2.10 Deep Learning (DL) Algorithms\nCác thuật toán DL là sự nâng cấp, mở rộng của thuật toán ANN. Chúng bao gồm các mạng ANN phức tạp hơn, giải quyết các bài toán với lượng dữ liệu lớn hơn.\nMột số thuật toán:\n Convolutional Neural Network (CNN) Recurrent Neural Networks (RNNs) Long Short-Term Memory Networks (LSTMs) Stacked Auto-Encoders Deep Boltzmann Machine (DBM) Deep Belief Networks (DBN)   Chi tiết hơn về các thuật toán ở nhóm này, mình sẽ đề cập trong bài tiếp theo. Mời các bạn đón đọc.\n2.11 Dimensionality Reduction Algorithms\nĐôi khi dữ liệu quá phức tạp sẽ làm giảm khả năng học của các ML model. Các thuật toán này sẽ giúp giải quyết vấn đề này bằng cách giảm bớt số chiều của dữ liệu (giảm độ phức tạp của dữ liệu).\nMột số thuật toán:\n Principal Component Analysis (PCA) Principal Component Regression (PCR) Partial Least Squares Regression (PLSR) Sammon Mapping Multidimensional Scaling (MDS) Projection Pursuit Linear Discriminant Analysis (LDA) Mixture Discriminant Analysis (MDA) Quadratic Discriminant Analysis (QDA) Flexible Discriminant Analysis (FDA)   2.12 Ensemble Algorithms\nEnsemble là phương pháp sử dụng kết hợp nhiều thuật toán khác nhau để tạo thành một thuật toán mới. Mỗi cách kết hợp khác nhau sẽ cho ra các thuật toán khác nhau. Trong ML, các thuật toán thuộc nhóm này được sử dụng rất phổ biến, đạt hiệu quả rất cao.\nMột số thuật toán:\n Boosting Bootstrapped Aggregation (Bagging) AdaBoost Weighted Average (Blending) Stacked Generalization (Stacking) Gradient Boosting Machines (GBM) Gradient Boosted Regression Trees (GBRT) Random Forest   2.13 Recommendation System Algorithms\nĐúng như tên gọi, đây là các thuật toán giải quyết bài toán khuyến nghị người dùng làm một việc gì đó bằng cách đưa cho họ những cái mà họ có thể quan tâm. Chúng thường được áp dụng trong các trang web thương mại điện tử, các ứng dụng xem phim trực tuyến, \u0026hellip;\nMột số thuật toán:\n Content based Collaborative filtering   2.14 Các thuật toán khác\nCòn rất nhiều thuật toán chưa được liệt kê bên trên, đó là những thuật toán giải quyết các bài toán cụ thể. Có thể kể ra một số như sau:\n Feature selection algorithms Algorithm accuracy evaluation Performance measures Optimization algorithms \u0026hellip;  Vậy là mình đã giới thiệu đến các bạn các thuật toán ML mà các bạn có thể gặp trong quá trình học ML. Hi vọng rằng các bạn đã có cái nhìn tổng quát về chúng, làm tiền đề để đi sâu hơn trong các bài toán ML về sau.\nTrong các bài viết tiếp theo, mình sẽ tổng hợp lại các thuật toán Deep Learning, sau đó sẽ đi chi tiết vào một số thụât toán với các ứng dụng cụ thể. Mời các bạn đón đọc!\n3. Tham khảo\n Machinelearningmastery Simplilearn  ","permalink":"https://tiensu.github.io/blog/02_machine_learning_algorithms_summary/","tags":["Machine Learning"],"title":"Tổng hợp các thuật toán Machine Learning"},{"categories":["Machine Learning","Project Management"],"contents":"Trong bất kỳ dự án nào, đứng ở góc độ của nhà đầu tư và người quản lý, họ đều muốn biết được mốc thời gian dự án có thể được hoàn thành trước khi thực sự bắt đầu dự án. Bởi vì thông tin này giúp cho họ đưa ra quyết định về ngân sách dành cho dự án, khả năng thu hồi vốn và sinh lời (ROI). Cuối cùng là kết luận xem dự án có đáng để đầu tư hay không?\nCác dự án trong lĩnh vực phát triển phần mềm cũng không ngoại lệ. Đối với các dự án phần mềm thông thường, hiện nay, có rất nhiều công cụ, kỹ thuật có thể giúp chúng ta lên kế hoạch, ước lượng khối lượng công việc ban đầu tuơng đối dễ dàng và đầy đủ. Tuy nhiên, các dự án AI sẽ có một vài điểm khác biệt cần lưu ý trong quá trình thực hiện các công đoạn như ước lượng chi phí, lập kế hoạch quản lý dự án. Trong bài viết này, chúng ta sẽ cùng bàn chiêt tiết về các vấn đề đó. Nội dung của mỗi phần được tham khảo từ nhiều nguồn, kết hợp với kinh nghiệm thực tế của bản thân.\n1. Ví dụ về 2 loại dự án phần mềm Loại dự án 1: Xây dựng một website bán hàng hoặc một ứng dụng mobile Ví dụ, dự án phát triên một website bán hàng. Yêu cầu của dự án là thiết kế các form, layout, button, database, và xử lý các hành vi của người dùng khi mua hàng. Những hành vi này thực chất là một chuỗi các bước được thực hiện tuần tự để đạt được một mục tiêu cụ thể nào đó. Sau đó, anh em developers chỉ cần code theo đúng các bước như vậy.\nLoại dự án 2: Phát triển một phần mềm nhận diện các giống mèo khác nhau trong ảnh Để thực hiện dự án này theo cách thông thường, cần phải hiểu rõ đặc tính sinh học, giải phẫu học của loài mèo, trích xuất những thông tin đó từ trong bức ảnh. Đây là một công việc không hề dễ dàng, và trong nhiều trường hợp không thể thực hiện được. Thay vì thế, ta có thể xây dựng một mô hình học máy (Machine Learning model - ML model) và cho nó \u0026ldquo;học\u0026rdquo; những đặc tính khác nhau của từng loài mèo. Từ đó, nó có thể dễ dàng phân loại các loài mèo khác nhau khi đưa cho nó một bức ảnh về mèo.\n2. Sơ lựơc về AI model Về bản chất, AI model là một tập hợp các công thức toán học, cùng với rất nhiều các tham số có thể điều chỉnh được trong suốt quá trình train (gọi là các parameters, phân biệt với hyper-parameters là các tham số không thay đổi thông qua quá trình train). Có thể hình dung một cách đơn giản, train một AI model giống như việc một người giáo viên đưa cho một học sinh bức ảnh và nói đó là con mèo. Người học sinh đóng vai trò là AI mode, sẽ cố gắng học những đặc điểm của bức ảnh để biết đó là con mèo. Còn người giáo viên chính là người train.\nCó rất nhiều thuật toán AI khác nhau, mỗi loại lại sử dụng những phép toán khác nhau, kiểu và số lượng các parameters cũng khác nhau. Điều này dẫn đến các AI model tương ứng cũng cho ra kết quả khác nhau trên cùng 1 tập dữ liệu. Và nhiệm vụ chính của người kỹ sư AI chính là làm sao xây dựng được AI model có độ chính xác cao nhất, thời gian xử lý nhanh nhất đối với 1 tập dữ liệu nhất định. Để xây dựng lên một AI model tốt, cần phải thực hiện rất nhiều xác thực nghiệm, thí nghiệm trên các thuật toán khác nhau, kết hợp với việc điều chỉnh các hyper-parameters hợp lý. Quá trình này gọi là \u0026ldquo;thử-sai\u0026rdquo; (try - error). Điều này nghe thì rất đơn giản nhưng thực tế thì không hề dễ dàng.\nVòng đời của một AI model có thể tóm tắt trong 6 bước:\n  Bussiness Understanding Hiểu yêu cầu bài toán, hiểu vấn đề của khách hàng. Data Understanding \u0026amp; Collection Hiểu dữ liệu, cần dữ liệu như thế nào để train được model tốt. Khách hàng có thể có rất nhiều dữ liệu dạng thô, nhưng họ không biết cách sử dụng, khai thác. Mình phải là người hướng dẫn họ. Data Preparation Dữ liệu thu thập được cần phải được chuẩn hóa trước khi đưa vào train AI model. Modeling Có dữ liệu rồi, lựa chọn thuật toán phù hợp và tiến hành train mô hình. Quá trình này có thể kéo dài vì chúng ta phải \u0026ldquo;thử-sai\u0026rdquo; rất nhiều lần. Evaluation Model sau khi train xong cần được đánh giá dựa theo 1 tiêu chí cụ thể nào đó. Nếu chưa thỏa mãn thì lại quay lại các bước trước đó. Deployment Sau khi model được train thoả mãn yêu cầu, nó sẽ được đem vào triển khai trong sản phẩm thực tế. Vòng đời của một AI model là tuần hoàn khép kín vì nó luôn phải được cập nhật theo dữ liệu mới. Để làm tốt các bước 1,2 thì cần phải trải qua kinh nghiệm thực tế để có cảm quan tốt về vấn đề, đồng thời có thể phải cần hiểu về quy trình nghiệp vụ của từng bài toán. Các bước còn lại, có khá nhiều kỹ thuật, phuơng pháp xử lý. Mình sẽ giới thiệu trong các bài sau.  3. Tại sao dự án AI khó ước lượng? Một người kỹ sư phần mềm với kinh nghiệm lâu năm của mình có thể dễ dàng ước lượng được thời gian cần thiết để có thể hoàn thành một công việc, bởi vì họ đã làm những công việc tương tự như thế rất nhiều lần rồi.\nMột người kỹ sư AI có kinh nghiệm, đã từng xây dựng nhiều AI model, độ chính xác lên đến 90%. Tuy nhiên, nếu được hỏi mất bao nhiêu thời gian để họ có thể phát triển AI model tương tự, đạt được độ chính xác như thế, họ sẽ rất khó để trả lời. Tại sao lại có sự khác biệt đó? Có một câu nói rất nổi tiếng trong giới \u0026ldquo;AI\u0026rdquo; rằng: No free launch. Điều này ngụ ý là không có một công thức, một cách tiếp cận hay một phuơng pháp nào chung cho các vấn đề trong AI. ML model của bạn có thể đạt độ chính xác cao đối với vấn đề A, nhưng nếu bạn sử dụng cách thức train ML model đó để tạo ra ML model cho vấn đề B, không có gì đảm bảo rằng ML mới cũng hoạt động tốt.\nMột vấn đề tối quan trọng nữa, đó là dữ liệu. Có thể nói dữ liệu là vấn đề sống còn của ML model. Thống kê chỉ ra rằng một người kỹ sư AI dành đến 80% thời gian của họ để làm việc với dữ liệu. 20% thời gian còn lại dành cho việc train và triển khai ML model. Nếu bạn chưa có chút hiểu biết gì về dữ liệu bạn cần sử dụng thì mọi ước lượng của bạn có thể coi như là vô nghĩa. \u0026ldquo;Garbage in, garbage out\u0026rdquo; - hãy nhớ điều này.\nNgoài ra, bạn cũng nên biết rằng mỗi lần điều chỉnh, cập nhật ML model đều phải dựa trên kết quả của lần train trước đó. Vì thế mà việc xây dựng một bản kế hoạch chi tiết trong thời gian dài cho 1 dự án AI là điều không thực tế.\n4. Rủi ro của dự án AI/ML Có 3 vấn đề cần lưu ý (risks) trong một dự án AI: 1. Chưa xác định được cách thức xây dựng ML model phù hợp: Chọn thuật toán, chọn tham số, \u0026hellip; 2. Thiếu dữ liệu train ML model 3. Khó lập kế hoạch chi tiết\nVấn đề đầu tiên có thể được giải quyết bằng cách thực hiện điều tra nghiên cứu tính khả thi của ML model trước khi thực sự bắt đầu dự án. Thậm chí, việc này có thể coi là một dự án PoC, làm tiền đề cho dự án thực sự phía sau. Trong giai đoạn PoC, chúng ta sẽ thử nghiệm các thuật toán, các bộ tham số, các cách thức tiếp cận khác nhau để giải quyết một vấn đề nhỏ nhưng tiêu biểu cho dự án lớn. PoC không chỉ giúp chúng ta xác định được các khả năng có thể và không thể của ML model đối với yêu cầu của dự án mà còn chỉ ra được những yêu cầu cụ thể đối với dữ liệu đầu vào. Thông qua PoC, người kỹ sư AI sẽ hiểu rõ hơn về nghiệp vụ, từ đó rút ngắn được danh sách các thuật toán PoC cần thử nghiệm. Và thậm chí là không cần dùng đến ML model cũng có thể giải quyết được yêu cầu của bài toán đặt ra. Nếu mà kết quả của PoC không được như mong đợi, không chọn ra được bất kỳ phuơng pháp nào đáp ứng yêu cầu dự án thì hoặc là cần phải dành thêm thời gian để nghiên cứu hoặc là quyết định dừng dự án lại, tránh lãng phí thời gian và tiền bạc. Sau khi đã giải quyết được vấn đề đầu tiên thì vấn đề thứ 2 đã sáng tỏ hơn. Chúng ta đã biết là cần dữ liệu như thế nào. Việc còn lại chỉ là làm thế nào để có được dữ liệu đó. Thông thường, khi đưa ra bài toán, khách hàng thường đã có dữ liệu (dạng thô) rồi. Người kỹ sư AI phải làm sao sử dụng được dữ liệu đó một cách hiệu quả và hợp lý để tăng được độ chính xác của ML model. Trong trường hợp, khách hàng không có sẵn dữ liệu thì chúng ta phải hướng dẫn họ cách thức thu thập dữ liệu, sao cho đơn giản, nhanh chóng và chính xác dữ liệu được mong đợi. Tuy nhiên, việc làm này có một hạn chế đó là việc thu thập dữ liệu trong một thời gian ngắn thường sẽ khó có thể bao quát hết các trường hợp xảy ra trong thực tế nghiệp vụ của khách hàng (vấn đề bias dữ liệu trong AI). Điều này chắc chắn sẽ làm giảm độ chính xác của ML model khi triển khai trong thực tế. Tất nhiên, theo thời gian, ML model sẽ được cập nhật theo dữ liệu thực tế thì độ chính xác cũng sẽ được tăng lên. Nhưng làm sao để khách hàng hiểu và chấp nhận điều này thì lại là một vấn đề không đơn giản. :D Vấn đề còn lại, giải quyết bằng cách áp dụng nguyên lý Agile-Scrum trong quản trị dự án. Chúng ta lập kế hoạch ngắn hạn cho từng Sprin. Kế hoạch của Sprin tiếp theo sẽ phụ thuộc vào kết quả của Sprin trước đó. Trong quá trình thực hiện dự án, cũng nên liên tục trao đổi, chia sẻ thông tin với khách hàng để điều chỉnh kế hoạch cho phù hợp, đảm bảo dự án đang đi đúng hướng.\n5. Kết luận Vấn đề về chất lượng dữ liệu và việc không có phương pháp chính xác ngay từ đầu yêu cầu phải có giai đoạn \u0026ldquo;research\u0026rdquo; - nghiên cứu tìm hiêu (PoC) trước khi chính thức bắt đầu một dự án AI. Tuy nhiên, những rủi ro này thường không đuợc xem xét trong quá trình ước lựong và lập kế hoạch cho một dự án AI. Cần nhấn mạnh một điều rằng, dự án AI thuộc thể loại \u0026ldquo;experiment-driven\u0026rdquo;, tức là dự án phải trải qua rất nhiều \u0026ldquo;experiments\u0026rdquo; - thực nghiệm, thí nghiệm. Và hành động tiếp theo phụ thuộc vào kết quả của hành động trước đó. Nếu bạn đã từng trải qua những dự án AI, hãy chia sẻ kinh nghiệm của bạn dưới phần bình luận! Rất vui nếu được tiếp thu những ý kiến đóng góp của mọi người!\nBài viết có tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/01_ai-project-planing/","tags":["Machine Learning","Project Management"],"title":"Lưu ý khi lập kế hoạch cho một dự án AI"}]