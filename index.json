[{"categories":["Model Deployment","Data Driff"],"contents":"Sau một thời gian nghỉ tết thì hôm nay mình đã trở lại. Trong bài viết mình sẽ cùng các bạn làm một ví dụ nhỏ về Data Driff để các bạn hiểu rõ hơn về nó. Cá nhận mình đánh giá, đây là một trong những vấn đề quan trọng nhất để giữ cho AI model chạy ổn định trong thực tế. Hãy xem lại bài này nếu bạn chưa biết về Data Driff.\n1. Ví dụ Giả sử chúng ta muốn dự đoán chất lượng của rượu tại một cửa hàng chuyên bán rượu, để quyết định xem có nên mua chai rượu đó hay không?\nChúng ta sẽ sử dụng UCI Wine Quality dataset để xây dựng một ML model dự đoán. Mỗi loại rượu có tất cả 12 features: type, fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, và alcohol rate. Nhãn là quality score có giá trị từ 0 đến 10.\nImport các thư viện sử dụng: Output:  Đọc và kiểm tra dữ liệu:  Thống kê dữ liệu:  Kiểm tra xem data có chứa giá trị NULL hay không?  Kiểm tra xem dữ liệu có bị trùng lặp hay không? Nếu có thì xóa bỏ những dữ liệu bị trùng đó.  Kiểm tra sự tương quan (liên hệ) giữa các features đôi một.  Từ đây, ta có thể loại bợt những features mà không có sự liên hệ nhiều đến nhãn (hệ số corr giữa feature đó và nhãn nhỏ).\nĐể đơn giản hóa model, chúng ta sẽ model hóa bài toán thành dạng binary classification. Cụ thể, rượu được coi là ngon khi quality score có giá trị lớn hơn 6 và ngược lại, rượu có quality score nhỏ hơn hoặc bằng 6 được coi là không ngon.  Kiểm tra sự phân phối dữ liệu giữa 2 nhãn.  Ta có thể thấy số lượng dữ liệu phân phối khá đồng đều giữa 2 nhãn. Điều này là cần thiết để tránh việc bias dữ liệu.\nĐể minh họa hiện tượng Data Driff, chúng ta chia tập dữ liệu thành 2 phần:\n Phần 1, chứa tất cả rượu có giá trị của alcohol rate lớn hơn 11%. Phần 2, chứa tất cả rượu có giá trị của alcohol rate nhỏ hơn hoặc bằng 11%.    Với việc phân chia như thế này, rõ ràng là dữ liệu ở phần 2 đã xảy ra hiện tượng Data Driff so với dữ liệu ở phần 1, cụ thể là ở feature alcohol.\nToàn bộ dữ liệu ở phần 1 sẽ được sử dụng để train và test model. Ở đây, mình không thực hiện việc tuning model mà chỉ xây dựng model đơn giản để minh họa ảnh hưởng của Data Driff.\nTách phần 1 thành 2 phần: features và labels. Sau đó lại chia mỗi phần đó thành 2 phần train và test theo tỉ lệ 80:20.  Thử kiểm tra sự phân bố dữ liệu giữa:\n Tập train và test của phần 1:   Tập train và phần 2:    Từ đồ thị phân bố có thể quan sát rõ ràng hiện tượng Data Driff khi mà feature alcohol của tập train và phần 2 nằm về 2 phía của giá trị 11. Tập train và test của phần 1 không có hiện tượng này.\nTiến hành tạo model và huấn luyện trên tập train:  Đánh giá model trên tập test:  Đánh giá model trên phần 2. Chúng ta dự đoán rằng, kết quả test trên 20% của phần 1 sẽ lớn hơn trên toàn bộ phần 2, vì hiện tượng Data Driff xảy ra ở phần 2 so với phần 1.  Ở đây, chúng ta sử dụng 3 metrics để đánh giá: accuracy score, f1 score và confusion matrix. Kết quả đánh giá chỉ ra, giá trị của các metrics trên phần 2 nhỏ hơn rất nhiều so với trên tập test, đúng như dự đoán ban đầu của chúng ta.\n2. Kết luận\nNhư vậy là chúng ta đã cùng nhau làm 1 ví dụ về hiện tượng Data Driff, một trong những vấn đề rất quan trọng của quá trình triển khai AI/ML model trong thực tế. Hi vọng là các bạn có cái nhiều sâu sắc hơn về nó thông qua bài này.\nToàn bộ source code của bài này, các bạn có thể tham khảo tại github cá nhân của mình tại đây.\nHẹn các bạn trong các bài viết tiếp theo.\n","permalink":"https://tiensu.github.io/blog/47_data_driff_cause_and_solution/","tags":["Model Deployment","Data Driff"],"title":"Một ví dụ về hiện tượng Data Driff trong Machine Learning"},{"categories":["Model Deployment","Kubernetes","Docker"],"contents":"Đây là bài viết cuối cùng về Kubernetes trên local. Bài sau (nếu có) thì sẽ là hướng dẫn cấu hình Kubernetes trên cloud.\nTrong bài này, chúng ta sẽ cùng tìm hiểu về Kubernetes Serice và áp dụng chúng vào bài toán AI.\n1. Kubernetes Service là gì?\nỞ bài trước, chúng ta đã biết rằng mặc dù Development rất hiệu quả trong việc giải quyết tác vụ Online Inference, nhưng nó có một nhược điểm là REST API chỉ có tác dụng trong phạm vi Cluster, không thể kết nối ra ngoài. Service chính là giải pháp để giải quyết cho vấn đề đó.\nService cung cấp một Stable Virtual IP (VIP) với mục đích forward dữ liệu đến tới các Pods. Một tiến trình kube-poluxy chịu trách nhiệm ánh xạ giữa VIP và các Pods (vì địa chỉ của các Pods luôn thay đổi).\n2. Làm việc với Kubernetes Service\nChúng ta sẽ sử dụng lại cấu hình của Deployment trong bài trước để tạo REST API cho tác vụ Online Inference trong bài toán AI. Sau đó, sử dụng Service để mở các REST API đó ra bên ngoài.\n2.1 Tạo Kubernetes Deployment\nChạy các lệnh sau để tạo và kiểm tra trạng thái của Deployment, Pods:\n$ kubectl create -f development-online-inference.yaml deployment.apps/online-inference-development created $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-world 5/5 5 5 37m online-inference-development 2/2 2 2 2m11s $ kubectl get pods NAME READY STATUS RESTARTS AGE online-inference-development-5d46c5c7dc-hrkrn 1/1 Running 0 12m online-inference-development-5d46c5c7dc-r4qfz 1/1 Running 0 12m 2.2 Tạo Kubernetes Service\nSử dụng lệnh sau để tạo Service:\n$ kubectl expose deployment online-inference-development --type NodePort --name online-inference-service service/online-inference-service exposed Một số thông tin:\n type: Loại Service. Ở đây sử dụng NodePort để mở REST API thông qua Port của các Worker Node. Chi tiết về các loại Type, tham khảo tại đây. name: Tên của Service đuọc tạo ra.  Kiểm tra Service vừa tạo:\n$ NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 72m online-inference-service NodePort 10.100.53.16 \u0026lt;none\u0026gt; 5000:31412/TCP 3m26s Chú ý đến 2 thông tin: EXTERNAL-IP và PORT(S). Đây là 2 thông tin để cho các yêu cầu đến truy cập vào trong các Pods.\n Giá trị \u0026lt;none\u0026gt; của EXTERNAL-IP được ngầm hiểu là IP của Worker Node, vì chúng ta đã chọn TYPE của Service là NodePort. Giá trị 5000:31412/TCP của PORT(S) có nghĩa là yêu cầu từ bên ngoài Cluster gửi đến Port 31412 của Worker Node sẽ được chuyển tiếp đến Port 5000 của các Pods. TCP là giao thức trao đổi dữ liệu.  Xem đầy đủ thông tin của Service:\n$ kubectl describe services online-inference-service Name: online-inference-service Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=model-api Type: NodePort IP Families: \u0026lt;none\u0026gt; IP: 10.100.53.16 IPs: 10.100.53.16 Port: \u0026lt;unset\u0026gt; 5000/TCP TargetPort: 5000/TCP NodePort: \u0026lt;unset\u0026gt; 31412/TCP Endpoints: 192.168.24.227:5000,192.168.24.228:5000 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; Ta có thể nhìn thấy 2 IP:PORT của 2 Pods là: 192.168.24.227:5000 và 192.168.24.228:5000\n2.3 Thực hiện Online Inference\nĐã cấu hình xong Service, chúng ta thử gửi một yêu cầu dự đoán từ bên ngoài Cluster xem sao:\n$ curl -i -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;CRIM\u0026#34;: 15.02, \u0026#34;ZN\u0026#34;: 0.0, \u0026#34;INDUS\u0026#34;: 18.1, \u0026#34;CHAS\u0026#34;: 0.0, \u0026#34;NOX\u0026#34;: 0.614, \u0026#34;RM\u0026#34;: 5.3, \u0026#34;AGE\u0026#34;: 97.3, \u0026#34;DIS\u0026#34;: 2.1, \u0026#34;RAD\u0026#34;: 24.0, \u0026#34;TAX\u0026#34;: 666.0, \u0026#34;PTRATIO\u0026#34;: 20.2, \u0026#34;B\u0026#34;: 349.48, \u0026#34;LSTAT\u0026#34;: 24.9}\u0026#39; 10.1.30.130:31412/predict HTTP/1.0 200 OK Content-Type: application/json Content-Length: 41 Server: Werkzeug/1.0.1 Python/3.8.6 Date: Tue, 02 Feb 2021 11:06:49 GMT { \u0026#34;prediction\u0026#34;: 12.273424794987877 } Có kết quả trả về, tức là chúng ta đã thành công, :)).\n3. Kết luận\nĐây là bài viết cuối cùng trong năm Canh Tý của mình. Xong bài này mình sẽ về quê đón tết cùng gia đình.\nBài viết đầu tiên trong năm Nhâm Sửu mình sẽ hướng dẫn các bạn cách nhận biết hiện tượng Data Driff, một vấn đề mà theo mình rất quan trong việc giải quyết các bài toán AI thực tế. Mời các bạn đón đọc!\nKính chúc mọi người năm mới AN KHANG THỊNH VƯỢNG!!!\n8. Tham khảo\n Mlinproduction Kubernetes Service  ","permalink":"https://tiensu.github.io/blog/46_kubernetes_services/","tags":["Model Deployment","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 5: Kubernetes Service"},{"categories":["Model Deployment","Kubernetes","Docker"],"contents":"Trong bài toán AI, nếu như Job và CronJob phù hợp nhất cho các tác vụ thực hiện không liên tục, không realtime (VD: Batch Inference, Training, \u0026hellip;) thì Deployment lại là lựa chọn tốt nhất cho các tác vụ cần chạy liên tục, realtime (VD: Online Inference, \u0026hellip;). Trong bài này, hãy cùng tìm hiểu về Deployment và cách sử dụng nó.\n1. Kubernetes Deployment là gì?\nDeployment có thể hiểu là một tập các Pods giống nhau chạy trên một Kubernetes Cluster. Giống như Job, nó cũng quản lý các Pods trong việc thực hiện một nhiệm vụ nào đó. Sự khác nhau giữa Job và Deployment ở tính chất nhiệm vụ mà chúng thực hiện. Đối với Job, các tasks của nó chỉ chạy một lần, sau đó kết thúc luôn. Ngược lại, các tasks của Deployment chạy liên tục từ lúc được khởi tạo và chỉ kết thúc khi có sự can thiệp của người quản trị hoặc một ngoại lệ bất thường.\nMột số đặc điểm trong cách quản lý Pod của Deployment:\n Trong quá trình làm việc, nếu một Pod bị chết, Deployment sẽ tạo ra một Pod khác thay thế. Deployment cũng có khả năng tự động scale up/down số lượng các Pods tùy thuộc vào mức độ nặng/nhẹ của công viêc mà nó thực hiện. Có thể thay đổi cấu hình của Deployment trực tiếp trong file cấu hình mà không phải downtime. Có thể quay lại những thay đổi trước đó trong trường hợp sự thay đổi mới gây ra lỗi.  Chính vì vậy mà Deployment rất phù hợp với nhiệm vụ Online Inference trong bài toán AI. Chúng ta train một model, tạo một REST API để lắng nghe các yêu cầu dự đoán. Sau đó, tạo ra một Deployment để chấp nhận và thực hiện các yêu cầu đó một các realtime. Nếu số lượng các yêu cầu tăng lên cao, Deployment sẽ tự động tạo thêm các Pod để xử lý và ngược lại. Nếu có một phiên bản mới của model, ta có thể dễ dàng đưa luôn vào sử dụng mà không phải downtime. Và nếu model mới đó không hiệu quả bằng model cũ, ta hoàn toàn có thể quay về sử dụng model cũ đó.\n2. Làm việc với Kubernetes Deployment\nChúng ta sẽ thực hiện tạo một Deployment để phục vụ nhiệm vụ Online Inference trong bài toán AI.\nHãy xem cấu trúc thư mục làm việc:\nkubernetes_deployment │ ├── deployment │ │ └── deployment-online-inference.yaml │ └── docker │ ├── api.py │ ├── Dockerfile │ └── train.py 2.1 Train model AI và tạo REST API\nTạo thư mục docker và hai file code python bên trong nó:\n File train.py: Train model AI và lưu file model. File api.py: Tạo API để cho phép yêu cầu dự đoán gửi đến và trả về kết quả.  Nội dung của file train.py như sau:\nimport json import os from joblib import dump import matplotlib.pyplot as plt import numpy as np from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Load data print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() print(\u0026#34;Splitting data...\u0026#34;) X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset]\tX_test, y_test = X[offset:], y[offset:] # ############################################################################# # Fit regression model print(\u0026#34;Fitting model...\u0026#34;) params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) train_mse = mean_squared_error(y_train, clf.predict(X_train)) test_mse = mean_squared_error(y_test, clf.predict(X_test)) metadata = { \u0026#34;train_mean_square_error\u0026#34;: train_mse, \u0026#34;test_mean_square_error\u0026#34;: test_mse } print(\u0026#34;Serializing model to: {}\u0026#34;.format(MODEL_PATH)) dump(clf, MODEL_PATH) print(\u0026#34;Serializing metadata to: {}\u0026#34;.format(METADATA_PATH)) with open(METADATA_PATH, \u0026#39;w\u0026#39;) as outfile: json.dump(metadata, outfile) Nội dung của file api.py như sau:\nimport os from flask import Flask from flask_restful import Resource, Api, reqparse from joblib import load import numpy as np MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) print(\u0026#34;Loading model from: {}\u0026#34;.format(MODEL_PATH)) clf = load(MODEL_PATH) app = Flask(__name__) api = Api(app) class Prediction(Resource): def __init__(self): self._required_features = [\u0026#39;CRIM\u0026#39;, \u0026#39;ZN\u0026#39;, \u0026#39;INDUS\u0026#39;, \u0026#39;CHAS\u0026#39;, \u0026#39;NOX\u0026#39;, \u0026#39;RM\u0026#39;, \u0026#39;AGE\u0026#39;, \u0026#39;DIS\u0026#39;, \u0026#39;RAD\u0026#39;, \u0026#39;TAX\u0026#39;, \u0026#39;PTRATIO\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;LSTAT\u0026#39;] self.reqparse = reqparse.RequestParser() for feature in self._required_features: self.reqparse.add_argument( feature, type = float, required = True, location = \u0026#39;json\u0026#39;, help = \u0026#39;No {} provided\u0026#39;.format(feature)) super(Prediction, self).__init__() def post(self): args = self.reqparse.parse_args() X = np.array([args[f] for f in self._required_features]).reshape(1, -1) y_pred = clf.predict(X) return {\u0026#39;prediction\u0026#39;: y_pred.tolist()[0]} api.add_resource(Prediction, \u0026#39;/predict\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True, host=\u0026#39;0.0.0.0\u0026#39;) Code của 2 files này khá đơn giản nên mình không giải thích gì thêm, hi vọng các bạn có thể tự hiểu được.\n2.2 Chuẩn bị Docker Image\nCũng trong thư mục docker, ta file Dockerfile như sau:\nFROM jupyter/scipy-notebook USER root WORKDIR /docker ADD . /docker RUN pip install flask flask-restful joblib RUN mkdir /docker/model ENV MODEL_DIR=/docker/model ENV MODEL_FILE=clf.joblib ENV METADATA_FILE=metadata.json RUN python3 train.py Sau đó tiến hành build Docker Image:\n$ docker build -t docker-ml-online . Sending build context to Docker daemon 6.656kB Step 1/10 : FROM jupyter/scipy-notebook ---\u0026gt; c1a7c7ef5e27 Step 2/10 : USER root ---\u0026gt; Using cache ---\u0026gt; 0d9f55e9c7e0 Step 3/10 : WORKDIR /docker ---\u0026gt; Using cache ---\u0026gt; 4ed21d81d110 Step 4/10 : ADD . /docker ---\u0026gt; a266bfc5ca35 Step 5/10 : RUN pip install flask flask-restful joblib ---\u0026gt; Running in 97888ed0b989 Collecting flask Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB) Collecting flask-restful Downloading Flask_RESTful-0.3.8-py2.py3-none-any.whl (25 kB) Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (1.0.0) Requirement already satisfied: click\u0026gt;=5.1 in /opt/conda/lib/python3.8/site-packages (from flask) (7.1.2) Collecting Werkzeug\u0026gt;=0.15 Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB) Requirement already satisfied: Jinja2\u0026gt;=2.10.1 in /opt/conda/lib/python3.8/site-packages (from flask) (2.11.2) Collecting itsdangerous\u0026gt;=0.24 Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB) Requirement already satisfied: MarkupSafe\u0026gt;=0.23 in /opt/conda/lib/python3.8/site-packages (from Jinja2\u0026gt;=2.10.1-\u0026gt;flask) (1.1.1) Requirement already satisfied: six\u0026gt;=1.3.0 in /opt/conda/lib/python3.8/site-packages (from flask-restful) (1.15.0) Requirement already satisfied: pytz in /opt/conda/lib/python3.8/site-packages (from flask-restful) (2020.5) Collecting aniso8601\u0026gt;=0.82 Downloading aniso8601-8.1.1-py2.py3-none-any.whl (44 kB) Installing collected packages: Werkzeug, itsdangerous, flask, aniso8601, flask-restful Successfully installed Werkzeug-1.0.1 aniso8601-8.1.1 flask-1.1.2 flask-restful-0.3.8 itsdangerous-1.1.0 Removing intermediate container 97888ed0b989 ---\u0026gt; d9f31d7e7c83 Step 6/10 : RUN mkdir /docker/model ---\u0026gt; Running in 89b237f6427c Removing intermediate container 89b237f6427c ---\u0026gt; b2778ed90f4a Step 7/10 : ENV MODEL_DIR=/docker/model ---\u0026gt; Running in d7a52c9249f9 Removing intermediate container d7a52c9249f9 ---\u0026gt; 5157d919abd5 Step 8/10 : ENV MODEL_FILE=clf.joblib ---\u0026gt; Running in a7f75c6f79e5 Removing intermediate container a7f75c6f79e5 ---\u0026gt; 790a21e54588 Step 9/10 : ENV METADATA_FILE=metadata.json ---\u0026gt; Running in b0b94567182c Removing intermediate container b0b94567182c ---\u0026gt; 92a98ce95a8d Step 10/10 : RUN python3 train.py ---\u0026gt; Running in d8055e4ef00d Loading data... Splitting data... Fitting model... Serializing model to: /docker/model/clf.joblib Serializing metadata to: /docker/model/metadata.json Removing intermediate container d8055e4ef00d ---\u0026gt; b1fb95b775ec Successfully built b1fb95b775ec Successfully tagged docker-ml-online:latest Có Docker Image rồi, tiến hành push nó lên Docker Hub:\n$ docker push tiensu/ml-model-online-infer:latest The push refers to repository [docker.io/tiensu/ml-model-online-infer] f0e40a44cb9c: Pushed a079ef4fd38e: Pushed 76cba4a3a958: Pushed 3451a539eae2: Pushed 66f4cc63b50c: Mounted from tiensu/docker-ml 5f70bf18a086: Mounted from tiensu/docker-ml 6f5a41ae77fd: Mounted from tiensu/docker-ml 5a1b9a3f9355: Mounted from tiensu/docker-ml b1d7816bac14: Mounted from tiensu/docker-ml c91fed2d1998: Mounted from tiensu/docker-ml cc70098d00e3: Mounted from tiensu/docker-ml 88727e93cbac: Mounted from tiensu/docker-ml cadaf24035f3: Mounted from tiensu/docker-ml 8f170f4774e3: Mounted from tiensu/docker-ml 33bd52db887f: Mounted from tiensu/docker-ml 21e5dd010f50: Mounted from tiensu/docker-ml ea370ab22368: Mounted from tiensu/docker-ml 421d1408f872: Mounted from tiensu/docker-ml 18fd1ca0de51: Mounted from tiensu/docker-ml 8f01aab6d756: Mounted from tiensu/docker-ml e18a1c4e1d31: Mounted from tiensu/docker-ml 8552f27c3cd8: Mounted from tiensu/docker-ml 1a4c57efcc23: Mounted from tiensu/docker-ml 94b8fe888eac: Mounted from tiensu/docker-ml 02473afd360b: Mounted from tiensu/docker-ml dbf2c0f42a39: Mounted from tiensu/docker-ml 9f32931c9d28: Mounted from tiensu/docker-ml latest: digest: sha256:67c219ed32f9748c0c3ce64e8c4274932a8dadaf05510402f5d64a038bca2165 size: 6790 2.3 Tạo Kubernetes Deployment\nTrong thư mục deployment, tạo file cấu hình (deployment-online-inference.yaml) của Deployment với nội dung như sau:\napiVersion: apps/v1 kind: Deployment metadata: name: online-inference-deployment spec: replicas: 2 selector: matchLabels: app: model-api template: metadata: labels: app: model-api spec: containers: - name: model-api imagePullPolicy: Always image: tiensu/ml-model-online-infer:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;api.py\u0026#34;] ports: - containerPort: 5000 Một số thông tin cần lưu ý ở đây:\n replicas: Số lượng Pods được tạo ra lúc ban đầu. selector: Định nghĩa tên của Pods/Containers mà nó quản lý.  Chạy các lệnh sau để tạo và kiểm tra Deployment:\n$ kubectl create -f deployment-online-inference.yaml deployment.apps/online-inference-deployment created $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE online-inference-deployment 2/2 2 2 41s Thực ra, Deployment không trực tiếp quản lý các Pods. Thay vào đó, nó sẽ tạo ra các ReplicaSet với mục đích duy trì sự ổn định của các Pods tại bất kì thời điểm nào trong suốt quá trình hoạt động.\nKiểm tra ReplicaSet được Deployment tạo ra:\n$ kubectl get rs NAME DESIRED CURRENT READY AGE online-inference-deployment-59c8579f48 2 2 2 68s Chú ý: Tên của ReplicaSet = Tên của Deployment + chuỗi ngẫu nhiên.\nKiểm tra thử các Pods được quản lý bởi online-inference-deployment-59c8579f48 ReplicaSet:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE online-inference-deployment-59c8579f48-bg2vj 1/1 Running 0 97s online-inference-deployment-59c8579f48-j9fl2 1/1 Running 0 97s Chú ý: Tên của Pod = Tên của ReplicaSet + chuỗi ngẫu nhiên.\nThử debug một Pod xem có gì bất thường không?\n$ kubectl describe pod online-inference-deployment-59c8579f48-bg2vj Name: online-inference-deployment-59c8579f48-bg2vj Namespace: default Priority: 0 Node: duynm-vostro-3670/10.1.30.130 Start Time: Mon, 01 Feb 2021 18:15:27 +0700 Labels: app=model-api pod-template-hash=59c8579f48 Annotations: cni.projectcalico.org/podIP: 192.168.24.197/32 cni.projectcalico.org/podIPs: 192.168.24.197/32 Status: Running IP: 192.168.24.197 IPs: IP: 192.168.24.197 Controlled By: ReplicaSet/online-inference-deployment-59c8579f48 Containers: model-api: Container ID: docker://4cde562c962b48ff4c6bc3c812b140d2555e1984f064108bd8bf607b122cef9a Image: tiensu/ml-model-online-infer Image ID: docker-pullable://tiensu/ml-model-online-infer@sha256:67c219ed32f9748c0c3ce64e8c4274932a8dadaf05510402f5d64a038bca2165 Port: 5000/TCP Host Port: 0/TCP Command: python3 api.py State: Running Started: Mon, 01 Feb 2021 18:15:45 +0700 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-wp4xr (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-wp4xr: Type: Secret (a volume populated by a Secret) SecretName: default-token-wp4xr Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m25s default-scheduler Successfully assigned default/online-inference-deployment-59c8579f48-bg2vj to duynm-vostro-3670 Normal Pulling 2m22s kubelet Pulling image \u0026#34;tiensu/ml-model-online-infer\u0026#34; Normal Pulled 2m8s kubelet Successfully pulled image \u0026#34;tiensu/ml-model-online-infer\u0026#34; in 14.038005704s Normal Created 2m7s kubelet Created container model-api Normal Started 2m7s kubelet Started container model-api OK, mọi thứ đều đang hoạt động đúng như mong muốn.\nTa cũng có thể xem logs của Pod khi chạy:\n$ kubectl logs -f online-inference-development-5d46c5c7dc-bg2vj Loading model from: /docker/model/clf.joblib * Serving Flask app \u0026#34;api\u0026#34; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 263-920-719 10.1.30.130 - - [02/Feb/2021 11:06:49] \u0026#34;POST /predict HTTP/1.1\u0026#34; 200 - Tham số -f dùng để xem log một các realtime.\n2.4 Chạy Online Inference\nBây giờ ta sẽ thử gửi một yêu cầu dự đoán thông qua REST API để xem kết quả trả về. Tuy nhiên, có một chú ý quan trọng là REST API này chỉ mới hoạt động được bên trong phạm vi của Kubernetes Cluster. Để mở rộng nó ra ngoài Internet, chúng ta cần phải sử dụng thêm Service. Service sẽ được trình bày trong bài viết tiếp theo.\nChúng ta sẽ thực hiện Online Inference từ một Pod trong cùng Cluster với Deployment. Sử dụng lệnh sau để chạy và truy cập vào Pod python3:\n$ kubectl run python3 -ti --image=python:3.6 --command=true bash If you don\u0026#39;t see a command prompt, try pressing enter. root@python3:/#  Phần xử lý Inference bây giờ đang nằm trên 2 Pods mà Deployment tạo ra. Ta sẽ gửi yêu cầu dự đoán đến chúng. Xem lại phần debug bên trên của Pod online-inference-deployment-59c8579f48-bg2vj ta thấy Internal IP của nó là 192.168.24.197\nTừ trong Pod python3, thực hiện lệnh sau để gửi yêu cầu dự đoán:\n$ curl -i -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;CRIM\u0026#34;: 15.02, \u0026#34;ZN\u0026#34;: 0.0, \u0026#34;INDUS\u0026#34;: 18.1, \u0026#34;CHAS\u0026#34;: 0.0, \u0026#34;NOX\u0026#34;: 0.614, \u0026#34;RM\u0026#34;: 5.3, \u0026#34;AGE\u0026#34;: 97.3, \u0026#34;DIS\u0026#34;: 2.1, \u0026#34;RAD\u0026#34;: 24.0, \u0026#34;TAX\u0026#34;: 666.0, \u0026#34;PTRATIO\u0026#34;: 20.2, \u0026#34;B\u0026#34;: 349.48, \u0026#34;LSTAT\u0026#34;: 24.9}\u0026#39; 192.168.24.197:5000/predict HTTP/1.0 200 OK Content-Type: application/json Content-Length: 41 Server: Werkzeug/1.0.1 Python/3.8.6 Date: Mon, 01 Feb 2021 11:22:25 GMT { \u0026#34;prediction\u0026#34;: 12.273424794987877 } Như vậy là ta đã nhận được kết quả dự đoán trả về, chứng tỏ Deployment của chúng ta đã hoạt động đúng như ta dự tính.\n2.5 Xóa Deployment khi không sử dụng\nNếu không sử dụng nữa, ta thực hiện lệnh sau để xóa Deployment và các tài nguyên của nó:\n$ kubectl delete deployment online-inference-development deployment.apps \u0026#34;online-inference-development\u0026#34; deleted Kiểm tra lại:\n$ kubectl get rs No resources found. $ kubectl get pods No resources found. 3. Kết luận\nXong, chúng ta đã thực hành thành công với Deployment, và ta cũng biết một thiếu sót của Deployment phải cần đến Service để giải quyết. Đó chính là nội dung của bài tiếp theo. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n8. Tham khảo\n Mlinproduction CronJob  ","permalink":"https://tiensu.github.io/blog/45_kubernetes_deployment/","tags":["Model Deployment","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 4: Kubernetes Deployment"},{"categories":["Model Deployment","Kubernetes","Docker"],"contents":"Ở bài trước, chúng ta đã tìm hiểu và thực hành với Kubernetes Job và thấy được sự phù hợp và hiệu quả của nó đối với các tác vụ trong bài toán AI. Tuy nhiên, có thể dễ dàng nhận thấy một nhược điểm của Job, đó là Job phải được tạo một cách thủ công. Điều này khá là bất tiện, vì chúng ta sẽ phải mất công giám sát hoạt động của hệ thống để can thiệp vào (tạo Job) khi cần. Liệu có cách nào làm cho Job có thể tự động được tạo ra và thực hiện nhiệm vụ của nó tại những thời điểm nhất định, theo chu kỳ? Kubernetes CronJob chính là câu trả lời. Trong bài này, hãy cùng nhau làm việc với CronJob nhé!\n1. Kubernetes CronJob là gì?\nCronJob là một bộ lập lịch, tương tự như Cron Task trong nhân Linux. Nó giúp chúng ta tạo ra một kế hoạch thực hiện một công việc nào đó (bằng cách tạo ra các Jobs), tại những thời điểm trong tương lai theo một chu kỳ mà ta định nghĩa.\nĐối với bài toán AI, CronJob phù hợp với các tác vụ cần chạy định kỳ, ví dụ như là Batch Inference, Feature Extraction, \u0026hellip;\n2. Làm việc với CronJob\nChúng ta sẽ sử dụng lại Docker Image ở bài trước.\nMục đích của mình ở đây là tạo ra một CronJob để thực hiện Batch Inference mỗi phút. File model được lưu trên AWS S3.\nHãy xem cấu trúc thư mục làm việc:\nkubernetes_cronjob │ ├── cronjob │ │ └── cronjob-inference.yaml │ └── docker │ ├── batch_inference.py │ ├── Dockerfile │ └── train.py Giống như Pod và Job, CronJob cũng được tạo thông qua file cấu hình (cronjob-inference.yaml):\napiVersion: batch/v1beta1 kind: CronJob metadata: name: inference-cronjob spec: schedule: \u0026#34;* * * * *\u0026#34; jobTemplate: spec: template: spec: containers: - name: inference-container imagePullPolicy: Always image: tiensu/docker-ml:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;inference.py\u0026#34;] env: - name: AWS_ACCESS_KEY_ID value: \u0026#34;\u0026#34; - name: AWS_SECRET_ACCESS_KEY value: \u0026#34;\u0026#34; restartPolicy: Never backoffLimit: 0 Các thông tin cấu hình khá giống với của Pod, Job. Chỉ có 1 thông tin mới cần lưu ý ở đây:\n schedule: Đây là giá trị chỉ ra chu kỳ chạy của Job, tuân theo các quy tắc định trước. Tham khảo các quy tắc tại đây. Ngoài ra, nếu bạn cảm thấy bối rối khi sử dụng những quy tắc để tạo schedule, bạn có thể sử dụng công cụ này để giúp đỡ bạn. Ở đây, mình đang cấu hình Schedule là 1 phút, tức cứ mỗi phút, CronJob sẽ tạo ra 1 Job để thực hiện lệnh ``python3 inference.py`.  Để tạo CronJob, chạy lệnh sau:\n$ kubectl create -f cronjob-inference.yaml cronjob.batch/inference-cronjob created Xem thông tin của CronJob vừa tạo:\nkubectl get cronjobs NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE inference-cronjob * * * * * False 0 \u0026lt;none\u0026gt; 19s Bởi vì bản chất của CronJob là tạo ra các Jobs, nên ta hãy xem thử các Jobs được tạo ra sau mỗi phút:\n$ kubectl get jobs --watch NAME COMPLETIONS DURATION AGE inference-cronjob-1611919440 1/1 15s 82s inference-cronjob-1611919500 1/1 14s 21s inference-cronjob-1611919560 0/1 0s inference-cronjob-1611919560 0/1 0s 0s inference-cronjob-1611919560 1/1 15s 15s inference-cronjob-1611919380 1/1 16s 3m21s inference-cronjob-1611919620 0/1 0s inference-cronjob-1611919620 0/1 0s 0s Cờ --watch sẽ cho phép lắng nghe sự kiện có bất kỳ sự thay đổi nào trong việc sử dụng tài nguyên của Job, chẳng hạn như tạo, hủy Job.\nChú ý: Tên của Job = Tên của CronJob + chuỗi số ngẫu nhiên.\nĐể xem các Pods tạo ra bởi Job sau mỗi phút, cần kết hợp với Job tại thời điểm đó. Ví dụ xem logs của Job inference-cronjob-1611919380:\nkubectl get pods --selector=job-name=inference-cronjob-1611919380 NAME READY STATUS RESTARTS AGE inference-cronjob-1611919380-cqdtd 0/1 Completed 0 78s Có Pod rồi, ta có thể xem logs tạo ra bởi Pod đó:\n$ kubectl logs inference-cronjob-1611919380-cqdtd Running inference... Loading data... Loading model from: /docker/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.21374322 31.94786177 10.40175849 34.31050209 22.05210667 11.58265489 13.19650094 42.84036647 33.03218733 15.77635169 23.93521876 19.85532224 25.43466604 20.55132127 13.67707622 47.44313586 17.6460682 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.77877263 30.23411048 19.67713185 23.19380271 24.96712102 18.65459129 30.35476911 8.9560549 13.8130382 14.18848318 17.3840622 19.83840166 24.09904134 20.52649052 15.32433651 25.8157052 16.47533793 19.2214524 19.86928427 21.47113681 21.56443118 24.64517965 22.43665872 22.1020877 ] Như ta thấy, Batch Inference đã được thực hiện thành công thông qua CronJob.\nCuối cùng, xóa CronJob khi không sử dụng nữa:\n$ kubectl delete inference-cronjob cronjob.batch/inference-cronjob deleted 3. Kết luận\nXong, mình đã cũng nhau tìm hiểu và sử dụng CronJob để thực hiện nhiệm vụ Batch Inference trong bài toán AI. Mình đặt lịch chạy Job mỗi phút mục đích là để nhanh chóng nhìn thấy kết quả cho lần demo này. Tùy theo yêu cầu thực tế bài toán, các bạn có thể set giá trị phù hợp cho mình.\nBài viết tiếp theo, chúng ta sẽ tìm hiểu và thực hành với Development. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n4. Tham khảo\n Mlinproduction CronJob Crontab  ","permalink":"https://tiensu.github.io/blog/44_kubernetes_cronjob/","tags":["Model Deployment","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 3: Kubernetes CronJob"},{"categories":["Model Deployment","Kubernetes","Docker"],"contents":"Trong bài trước, chúng ta đã tìm hiểu về Pod, cách tương tác với Pod và hạn chế của nó. Bài này, chúng ta sẽ làm việc với một thành phần ở mức high level hơn của Kubernetes, đó là Job. Cụ thể, mình sẽ cùng nhau tạo ra các Job để train model và thực hiện Batch Inference.\n1. Kubernetes Job là gì?\nTheo định nghĩa từ trang chủ của Kubernetes thì:\nA Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.\nHiểu một cách đơn giản thì Jobs chịu trách nhiệm quản lý một hoặc nhiều Pods để thực hiện một công việc nào đó. Trong quá trình làm việc, các Pods có thể chạy song song với nhau, và nếu một Pod bị chết thì Job sẽ tạo ra một Pod khác để thay thể. Job chỉ được coi là hoàn thành thì tất cả các Pod của nó hoàn thành. Khi xóa Job, các Pods được quản lý bởi nó cũng bị xóa theo.\nJob rất phù hợp để chạy các tác vụ kiểu Batch, tức là các tác vụ mà chạy trong một khoảng thời gian nào đó rồi kết thúc. Trong AI, có khá nhiều tác vụ kiểu như vậy, có thể kể ra như Feature Engineering, Cross-Validation, Model Training, Batch Inference. Ví dụ, chúng ta tạo ra một Job để train một model, sau đó lưu model đó vào Storage. Một Job khác sẽ sử dụng model đó để thực hiện Batch Inference.\n2. Sử dụng Job cho các tác vụ AI\nChúng ta sẽ thử tạo 2 Jobs:\n Job thứ nhất để train ML model, lưu model ra file trên AWS S3. Job thứ hai sử dụng model đã trained để thực hiện Batch Inference.  Hãy xem cấu trúc thư mục làm việc:\nkubernetes_job │ ├── docker │ │ ├── batch_inference.py │ │ ├── Dockerfile │ │ └── train.py │ └── job │ ├── job-inference.yaml │ └── job-train.yaml 2.1 Code train \u0026amp; inference model\nTạo thư mục docker và copy 2 file train.py và batch_inference.py đã sử dụng trong các bài trước vào thư mục vừa tạo. Sử a lại nội dung của file train.py như sau:\nimport json import os import boto3 from joblib import dump import matplotlib.pyplot as plt import numpy as np from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] BUCKET_NAME = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Load data print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() print(\u0026#34;Splitting data...\u0026#34;) X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] # ############################################################################# # Fit regression model print(\u0026#34;Fitting model...\u0026#34;) params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) train_mse = mean_squared_error(y_train, clf.predict(X_train)) test_mse = mean_squared_error(y_test, clf.predict(X_test)) metadata = { \u0026#34;train_mean_square_error\u0026#34;: train_mse, \u0026#34;test_mean_square_error\u0026#34;: test_mse } print(\u0026#34;Serializing model to: {}\u0026#34;.format(MODEL_PATH)) dump(clf, MODEL_PATH) print(\u0026#34;Serializing metadata to: {}\u0026#34;.format(METADATA_PATH)) with open(METADATA_PATH, \u0026#39;w\u0026#39;) as outfile: json.dump(metadata, outfile) print(\u0026#34;Moving to S3\u0026#34;) s3 = boto3.client(\u0026#39;s3\u0026#39;) s3.upload_file(MODEL_PATH, BUCKET_NAME, MODEL_FILE) Sửa lại code của file batch_inference.py như sau:\nimport os import boto3 from joblib import load import numpy as np from sklearn import datasets from sklearn.utils import shuffle MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] BUCKET_NAME = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) def load_model(): s3 = boto3.resource(\u0026#39;s3\u0026#39;) try: s3.Bucket(BUCKET_NAME).download_file(MODEL_FILE, MODEL_PATH) except Exception as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] == \u0026#34;404\u0026#34;: print(\u0026#34;The object does not exist.\u0026#34;) else: raise return load(MODEL_PATH) def get_data(): \u0026#34;\u0026#34;\u0026#34; Return data for inference. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] return X_test, y_test print(\u0026#34;Running inference...\u0026#34;) X, y = get_data() # ############################################################################# # Load model print(\u0026#34;Loading model from: {}\u0026#34;.format(MODEL_PATH)) clf = load_model() # ############################################################################# # Run inference print(\u0026#34;Scoring observations...\u0026#34;) y_pred = clf.predict(X) print(y_pred) 2.2 Tạo Docker Images\n Tạo file Dokerfile  Cũng trong cùng thư mục docker, tạo file Dockerfile với nội dung như sau:\nFROM jupyter/scipy-notebook USER root WORKDIR /docker ADD . /docker RUN pip install awscli joblib boto3 RUN mkdir /docker/model # Env variables ENV MODEL_DIR=/docker/model ENV MODEL_FILE=clf.joblib ENV METADATA_FILE=metadata.json ENV BUCKET_NAME=kubernetes-job  Build Docker Image này:  $ docker build -t docker-ml . Sending build context to Docker daemon 6.656kB Step 1/10 : FROM jupyter/scipy-notebook ---\u0026gt; c1a7c7ef5e27 Step 2/10 : USER root ---\u0026gt; Using cache ---\u0026gt; 0c1dbc43bef8 Step 3/10 : WORKDIR /docker ---\u0026gt; Running in fdae735976d0 Removing intermediate container fdae735976d0 ---\u0026gt; b795fe3bbd80 Step 4/10 : ADD . /docker ---\u0026gt; 16082b6c9bda Step 5/10 : RUN pip install awscli joblib boto3 ---\u0026gt; Running in e4f9036ae9fc Collecting awscli Downloading awscli-1.18.221-py2.py3-none-any.whl (3.5 MB) Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (1.0.0) Collecting boto3 Downloading boto3-1.16.61-py2.py3-none-any.whl (130 kB) Collecting s3transfer\u0026lt;0.4.0,\u0026gt;=0.3.0 Downloading s3transfer-0.3.4-py2.py3-none-any.whl (69 kB) Collecting botocore==1.19.61 Downloading botocore-1.19.61-py2.py3-none-any.whl (7.2 MB) Collecting PyYAML\u0026lt;5.4,\u0026gt;=3.10 Downloading PyYAML-5.3.1.tar.gz (269 kB) Collecting colorama\u0026lt;0.4.4,\u0026gt;=0.2.5 Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB) Collecting rsa\u0026lt;=4.5.0,\u0026gt;=3.1.2 Downloading rsa-4.5-py2.py3-none-any.whl (36 kB) Collecting docutils\u0026lt;0.16,\u0026gt;=0.10 Downloading docutils-0.15.2-py3-none-any.whl (547 kB) Collecting jmespath\u0026lt;1.0.0,\u0026gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Requirement already satisfied: urllib3\u0026lt;1.27,\u0026gt;=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore==1.19.61-\u0026gt;awscli) (1.26.3) Requirement already satisfied: python-dateutil\u0026lt;3.0.0,\u0026gt;=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore==1.19.61-\u0026gt;awscli) (2.8.1) Requirement already satisfied: six\u0026gt;=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil\u0026lt;3.0.0,\u0026gt;=2.1-\u0026gt;botocore==1.19.61-\u0026gt;awscli) (1.15.0) Collecting pyasn1\u0026gt;=0.1.3 Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB) Building wheels for collected packages: PyYAML Building wheel for PyYAML (setup.py): started Building wheel for PyYAML (setup.py): finished with status \u0026#39;done\u0026#39; Created wheel for PyYAML: filename=PyYAML-5.3.1-cp38-cp38-linux_x86_64.whl size=44618 sha256=421030371a2f82fdfd722d0b032ce5b0c8d01e02a5ca379c9a0e2eea3a03fd78 Stored in directory: /tmp/pip-ephem-wheel-cache-cs65titp/wheels/13/90/db/290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c Successfully built PyYAML Installing collected packages: jmespath, pyasn1, botocore, s3transfer, rsa, PyYAML, docutils, colorama, boto3, awscli Attempting uninstall: PyYAML Found existing installation: PyYAML 5.4.1 Uninstalling PyYAML-5.4.1: Successfully uninstalled PyYAML-5.4.1 Successfully installed PyYAML-5.3.1 awscli-1.18.221 boto3-1.16.61 botocore-1.19.61 colorama-0.4.3 docutils-0.15.2 jmespath-0.10.0 pyasn1-0.4.8 rsa-4.5 s3transfer-0.3.4 Removing intermediate container e4f9036ae9fc ---\u0026gt; 0f7e7ec1c6a0 Step 6/10 : RUN mkdir /docker/model ---\u0026gt; Running in 7f4c3bd5253a Removing intermediate container 7f4c3bd5253a ---\u0026gt; 0b2c845fbb40 Step 7/10 : ENV MODEL_DIR=/docker/model ---\u0026gt; Running in 643ef25a50eb Removing intermediate container 643ef25a50eb ---\u0026gt; 339c22e0a4f9 Step 8/10 : ENV MODEL_FILE=clf.joblib ---\u0026gt; Running in d81f810dc092 Removing intermediate container d81f810dc092 ---\u0026gt; cd7ecdc2f380 Step 9/10 : ENV METADATA_FILE=metadata.json ---\u0026gt; Running in 701460e9b463 Removing intermediate container 701460e9b463 ---\u0026gt; 7646e477d5a9 Step 10/10 : ENV BUCKET_NAME=kubernetes-job ---\u0026gt; Running in ce92e3cdbc3b Removing intermediate container ce92e3cdbc3b ---\u0026gt; 31d25c8be720 Successfully built 31d25c8be720 Successfully tagged docker-ml:latest  Push Docker Image lên Docker Hub:  Sử dụng các lệnh sau để push Docker Image vừa build lên Docker Hub\n$ docker tag docker-ml:latest tiensu/docker-ml:latest $ docker push tiensu/docker-ml:latest The push refers to repository [docker.io/tiensu/docker-ml] 76fba3826ca9: Pushed 59928edb97b5: Pushed b5e012598fbb: Pushed c4e3257e6eb5: Pushed 5f70bf18a086: Mounted from tiensu/ml-model-batch-infer 6f5a41ae77fd: Mounted from tiensu/ml-model-batch-infer 5a1b9a3f9355: Mounted from tiensu/ml-model-batch-infer b1d7816bac14: Mounted from tiensu/ml-model-batch-infer c91fed2d1998: Mounted from tiensu/ml-model-batch-infer cc70098d00e3: Mounted from tiensu/ml-model-batch-infer 88727e93cbac: Mounted from tiensu/ml-model-batch-infer cadaf24035f3: Mounted from tiensu/ml-model-batch-infer 8f170f4774e3: Mounted from tiensu/ml-model-batch-infer 33bd52db887f: Mounted from tiensu/ml-model-batch-infer 21e5dd010f50: Mounted from tiensu/ml-model-batch-infer ea370ab22368: Mounted from tiensu/ml-model-batch-infer 421d1408f872: Mounted from tiensu/ml-model-batch-infer 18fd1ca0de51: Mounted from tiensu/ml-model-batch-infer 8f01aab6d756: Mounted from tiensu/ml-model-batch-infer e18a1c4e1d31: Mounted from tiensu/ml-model-batch-infer 8552f27c3cd8: Mounted from tiensu/ml-model-batch-infer 1a4c57efcc23: Mounted from tiensu/ml-model-batch-infer 94b8fe888eac: Mounted from tiensu/ml-model-batch-infer 02473afd360b: Mounted from tiensu/ml-model-batch-infer dbf2c0f42a39: Mounted from tiensu/ml-model-batch-infer 9f32931c9d28: Mounted from tiensu/ml-model-batch-infer latest: digest: sha256:40678bdd8d763129322db38be9f83bc70d1278b7836c7c7f4f4ac3ef6af20e5e size: 6582 2.3 Tạo Kubernetes Job để train ML model\nTương tự như tạo Pod, để tạo Job ta cũng cần khai báo các thông tin cần thiết trong file cấu hình job-train.yaml:\napiVersion: batch/v1 kind: Job metadata: name: job-train-ml-model spec: template: spec: containers: - name: train-container imagePullPolicy: Always image: tiensu/docker-ml:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;train.py\u0026#34;] env: - name: AWS_ACCESS_KEY_ID value: \u0026#34;\u0026#34; - name: AWS_SECRET_ACCESS_KEY value: \u0026#34;\u0026#34; restartPolicy: Never backoffLimit: 0 Một số thông tin như sau:\n apiVersion: Phiên bản của Kubernetes API. kind: Loại tài nguyên của Kubernetes cần tạo, ở đây là Job. metadata: Danh sách các nhãn, các thuộc tính tùy ý mà người phát triển có thể gắn cho Job. Thường các thông tin về Metadata của ML model được gắn ở đây. Kubernetes cũng khuyến nghị một số nhãn ở đây. spec.template: Chính là phần cấu hình của Pod mà ta cần khai báo, tương tự như cấu hình của Pod mà ta đã tạo ở bài trước.  imagePullPolicy: Cho phép Kubernetes luôn luôn sử dụng Docker Image từ Docker Hub thay vì Cache Image. env: Danh sách các biến môi trường để Pod sử dụng. Ở đây, chúng ta khai bào 2 biến liên quan đến AWS để làm việc với AWS S3. Mình đã xóa các key mà mình sử dụng. Nếu bạn muốn chạy thử thì hãy thêm key của bạn vào nhé!   restartPolicy: Có khởi động lại Container khi nó bị chết hay không? backoffLimit: Số lần cố gắng thực hiện lại Job khi nó bị thất bị.  Chạy lệnh sau để tạo và kiểm tra trạng thái của Job:\n$ kubectl create -f job-train.yaml job.batch/job-train-ml-model created $ kubectl get jobs NAME COMPLETIONS DURATION AGE job-train-ml-model 1/1 58s 2m19s Kiểm tra xem các pods của Job là gì và trạng thái của chúng:\n$ kubectl get pods --selector=job-name=job-train-ml-model NAME READY STATUS RESTARTS AGE job-train-ml-model-6fkcd 0/1 Completed 0 2m19s Xem logs Job/Pod:\n$ kubectl logs job-train-ml-model-6fkcd Loading data... Splitting data... Fitting model... Serializing model to: /docker/model/clf.joblib Serializing metadata to: /docker/model/metadata.json Moving to S3 Như vậy, có thể thấy là Job đã chạy xong, file model đã được lưu trên S3.\nCuối cùng, ta có thể xóa Job sau khi chúng đã hoàn thành nhiệm vụ của mình:\n$ kubectl delete job job-train-ml-model job.batch \u0026#34;job-train-ml-model\u0026#34; deleted 2.4 Tạo Kubernetes Job để thực hiện Batch Inference\nChúng ta sẽ sử dụng lại Docker Image đã tạo ở trên cho Job này.\nFile cấu hình của Job (job-inference.yaml) như sau:\napiVersion: batch/v1 kind: Job metadata: name: job-inference-ml-model spec: template: spec: containers: - name: inference-container imagePullPolicy: Always image: tiensu/docker-ml:latest command: [\u0026#34;python3\u0026#34;, \u0026#34;batch_inference.py\u0026#34;] env: - name: AWS_ACCESS_KEY_ID value: \u0026#34;\u0026#34; - name: AWS_SECRET_ACCESS_KEY value: \u0026#34;\u0026#34; restartPolicy: Never backoffLimit: 0 So với cấu hình của Job phía trên, chỉ có các thông tin sau thay đổi: Job name, container name, container command.\nĐể tạo và liểm tra trạng thái của Job, chạy lệnh sau:\n$ kubectl create -f job-inference.yaml job.batch/job-inference-ml-model created $ kubectl get jobs NAME COMPLETIONS DURATION AGE job-inference-ml-model 1/1 13s 66s Kiểm tra xem các Pods của Job và trạng thái tương ứng:\n$ kubectl get pods --selector=job-name=job-inference-ml-model NAME READY STATUS RESTARTS AGE job-inference-ml-model-sk2m4 0/1 Completed 0 2m11s Chú ý: Tên của Pod = Tên của Job + chuỗi ngẫu nhiên.\nXem logs của Job/Pod:\n$ kubectl logs job-inference-ml-model-sk2m4 Running inference... Loading data... Loading model from: /docker/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.21374322 31.94786177 10.40175849 34.31050209 22.05210667 11.58265489 13.19650094 42.84036647 33.03218733 15.77635169 23.93521876 19.85532224 25.43466604 20.55132127 13.67707622 47.44313586 17.6460682 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.77877263 30.23411048 19.67713185 23.19380271 24.96712102 18.65459129 30.35476911 8.9560549 13.8130382 14.18848318 17.3840622 19.83840166 24.09904134 20.52649052 15.32433651 25.8157052 16.47533793 19.2214524 19.86928427 21.47113681 21.56443118 24.64517965 22.43665872 22.1020877 ] Như vậy là Job đã thực hiện Batch Inference thành công bằng model nhận được từ S3.\nCuối cùng, xóa Job sau khi nó đã hoàn thành nhiệm vụ để tiết kiệm tài nguyên server:\n$ kubectl delete job job-inference-ml-model job.batch \u0026#34;job-inference-ml-model\u0026#34; deleted 3. Kết luận\nNhư vậy là mình đã cùng các bạn tìm hiểu và sử dụng Kubernetes Job để thực hiện các tác vụ của một bài toán AI. Có một lưu ý dành cho các bạn đó là trong trường hợp việc thực hiện tạo Job thất bại, hãy nhớ sử dụng lệnh kubectl describe pod \u0026lt;pod_name\u0026gt;, trong đó pod_name là tên Pod của Job để xem đầy đủ logs. Dựa vào logs này, các bạn có thể dễ dàng phát hiện ra nguyên nhân lỗi và cách khắc phục chúng.\nbài viết tiếp theo, chúng ta sẽ tìm hiểu và thực hành với CronJob. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n4. Tham khảo\n Mlinproduction Kubernetes Jobs  ","permalink":"https://tiensu.github.io/blog/43_kubernetes_job/","tags":["Model Deployment","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 2: Kubernetes Job"},{"categories":["Model Deployment","Kubernetes","Docker"],"contents":"Trong các bài viết trước, mình đã giới thiệu về Docker, sử dụng kết hợp với Nginx, uWSGI, Flask để deploy model trong môi trường production. Nhìn chung mà nói, cách kết hợp 4 dịch vụ này đủ để áp ứng cho hầu hết các bài toán AI, ngoại trừ vấn đề cấu hình tương đối phức tạp và khó triển khai trên cloud (thực tế là AWS và GCP đề không hỗ trợ cách này, nếu muốn chúng ta vẫn phải cấu hình bằng tay như dưới local).\nGần đây, Kubernetes nổi lên như là một xu hướng mới, đáp ứng đầy đủ các yêu cầu của việc triển khai model trong môi trường production. Hơn thế nữa, việc cấu hình rất đơn giản và được hỗ trợ bởi các ông lớn cloud (AWS và GCP đều có dịch vụ Kubernetes). Trong loạt bài tiếp theo, mình sẽ cùng mọi người tìm hiểu về hot trend này và cách thức sử dụng nó để deploy các AI model của chúng ta nhé!\n1. Kubernetes là gì?\nTheo định nghĩa từ trang chủ của Kubernetes thì:\nKubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.\nPhần tử hạt nhân của Kubernetes chính là các Containers (Docker Container). Nói theo một cách khác, Kubernetes giúp chúng ta:\n Manage containers: Quản lý đồng thời nhiều docker containers của cùng một ứng dụng hoặc thậm chí là nhiều ứng dụng khác nhau. Manage lifecycle: Quản lý toàn bộ lifecycle của các containers, từ lúc được tạo ra đến khi bị xóa bỏ. Hardware optimization: Tối ưu hóa, tối đa hóa khả năng của phần cứng thiết bị. Schedule: Lập lịch khi nào cần bật/tắt containers. Load balancer: Phân tải xử lý đều cho các containers. Backup: Khi một container chết, sẽ có một container khác đươc tạo để thay thế. Scalling: Dễ dàng scale các containers up/down theo một trong 2 chế độ: tự động hoặc bằng tay. Monitor system: Dễ dàng giám sát hoạt động của toàn bộ hệ thống.  Việc cấu hình cho Kubernetes khá đơn giản, tất cả chỉ thông qua một file cấu hình duy nhất.\nTrong tiếng Hy Lạp, Kubernetes có nghĩa là người chỉ huy hay thuyền trưởng.\nTất cả những đặc điểm trên đều phù hợp với giải pháp mà chúng ta tìm kiếm để đưa AI model vào môi trường production. Tất nhiên là phạm vi ứng dụng của Kubernetes còn rộng lớn hơn rất nhiều, nhưng trong lĩnh vực làm việc và nghiên cứu của mình, mình chỉ tập trung tìm hiểu và sử dụng Kubernetes cho các bài toán về AI.\n2. Kiến trúc và thành phần của Kubernetes\n Kubernetes bao gòm các Nodes, được chia thành 2 loại: Master Node và Worker Nodes. Master Nodes chịu trách nhiệm quản lý các Worker Nodes, trong khi các Worker Nodes làm nhiệm vụ thực hiện các công việc tính toán, \u0026hellip; Mỗi Worker Node lại được chia nhỏ thành các Pods, và trong mỗi Pod chính là các Containers.\nPhần còn lại của bài hôm nay, mình sẽ cùng các bạn tìm hiểu về Pod. Các bài tiếp theo, chúng ta sẽ làm việc với Job, CronJob, Deployment, Service.\n3. Kubernetes Pod\n3.1 Kubernetes Pod là gì?\nTheo định nghĩa, Pod là đối tượng nhỏ nhất có khả năng triển khai trong kiến trúc của Kubernetes, tức là bạn có thể tạo, sử dụng, hay xóa Pod. Có thể coi Pod chính là đại diện của một ứng dụng (instance application) chạy trong Kubernetes.\nNhư đã nói ở phần 2, mỗi Pod chứa một hoặc nhiều Containers để thực hiện một công việc (Job) nào đó. Các Containers trong cùng Pod nằm trong cùng một mạng local và chia sẻ tài nguyên sử dụng với nhau. Chính vì thế mà chúng dễ dàng giao tiếp và làm việc với nhau.\nVì Pod là Single Instance của ứng dụng chạy trong Kebernetes, số lượng Pod được tạo ra hay xóa đi một cách tự động (Load Balancing \u0026amp; Failure Recovery), tùy theo tải mà ứng dụng phải phục vụ.\n3.2 Tạo Pod từ Docker Image có sẵn\nĐể làm việc được với Pod, trước tiên cần phải cài đặt kubectl theo hướng dẫn trên trang chủ của Kubernetes tại đây hoặc tại đây\nCách dễ nhất để tạo và triển khai Kubernetes là sử dụng config file. File này sẽ chỉ định đối tượng được tạo là gì, các metadata gắn với đối tượng đó, tài nguyên cần thiết là bao nhiêu, \u0026hellip;\nDưới đây là template của config file (pod_public.yaml) để tạo một Pod:\napiVersion: v1 kind: Pod metadata: name: python3-pod labels: app: python3 spec: containers: - name: python3-container image: python:3.6 command: [\u0026#39;python3\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;print(\u0026#34;Hello, World!\u0026#34;)\u0026#39;] restartPolicy: Never File config này bao gồm những thông tin sau:\n apiVersion: Phiên bản của Kubernetes API đang sử dụng. kind: Loại tài nguyên (đối tượng) của Kubernetes được tạo ra: Pod, Job, Development, \u0026hellip; Ở đây là Pod object. metadata: Là một tập hợp các labels và các thuộc tính của model mà người phát triển có thể thêm vào tùy ý giống như phiên bản, độ chính xác, thuật toán, \u0026hellip; spec: Bao gồm thông tin của các Containers chạy bên trong Pod: tên, docker image, command. Như trong cấu hình hiện tại thì chỉ có 1 Container. restartPolicy: Cho phép Container có restart hay không khi nó bị lỗi. Giá trị never ở đây tức là không cho phép restart.  Thực hiên lệnh sau để tạo Pod:\n$ kubectl create -f pod_public.yaml pod \u0026#34;python3-pod\u0026#34; created Kiểm tra trạng thái của pod vừa tạo:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE python3-pod 0/1 Completed 0 3s Xem log của pod vừa tạo:\n$ kubectl logs python3-pod Hello, World! Xóa pod vừa tạo:\n$ kubectl delete -f pod_public.yaml pod \u0026#34;python3-pod\u0026#34; deleted 3.3 Tạo Pod từ Docker Image tự tạo\nMình sẽ sử dụng Docker Image đã tạo từ bài này để đưa vào Pod.\nTrước tiên, bạn hãy login vào Docker Hub để tạo một Repository. Giả sử mình tạo Repository tên là ml-model-batch-infer.\nở máy local, thực hiện các bước sau để đưa Docker Image đã tạo lên Repository:\n Login vào Docker Hub  $ docker login -u tiensu Trong đó, tiensu là tên đăng nhập của mình, bạn hãy thay bằng tên đăng nhập của bạn. Nhập mật khẩu khi được hỏi.\n Gán Tag cho Docker Image theo tên mới trên Repository  docker tag docker-model-batch-infer:latest tiensu/ml-model-batch-refer:latest  Push Docker Image đã gắn Tag lên Repository  docker push tiensu/ml-model-batch-infer:latest Output:\nThe push refers to repository [docker.io/tiensu/ml-model-batch-infer] 2a0a8f09fca2: Pushed ea3e588d9e9f: Pushed 2b8e8179f02d: Pushed 254c54a05297: Pushed bdeb303132f3: Pushed 5f70bf18a086: Mounted from jupyter/scipy-notebook 6f5a41ae77fd: Mounted from jupyter/scipy-notebook 5a1b9a3f9355: Mounted from jupyter/scipy-notebook b1d7816bac14: Mounted from jupyter/scipy-notebook c91fed2d1998: Mounted from jupyter/scipy-notebook cc70098d00e3: Mounted from jupyter/scipy-notebook 88727e93cbac: Mounted from jupyter/scipy-notebook cadaf24035f3: Mounted from jupyter/scipy-notebook 8f170f4774e3: Mounted from jupyter/scipy-notebook 33bd52db887f: Mounted from jupyter/scipy-notebook 21e5dd010f50: Mounted from jupyter/scipy-notebook ea370ab22368: Mounted from jupyter/scipy-notebook 421d1408f872: Mounted from jupyter/scipy-notebook 18fd1ca0de51: Mounted from jupyter/scipy-notebook 8f01aab6d756: Mounted from jupyter/scipy-notebook e18a1c4e1d31: Mounted from jupyter/scipy-notebook 8552f27c3cd8: Mounted from jupyter/scipy-notebook 1a4c57efcc23: Mounted from jupyter/scipy-notebook 94b8fe888eac: Mounted from jupyter/scipy-notebook 02473afd360b: Mounted from jupyter/scipy-notebook dbf2c0f42a39: Mounted from jupyter/scipy-notebook 9f32931c9d28: Mounted from jupyter/scipy-notebook latest: digest: sha256:2552cb24c104d9b4fe3a43cc952371a7a1b0cce84e1c95821622b4fe508a6877 size: 6786 Để tạo Pod với Docker Image này, cập nhật lại file config (đổi tên thành pod_custom.yaml) của Pod như sau:\napiVersion: v1 kind: Pod metadata: name: pod-ml-model-batch-infer labels: app: python3 spec: containers: - name: container-ml-model-batch-infer image: tiensu/ml-model-batch-infer:latest command: [\u0026#39;python3\u0026#39;, \u0026#39;batch_inference.py\u0026#39;] restartPolicy: Never Chạy lệnh sau để tạo Pod:\n$ kubectl create -f pod_custom.yaml pod/pod-ml-model-batch-infer created Kiểm tra trạng thái của Pod vừa tạo:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE command-demo 0/1 Completed 0 113m pod-ml-model-batch-infer 0/1 Completed 0 5m28s python3-pod 0/1 Completed 0 90m Chú ý là Docker Image của chúng ta được tải về trên Worker Node. Bạn có thể kiểm tra trên đó bằng lệnh $ docker ps.\nChúng ta có thể xem miêu tả chi tiết quá trình tạo Pod như sau:\n$ kubectl describe pod pod-ml-model-batch-infer Name: pod-ml-model-batch-infer Namespace: default Priority: 0 Node: duynm-vostro-3670/10.1.34.169 Start Time: Wed, 27 Jan 2021 16:44:15 +0700 Labels: app=python3 Annotations: cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs: Status: Succeeded IP: 192.168.24.198 IPs: IP: 192.168.24.198 Containers: container-ml-model-batch-infer: Container ID: docker://535749cae10e6dd605030b6d84ba978cc245bfd44bb6981d3307a3ffa8a5bf94 Image: tiensu/ml-model-batch-infer:latest Image ID: docker-pullable://tiensu/ml-model-batch-infer@sha256:2552cb24c104d9b4fe3a43cc952371a7a1b0cce84e1c95821622b4fe508a6877 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: python3 batch_inference.py State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 27 Jan 2021 16:44:24 +0700 Finished: Wed, 27 Jan 2021 16:44:24 +0700 Ready: False Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-gmswp (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: default-token-gmswp: Type: Secret (a volume populated by a Secret) SecretName: default-token-gmswp Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m18s default-scheduler Successfully assigned default/pod-ml-model-batch-infer to duynm-vostro-3670 Normal Pulling 4m15s kubelet Pulling image \u0026#34;tiensu/ml-model-batch-infer:latest\u0026#34; Normal Pulled 4m11s kubelet Successfully pulled image \u0026#34;tiensu/ml-model-batch-infer:latest\u0026#34; in 4.353859959s Normal Created 4m9s kubelet Created container container-ml-model-batch-infer Normal Started 4m9s kubelet Started container container-ml-model-batch-infer Cuối cùng, hãy xem log tạo ra khi thực hiên Inference:\n$ kubectl logs pod-ml-model-batch-infer Running inference... Loading data... Loading model from: /code/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.20025598 31.94786177 10.42732759 34.12058193 22.05210667 11.58265489 13.1649368 42.84036647 33.03218733 15.77635169 23.93521876 19.91587166 25.43466604 20.55132127 13.65254047 47.47279364 17.58214889 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.65978361 30.18436261 19.67895051 23.22841646 24.94197905 18.65459129 30.19731636 8.9560549 13.8130382 14.23277857 17.3840622 19.83840166 24.91315811 20.44991809 15.32433651 25.8157052 16.47533793 19.2214524 19.87110293 21.47113681 21.56443118 24.64517965 22.43665872 22.18289286] 7. Kết luận\nMặc dù Pod là đối tượng quan trọng, không thể thiếu trong bất kỳ kiến trúc Kubernetes nào nhưng các Best Practice đều không khuyến khích việc sử dung nó một cách trực tiếp, mà nên được triển khai cùng với các đối tượng khác ở mức cao hơn của Kubernetes để quản lý nó, như Job chẳng hạn. Job sẽ tạo ra một hoặc nhiều Pods, và khi một Pod bị chết thì Pod khác sẽ được bật lên để sẵn sàng thay thế cho nó.\nChúng ta sẽ tìm hiểu vấn đề này trong bài viết tiếp theo. Mời các bạn đón đọc!\nSource code của bài này các bạn tham khảo tại đây.\n8. Tham khảo\n Mlinproduction Docker Hub  ","permalink":"https://tiensu.github.io/blog/42_kubernetes_in_ai/","tags":["Model Deployment","Kubernetes","Docker"],"title":"Tìm hiểu về Kubernetes và áp dụng vào bài toán AI - Phần 1: Kubernetes Pod"},{"categories":["Model Deployment"],"contents":"Hẳn các bạn đã biết, trong hầu hết các bài toán AI, chúng ta không chỉ train model 1 lần rồi thôi (mình không nói đến việc thử-sai trong quá trình tuning model). Tại thời điểm này, model hoạt động tốt đúng như những gì ta mong đợi, nhưng sau một thời gian, hiệu năng của model có thể giảm xuống. Đó là một trong những dấu hiệu chỉ ra rằng ta phải retrain lại model. Trong bài hôm nay, mình sẽ cùng các bạn tìm hiểu chi tiết hơn về vấn đề này.\n1. Model Drift\nModel Drift là khái niệm mô tả hiện tượng hiệu năng dự đoán của model suy giảm theo thời gian do có sự thay đổi của môi trường làm sai lệch các giả thiết ban đầu của model. Thuật ngữ Model Drift (model chuyển dịch) có thể khiến chúng ta hơi bối rối 1 chút, vì bản chất là model không thay đổi, chỉ có các yếu tố môi trường bên ngoài thay đổi, input data thay đổi.\n2. Làm sao để nhận biệt hiện tượng Model Drift\n2.1 Kiểm tra độ chính xác của model\nBiểu hiên trực tiếp và rõ ràng nhất của Model Drift là độ chính xác dự đoán (độ chính xác ở đây dùng chung cho tất cả các metrics đánh giá model) giảm dần theo thời gian. Nhưng việc giám sát việc này không phải lúc nào cũng đơn giản bởi vì ta phải có cả kết quả dự đoán của model và ground truth, đặc biệt khi model đang chạy trong sản phầm thực tế (môi trường production hay online).\nCó một cách đơn giản hơn để kiểm tra độ chính xác của model có bị suy giảm hay không, đó là offline monitor. Cách này được thực hiện trước khi model triển khai model vào môi trường production. Giả sử ra có dữ liệu từ 01/2019 đến 01/2021. Ta sẽ sử dụng dữ liệu từ 01/2019 đến 06/2020 đê train và đánh giá model, sau đó sử dụng model này để dự đoán trên dữ liệu tháng 07/2020 đến 01/2021. Kết quả dự đoán được lưu lại để đánh giá xem độ chính xác của model có suy giảm hay không, nếu có thì mức độ suy giảm như thế nào? \u0026hellip; Sử dụng cách này cho phép chúng ta ước lượng được tốc độ suy giảm độ chính xác, từ đó lên kế hoạch retrain lại model.\n2.2 Kiểm tra phân bố của dữ liệu\nNếu phân bố của dữ liệu mới có sự sai khác so với dữ liệu huấn luyện model từ ban đầu thì độ chính xác của model cũng sẽ giảm. Vì thế, đây cũng là một dấu hiệu nhận biết sớm của hiện tượng Model Drift.\nĐể đánh giá sự phân bố của dữ liệu, có thể dựa vào các yếu tố sau:\n Phạm vi giá trị của các features Đồ thị histogram của các features Các features có được cho phép nhận giá trị NULL hay không? \u0026hellip;  Facets là một công cụ cho phép chúng ta nhanh chóng nhận ra sự thay đổi trong phân bố dữ liệu dựa trên sự quan sát các đồ thị phân bố trên dashboards. Việc theo dõi này có thể được thực hiện một cách tự động và nó sẽ gửi thống báo cho chúng ta khi sự phân bố dữ liệu thay đổi vượt quá một ngưỡng nào đó.\n2.3 Kiểm tra sự tương quan giữa các features trong dữ liệu\nMối qua hệ giữa các features cũng ảnh hướng đến độ chính xác của model. Vì vậy, kiểm tra sự tương quan giữa các features từng đôi một xem chúng thay đổi ra sao cũng là một cách để nhận biết Model Drift.\n3. Hiểu đúng về Model Retraining\nChúng ta đều hiểu rằng Model Retraining tức là training lại model, tạo ra model mới tốt hơn model cũ. Nhưng nếu chỉ chung chung như thế thì có rất nhiều cách để retraining model:\n Thay đổi hyper-parameters Thay đổi thuật toán ML/DL (model algorithm) Thêm/bớt các features \u0026hellip;  Giữa những cách retraining model kể trên, đâu là cách đúng nhất để loại bỏ hiện tượng Model Drift?\nQuay lại khái niệm của Model Drift, đó là hiện tượng độ chính xác của model suy giảm do có sự thay đổi trong phân phối dữ liệu. Vậy ta chỉ cần train lại model trên tập dữ liệu mới và giữ nguyên tất cả những cái khác: hyper-parameters, thuật toán, features, \u0026hellip; Hiểu một cách đơn giản hơn thì tức là ta sẽ không thay đổi dòng code nào cả, chỉ thay đổi nội dung của file chứa dữ liệu mới để train model.\nNói vậy, không có nghĩa là chúng ta bỏ qua hoàn toàn các cách retraing model khác. Nếu bạn có đủ thời gian, công sức, bạn hay các thành viên trong dự án của bạn hoàn toàn có thể thử nghiệm cách retraining model kể trên. Sau đó sử dụng chiến lược A/B Test để đánh giá các models dựa trên các tiêu chí của bài toán. Model nào cho cho kết quả tốt hơn thì sẽ được sử dụng trong môi trường production.\n4. Tần suất Retrain Model\nMột vấn đề tiếp theo cần quan tâm là tần suất retrain model như thế nào là hợp lý?\nCâu trả lời là không có một quy định, quy tắc cụ thể nào cả. Tùy từng bài toán mà ta có cách xử lý khác nhau.\n Retrain model tại một thời điểm cố định nếu ta biết trước chính xác thời điểm dữ liệu có sự thay đổi lớn. VD: tại các trường đại học, đầu mỗi năm học đều có số lượng lớn sinh viên nhập học và ra trường thì ta nên retrain lại model tại thời điểm đó. Retrain model khi thu thập được đủ một lượng dữ liệu nhất định Retrain model khi các metrics mà ta theo dõi (như đề cập trong mục 2) thay đổi vượt quá một ngưỡng nào đó.  Đối với cách thứ 2\u0026amp;3, cần phải có một hạ tầng độc để giám sát và đưa ra cảnh báo khi sự thay đổi đạt đến mức quy định. Việc chọn ngưỡng cho các metrics cũng cần phải xem xét cẩn thận. Ngưỡng quá thấp sẽ làm cho tần suất retrain model thường xuyên hơn, dẫn đến tốn kém chi phí tính toán (đặc biệt quan trong trường trường hợp sử dụng tài nguyên trên cloud). Ngưỡng quá cao làm cho model không thay đổi kịp với sự thay đổi của môi trường, dẫn đến không tối ưu hóa lợi nhuận, \u0026hellip;\nĐặc biệt trong trường hợp model cần thay đổi realtime mỗi khi có bất cứ dữ liệu mới (VD model dự đoán giao dịch ngân hàng an toàn hay không an toàn) thì nên sử dụng phương pháp học tăng dần, Incremental Learning / Online Learning. Phương pháp này khác các cách Retrain Model đã đề cập ở chỗ model được retrain (cập nhật) chỉ sử dụng dữ liệu mới, không phải retrain trên toàn bộ dữ liệu.\n5. Chạy Retrain Model tự động\nCách cấu hình để Retrain Model tự động liên quan đến tần suất retrain model của bạn.\n  Nếu model được retrained định kỳ, chúng ta có thể sử dụng Kubernetes CronJobs hoặc Jenkins để lập lịch cho model chạy retrain.\n  Nếu model đươc retrained dựa vào trigger khi các metrics thay đổi đến ngưỡng được phát hiện, chúng ta có thể sử dụng Kubernetes Jobs hoặc Jenkins để làm việc này.\n  Cuối cùng, nếu model cần retrain realtime, sử dụng phương pháp Online Learning. River là thư viện lý tưởng cho việc này. Tên cũ của nó là Creme.\n  6. Implement code prototype\n6.1 Query Data by Date Range function\nBởi vì quá trình retraining dựa trên dữ liệu mới, nên chúng ta cần 1 hàm lấy ra những dữ liệu đó, theo 1 khoảng thời gian quy đinh. Dữ liệu mới có thể được lưu ở SQL database, S3, local storage, \u0026hellip;\ndef get_raw_data(end_date, date_window=365): \u0026#39;\u0026#39;\u0026#39; Retrieve all data in date range (end_date - date_window, end_date) \u0026#39;\u0026#39;\u0026#39; Trong đó:\n end_date: ngày cuối cùng trong khoảng thời gian của dữ liệu mới. date_window: số lượng ngày trong khoảng thời gian của dữ liệu mới, tính từ end_date trở lại.  Để nhận dữ liệu mới cho việc retraining model, chúng ta sẽ gọi:\nfrom datetime import date training_data = get_new_data(date.today()) 6.2 Generate a Machine Learning Model function\nHàm này chịu trách nhiệm train AI model: chia dataset thành tập train và tập test, trích xuất vector đặc trừng từ dữ liệu, thực hiện tuning hyper-parameters, huấn luyện model, đánh giá model, \u0026hellip;\nfind_optimal_model(data, ...): \u0026#39;\u0026#39;\u0026#39; Split data, generate features, tune hyper-parameters, train model, ... \u0026#39;\u0026#39;\u0026#39; Tham số data là dữ liệu để huấn luyện mode. Kết quả thực thi của hàm sẽ trả về model đã được trained và các training metrics.\n6.3 Store Trained Model\nMột khi model được trained xong, ta cần lưu nó lại để sử dụng về sau. Cách đơn giản nhất là sử dụng thư viện pickle có sẵn của python. Ngoài ta, bạn cũng có thể sử dụng ONNX hoặc PMML.\n# Serialize and store model on local storage def serialize_model(training_arfifacts): \u0026#39;\u0026#39;\u0026#39; Return a local path to serialized model \u0026#39;\u0026#39;\u0026#39; 6.4 Registry Model\nTham khảo bài Model Registry\n6.5 Model Retraining Enpoint\nTập hợp tất cả các hàm lại trong một script để đơn giản hóa quy trình, retrain.py. Script sẽ chấp nhận một tham số từ command line là end_date, nhận về dữ liệu mới, train mode, store model và registry model.\nfrom datetime import date import sys def retrain(end_date): \u0026#39;\u0026#39;\u0026#39;Model retraining loop.\u0026#39;\u0026#39;\u0026#39; data = get_raw_data(end_date) training_artifacts = find_optimal_model(data, ...) local_path = serialize_model(training_artifacts) model_registry(local_path, training_artifacts) if __name__ == \u0026#39;__main__\u0026#39;: retrain(sys.argv[1]) 6.6 Scheduling the Retraining Procedure\nScript retrain.py lại tiếp tục được đóng gói trong bash script, retrain.sh:\ntoday_date=\u0026#39;date +”%m/%d/%Y”\u0026#39; python retrain.py $today_date Để trigger event gọi đến bash script này, chúng ta có thể lập lịch, sử dụng một trong các công cụ sau:\n Jenkins Airflow Kubernetes Cronjobs Crontab  Các công cụ này đều hỗ trợ đầy đủ việc xử lý ngoại lệ, cơ chế retry, \u0026hellip; Tuy nhiên, viêc thiết lập và cài đặt sẽ tương đối mất thời gian nếu bạn chưa quen thuộc với chúng.\n6.7 Retrieve the Model at Inference Time\nTham khảo bài Model Registry\n7. Kết luận\nBài này, chúng ta đã bàn rất nhiều về Model Retraining. Hi vọng là bạn đã hiểu được phần nào tất cả các khía cạnh của nó để xem xét áp dụng vào dự án của bạn.\nBài viết tiếp theo, chúng ta sẽ cùng tìm hiểu về Kubernetes và áp dụng nó cho các bài toàn AI. Mời các bạn đón đọc!\n8. Tham khảo\n Mlinproduction  ","permalink":"https://tiensu.github.io/blog/41_ai_model_data_driff_and_retraining/","tags":["Model Deployment","Data Driff","Model Retraining"],"title":"Tìm hiểu về hiện tượng Data Driff và cách cấu hình AI Model Retraining"},{"categories":["Model Deployment"],"contents":"Khi giải quyết một bài toán AI, rất hiếm khi số lượng model được huấn luyện và đưa vào sử dụng dừng lại ở con số 1. Bởi vì theo thời gian, dữ liệu thay đổi, yêu cầu thay đổi, \u0026hellip; dẫn đến việc chúng ta cần cập nhật lên model mới hơn. Trong trường hợp đó, làm thế nào để quản lý được tất cả các models đó một các hợp lý, đảm bảo sử dụng đúng model mong muốn để thực hiện inference và không làm gián đoạn quá trình inference đang chạy? Đó chính là câu chuyện của Model Registry.\nTrong bài hôm nay, chúng ta sẽ cùng nhau implement một Model Registry đơn giản, sử dụng SQLite. Bạn cũng sử dụng bất kỳ database nào bạn muốn.\n1. Model Registry Database\nĐầu tiên, hãy tạo một database, registry.db, như sau:\nimport sqlite3 conn = sqlite3.connect(\u0026#39;registry.db\u0026#39;) Đối tượng conn tạo ra một kết nối đến registry.db. Chúng ta sẽ sử dụng nó để thực thi các câu lệnh truy vấn sql.\nTiếp theo, tạo bảng model_registry bao gồm các trường thông tin của model.\ncur = conn.cursor() cur.execute(\u0026#34;\u0026#34;\u0026#34; CREATE TABLE model_registry ( id INTEGER PRIMARY KEY ASC, name TEXT UNIQUE NOT NULL, version TEXT NOT NULL, registered_date TEXT DEFAULT CURRENT_TIMESTAMP, metrics TEXT NOT NULL, remote_path TEXT NOT NULL, stage TEXT DEFAULT \u0026#39;DEVELOPMENT\u0026#39; NOT NULL ); \u0026#34;\u0026#34;\u0026#34;) cur.close() 2. Xây dựng Model Registry API\nMục đích của việc xây dựng các API là làm đơn giản hóa quá trình thao tác với database. Tất cả các công việc chung một hành động sẽ được gom vào thành một API.\nChúng ta sẽ xây dựng các API sau:\n Thêm một model mới train vào database. Cập nhật trạng thái của model. Có 2 trạng thái là Development và Production. Lấy tất cả thông tin của model.  Dưới đây là implement các API:\nimport panda as pd class ModelRegistry: def __init__(self, conn, table_name=\u0026#39;model_registry\u0026#39;): self.conn = conn self.table_name = table_name def _insert(self, values): query = \u0026#34;\u0026#34;\u0026#34; INSERT INTO {} (name, version, metrics, remote_path) VALUES (?, ?, ?, ?)\u0026#34;\u0026#34;\u0026#34;.format(self.table_name) self._query(query, values) def _query(self, query, values=None): cur = self.conn.cursor() cur.execute(query, values) cur.close() def publish_model(self, model_name, model_metrics): model_version_query = \u0026#34;\u0026#34;\u0026#34; SELECT version FROM {} WHERE name = \u0026#39;{}\u0026#39; ORDER BY registered_date DESC LIMIT 1 ;\u0026#34;\u0026#34;\u0026#34;.format(self.table_name, model_name) model_version = pd.read_sql_query(model_version_query, conn) if model_version is not None: model_version = int(version.iloc[0][\u0026#39;version\u0026#39;]) model_version = model_version + 1 # Assume that trained models are stored on S3 model_path = \u0026#39;s3://models/{}::v{}\u0026#39;.format(model_name, model_version) self._insert((model_name, model_version, model_metrics, model_path)) def update_stage(self, model_name, model_version, model_stage): query = \u0026#34;\u0026#34;\u0026#34; UPDATE {} SET stage = ? WHERE name = ? AND version = ? ;\u0026#34;\u0026#34;\u0026#34;.format(self.table_name) self._query(query, (model_stage, model_name, model_version)) def get_production_model(self, model_name): query = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM {} WHERE name = \u0026#39;{}\u0026#39; AND stage = \u0026#39;PRODUCTION\u0026#39; ;\u0026#34;\u0026#34;\u0026#34;.format(self.table_name, model_name) return pd.read_sql_query(query, self.conn) Code implement API khá đơn giản, hi vọng bạn có thể hiểu được dễ dàng, :)\n3. Sử dụng Model Registry API\nTrên thực tế , Training và Inference là 2 quá trình cùng chạy đồng thời và Model Registry cung cấp cơ chế trao đổi thông tin giữa 2 quá trình này thông qua database.\nGiả sử rằng chúng ta đã trained xong model thoả mãn yêu cầu đề bài, giờ là lúc ta sử dụng Model Registry API.\nChú ý: Sau mỗi đoạn code ví dụ, ta sẽ sử dụng câu truy vấn sau đây để kiểm tra kết quả:\npd.read_sql_query(\u0026#34;SELECT * FROM model_registry;\u0026#34;, conn) 3.1 Model Training\n Training lần đầu  conn = sqlite3.connect(\u0026#39;registry.db\u0026#39;) model_registry = ModelRegistry(conn=conn) model = None # This would be replaced by the trained model. name = \u0026#39;house_price_prediction\u0026#39; metrics = {\u0026#39;accuracy\u0026#39;: 0.87} model_registry.publish_model(model=model, name=name, metrics=metrics)    id name version registered_data remote_path stage     1 house_price_prediction 1 2021-01-10 12:42:25 s3://models/house_price_prediction::v1 DEVELOPMENT     Training lần thứ 2  model = None # This would be replaced by the trained model. name = \u0026#39;house_price_prediction\u0026#39; metrics = {\u0026#39;accuracy\u0026#39;: 0.89} model_registry.publish_model(model=model, name=name, metrics=metrics)    id name version registered_data remote_path stage     1 house_price_prediction 1 2020-07-12 12:45:27 s3://models/house_price_prediction::v1 DEVELOPMENT   2 house_price_prediction 2 2021-01-10 12:42:25 s3://models/house_price_prediction::v2 DEVELOPMENT    3.2 Chuyển model sang trạng thái sẵn sàng sử dụng cho sản phẩm thực tế\nmodel_registry.update_stage(name=name, version=\u0026#39;2\u0026#39;, stage=\u0026#34;PRODUCTION\u0026#34;)    id name version registered_data remote_path stage     1 house_price_prediction 1 2020-07-12 12:45:27 s3://models/house_price_prediction::v1 DEVELOPMENT   2 house_price_prediction 2 2021-01-10 12:42:25 s3://models/house_price_prediction::v2 PRODUCTION    3.3 Lấy thông tin model\nmodel_registry.get_production_model(name=name)    id name version registered_data remote_path stage     2 house_price_prediction 2 2021-01-10 12:42:25 s3://models/house_price_prediction::v2 PRODUCTION    4. Kết luận\nNhư vậy là chúng ta đã implemented xong Model Register, sử dụng SQLite database. Bạn hoàn toàn có thể áp dụng những gì được trình bày trong bài viết này vào trong dự án của bạn.\nHiện nay cũng có một số open-source giúp bạn thực hiện việc này một cách trực quan hơn. Nổi bật trong số đó là MLflow. Mình sẽ có một bài viết hướng dẫn sử dụng MLflow cho Model Registry trong tương lai.\nBài viết tiếp theo, mình sẽ thảo luận về vấn đề Retraining model. Mời các bạn đón đọc!\n5. Tham khảo\n Mlinproduction MLflow  ","permalink":"https://tiensu.github.io/blog/40_ai_model_registry/","tags":["Model Deployment"],"title":"Cấu hình AI Model Registry"},{"categories":["Model Deployment","Docker"],"contents":"Trong bài trước, chúng ta đã tìm hiểu và sử dụng Docker để triển khai AI model theo kiểu online inference. Trong bài này, ta sẽ train một model khác để inference theo kiểu thứ 2, batch inference, sử dụng docker. Mình cũng sẽ thực hiện việc train model bên trong docker luôn (các bài trước đó là train model bên ngoài docker, sau đó chỉ copy model đã train vào trong docker để thực hiện online inference).\n1. Tạo cấu trúc thư mục\nTrước tiên, chúng ta sẽ tạo ra một thư mục để lát nữa khi build docker image, nó sẽ được copy vào trong docker image đó.\ncode ├── Dockerfile ├── batch_inference.py └── train.py Trong đó:\n code: thư mục làm việc chính. Dockerfile: File cấu hình để build docker image. train.py: File code python để thực hiện train model. batch_inference.py: File code python để thực hiện batch inference.  2. Tạo code python để train model và inference data\nLần này, chúng ta sẽ thực hiện train model để dự đoán giá nhà tại Boson, sử dụng tập dữ liệu boson trong thư viện scikit-learn. Đây là kiểu model Leaner Regression.\nTạo file train.py với nội dung như sau:\nimport json import os from joblib import dump import matplotlib.pyplot as plt import numpy as np from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error # ############################################################################# # Load directory paths for persisting model and metadata MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Load and split data print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() print(\u0026#34;Splitting data...\u0026#34;) X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] # ############################################################################# # Fit regression model print(\u0026#34;Fitting model...\u0026#34;) params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) train_mse = mean_squared_error(y_train, clf.predict(X_train)) test_mse = mean_squared_error(y_test, clf.predict(X_test)) metadata = { \u0026#34;train_mean_square_error\u0026#34;: train_mse, \u0026#34;test_mean_square_error\u0026#34;: test_mse } # ############################################################################# # Serialize model and metadata print(\u0026#34;Serializing model to: {}\u0026#34;.format(MODEL_PATH)) dump(clf, MODEL_PATH) print(\u0026#34;Serializing metadata to: {}\u0026#34;.format(METADATA_PATH)) with open(METADATA_PATH, \u0026#39;w\u0026#39;) as outfile: json.dump(metadata, outfile) Tạo file batch_inference.py với nội dung như sau:\nimport os from joblib import load import numpy as np from sklearn import datasets from sklearn.utils import shuffle MODEL_DIR = os.environ[\u0026#34;MODEL_DIR\u0026#34;] MODEL_FILE = os.environ[\u0026#34;MODEL_FILE\u0026#34;] METADATA_FILE = os.environ[\u0026#34;METADATA_FILE\u0026#34;] MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE) METADATA_PATH = os.path.join(MODEL_DIR, METADATA_FILE) # ############################################################################# # Get a batch of data to inference def get_data(): \u0026#34;\u0026#34;\u0026#34; Return data for inference. \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Loading data...\u0026#34;) boston = datasets.load_boston() X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] return X_test, y_test print(\u0026#34;Running inference...\u0026#34;) X, y = get_data() # ############################################################################# # Load model print(\u0026#34;Loading model from: {}\u0026#34;.format(MODEL_PATH)) clf = load(MODEL_PATH) # ############################################################################# # Run inference print(\u0026#34;Scoring observations...\u0026#34;) y_pred = clf.predict(X) print(y_pred) Mình tin chắc các bạn có thể dễ dàng hiểu được đoạn code trên. Ở đây, có một chú ý là mình sử dụng một số biến môi trường MODEL_DIR, MODEL_FILE, \u0026hellip;. Mình không muốn hard-code những biến này vì chúng được sử dụng ở 2 nơi, train và inference model. Thay vào đó, giá trị của chúng sẽ được truyền vào lúc build docker.\n3. Build Docker image để train model\nTạo file Dockerfile với nội dung như sau:\nFROM jupyter/scipy-notebook USER root WORKDIR /code ADD . /code RUN pip install joblib RUN mkdir model # Env variables ENV MODEL_DIR=/code/model ENV MODEL_FILE=clf.joblib ENV METADATA_FILE=metadata.json # COPY train.py ./train.py # COPY batch_inference.py ./batch_inference.py RUN python3 train.py Để build docker image, chạy lệnh sau:\n$ docker build -t docker-model-batch-infer . Output:\nSending build context to Docker daemon 4.608kB Step 1/10 : FROM jupyter/scipy-notebook ---\u0026gt; 069532086d63 Step 2/10 : USER root ---\u0026gt; Running in c580ea3bbd7f Removing intermediate container c580ea3bbd7f ---\u0026gt; efcad69c0b79 Step 3/10 : WORKDIR /code ---\u0026gt; Running in 08ee819c1e52 Removing intermediate container 08ee819c1e52 ---\u0026gt; d05432266229 Step 4/10 : ADD . /code ---\u0026gt; 6b1f81f3a9f6 Step 5/10 : RUN pip install joblib ---\u0026gt; Running in 9a2c41424c59 Removing intermediate container 9a2c41424c59 ---\u0026gt; 11e642103f1f Step 6/10 : RUN mkdir /code/model ---\u0026gt; Running in 5a4068194bfa Removing intermediate container 5a4068194bfa ---\u0026gt; 6ae6727bf9aa Step 7/10 : ENV MODEL_DIR=/code/model ---\u0026gt; Running in f8992466e635 Removing intermediate container f8992466e635 ---\u0026gt; c3491c25966e Step 8/10 : ENV MODEL_FILE=clf.joblib ---\u0026gt; Running in 15f321f925e4 Removing intermediate container 15f321f925e4 ---\u0026gt; a643969fdfd1 Step 9/10 : ENV METADATA_FILE=metadata.json ---\u0026gt; Running in 72c0b8ef67db Removing intermediate container 72c0b8ef67db ---\u0026gt; efce67f3494c Step 10/10 : RUN python3 train.py ---\u0026gt; Running in 10bec25fc25e Loading data... Splitting data... Fitting model... Serializing model to: /code/model/clf.joblib Serializing metadata to: /code/model/metadata.json Removing intermediate container 10bec25fc25e ---\u0026gt; 05d1c3a12437 Successfully built 05d1c3a12437 Successfully tagged docker-model-batch-infer:latest Kiểm tra nội dung file metadata:\n$ docker run docker-model-batch-infer cat /code/model/metadata.json Kết quả:\n{\u0026#34;train_mean_square_error\u0026#34;: 1.7677391462344387, \u0026#34;test_mean_square_error\u0026#34;: 6.588673999729974} Chú ý: Model ở đây chưa được tuning để tối ưu hóa hiệu năng, các thông tin metadata khác như thuật toán, phân phối dữ liệu, phiên bản model, \u0026hellip; cũng không được lưu lại. Nếu bạn sử dụng hướng dẫn này trong thực tế thì cần lưu ý những điểm trên.\n4. Thực hiện Batch Inference\nChúng ta đã có một model đã train, được lưu thành file clf.joblib trong docker image docker-model-batch-infer. Giờ là lúc sử dụng nó để thực hiện inference.\nĐể thực hiện batch inference, ta sẽ khởi động docker image và chạy lệnh thực thi code kèm theo đó:\n$ docker run docker-model-batch-infer python3 batch_inference.py Kết quả:\nRunning inference... Loading data... Loading model from: /code/model/clf.joblib Scoring observations... [15.32448686 27.68741572 24.23789723 31.94786177 10.43966955 34.25663827 22.05210667 11.58265489 13.36407623 42.87157933 33.03218733 15.77635169 23.93521876 19.88239305 25.43466604 20.55132127 13.65254047 47.45491473 17.5734174 21.51806638 22.57388848 16.97645106 16.25503893 20.57862843 14.57438158 11.81385445 24.78353556 37.51637481 30.34664466 19.67895051 23.22841646 25.02203256 18.65459129 30.09762517 8.96667041 13.8130382 14.18734797 17.3840622 19.83840166 24.23822033 20.52076144 15.32433651 25.8157052 16.47533793 19.2214524 19.87110293 21.47113681 21.56443118 24.64517965 22.43665872 22.22261406] 5. Kết luận\nNhư vậy là chúng ta đã thực hiên xong việc train một ML model và sử dụng nó để inference dữ liệu bằng docker. Tuy nhiên, để mang nó vào sử dụng trong môi trường production thì chúng ta cần thực hiện thêm một số công việc nữa như là lập lịch đê thực hiện batch inference định kỳ, tuning hyper-parameter, lưu trữ metadata của model, \u0026hellip;\nToàn bộ source code sử dụng trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong các bài viết tiếp theo, mình sẽ tổng hợp lại các thuật toán Deep Learning, sau đó sẽ đi chi tiết vào một số thụât toán với các ứng dụng cụ thể. Mời các bạn đón đọc!\n6. Tham khảo\n Docker Mlinproduction  ","permalink":"https://tiensu.github.io/blog/39_package_ai_model_using_docker_batch_inference/","tags":["Model Deployment","Docker"],"title":"Đóng gói quá trình Train AI model và Batch Inference sử dụng Docker"},{"categories":["Model Deployment","Docker"],"contents":"Đã bao giờ bạn gặp tình huống:\nTại sao code trên máy tính của tôi chạy mà mang sang máy tính của bạn lại không chạy?\n99% câu trả lời cho câu hỏi này là do sự khác biệt về môi trường giữa 2 máy tính, 1% còn lại là do các nguyên nhân khác như copy thiếu file, sai đường dẫn, sử dụng câu lệnh không đúng, \u0026hellip;\nTrong bài toán AI, tình trạng này lại càng phổ biến hơn, bởi vì một model AI yêu cầu cơ man nào là thư viện đi kèm, thư viện này liên kết, ràng buộc với thư viện kia. Không những thế, với tốc độ phát triển như vũ bão hiện nay của AI, các thư viện cũng liên tục cập nhật phiên bản mới, và có khi code sử dụng phiên bản cũ lại không chạy được trên phiên bản mới. Rồi thì thư viện A phiên bản 1.x lại chỉ tương thích với thư viện B phiên bản 1.x.x, nếu ta cứ nhắm mắt gõ pip install abc thì mặc định sẽ là phiên bản mới nhất. Rất rất nhiều vấn đề xung đột thư viện xảy ra trong phát triển một bài toán AI trên nhiều máy tính khác nhau hoặc nhiều người cùng làm việc.\nVấn đề đặt ra lúc này là phải làm sao cô lập được môi trường phát triển dành riêng cho 1 bài toán AI cụ thể. Môi trường đó phải tách biệt hoàn toàn với môi trường trên máy tính, và phải dễ dàng di chuyển giữa nhiều máy tính với nhau.\nMột số công cụ đã ra đời để hỗ trợ giải quyết vấn đề này bằng cách tạo ra các môi trường ảo. Có thể kể đến như anaconda, venv`, \u0026hellip; Mỗi loại đều có những ưu nhược điểm riêng, và phụ thuộc vào thói quen sử dụng của mỗi người. Cá nhân mình cũng đã từng sử dụng qua các loại kể trên nhưng thấy chúng vẫn chưa thể giải quyết được triệt để vấn đề về xung đột môi trường \u0026hellip;\nCho đến khi mình biết đến Docker, một công cụ rất powerfull, rất tuyệt vời. Có thể nói docker đã giải quyết được tận gốc vấn đề làm đau đầu những nhà phát triể n AI bấy lâu nay.\nTrong bài này, chúng ta sẽ cùng nhau tìm hiểu về docker và cách sử dụng nó trong viêc đóng gói một AI model để thực hiện Inference theo kiểu Online Inference.\n1. Docker là gì?\nTheo định nghĩa chính thức tại trang chủ của docker thì:\nDocker is an open platform for developing, shipping, and running applications.   Hiểu một cách đơn giản thì docker là một nền tảng mã nguồn mở cho việc phát triển, chạy và phân phối các ứng dụng. Nó cho phép chúng ta tách biệt ứng dụng ra khỏi kiến trúc hạ tầng chung của toàn hệ thống và dễ dang mang toàn bộ ứng dụng đó (bao gồm cả môi trường thực thi) sang một máy tính hoàn toàn mới. Điều này giúp các nhà phát triển ứng dụng giảm được thời gian đáng kể ở công đoạn đưa sản phẩm vào sử dụng trong thực tế.\nMột số khái niệm cần biết khi làm việc với docker:\n Docker image: Là một file không thể thay đổi (read-only), chứa toàn bộ source code, thư viện, công cụ, \u0026hellip; cần thiết để một ứng dụng có thể chạy được. Docker container: Là một \u0026ldquo;bản sao\u0026rdquo;, hay một \u0026ldquo;instance\u0026rdquo; của docker image tại thời điểm khởi chạy docker image. Và thực tế là chúng ta chỉ làm việc trên các containers chứ không làm việc với các images. Sau khi kết thúc phiên làm việc thì container sẽ biến mất, và các thay đổi của container đó sẽ không được lưu lại vào docker image sinh ra container đó. Nếu bạn muốn lưu lại các thay đổi bạn đã thực hiện thì có thể sử dụng lệnh \u0026ldquo;docker commit\u0026rdquo;, nhưng nó sẽ tạo ra một docker image mới bao gồm docker image cũ và phần thay đổi. Đối với sự thay đổi trên các file, thư mục, ta có thể lưu lại sự thay đổi để sử dụng ở nơi khác mà không cần tạo docker image mới bằng cách đặt các file cần thay đổi ở thư mục chung, chia sẻ với máy tính bên ngoài (host). Docker hub: Là một kho (repository) chứa các docker images, cho phép bạn chia sẻ các docker images của bạn cho người khác bằng cách upload nó lên docker hub. Khi người nào muốn sử dụng docker image của bạn, họ chỉ cần tải về để sử dụng. Host: Là máy tính cài đặt docker và chạy các docker containers.  2. Cài đặt Docker\nĐể sử dụng docker thì trước tiên cần phải cài đặt docker engine. Các cài đặt khá đơn giản, hãy làm theo hướng dẫn trên trang chủ của docker.\nSau khi cài xong docker, hãy thử chạy lệnh sau:\n$ docker run tensorflow/tensorflow:2.3.0-gpu Nếu thấy output như sau tức là ta đã cài đặt thành công:  Ở đây, tensorflow/tensorflow:2.3.0-gpu là docker image trên docker hub, được cài đặt sẵn tensorflow 2.3.0 và cuda.\nKiểm tra image vừa tải về trong danh sách:\n$ ``` Kết quả: ```python REPOSITORY TAG IMAGE ID CREATED SIZE tensorflow/tensorflow 2.3.0-gpu 3b8d4cbd6723 3 weeks ago 3.18GB Nếu bạn muốn sử dụng GPU (giả sử là NVIDIA) trong docker thì bạn cần thêm 2 điều kiện:\n Máy tính của bạn phải có GPU và đã cài đặt đầy đủ driver, cuda, cudnn (có thể sử dụng GPU bình thường trong các task DL mà không sử dụng docker). Cài thêm NVIDIA Container Tookit theo hướng dẫn ở đây  Như trên máy tính của mình đã có đủ 2 điều kiện trên, mình kiểm tra GPU bên trong docker như sau:\n$ docker run --gpus all --rm tensorflow/tensorflow:2.3.0-gpu nvidia-smi Kết quả:  Hoặc chi tiết hơn:\n$ docker run --gpus all --rm tensorflow/tensorflow:2.3.0-gpu python -c \u0026#34;import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\u0026#34; Kết quả:  Trong đó:\n \u0026ndash;gpus all: Cho phép sử dụng GPU trong docker. \u0026ndash;rm: Xóa docker container sau khi chạy lệnh xong.  Để cho phép mặc định sử dụng GPU trong docker (không cần sử dụng \u0026ndash;gpus all), bạn có thể làm như sau:\n Thêm default-runtime\u0026quot;: \u0026quot;nvidia\u0026quot; vào trong file /etc/docker/daemon.json  # filename: /etc/docker/daemon.json { \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } }  Khởi động lại docker:  $ sudo pkill -SIGHUP dockerd Mình đã tổng hợp lại các lệnh hay sử dụng của docker ở phần phụ lục. Các bạn có thể tham khảo thêm.\n3. Xây dựng uWSGI docker image\nBên cạnh những images được xây dựng sẵn và chia sẻ trên docker hub, docker cũng hỗ trợ bạn tự build các images cho riêng bạn, phù hợp với từng nhu cầu của bạn. Tất cả được thực hiện thông qua 1 file cấu hình, gọi là Dockerfile.\nTrong phần này, ta sẽ cùng nhau xây dựng một DL docker image, bao gồm toàn bộ source code ở bài trước. Ta cũng cấu hình Flask, uWSGI trong docker image để chạy được source code đó.\n 3.1 Bước 1\nTạo một thư mục tên là uwsgi, và copy toàn bộ source code của bài trước (bao gồm cả model, bỏ đi file client.py vì ta sẽ chạy client ở bên ngoài docker) vào thư mục vừa tạo.\n3.2 Bước 2\nTạo file requirements.txt chứa toàn bộ thư viện cần dùng để chạy code (cũng đặt trong thư mục uWSGI). Bạn có thể sinh ra file này tự động bằng lệnh pip freeze \u0026gt; requirements.txt. Tuy nhiên, nếu làm theo cách này thì file requirements.txt sẽ chứa rất nhiều thư viện không cần thiết, bởi vì hầu hết chúng phụ thuộc vào các thư viện khác. Nếu bạn theo dõi từ đầu, bạn chắc chắc biết rằng, ta sẽ chỉ cần những những thư viện sau là đủ: tensorflow, uwsgi, flask, opencv. Trong đó, vì mình dự định không build docker image từ đầu mà kế thừa từ 1 image đã build sẵn (cụ thể là tensorflow/tensorflow:2.3.0-gpu đã tải về ở phần trước) nên tensorflow đã được tích hợp sẵn, không cần cài lại nữa. Cuối cùng, file requirements.txt của chúng ta chỉ như sau:\nFlask==1.1.2 uWSGI==2.0.18 opencv-python==4.4.0.46 3.3 Bước 3\nTạo file cấu hình cho uWSGI. Vì mình muốn kiểm tra riêng sự hoạt động của uWSGI nên file cấu hình sẽ như sau:\n[uwsgi] http = 0.0.0.0:8080 wsgi-file = server.py callable = app die-on-term = true processes = 4 threads = 2 chdir = /uwsgi master = false vacuum = truemodule Trong phần sau, chúng ta sẽ kết hợp thêm Nginx. Khi đó sẽ cần thay đổi lại cấu hình của uWSGI lại một chút.\n3.3 Bước 4\nTạo Dockerfile (trong thư mục uwsgi) chứa các thông tin cấu hình cần thiết để tạo docker image. Nội dung của file này như sau:\nFROM tensorflow/tensorflow:2.3.0-gpu # kế thừa từ image tensorflow/tensorflow:2.3.0-gpu WORKDIR /uwsgi # thư mục làm viêc mặc định bên trong docker ADD . /uwsgi # local folder để copy vào thư mục làm việc của docker, chính là thư mục chúng ta tạo ở bước 1 RUN pip install -r requirements.txt # cài đặt các thư viện cần thiết trong file requirements.txt RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y # sửa lỗi opencv, thử bỏ đi để xem điều gì xảy ra? CMD [\u0026#34;uwsgi\u0026#34;, \u0026#34;app.ini\u0026#34;] # chạy lệnh \u0026#34;uwsgi app.ini\u0026#34; khi khởi chạy docker image Có rất rất nhiều tùy chọn khi viết Dockerfile, tham khảo ở đây nếu bạn cần thêm thông tin.\nThự mục uwsgi lúc này sẽ như sau:\n├── animal_model_classification.h5 ├── app.ini ├── cat.1.jpg ├── Dockerfile ├── dog.1.jpg ├── requirements.txt └── server.py Trong đó, 2 files ảnh là để chúng ta thực hiện việc test về sau.\n4 Build docker image\nOK, mọi thứ cần thiết đã chuẩn bị xong, ta sẽ chạy lệnh sau để buidl docker image:\n$ docker build -t image-classification-production:1.0 . Docker image được sinh ra sẽ có tên là image-classification-production, kèm theo tag 1.0 để phân biệt nó với các phiên bản khác trong tương lai.\nNếu build, thành công, output sẽ như sau:\n---\u0026gt; a2a60f32471b Step 7/7 : CMD [\u0026#34;uwsgi\u0026#34;, \u0026#34;app.ini\u0026#34;] ---\u0026gt; Running in 3776d496ca68 Removing intermediate container 3776d496ca68 ---\u0026gt; 53150d1373a3 Successfully built 53150d1373a3 Successfully tagged image-classification-production:1.0 Kiểm tra docker image trong danh sách:\ndocker images Kết quả:\nREPOSITORY TAG IMAGE ID CREATED SIZE image-classification-production 1.0 53150d1373a3 About a minute ago 5.37GB tensorflow/tensorflow 2.3.0-gpu 3b8d4cbd6723 3 weeks ago 3.18GB 5 Chạy docker image\nĐể chạy docker image vừa tạo, ta sử dụng lệnh sau:\n$ docker run --rm --publish 80:8080 --name dlp image-classification-production:1.0 Có 2 cái mà ta phải chú ý ở đây:\n Tham số --public 80:8080 sẽ \u0026ldquo;expose\u0026rdquo; port 8080 của container tới port 80 của host. Nói cách khác, tất cả các requests đến địa chỉ localhost:80 sẽ được chuyển tiếp đến địa chỉ 0.0.0.0:8080 bên trong container. 8080 được gọi là listening port của uWSGI. Tham số --name dlp sẽ đặt tên cho container là dlp. Ta nên đặt tên cho container để dễ làm việc với nó hơn. Ngược lại, docker sẽ tạo cho nó một ID ngẫu nhiên.  Kết quả:\n2021-01-06 02:52:45.347188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2021-01-06 02:52:45.347580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2021-01-06 02:52:45.347925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4676 MB memory) -\u0026gt; physical GPU (device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5) 2021-01-06 02:52:47.271932: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:47.621364: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:47.918398: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:48.759371: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. 2021-01-06 02:52:49.637126: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. WSGI app 0 (mountpoint=\u0026#39;\u0026#39;) ready in 8 seconds on interpreter 0x55cf948bc720 pid: 1 (default app) uWSGI running as root, you can use --uid/--gid/--chroot options *** WARNING: you are running uWSGI as root !!! (use the --uid flag) *** *** uWSGI is running in multiple interpreter mode *** spawned uWSGI worker 1 (and the only) (pid: 1, cores: 1) Ta quan sá thấy container đã chạy thành công và uWSGI cũng đã được khởi động.\nLưu ý: Trong trường hợp port 80 đã được sử dụng bới ứng dụng khác (nginx trong bài trước chẳng hạn), bạn phải close ứng dụng đó hoặc sử dụng một port khác.\nĐể kiểm tra xem uWSGI có làm viêc đúng hay không, ta có thể sử dụng lại client đã chuẩn bị từ bài trước (nhớ đổi port của ENDPOINT_URL từ 8080 thành 80).\n$ python client.py Kết quả:\nb\u0026#39;cat\u0026#39; Như vậy là uWSGI đã được cài đặt thành công vào docker.\n Lưu ý: Trong quá trình viết bài này, mình gặp lỗi liên quan đến cuda khi chạy test uWSGI. Mình xóa hết các docker images đi và chạy lại từ đầu thì không bị lỗi nữa.\n6. Xây dựng Nginx docker image\nTương tự như việc xây dựng uWSGI docker image, chúng ta sẽ đi build một Nginx docker image, đặt trước uWSGI server thực hiện vai trò như một reverse proxy.\n6.1 Bước 1\nTạo thư mục nginx, cùng cấp với thư mục uwsgi.\n6.2 Bước 2\nTạo file cấu hình Nginx, tên là nginx.conf, đặt trong thư mục nginx, với nội dung như sau:\nserver { listen 80; location / { include uwsgi_params; uwsgi_pass uwsgi:660 ; } Với cấu hình này thì Nginx sẽ lắng nghe trên port 80, chuyển tiếp các requests đến port 660 của uWSGI server thông qua socket (sử dụng giao thức uwsgi).\n6.3 Bước 3\nCập nhật lại cấu hình của uWSGI (file app.ini) để làm việc được với Nginx, như sau:\nmodule = server socket= :660 callable = app die-on-term = true processes = 1 master = false vacuum = true 6.4 Bước 4\nTạo file Dockerfile cho Nginix docker trong thư mục nginx. Nginix docker image được kế thừa từ nginx image trên docker hub, ta chỉ việc thay thế cấu hình mặc định của nó bằng cấu hình mà ta vừa tạo ở bước 3.\nFROM nginx RUN rm /etc/nginx/conf.d/default.conf COPY nginx.conf /etc/nginx/conf.d/ Đến đây, nếu chạy lệnh docker build ... thì ta sẽ có được nginx docker image. Nhưng nếu chỉ chạy một mình image này thì không có tác dụng gì cả. Ta cần phải kết hợp cả 2 docker images uwsgi và nginx. Đó chính là công viêc của docker-compose.\n7. Chạy đồng thời nhiều docker containers với Docker Compose\nLiệu bạn có thắc mắc rằng tại sao ta không build cả Nginx và uWSGI vào chung 1 docker image? Chẳng phải như thế sẽ tiện hơn hay sao?\nCâu trả lời là không nên làm vậy. Theo kiến trúc làm việc kết hợp giữa Nginx và uWSGI thì một Nginx instance có thể kết hợp với nhiề u uWSGI instances. Nếu ta kết hợp chung lại, sẽ không tận dụng được khả năng này. Thêm nữa, dung lượng của docker image sẽ rất lớn nếu ta kết hợp lại.\n Mở rộng ra, nếu một hệ thống của chúng ta bao gồm cả database, backend, front-end, messaging systems, task queue, \u0026hellip; ta không thể chạy tất tần tật mọi thứ trong một docker container được.\nTừ góc độ của nhà phát triển phần mềm, docker-compose chỉ là một file cấu hình, định nghĩa tất cả containers và cách thức mà các containers đó tương tác với nhau.\n7.1 Cài đặt docker-compose\nĐể cài đặt docker-compose. Bạn hãy làm theo hướng dẫn sau trên trang chủ của docker.\n7.2 Định nghĩa cấu hình của docker-compose\nTạo file docker-compose.yml (bên ngoài 2 thư mục uwsgi và nginx), với nội dung như sau:\nversion : \u0026#34;3.7\u0026#34; services: uwsgi: build: ./uwsgi container_name: uwsgi_img_classification restart: always expose: - 660 nginx: build: ./nginx container_name: nginx restart: always ports: - \u0026#34;80:80\u0026#34; Phần chính của cấu hình này là khai báo 2 containers, gọi là 2 services. Hai tham số quan trọng của mỗi services là:\n build: thư mục chứa Dockerfile và các files cần thiết của mỗi container. restart: tự động khởi động lại service nếu xay ra lỗi. expose: uwsgi lắng nghe request đến trên port 660 (chỉ trong phạm vi docker). port: nginx mở port 80 ra bên ngoài (có thể chọn tùy ý) để lắng nghe requests đến, ánh xạ đến port 80 (theo như cấu hình trong file nginx.conf) của container.  Như vậy, có thể tóm tắt lại flow như sau:\n Các requests từ clients đến port 80 của host. Các requests được ánh xạ sang port 80 của nginx container. Các requests tiếp tục được chuyển tiếp đến port 660 của uwsgi container. uwsgi gọi Flask endpoint và thực hiện quá trình nhận diện. uwsgi gửi lại kết quả nhận diện theo hướng ngược lại.  7.3 Build docker-compose\nChạy lệnh sau để build docker-compose với cả 2 containers.\n$ docker-compose build Nếu build thành công, output sẽ như sau:\nStep 7/7 : CMD [\u0026#34;uwsgi\u0026#34;, \u0026#34;app.ini\u0026#34;] ---\u0026gt; Running in 9608e1187e82 Removing intermediate container 9608e1187e82 ---\u0026gt; 357fe8e41768 Successfully built 357fe8e41768 Successfully tagged docker_uwsgi:latest Building nginx Step 1/3 : FROM nginx latest: Pulling from library/nginx 6ec7b7d162b2: Pull complete cb420a90068e: Pull complete 2766c0bf2b07: Pull complete e05167b6a99d: Pull complete 70ac9d795e79: Pull complete Digest: sha256:4cf620a5c81390ee209398ecc18e5fb9dd0f5155cd82adcbae532fec94006fb9 Status: Downloaded newer image for nginx:latest ---\u0026gt; ae2feff98a0c Step 2/3 : RUN rm /etc/nginx/conf.d/default.conf ---\u0026gt; Running in 11140e051282 Removing intermediate container 11140e051282 ---\u0026gt; 1fcc92cfdfc4 Step 3/3 : COPY nginx.conf /etc/nginx/conf.d/ ---\u0026gt; 21bda0089cca Successfully built 21bda0089cca Successfully tagged docker_nginx:latest Kiểm tra thử danh sách images bằng lệnh docker images:\nREPOSITORY TAG IMAGE ID CREATED SIZE docker_nginx latest 21bda0089cca 4 minutes ago 133MB docker_uwsgi latest 357fe8e41768 5 minutes ago 5.37GB image-classification-production 1.0 bd9928abee21 2 hours ago 5.37GB nginx latest ae2feff98a0c 3 weeks ago 133MB tensorflow/tensorflow 2.3.0-gpu 3b8d4cbd6723 3 weeks ago 3.18GB Ta thấy hai containers docker_nginx và docker_uwsgi đã xuất hiện.\n7.4 Kiểm tra hoạt động của hệ thống\nTa sẽ khởi động các containers lên để kiểm tra thử xem hê thống có làm việc chính xác không.\n$ docker-compose up Khởi động thành công:\nStarting nginx ... done Starting uwsgi_img_classification ... done Attaching to nginx, uwsgi_img_classification nginx | /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration nginx | /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ uwsgi_img_classification | [uWSGI] getting INI configuration from app.ini ..... uwsgi_img_classification | 2021-01-06 09:18:12.292414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4662 MB memory) -\u0026gt; physical GPU (device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5) uwsgi_img_classification | 2021-01-06 09:18:14.386546: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:14.737805: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:15.045424: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:16.238951: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | 2021-01-06 09:18:17.246651: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 536870912 exceeds 10% of free system memory. uwsgi_img_classification | WSGI app 0 (mountpoint=\u0026#39;\u0026#39;) ready in 7 seconds on interpreter 0x56494b98c680 pid: 1 (default app) uwsgi_img_classification | uWSGI running as root, you can use --uid/--gid/--chroot options uwsgi_img_classification | *** WARNING: you are running uWSGI as root !!! (use the --uid flag) *** uwsgi_img_classification | *** uWSGI is running in multiple interpreter mode *** uwsgi_img_classification | spawned uWSGI worker 1 (and the only) (pid: 1, cores: 1) Chạy client để nhận diện: python client.py.\nKết quả:\nb\u0026#39;cat\u0026#39; 8. Kết luận\nPhù, thật tuyệt vời, mọi thứ đã chạy đúng như mong muốn.\nBài hôm nay khá là dài và khó. Mình đã phải thực hiện cài cắm rất nhiều lần để có thể hoàn thành bài viết này. Hi vọng sẽ có ích cho các bạn trong việc tìm kiếm giải pháp triể n khải AI model vào trong các sản phẩm để đưa đến tay người dùng!\nToàn bộ source code sử dụng trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây. Giống như bài trước, vì model animal_model_classification.h5 có dung lượng khá lớn (\u0026gt; 1.5GB) nên mình không upload lên github được. Các bạn hãy sử dụng model của chính mình để thực hành nhé!\nTrong các bài viết tiếp theo, mình sẽ sử dụng docker để train một model khác và thực hiện batch inference bằng model đó. Mời các bạn đón đọc!\n9. Phụ lục một số lệnh cơ bản của Docker\n1. List docker image $ docker images 2. List container $ docker ps \u0026lt;-a\u0026gt; 3. Run a docker $ docker run -it [image_name] bash 4. Access to running container $ docker exec -it [container_id or container_name] bash 5. Commit change of container to docker image $ docker commit [container_name or container_id] [new_image_name] 6. Stop running container $ docker stop [container_id or container_name] $ docker stop $(docker ps -aq) # Stop all container 7. Start stoped container $ docker start [container_id or container_name] 8. Remove container $ docker rm [container_id or container_name] $ docker rm $(docker ps -aq) # Remove all 9. Export container $ docker export [container_id or container_name] | gzip \u0026gt; file_export.tar.gz 10. Import docker =\u0026gt; images $ zcat file_export.tar.gz | docker [new_name_image] $ docker images # check 11. Remove docker image $ docker rmi [image_name] Loi: docker: Error response from daemon: Unknown runtime specified nvidia. Solution: 1. $ sudo systemctl daemon-reload $ sudo systemctl restart docker 2. $ sudo mkdir -p /etc/systemd/system/docker.service.d $ sudo tee /etc/systemd/system/docker.service.d/override.conf \u0026lt;\u0026lt;EOF [Service] ExecStart= ExecStart=/usr/bin/dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime EOF $ sudo systemctl daemon-reload $ sudo systemctl restart docker ** Move docker image to other computer 1. Save images $ docker save \u0026lt;REPOSITORY\u0026gt; \u0026gt; \u0026lt;images_name\u0026gt;.tar 2. Load images $ docker load \u0026lt; \u0026lt;images_name\u0026gt;.tar 3. Run images $ docker run -it --runtime=nvidia --rm --net=host --privileged \u0026lt;Image ID\u0026gt; 10. Tham khảo\n Docker Nginx uWSGI Flask AI Summer  ","permalink":"https://tiensu.github.io/blog/38_package_ai_model_using_docker_online_inference/","tags":["Model Deployment","Docker"],"title":"Đóng gói AI model theo kiểu Online Inference sử dụng Docker"},{"categories":["Model Deployment"],"contents":"Như đã giới thiệu trong bài trước, mặc dù Flask rất dễ để sử dụng nhưng nó không có đầy đủ chức năng để có thể áp dụng vào các sản phẩm trong thực tế. Đó là tính bảo mật, khả năng xử lý đồng thời nhiều kết nối, khả năng mở rộng và nâng cấp model, \u0026hellip; Sử dụng kết hợp bộ ba Flask, uWSGI và Nginx chính là giải pháp hữu hiệu khắc phục những thiếu sót này. Trong bài viết này, chúng ta sẽ cùng tìm hiểu cách cài đặt, cấu hình và sử dụng bộ 3 kể trên để triển khai Animal Classification model dưới dạng server phục vụ các yêu cầu nhận dạng từ các clients.\n1. WSGI, uWSGI, và uwsgi là gì?\nTrước tiên cần hiểu rõ một số thuật ngữ mà ta sử dụng trong bài này.\n WSGI: Viết tắt của Web Server Gateway Interface, là một Interface giữa server và client, được viết bằng python. Hiểu một cách đơn giản, nó quy định các thức để client có thể kết nối và gửi nhận dữ liệu với server. uWSGI: Là một server, sử dụng WSGI để giao tiếp với client (hoặc sử dụng giao thức HTTP trong trường hợp client là web application). uwsgi: Là một giao thức ở tầng thấp hơn, cho phép các servers giao tiếp với nhau.  Bạn có thể xem sơ đồ kiến trúc triển khai sử dụng uWSGI như hình bên dưới đây:\n Phần xử lý nhận diện của ta vẫn được gọi thông qua Flask. Phía trước Flask ta đặt uWSGI rồi đến web application (client).\nViệc đặt uWSGI như vậy mang lại cho ta các lợi ích như sau:\n Quản lý tiến trình: Quản lý việc tạo và duy trì các tiến trình trong quá trình làm việc. Các tiến trình được đồng bộ với nhau trong cùng 1 môi trường và có khả năng scale-up để phục vụ cho nhiều users. Cân bằng tải: Phân phối tải (các requests) đến các tiến trình khác nhau. Giám sát: Giám sát hiệu năng và tài nguyên sử dụng. Hạn chế tài nguyên: Cho phép chỉ định mức tối đa tài nguyên có thể sử dụng.  2. Nginx là gì và tại sao phải sử dụng nó?\nNginx là một webserver với các đặc tính:\n High performance: Hiệu năng cao Highly scalable: Khả năng mở rộng cao Highly available: Tính sẵn sàng cao  Nó hoạt động giống như một bộ cân bằng tải, một reverse proxy cùng với cơ chế caching, cơ chế mã hóa và bảo mật trên các bản tin giao tiếp giữa client và server. Nginx được cho là có thể phục vụ hơn 10,000 kết nối đồng thời.\nNginx được sử dụng khá phổ biến trong các công ty công nghệ lớn, trong nhiều sản phẩm, ứng dụng cần phục vụ số lượng lớn người dùng đồng thời.\nXét về kiến trúc tổng thể, nó thường được sử dụng cùng với uWSGI, đứng trước uWSGI như trong hình sau:\n Mục đích của việc sử dụng đồng thời cả uWSGI và Nginx là để tận dụng những ưu điểm của cả 2. Tất nhiên điều này là không bắt buộc nếu ứng dụng của chúng ta ở mức đơn giản, không cần phải phục vụ số lượng users đồng thời quá lớn. Nhưng dù sao vẫn nên sử dụng kiến trúc này để có thể dễ dàng mở rộng ứng dụng về sau.\n3. Chuẩn bị Flask server\nTa vẫn cần có Flask làm server trực tiếp xử lý request từ client. Có thể hiểu Flask server ở đây là endpoint cũng được. Mình sẽ sử dụng lại file server.py ở bài trước, nhưng đã bỏ đi phần phục vụ web client.\nimport cv2 import os import numpy as np import tensorflow as tf from flask_cors import CORS from tensorflow.keras.models import load_model from flask import Flask, request, render_template, make_response, jsonify config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) app = Flask(__name__) CORS(app) image_width = 300 image_height = 300 classes = [\u0026#39;cat\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;pandas\u0026#39;] APP_ROOT_2 = os.getenv(\u0026#39;APP_ROOT\u0026#39;, \u0026#39;/infer\u0026#39;) model = load_model(\u0026#39;animal_model_classification.h5\u0026#39;) @app.route(APP_ROOT_2, methods=[\u0026#34;POST\u0026#34;]) def infer(): data = request.json img_path = data[\u0026#39;img_path\u0026#39;] return classify_animal(img_path) def classify_animal(img_path): # read image image = cv2.imread(img_path) image = image/255 image = cv2.resize(image, (image_width,image_height)) image = np.reshape(image, [1,image_width,image_height,3]) # pass the image through the network to obtain our predictions preds = model.predict(image) label = classes[np.argmax(preds)] return label if __name__ == \u0026#39;__main__\u0026#39;: app.run() 4. Cài đặt và cấu hình uWSGI\nĐể cài đặt uWSGI, sử dụng lệnh sau:\n$ pip install uwsgi Kiểm tra cài đặt bằng cách chạy câu lệnh sau:\n$ uwsgi --http 0.0.0.0:8080 --wsgi-file server.py --callable app Nếu output như sau thì tức là viêc cài đặt thành công:\n Câu lệnh trên có ý nghĩa là chạy một server tại địa chỉ 0.0.0.0, port 8080, sử dụng ứng dụng đặt trong file server.py.\nuWSGI có rất nhiều tùy chọn cấu hình. Do đó, để thuận tiện, ta thường tạo một file cấu hình, tên là app.ini như sau:\n[uwsgi] http = 0.0.0.0:8080 # địa chỉ server socket = service.sock # socket giao tiếp với Nginx chmod-socket = 660 # cấp quyền truy câp socket wsgi-file = server.py # file chứa code xử lý yêu cầu từ client callable = app # function được gọi khi tạo uWSGI die-on-term = true # cho phép kill server từ terminal processes = 4 # số lượng process threads = 2 # số lượng thread chdir = /media/sunt/DATA/GITHUB/Model_Deployment/uWSGI/ # thư mục dự án virtualenv = /home/sunt/anaconda3/envs/tf2/ # môi trường ảo (nếu có) master = false vacuum = truemodule # định kỳ xóa những file ko cần thiết được sinh ra Để chạy uWSGI, dùng lệnh:\n$ uwsgi app.ini Nếu thành công, output trên terminal sẽ như sau:\n 5. Cài đặt và cấu hình Nginx\nĐể cài đặt Nginx, sử dụng lệnh sau:\n$ sudo apt-get install nginx Tiếp theo, tạo một file cấu hình đặt trong thư mục /etc/nginx/sites-available, tên là service.conf, với nội dụng như sau:\nserver { listen 80; server_name 0.0.0.0; location / { include uwsgi_params; uwsgi_pass unix:/media/sunt/DATA/GITHUB/Model_Deployment/uWSGI/service.sock; } } Theo cấu hình này, Nginx sẽ lắng nghe trên cổng 80 (mặc định) cho tất cả các yêu cầu đến server đặt tại địa chỉ 0.0.0.0. Các yêu cầu sau đó được chuyển đến uWSGI server thông qua socket service.sock (sử dụng giao thức uwsgi).\nTiếp theo, để áp dụng các cấu hình trên cho Nginx, ta cần trỏ liên kết của chúng tới thư mục sites-enabled:\n$ sudo ln -s /etc/nginx/sites-available/service.config /etc/nginx/sites-enabled Kiểm tra lại xem các cấu hình đã đúng hay chưa?:\n$ sudo nginx -t Nếu mọi thứ OK, sẽ có output như sau:\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful Cuối cùng, restart lại Nginx server:\n$ sudo systemctl status nginx   6. Tạo client và kiểm tra kết quả\nSử dụng lại client viết bằng python như ở bài trước:\nimport requests from PIL import Image import numpy as np ENDPOINT_URL = \u0026#39;http://0.0.0.0:8080/infer2\u0026#39; def infer(): data = { \u0026#39;img_path\u0026#39;: \u0026#39;dog.1.jpg\u0026#39; } response = requests.post(ENDPOINT_URL, json = data) response.raise_for_status() print(response.content) if __name__ ==\u0026#34;__main__\u0026#34;: infer() Để kiểm tra hoạt động của client và server, đầu tiên khởi chạy uWSGI (như phần 4.), sau đó chạy client:\n$ python client.py Nếu nhận được kết quả trả vê từ server tức là hệ thống đã hoạt động chính xác:\nb`dog` 7. Kết luận\nNhư vậy là chúng ta đã cùng nhau triển khai thành công DL model sử dụng Nginx, uWSGI và Flask. Nginx thì mặc định được chạy dưới dạng service sau khi cài đặt xong, còn uWSGI thì không. Ta nên cấu hình uWSGI để nó cũng chạy dưới dạng service cho thuận tiện sử dụng. Hi vọng qua bài này, các bạn đã hiểu rõ hơn về cách thức triển khai một AI model trong các sản phẩm thực tế, đáp ứng số lượng lớn user sử dụng đồng thời.\nToàn bộ source code của backend và front-end trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây. Vì model animal_model_classification.h5 có dung lượng khá lớn (\u0026gt; 1.5GB) nên mình không upload lên github được. Các bạn hãy sử dụng model của chính mình để thực hành nhé!\nBài viết tiếp theo, chúng ta sẽ đi nâng cao hơn 1 chút nữa, đó là đóng gói tất cả những phần đã làm hôm nay vào một cái gọi là docker. Docker là gì, và tại sao lại nên dùng nó? Tất cả sẽ được giải đáp trong bài viết đó. Mời các bạn đón đọc!\n8. Tham khảo\n Nginx uWSGI Flask AI Summer  ","permalink":"https://tiensu.github.io/blog/37_deploy_ai_model_with_uwsgi_online_inference/","tags":["Model Deployment"],"title":"Triển khai AI model sử dụng uWSGI và Nginx"},{"categories":["Text Classification"],"contents":"Bài này mình xin phép đổi chủ đề một chút. Chúng ta sẽ thử làm bài toán phân loại text theo các chủ đề khác nhau. Đây là một trong những bài toán thuộc phạm vi của chủ đề xử lý ngôn ngữ tự nhiên (NLP).\nMình sẽ sử dụng bộ dữ liệu BBC news để thực hành. Bạn hãy download của 2 file Train.csv và Test.csv, sau đó gộp chung chúng lại thành 1 file để làm dữ liệu huấn luyện. Tổng số records là 2225, chia thành 6 chủ đề.\nĐầu tiên, import các thư viện sẽ sử dụng:\nimport csv import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences Khai báo một số tham số:\nvocab_size = 1000 embedding_dim = 16 max_length = 120 trunc_type = \u0026#39;post\u0026#39; padding_type = \u0026#39;post\u0026#39; oov_tok = \u0026#39;\u0026lt;OOV\u0026gt;\u0026#39; training_portion = 0.8 sentences = [] labels = [] stopwords = [ \u0026#34;a\u0026#34;, \u0026#34;about\u0026#34;, \u0026#34;above\u0026#34;, \u0026#34;after\u0026#34;, \u0026#34;again\u0026#34;, \u0026#34;against\u0026#34;, \u0026#34;all\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;and\u0026#34;, \u0026#34;any\u0026#34;, \u0026#34;are\u0026#34;, \u0026#34;as\u0026#34;, \u0026#34;at\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;because\u0026#34;, \u0026#34;been\u0026#34;, \u0026#34;before\u0026#34;, \u0026#34;being\u0026#34;, \u0026#34;below\u0026#34;, \u0026#34;between\u0026#34;, \u0026#34;both\u0026#34;, \u0026#34;but\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;could\u0026#34;, \u0026#34;did\u0026#34;, \u0026#34;do\u0026#34;, \u0026#34;does\u0026#34;, \u0026#34;doing\u0026#34;, \u0026#34;down\u0026#34;, \u0026#34;during\u0026#34;, \u0026#34;each\u0026#34;, \u0026#34;few\u0026#34;, \u0026#34;for\u0026#34;, \u0026#34;from\u0026#34;, \u0026#34;further\u0026#34;, \u0026#34;had\u0026#34;, \u0026#34;has\u0026#34;, \u0026#34;have\u0026#34;, \u0026#34;having\u0026#34;, \u0026#34;he\u0026#34;, \u0026#34;he\u0026#39;d\u0026#34;, \u0026#34;he\u0026#39;ll\u0026#34;, \u0026#34;he\u0026#39;s\u0026#34;, \u0026#34;her\u0026#34;, \u0026#34;here\u0026#34;, \u0026#34;here\u0026#39;s\u0026#34;, \u0026#34;hers\u0026#34;, \u0026#34;herself\u0026#34;, \u0026#34;him\u0026#34;, \u0026#34;himself\u0026#34;, \u0026#34;his\u0026#34;, \u0026#34;how\u0026#34;, \u0026#34;how\u0026#39;s\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;i\u0026#39;d\u0026#34;, \u0026#34;i\u0026#39;ll\u0026#34;, \u0026#34;i\u0026#39;m\u0026#34;, \u0026#34;i\u0026#39;ve\u0026#34;, \u0026#34;if\u0026#34;, \u0026#34;in\u0026#34;, \u0026#34;into\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;it\u0026#34;, \u0026#34;it\u0026#39;s\u0026#34;, \u0026#34;its\u0026#34;, \u0026#34;itself\u0026#34;, \u0026#34;let\u0026#39;s\u0026#34;, \u0026#34;me\u0026#34;, \u0026#34;more\u0026#34;, \u0026#34;most\u0026#34;, \u0026#34;my\u0026#34;, \u0026#34;myself\u0026#34;, \u0026#34;nor\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;on\u0026#34;, \u0026#34;once\u0026#34;, \u0026#34;only\u0026#34;, \u0026#34;or\u0026#34;, \u0026#34;other\u0026#34;, \u0026#34;ought\u0026#34;, \u0026#34;our\u0026#34;, \u0026#34;ours\u0026#34;, \u0026#34;ourselves\u0026#34;, \u0026#34;out\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;own\u0026#34;, \u0026#34;same\u0026#34;, \u0026#34;she\u0026#34;, \u0026#34;she\u0026#39;d\u0026#34;, \u0026#34;she\u0026#39;ll\u0026#34;, \u0026#34;she\u0026#39;s\u0026#34;, \u0026#34;should\u0026#34;, \u0026#34;so\u0026#34;, \u0026#34;some\u0026#34;, \u0026#34;such\u0026#34;, \u0026#34;than\u0026#34;, \u0026#34;that\u0026#34;, \u0026#34;that\u0026#39;s\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;their\u0026#34;, \u0026#34;theirs\u0026#34;, \u0026#34;them\u0026#34;, \u0026#34;themselves\u0026#34;, \u0026#34;then\u0026#34;, \u0026#34;there\u0026#34;, \u0026#34;there\u0026#39;s\u0026#34;, \u0026#34;these\u0026#34;, \u0026#34;they\u0026#34;, \u0026#34;they\u0026#39;d\u0026#34;, \u0026#34;they\u0026#39;ll\u0026#34;, \u0026#34;they\u0026#39;re\u0026#34;, \u0026#34;they\u0026#39;ve\u0026#34;, \u0026#34;this\u0026#34;, \u0026#34;those\u0026#34;, \u0026#34;through\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;too\u0026#34;, \u0026#34;under\u0026#34;, \u0026#34;until\u0026#34;, \u0026#34;up\u0026#34;, \u0026#34;very\u0026#34;, \u0026#34;was\u0026#34;, \u0026#34;we\u0026#34;, \u0026#34;we\u0026#39;d\u0026#34;, \u0026#34;we\u0026#39;ll\u0026#34;, \u0026#34;we\u0026#39;re\u0026#34;, \u0026#34;we\u0026#39;ve\u0026#34;, \u0026#34;were\u0026#34;, \u0026#34;what\u0026#34;, \u0026#34;what\u0026#39;s\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;when\u0026#39;s\u0026#34;, \u0026#34;where\u0026#34;, \u0026#34;where\u0026#39;s\u0026#34;, \u0026#34;which\u0026#34;, \u0026#34;while\u0026#34;, \u0026#34;who\u0026#34;, \u0026#34;who\u0026#39;s\u0026#34;, \u0026#34;whom\u0026#34;, \u0026#34;why\u0026#34;, \u0026#34;why\u0026#39;s\u0026#34;, \u0026#34;with\u0026#34;, \u0026#34;would\u0026#34;, \u0026#34;you\u0026#34;, \u0026#34;you\u0026#39;d\u0026#34;, \u0026#34;you\u0026#39;ll\u0026#34;, \u0026#34;you\u0026#39;re\u0026#34;, \u0026#34;you\u0026#39;ve\u0026#34;, \u0026#34;your\u0026#34;, \u0026#34;yours\u0026#34;, \u0026#34;yourself\u0026#34;, \u0026#34;yourselves\u0026#34; ] Chúng ta có một mảng chứa các stop words, tức là các từ thường hay xuất hiện trong câu nhưng lại không mang nhiều ý nghĩa. Chúng ta sẽ loại bỏ chúng đi trước khi huấn luyện model phân loại.\nBây giờ, ta sẽ đọc dataset và chuẩn bị dữ liệu training:\nwith open(\u0026#39;bbc-text.csv\u0026#39;, \u0026#39;r\u0026#39;) as csvfile: reader = csv.reader(csvfile, delimiter=\u0026#39;,\u0026#39;) next(reader) for row in reader: labels.append(row[0]) sentence = row[1] # remove stop words for word in stopwords: token = \u0026#39; \u0026#39; + word + \u0026#39; \u0026#39; sentence = sentence.replace(token, \u0026#39; \u0026#39;) sentence = sentence.replace(\u0026#39; \u0026#39;, \u0026#39; \u0026#39;) sentences.append(sentence) print(len(sentences)) Chia dataset thành 2 phần: train và validation:\ntrain_size = int(len(sentences) * training_portion) train_sentences = sentences[:train_size] train_labels = labels[:train_size] validation_sentences = sentences[train_size:] validation_labels = labels[train_size:] Để model có thể hiểu được dataset, cần phải chuyển các câu dạng text sang dạng vector:\n# chuyển text sang vector tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok) tokenizer.fit_on_texts(train_sentences) word_index = tokenizer.word_index label_tokenizer = Tokenizer() label_tokenizer.fit_on_texts(labels) training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels)) validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels)) # padding để các câu có cùng chiều dài train_sequences = tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length) validation_sequences = tokenizer.texts_to_sequences(validation_sentences) validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length) Mình sẽ đi chi tiết phần này trong 1 bài viết khác. Hôm nay các bạn chỉ cần hiểu ý tưởng của nó như vậy là được rồi.\nSau khi đã có dữ liệu huấn luyện, giờ là lúc chúng ta định nghĩa kiến trúc model.\nmodel = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), tf.keras.layers.GlobalAveragePooling1D(), tf.keras.layers.Dense(24, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(6, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) model.summary() Ở bài này, mình chỉ sử dụng một model đơn giản gồm các lớp Embedding, GlobalAveragePooling1D, và Dense.\nHàm plot để vẽ đồ thị quá trình training:\ndef plot_graph(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10,6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training \u0026amp; Validation, Accuracy \u0026amp; Loss\u0026#39;) plt.legend(loc=0) plt.show() Bước cuối cùng là chạy train model, ta sẽ train model với 30 epochs:\nnum_epochs = 30 history = model.fit( train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=1 ) Training output:\nEpoch 1/30 56/56 [==============================] - 0s 4ms/step - loss: 1.7737 - acc: 0.2180 - val_loss: 1.7481 - val_acc: 0.2270 Epoch 2/30 56/56 [==============================] - 0s 2ms/step - loss: 1.7163 - acc: 0.2303 - val_loss: 1.6755 - val_acc: 0.2270 Epoch 3/30 56/56 [==============================] - 0s 2ms/step - loss: 1.6299 - acc: 0.2371 - val_loss: 1.5792 - val_acc: 0.2539 .................... Epoch 27/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0580 - acc: 0.9949 - val_loss: 0.2115 - val_acc: 0.9506 Epoch 28/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0521 - acc: 0.9961 - val_loss: 0.2080 - val_acc: 0.9506 Epoch 29/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0467 - acc: 0.9961 - val_loss: 0.2051 - val_acc: 0.9506 Epoch 30/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0420 - acc: 0.9989 - val_loss: 0.2014 - val_acc: 0.9506 Quá trình train diễn ra khá nhanh, mất khoảng 2 phút trên máy tính của mình. Tại epochs cuối cùng, độ chính xác trên tập validation là 95,06%.\nToàn bộ quá trình này được thể hiện trên đồ thị như sau:\nplot_graph(history)   Model hội tụ khá nhanh và cho kết quả tốt, không có hiện tượng overfitting. Có lẽ train thêm một số epochs nữa sẽ cho kết quả tốt hơn. Bạn có thể thử.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong bài viết tiếp theo, mình sẽ vẫn thực hành bài toán phân loại văn bản nhưng sử dụng kỹ thuật Transfer Learning giống như bên Computer Vision. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/ddd_bbc_text_topic_classification-copy/","tags":["Text Classification"],"title":"Phân loại text theo chủ đề  - Transfer Learning"},{"categories":["Text Classification"],"contents":"Bài này mình xin phép đổi chủ đề một chút. Chúng ta sẽ thử làm bài toán phân loại text theo các chủ đề khác nhau. Đây là một trong những bài toán thuộc phạm vi của chủ đề xử lý ngôn ngữ tự nhiên (NLP).\nMình sẽ sử dụng bộ dữ liệu BBC news để thực hành. Bạn hãy download của 2 file Train.csv và Test.csv, sau đó gộp chung chúng lại thành 1 file để làm dữ liệu huấn luyện. Tổng số records là 2225, chia thành 6 chủ đề.\nĐầu tiên, import các thư viện sẽ sử dụng:\nimport csv import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences Khai báo một số tham số:\nvocab_size = 1000 embedding_dim = 16 max_length = 120 trunc_type = \u0026#39;post\u0026#39; padding_type = \u0026#39;post\u0026#39; oov_tok = \u0026#39;\u0026lt;OOV\u0026gt;\u0026#39; training_portion = 0.8 sentences = [] labels = [] stopwords = [ \u0026#34;a\u0026#34;, \u0026#34;about\u0026#34;, \u0026#34;above\u0026#34;, \u0026#34;after\u0026#34;, \u0026#34;again\u0026#34;, \u0026#34;against\u0026#34;, \u0026#34;all\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;an\u0026#34;, \u0026#34;and\u0026#34;, \u0026#34;any\u0026#34;, \u0026#34;are\u0026#34;, \u0026#34;as\u0026#34;, \u0026#34;at\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;because\u0026#34;, \u0026#34;been\u0026#34;, \u0026#34;before\u0026#34;, \u0026#34;being\u0026#34;, \u0026#34;below\u0026#34;, \u0026#34;between\u0026#34;, \u0026#34;both\u0026#34;, \u0026#34;but\u0026#34;, \u0026#34;by\u0026#34;, \u0026#34;could\u0026#34;, \u0026#34;did\u0026#34;, \u0026#34;do\u0026#34;, \u0026#34;does\u0026#34;, \u0026#34;doing\u0026#34;, \u0026#34;down\u0026#34;, \u0026#34;during\u0026#34;, \u0026#34;each\u0026#34;, \u0026#34;few\u0026#34;, \u0026#34;for\u0026#34;, \u0026#34;from\u0026#34;, \u0026#34;further\u0026#34;, \u0026#34;had\u0026#34;, \u0026#34;has\u0026#34;, \u0026#34;have\u0026#34;, \u0026#34;having\u0026#34;, \u0026#34;he\u0026#34;, \u0026#34;he\u0026#39;d\u0026#34;, \u0026#34;he\u0026#39;ll\u0026#34;, \u0026#34;he\u0026#39;s\u0026#34;, \u0026#34;her\u0026#34;, \u0026#34;here\u0026#34;, \u0026#34;here\u0026#39;s\u0026#34;, \u0026#34;hers\u0026#34;, \u0026#34;herself\u0026#34;, \u0026#34;him\u0026#34;, \u0026#34;himself\u0026#34;, \u0026#34;his\u0026#34;, \u0026#34;how\u0026#34;, \u0026#34;how\u0026#39;s\u0026#34;, \u0026#34;i\u0026#34;, \u0026#34;i\u0026#39;d\u0026#34;, \u0026#34;i\u0026#39;ll\u0026#34;, \u0026#34;i\u0026#39;m\u0026#34;, \u0026#34;i\u0026#39;ve\u0026#34;, \u0026#34;if\u0026#34;, \u0026#34;in\u0026#34;, \u0026#34;into\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;it\u0026#34;, \u0026#34;it\u0026#39;s\u0026#34;, \u0026#34;its\u0026#34;, \u0026#34;itself\u0026#34;, \u0026#34;let\u0026#39;s\u0026#34;, \u0026#34;me\u0026#34;, \u0026#34;more\u0026#34;, \u0026#34;most\u0026#34;, \u0026#34;my\u0026#34;, \u0026#34;myself\u0026#34;, \u0026#34;nor\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;on\u0026#34;, \u0026#34;once\u0026#34;, \u0026#34;only\u0026#34;, \u0026#34;or\u0026#34;, \u0026#34;other\u0026#34;, \u0026#34;ought\u0026#34;, \u0026#34;our\u0026#34;, \u0026#34;ours\u0026#34;, \u0026#34;ourselves\u0026#34;, \u0026#34;out\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;own\u0026#34;, \u0026#34;same\u0026#34;, \u0026#34;she\u0026#34;, \u0026#34;she\u0026#39;d\u0026#34;, \u0026#34;she\u0026#39;ll\u0026#34;, \u0026#34;she\u0026#39;s\u0026#34;, \u0026#34;should\u0026#34;, \u0026#34;so\u0026#34;, \u0026#34;some\u0026#34;, \u0026#34;such\u0026#34;, \u0026#34;than\u0026#34;, \u0026#34;that\u0026#34;, \u0026#34;that\u0026#39;s\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;their\u0026#34;, \u0026#34;theirs\u0026#34;, \u0026#34;them\u0026#34;, \u0026#34;themselves\u0026#34;, \u0026#34;then\u0026#34;, \u0026#34;there\u0026#34;, \u0026#34;there\u0026#39;s\u0026#34;, \u0026#34;these\u0026#34;, \u0026#34;they\u0026#34;, \u0026#34;they\u0026#39;d\u0026#34;, \u0026#34;they\u0026#39;ll\u0026#34;, \u0026#34;they\u0026#39;re\u0026#34;, \u0026#34;they\u0026#39;ve\u0026#34;, \u0026#34;this\u0026#34;, \u0026#34;those\u0026#34;, \u0026#34;through\u0026#34;, \u0026#34;to\u0026#34;, \u0026#34;too\u0026#34;, \u0026#34;under\u0026#34;, \u0026#34;until\u0026#34;, \u0026#34;up\u0026#34;, \u0026#34;very\u0026#34;, \u0026#34;was\u0026#34;, \u0026#34;we\u0026#34;, \u0026#34;we\u0026#39;d\u0026#34;, \u0026#34;we\u0026#39;ll\u0026#34;, \u0026#34;we\u0026#39;re\u0026#34;, \u0026#34;we\u0026#39;ve\u0026#34;, \u0026#34;were\u0026#34;, \u0026#34;what\u0026#34;, \u0026#34;what\u0026#39;s\u0026#34;, \u0026#34;when\u0026#34;, \u0026#34;when\u0026#39;s\u0026#34;, \u0026#34;where\u0026#34;, \u0026#34;where\u0026#39;s\u0026#34;, \u0026#34;which\u0026#34;, \u0026#34;while\u0026#34;, \u0026#34;who\u0026#34;, \u0026#34;who\u0026#39;s\u0026#34;, \u0026#34;whom\u0026#34;, \u0026#34;why\u0026#34;, \u0026#34;why\u0026#39;s\u0026#34;, \u0026#34;with\u0026#34;, \u0026#34;would\u0026#34;, \u0026#34;you\u0026#34;, \u0026#34;you\u0026#39;d\u0026#34;, \u0026#34;you\u0026#39;ll\u0026#34;, \u0026#34;you\u0026#39;re\u0026#34;, \u0026#34;you\u0026#39;ve\u0026#34;, \u0026#34;your\u0026#34;, \u0026#34;yours\u0026#34;, \u0026#34;yourself\u0026#34;, \u0026#34;yourselves\u0026#34; ] Chúng ta có một mảng chứa các stop words, tức là các từ thường hay xuất hiện trong câu nhưng lại không mang nhiều ý nghĩa. Chúng ta sẽ loại bỏ chúng đi trước khi huấn luyện model phân loại.\nBây giờ, ta sẽ đọc dataset và chuẩn bị dữ liệu training:\nwith open(\u0026#39;bbc-text.csv\u0026#39;, \u0026#39;r\u0026#39;) as csvfile: reader = csv.reader(csvfile, delimiter=\u0026#39;,\u0026#39;) next(reader) for row in reader: labels.append(row[0]) sentence = row[1] # remove stop words for word in stopwords: token = \u0026#39; \u0026#39; + word + \u0026#39; \u0026#39; sentence = sentence.replace(token, \u0026#39; \u0026#39;) sentence = sentence.replace(\u0026#39; \u0026#39;, \u0026#39; \u0026#39;) sentences.append(sentence) print(len(sentences)) Chia dataset thành 2 phần: train và validation:\ntrain_size = int(len(sentences) * training_portion) train_sentences = sentences[:train_size] train_labels = labels[:train_size] validation_sentences = sentences[train_size:] validation_labels = labels[train_size:] Để model có thể hiểu được dataset, cần phải chuyển các câu dạng text sang dạng vector:\n# chuyển text sang vector tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok) tokenizer.fit_on_texts(train_sentences) word_index = tokenizer.word_index label_tokenizer = Tokenizer() label_tokenizer.fit_on_texts(labels) training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels)) validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels)) # padding để các câu có cùng chiều dài train_sequences = tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length) validation_sequences = tokenizer.texts_to_sequences(validation_sentences) validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length) Mình sẽ đi chi tiết phần này trong 1 bài viết khác. Hôm nay các bạn chỉ cần hiểu ý tưởng của nó như vậy là được rồi.\nSau khi đã có dữ liệu huấn luyện, giờ là lúc chúng ta định nghĩa kiến trúc model.\nmodel = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), tf.keras.layers.GlobalAveragePooling1D(), tf.keras.layers.Dense(24, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(6, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) model.summary() Ở bài này, mình chỉ sử dụng một model đơn giản gồm các lớp Embedding, GlobalAveragePooling1D, và Dense.\nHàm plot để vẽ đồ thị quá trình training:\ndef plot_graph(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10,6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training \u0026amp; Validation, Accuracy \u0026amp; Loss\u0026#39;) plt.legend(loc=0) plt.show() Bước cuối cùng là chạy train model, ta sẽ train model với 30 epochs:\nnum_epochs = 30 history = model.fit( train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=1 ) Training output:\nEpoch 1/30 56/56 [==============================] - 0s 4ms/step - loss: 1.7737 - acc: 0.2180 - val_loss: 1.7481 - val_acc: 0.2270 Epoch 2/30 56/56 [==============================] - 0s 2ms/step - loss: 1.7163 - acc: 0.2303 - val_loss: 1.6755 - val_acc: 0.2270 Epoch 3/30 56/56 [==============================] - 0s 2ms/step - loss: 1.6299 - acc: 0.2371 - val_loss: 1.5792 - val_acc: 0.2539 .................... Epoch 27/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0580 - acc: 0.9949 - val_loss: 0.2115 - val_acc: 0.9506 Epoch 28/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0521 - acc: 0.9961 - val_loss: 0.2080 - val_acc: 0.9506 Epoch 29/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0467 - acc: 0.9961 - val_loss: 0.2051 - val_acc: 0.9506 Epoch 30/30 56/56 [==============================] - 0s 2ms/step - loss: 0.0420 - acc: 0.9989 - val_loss: 0.2014 - val_acc: 0.9506 Quá trình train diễn ra khá nhanh, mất khoảng 2 phút trên máy tính của mình. Tại epochs cuối cùng, độ chính xác trên tập validation là 95,06%.\nToàn bộ quá trình này được thể hiện trên đồ thị như sau:\nplot_graph(history)   Model hội tụ khá nhanh và cho kết quả tốt, không có hiện tượng overfitting. Có lẽ train thêm một số epochs nữa sẽ cho kết quả tốt hơn. Bạn có thể thử.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong bài viết tiếp theo, mình sẽ vẫn thực hành bài toán phân loại văn bản nhưng sử dụng kỹ thuật Transfer Learning giống như bên Computer Vision. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/ddd_bbc_text_topic_classification/","tags":["Text Classification"],"title":"Phân loại text theo chủ đề "},{"categories":["Model Deployment"],"contents":"Bạn đã xây dựng thành công một DL model với độ chính xác rất cao, 99%. Xin chúc mừng bạn!\nVấn đề tiếp theo bạn cần nghĩ đến là làm sao đưa model đó vào trong sản phẩm thực tế, để mọi người có thể sử dụng model của bạn một cách đơn giản và dễ dàng. Xây dựng và triển khai model luôn là 2 công đoạn bắt buộc trong một bài toán về AI. Trong các bài tiếp theo, mình sẽ chia sẻ các cách thức mà chúng ta có thể sử dụng để triển khai một DL model vào trong ứng dụng để sử dụng trong thưc tế.\nCó 2 kiểu Inference mà một AI model có thể được sử dụng:\n  Online Inference: Model phải liên tục xử lý và trả về kết quả dự đoán gần như real-time khi nó nhận được yêu cầu. Số lượng yêu cầu đến thường rất lớn, thậm chí là nhiều yêu cầu đến tại cùng 1 thời điểm. Chính vì vậy mà việc triển khai model theo kiểu này thường phức tạp hơn rất nhiều so với kiểu thứ 2.\n  Batch Inference: Model chỉ chạy Inference tại một số thời điểm cố định trong ngày, và mỗi lần Inference sẽ xử lý một tập hợp (batch) các input data nhất định.\n  Mỗi kiểu Inference phù hợp với các bài toán khác nhau, có lẽ mình sẽ viết một bài so sánh chi tiết hơn về 2 kiểu Inference này.\nTrong bài đầu tiên này chúng ta sẽ sử dụng Flask, một web server framework nhỏ nhẹ, dễ dàng trong việc cài đạt và sử dụng, để triển khai model phân loại Cat\u0026amp;Dog\u0026amp;Panda trong bài trước theo kiểu Online Inference.\n1. Giới thiệu về kiến trúc Client-Server và Flask\nClient-server là kiểu kiến trúc xử lý phân tán, gồm 2 thành phần chính là client và server:\n Client gửi các yêu cầu (requests) đến server. Server xử lý các yêu cầu đó và trả lại kết quả cho client. Yêu cầu có thể là truy vấn database, tính toán, so sánh, dự đoán, \u0026hellip;  Dữ liệu trao đổi giữa client và server gọi là các messages.\nGiao thức trao đổi giữa client-server thường là HTTP/HTTPS trong trường hợp client là website, và server khi đó gọi là webserver. Nếu client không phải là website thì giao thức có thể là TCP/UDP hoặc uwsgi. Giao thức sẽ định nghĩa định dạng dữ liệu, cơ chế truyền, truyền lại, cơ chế xác thực dữ liệu, \u0026hellip;. của các bản tin trao đổi giữa 2 bên. Ví dụ, một HTTP request/Response bao gồm 4 thành phần cơ bản:\n URL đích: đường dẫn (path) đến một dịch vụ cụa thể của server mà client cần giao tiếp. Phương pháp giao tiếp (method): có 4 phương pháp là GET, POST, UPDATE, DELETE. Tùy từng yêu cầu cụ thể của bài toán mà ta sử dụng phương pháp phù hợp. Header: là các metadata kiểu như ngày tháng năm (date), tình trạng(status), kiểu dữ liệu (content-type), \u0026hellip; giúp server xử lý và đồng bộ dữ liệu với client. Body: chứa dữ liệu thực tế mà ta cần gửi từ client đến server hoặc ngược lại.  Flask là một framework để tạo ra thành phần server, bao gồm cả web server. Một số ưu điểm của Flask có thể kể đến như sau:\n Nó giúp triển khai DL model dưới dạng web application nếu bạn muốn cung cấp giao diện web cho người dùng. Đơn giản, dễ dàng cài đặt và sử dụng. Hỗ trợ nhiều chức năng thông qua các end-point URL khác nhau.  Tuy nhiên, Flask không hỗ trợ đầy đủ các chức năng cần thiết của 1 server để có thể sử dụng trong môi trường sản phẩm thực tế giống như là tính bảo mật và sự hỗ trợ cùng lúc nhiều client truy cập. Nó chỉ phù hợp cho các ứng dụng mang tính demo, kiểm thử tính năng model. Nếu cần triển khai thực tế, uWSGI là một sự lựa chọn phù hợp (sẽ được đề cập chi tiết trong bài sau).\n2. Xây dựng Server với Flask\nTa sẽ sử dụng Flask để xây dựng một server phục vụ cả 2 loại client: dạng web và dạng code python (dạng thông thường).\nĐầu tiên, tạo file server.py và import Flask và các thư viện cần thiết:\nimport cv2 import os import numpy as np import tensorflow as tf from flask_cors import CORS from tensorflow.keras.models import load_model from flask import Flask, request, render_template, make_response, jsonify Tạo một instance của Flask:\napp = Flask(__name__) Định nghĩa 1 vài hằng số sử dụng:\nimage_width = 300 image_height = 300 classes = [\u0026#39;cat\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;pandas\u0026#39;] APP_ROOT_1 = os.getenv(\u0026#39;APP_ROOT\u0026#39;, \u0026#39;/infer1\u0026#39;) APP_ROOT_1 = os.getenv(\u0026#39;APP_ROOT\u0026#39;, \u0026#39;/infer2\u0026#39;) Hàm render UI mặc định khi truy cập vào địa chỉ server:\n# render default webpage @app.route(\u0026#39;/\u0026#39;) def home(): return render_template(\u0026#39;home.html\u0026#39;) Hàm này chỉ phục vụ client dạng web. File home.html chính là phần front-end mà chúng ta sẽ viết code để tạo giao diện tương tác với người dùng trên web.\nHàm nhận và xử lý request từ client dạng web:\n@app.route(APP_ROOT_1, methods = [\u0026#39;POST\u0026#39;, \u0026#39;GET\u0026#39;]) def classify_image(): if request.method == \u0026#39;POST\u0026#39;: # geting data from html form img_path = request.form[\u0026#34;img_path\u0026#34;] # call funtion to classify image and receive result result = classify_animal(img_path) # return result to client response = {\u0026#39;result\u0026#39;: result, \u0026#39;image\u0026#39;: img_path} return make_response(jsonify(response), 200) Hàm nhận và xử lý request từ client dạng thông thường:\n@app.route(APP_ROOT_2, methods=[\u0026#34;POST\u0026#34;]) def infer(): data = request.json image = data[\u0026#39;image_path\u0026#39;] return classify_animal(img_path) Hãy nhớ lại ở bài trước, sau khi huấn luyện xong model, ta đã lưu nó thành file animal_model_classification.h5 vào ổ cứng. File này có kích thước khá nặng, khoảng 1.7GB. Bây giờ, ta sẽ sử dụng model đó đã nhận diện.\nLoad DL model:\nmodel = load_model(\u0026#39;animal_model_classification.h5\u0026#39;) Hàm dưới đây nhận vào tham số là đường dẫn đến ảnh cần nhận diện và trả về kết quả:\ndef classify_animal(img_path): # read image image = cv2.imread(img_path) image = image/255 image = cv2.resize(image, (image_width,image_height) image = np.reshape(image, [1,image_width,image_height,3]) # pass the image through the network to obtain our predictions preds = model.predict(image) label = classes[np.argmax(preds)] return label Cuối cùng, sử dụng hàm run() để khởi tạo server:\nif __name__ == \u0026#39;__main__\u0026#39;: app.run() Để chạy server, mở cửa sổ terminal và gõ lệnh:\n$ python server.py Nếu mọi thứ Ok thì cửa sổ terminal sẽ xuất hiện như sau:\n Như vậy là đã xong phần backend, tiếp theo ta sẽ viết code cho front-end.\n3. Web client\nWeb client có giao diện đơn giản gồm 1 button cho phép user chọn ảnh cần nhận diện và 1 khu vực để hiển thị ảnh kèm kết quả.\n Vì web không phải là lĩnh vực chuyên sâu của mình nên mình sẽ không đi chi tiết code ở đây. Các bạn có thể tham khảo code web trên github của mình.\nĐể kiểm tra hoạt động, truy cập vào địa chỉ http://localhost:5000, upload một bức ảnh, click Detect button. Kết quả phân loại sẽ được hiển thị.\n 4. Python client\nTạo file client.py và code như sau:\nimport requests import numpy as np ENDPOINT_URL = \u0026#39;http://0.0.0.0:5000/infer2\u0026#39; def infer(): img_path = \u0026#39;dog1.jpg\u0026#39; data = { \u0026#39;image\u0026#39;: image_path } response = requests.post(ENDPOINT_URL, json = data) response.raise_for_status() print(response.content) if __name__ ==\u0026#34;__main__\u0026#34;: infer() Khởi chạy client:\n$ python client.py Kết quả trả về từ server:\nb\u0026#39;dog\u0026#39; Thử lại với một ảnh ảnh con mèo, cat.1.jpg. Kết qủa trả về:\nb\u0026#39;cat\u0026#39; 5. Kết luận\nNhư vậy là chúng ta đã cùng nhau triển khai xong DL model sử dụng Flask. Mặc dù tồn tại nhiều nhược điểm nhưng khi cần nhanh chóng đạt được kết quả để thử nghiệm thì Flask vẫn được tin dùng.\nBài viết tiếp theo, mình sẽ giới thiệu một cách nâng cao hơn để triển khai DL model, thường được áp dụng trong các bài toán thực tế. Mời các bạn đón đọc!\nToàn bộ source code của backend và front-end trong bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây. Vì model animal_model_classification.h5 có dung lượng khá lớn (\u0026gt; 1.5GB) nên mình không upload lên github được. Các bạn hãy sử dụng model của chính mình để thực hành nhé!\n6. Tham khảo\n Flask AI Summer  ","permalink":"https://tiensu.github.io/blog/36_deploy_ai_model_with_flask_online_inference/","tags":["Model Deployment"],"title":"Triển khai AI model sử dụng Flask"},{"categories":["Convolution Neural Network","Image Classification"],"contents":"Những bài toán mà chỉ có 2 lớp cần phân biệt gọi là binary classification, còn những bài toán có nhiều hơn 2 lớp được gọi là multiple classification.\nCó một vài sự khác biệt trong cách cài đặt CNN model giữa 2 loại bài toán này. Trong hôm nay chúng ta sẽ cùng tìm hiểu điều đó thông qua thực hiện phân loại 3 classes: Cat, Dog và Panda của bộ dữ liệu animals. Bộ dataset này gồm 3000 ảnh chia đều cho mỗi class.\nTa sẽ bắt tay vào thực hiện code luôn, những điểm khác biệt sẽ được đề cập trong quá trình viết code.\nSử dụng kiến thức đã học từ bài trước, ta sẽ thực hiện bài này theo 2 cách và so sánh kết quả:\n Không sử dụng Transfer Learning Sử dụng Transfer Learning  Đầu tiên, download dataset về thư mục làm việc và dùng thư viện split-folers để chia dữ liệu thành 2 tập train và validation.\nImport các thư viện sẽ sử dụng:\nimport os import random import shutil import tensorflow as tf import matplotlib.pyplot as plt from tensorflow import keras from tensorflow.keras import layers, Model from tensorflow.keras.applications.inception_v3 import InceptionV3 from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping Chuẩn bị dữ liệu training:\ndef data_gen(): train_gen = ImageDataGenerator( rescale=1/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\u0026#39;nearest\u0026#39; ) validation_gen = ImageDataGenerator( rescale=1/255 ) train_datagen = train_gen.flow_from_directory( \u0026#39;Animals/training\u0026#39;, target_size=(300,300), batch_size=32, class_mode=\u0026#39;categorical\u0026#39; ) validation_datagen = validation_gen.flow_from_directory( \u0026#39;Animals/validation\u0026#39;, target_size=(300,300), batch_size=32, class_mode=\u0026#39;categorical\u0026#39; ) return train_datagen, validation_datagen Model tự định nghĩa:\ndef create_own_model(): model = keras.Sequential([ # CONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; POOL =\u0026gt; DO layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, input_shape=(300, 300, 3)), layers.BatchNormalization(), layers.MaxPooling2D(3,3), layers.Dropout(0.25), # (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.MaxPooling2D(2,2), layers.Dropout(0.25), # (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;), layers.BatchNormalization(), layers.MaxPooling2D(2,2), layers.Dropout(0.25), # (FC =\u0026gt; RELU =\u0026gt; BN =\u0026gt; DO)*2 =\u0026gt; FC =\u0026gt; SOFTMAX layers.Flatten(), layers.Dense(1024, activation=\u0026#39;relu\u0026#39;), layers.BatchNormalization(), layers.Dropout(0.25), layers.Dense(512, activation=\u0026#39;relu\u0026#39;), layers.BatchNormalization(), layers.Dropout(0.25), layers.Dense(3, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Kiến trúc model:\nCONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; POOL =\u0026gt; DO =\u0026gt; (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO =\u0026gt; (CONV =\u0026gt; RELU =\u0026gt; BN)*2 =\u0026gt; POOL =\u0026gt; DO =\u0026gt; (FC =\u0026gt; RELU =\u0026gt; BN =\u0026gt; DO)*2 =\u0026gt; FC =\u0026gt; SOFTMAX\nModel sử dụng Transfer Learning:\ndef create_transfer_learning_model(): base_model = InceptionV3( input_shape=(300,300,3), include_top=False, weights=\u0026#39;imagenet\u0026#39; ) for layer in base_model.layers: layer.trainable = False head_model = base_model.output head_model = layers.Flatten()(head_model) head_model = layers.Dense(1024, activation=\u0026#39;relu\u0026#39;)(head_model) head_model = layers.BatchNormalization()(head_model) head_model = layers.Dropout(0.25)(head_model) head_model = layers.Dense(512, activation=\u0026#39;relu\u0026#39;)(head_model) head_model = layers.BatchNormalization()(head_model) head_model = layers.Dropout(0.5)(head_model) head_model = layers.Dense(3, activation=\u0026#39;softmax\u0026#39;)(head_model) model = Model(inputs=base_model.input, outputs=head_model) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Trong phương pháp Transfer Learning, ta sử dụng pre-trained InceptionV3 làm base_model.\nNhư chúng ta thấy, có 2 điểm khác biệt ở đây:\n Hàm activation ở layer cuối Nếu như bài toán binary classification sử dụng hàm sigmoid chỉ có 1 output thì bài toán multiple classification sử dụng hàm softmax, số lượng output bằng số classes cần phân loại. Mình sẽ có bài phân tích chi tiết về các loại hàm này sau. Hàm loss Binary classification sử dụng hàm binary_crossentropy, còn multiple classification sử dụng categorical_crossentropy hoặc sparse_categorical_crossentropy. Mình cũng sẽ viết một bài về các dạng hàm loss trong tương lai.  Định nghĩa callback functions:\ndef create_callbacks(): callback_1 = EarlyStopping(monitor=\u0026#39;val_acc\u0026#39;, patience=4) callback_2 = ModelCheckpoint( \u0026#39;Animals_ModelCheckpoints/model-{epoch:02d}-{val_acc:.2f}.hdf5\u0026#39;, save_best_only=True, save_weights_only=True, monitor=\u0026#39;val_acc\u0026#39;, save_freq=\u0026#39;epoch\u0026#39;, mode=\u0026#39;auto\u0026#39;, verbose=1 ) return [callback_1, callback_2] Hàm vẽ đồ thị:\ndef plot_graph(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epoch = range(len(acc)) plt.plot(epoch, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epoch, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epoch, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epoch, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training \u0026amp; Validation, Accuracy \u0026amp; Loss\u0026#39;) plt.legend(loc=0) plt.show() Train model với kiến trúc tự định nghĩa:\ntraining_datagen, validation_datagen = data_gen() model = create_own_model() history = model.fit( training_datagen, epochs=30, validation_data=validation_datagen, callbacks=create_callbacks(), verbose=1 ) Training output:\nFound 2400 images belonging to 3 classes. Found 600 images belonging to 3 classes. Epoch 1/30 2/75 [..............................] - ETA: 4s - loss: 1.8446 - acc: 0.3906WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0442s vs `on_train_batch_end` time: 0.0907s). Check your callbacks. 75/75 [==============================] - ETA: 0s - loss: 1.2381 - acc: 0.5292 Epoch 00001: val_acc improved from -inf to 0.33333, saving model to Animals_ModelCheckpoints/model-01-0.33.hdf5 75/75 [==============================] - 40s 533ms/step - loss: 1.2381 - acc: 0.5292 - val_loss: 4.0135 - val_acc: 0.3333 Epoch 2/30 75/75 [==============================] - ETA: 0s - loss: 0.8709 - acc: 0.6042 Epoch 00002: val_acc did not improve from 0.33333 75/75 [==============================] - 40s 533ms/step - loss: 0.8709 - acc: 0.6042 - val_loss: 3.9076 - val_acc: 0.3333 Epoch 3/30 75/75 [==============================] - ETA: 0s - loss: 0.8323 - acc: 0.6008 Epoch 00003: val_acc improved from 0.33333 to 0.35000, saving model to Animals_ModelCheckpoints/model-03-0.35.hdf5 75/75 [==============================] - 40s 535ms/step - loss: 0.8323 - acc: 0.6008 - val_loss: 3.0383 - val_acc: 0.3500 ........ Epoch 11/30 75/75 [==============================] - ETA: 0s - loss: 0.6230 - acc: 0.6958 Epoch 00011: val_acc did not improve from 0.72667 75/75 [==============================] - 40s 536ms/step - loss: 0.6230 - acc: 0.6958 - val_loss: 0.9326 - val_acc: 0.5933 Epoch 12/30 75/75 [==============================] - ETA: 0s - loss: 0.5867 - acc: 0.7133 Epoch 00012: val_acc did not improve from 0.72667 75/75 [==============================] - 40s 539ms/step - loss: 0.5867 - acc: 0.7133 - val_loss: 0.6075 - val_acc: 0.7067 Epoch 13/30 75/75 [==============================] - ETA: 0s - loss: 0.5752 - acc: 0.7262 Epoch 00013: val_acc did not improve from 0.72667 75/75 [==============================] - 41s 540ms/step - loss: 0.5752 - acc: 0.7262 - val_loss: 0.5461 - val_acc: 0.7250 Model dừng train sau 13 epochs do giá trị của val_acc không tăng sau 5 epochs liên tiếp từ epoch 9 đến epoch 13. Độ chính xác cao nhất đạt được trên tập validation 72.67% tại epoch thứ 9.\nĐồ thị quá trình training:  Bây giờ, thử sử dụng pre-trained model:\nmodel = create_transfer_learning_model() training_datagen, validation_datagen = data_gen() history = model.fit( training_datagen, validation_data=validation_datagen, epochs=30, callbacks=create_callbacks(), verbose=1 ) Traning output:\nFound 2400 images belonging to 3 classes. Found 600 images belonging to 3 classes. Epoch 1/30 75/75 [==============================] - ETA: 0s - loss: 0.1724 - acc: 0.9525 Epoch 00001: val_acc improved from -inf to 0.95667, saving model to Animals_ModelCheckpoints/model-01-0.96.hdf5 75/75 [==============================] - 41s 548ms/step - loss: 0.1724 - acc: 0.9525 - val_loss: 0.2077 - val_acc: 0.9567 Epoch 2/30 75/75 [==============================] - ETA: 0s - loss: 0.1153 - acc: 0.9679 Epoch 00002: val_acc improved from 0.95667 to 0.99167, saving model to Animals_ModelCheckpoints/model-02-0.99.hdf5 75/75 [==============================] - 42s 559ms/step - loss: 0.1153 - acc: 0.9679 - val_loss: 0.0331 - val_acc: 0.9917 Epoch 3/30 75/75 [==============================] - ETA: 0s - loss: 0.0719 - acc: 0.9771 Epoch 00003: val_acc improved from 0.99167 to 0.99333, saving model to Animals_ModelCheckpoints/model-03-0.99.hdf5 75/75 [==============================] - 42s 557ms/step - loss: 0.0719 - acc: 0.9771 - val_loss: 0.0082 - val_acc: 0.9933 ........ Epoch 8/30 75/75 [==============================] - ETA: 0s - loss: 0.0410 - acc: 0.9833 Epoch 00008: val_acc did not improve from 0.99500 75/75 [==============================] - 43s 569ms/step - loss: 0.0410 - acc: 0.9833 - val_loss: 0.0288 - val_acc: 0.9933 Epoch 9/30 75/75 [==============================] - ETA: 0s - loss: 0.0554 - acc: 0.9842 Epoch 00009: val_acc did not improve from 0.99500 75/75 [==============================] - 44s 588ms/step - loss: 0.0554 - acc: 0.9842 - val_loss: 0.0140 - val_acc: 0.9917 Epoch 10/30 75/75 [==============================] - ETA: 0s - loss: 0.0632 - acc: 0.9804 Epoch 00010: val_acc did not improve from 0.99500 75/75 [==============================] - 43s 572ms/step - loss: 0.0632 - acc: 0.9804 - val_loss: 0.0219 - val_acc: 0.9933 Model dừng train sớm hơn, tại epoch thứ 10 sau 5 epochs liên tiếp không cải thiện về giá trị của val_acc (từ epoch 6 đến epoch 10). Độ chính xác trên tập validation cao nhất đạt được là 99,5% tại epoch thứ 5. Kết quả tốt hơn so với sử dụng model tự định nghĩa rất nhiều.\nĐồ thị quá trình training:  Như vậy, qua bài này ta đã biết được cách thức xây dựng kiến trúc CNN model cho bài toán multiple classification. Đồng thời ta cũng nhận thấy rõ ràng ưu điểm của kỹ thuật Transfer Learning so với cách tự xây dựng CNN model. Có thể nói rằng, sử dụng pre-trained model luôn cho kết quả tốt hơn, trừ khi chúng ta có lý do cụ thể để không sử dụng chúng.\nCuối cùng, ta lưu lại model để sử dụng cho việc dự đoán về sau khi triển khai model vào sản phẩm thực tế:\nmodel.save(\u0026#39;animal_classification_model.h5\u0026#39;) Source code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong các bài viết tiếp theo, mình sẽ viết về một số bài toán NLP và các kỹ thuật cần dùng để giải quyết chúng. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/35_cnn_model_multiple_classification/","tags":["Convolution Neural Network","Image Classification"],"title":"Xây dựng CNN model cho bài toán đa lớp"},{"categories":["Convolution Neural Network","Image Classification"],"contents":"Trong các bài toán thực tế, khi làm việc với bộ dataset lớn và kiến trúc model phức tạp, việc huấn luyện model sẽ mất rất nhiều thời gian. Vài ngày hoặc thậm chí cả tuần mới ra được kết quả. Nếu ta chỉ train một lần thì không có gì đáng nói, nhưng thường thì ta sẽ train đi train lại rất nhiều lần, mỗi lần điều chỉnh hyper-parameter lại phải chạy train lại. Việc này quả thực rât rất mất thời gian và chán nản.\nHoặc khi chỉ có một lượng nhỏ dữ liệu để train model, chắc chắn sẽ cho ra một model không tốt, vì nó không học được kết các khía cạnh của dữ liệu. Khi triển khai thực tế chắc chắn sẽ thất bại.\nKỹ thuật Transfer Learning ra đời để giải quyết khó khăn này. Ý tưởng chính của nó là tận dụng lại những model kinh diển (VGG, Resnet, InceptionNet, \u0026hellip;), đã được train trên những tập dữ liệu lớn (pre-trained models), loại bỏ các layers classification ở gần cuối (thường là các lớp FC), chỉ giữ lại các layers ở đầu làm nhiệm vụ trích xuất đặc trưng của dữ liệu.\nCó 2 loại transfer learning:\n Feature extractor: Sử dụng pre-trained model như là bộ trích xuất đặc trưng của dữ liệu. Các đặc trưng này sau đó sẽ được phân loại sử dụng các thuật toán ML như kNN, SVM, Decision Tree, \u0026hellip; Fine tuning: Loại bỏ các layers cuối làm nhiệm vụ phân loại trong pre-trained model, thêm vào các layers mới dựa theo bộ dữ liệu mà chúng ta có. Sau đó, train lại model tại những layers mà ta mới thêm vào.  Vậy thì khi nào ta nên sử dụng Transfer Learning?\nDựa trên kích thước và độ tương quan giữa CSDL mới và CSDL gốc (chủ yếu là ImageNet) để train các mô hình có sẵn, CS231n đưa ra một vài lời khuyên:\n  CSDL mới là nhỏ và tương tự như CSDL gốc. Vì CSDL mới nhỏ, việc tiếp tục train model dễ dẫn đến hiện tượng overfitting. Cũng vì hai CSDL là tương tự nhau, ta dự đoán rằng các high-level features là tương tự nhau. Vậy nên ta không cần train lại model mà chỉ cần train một classifer dựa trên feature vectors ở đầu ra ở layer gần cuối.\n  CSDL mới là lớn và tương tự như CSDL gốc. Vì CSDL này lớn, overfitting ít có khả năng xảy ra hơn, ta có thể train mô hình thêm một chút nữa (toàn bộ hoặc chỉ một vài layers cuối).\n  CSDL mới là nhỏ và rất khác với CSDL gốc. Vì CSDL này nhỏ, tốt hơn hết là dùng các classifier đơn giản (các linear classifiers) để tránh overfitting). Nếu muốn train thêm, ta cũng chỉ nên train các layer cuối. Hoặc có một kỹ thuật khác là coi đầu ra của một layer xa layer cuối hơn làm các feature vectors.\n  CSDL mới là lớn và rất khác CSDL gốc. Trong trường hợp này, ta vẫn có thể sử dụng mô hình đã train như là điểm khởi tạo cho mô hình mới, không nên train lại từ đầu.\n  Có một điểm đáng chú ý nữa là khi tiếp tục train các mô hình này, ta chỉ nên chọn learning rate nhỏ để các weights mới không đi quá xa so với các weights đã được trained ở các mô hình trước.\nOK, ta sẽ bắt tay vào thực hành kỹ thuật này (theo cách thứ 2) ngay bây giờ!\nYêu cầu bài toán là huấn luyện một CNN model để phân loại ảnh chứa ngựa và người trong bộ dataset horse-or-humand.\nDownload bộ dataset horse-or-humand về máy tính, giải nén và sử dụng thư viện split-folders để chia thành 2 phần train set và validation set.\nĐầu tiên, import các thư viện cần thiết:\nimport os import tensorflow as tf import matplotlib.pyplot as plt from tensorflow.keras import layers from tensorflow.keras import Model from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.applications.inception_v3 import InceptionV3 config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Các pre-trained model phổ biến đã được tích hợp sẵn trong tensorflow. Ở đây ta khai báo lớp InceptionV3 để sử dụng InceptionNet pre-trained model.\nTiếp theo là chuẩn bị dữ liệu huấn luyện:\ndef data_gen(): training_datagen = ImageDataGenerator( rescale=1/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, horizontal_flip=True, fill_mode=\u0026#39;nearest\u0026#39; ) validation_datagen = ImageDataGenerator(rescale=1/255) training_generator = training_datagen.flow_from_directory( \u0026#39;horse-and-humand/train\u0026#39;, target_size=(224,224), batch_size=16, class_mode=\u0026#39;binary\u0026#39; ) validation_generator = validation_datagen.flow_from_directory( \u0026#39;horse-and-humand/validation\u0026#39;, target_size=(224,224), batch_size=16, class_mode=\u0026#39;binary\u0026#39; ) return training_generator, validation_generator Khai báo 2 hàm callback: EarlyStopping và ModelCheckpoint:\ndef create_callbacks(): callback_1 = ModelCheckpoint( \u0026#39;horse-humand_model_checkpoint/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\u0026#39;, monitor=\u0026#39;val_acc\u0026#39;, save_best_only=True, save_weights_only=True, save_freq=\u0026#39;epoch\u0026#39;, mode=\u0026#39;auto\u0026#39;, verbose=1 ) callback_2 = EarlyStopping(monitor=\u0026#39;val_acc\u0026#39;, patience=5) return [callback_1, callback_2] Phần quan trọng nhất trong bài này là định nghĩa model, sử dụng kỹ thuật Transfer Learning:\ndef create_model(): # Load pre-trained model base_model = InceptionV3( input_shape=(224,224,3), # Kích thước ảnh đầu vào include_top=False, # Loại bỏ các FC layers ở cuối weights=\u0026#39;imagenet\u0026#39; # Sử dụng các weights được train trên tập imagenet ) # Đóng băng các layers của pre-trained model, không cho chúng update for layer in base_model.layers: layer.trainable = False # Tạo head_model head_model = base_model.output head_model = layers.Flatten()(head_model) head_model = layers.Dense(1024, activation=\u0026#39;relu\u0026#39;)(head_model) head_model = layers.Dropout(0.2)(head_model) head_model = layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;)(head_model) model = Model(inputs=base_model.input, outputs=head_model) model.compile(optimizer=RMSprop(lr=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Model được tạo thành gồm 2 phần:\n base_model: chính là pre-trained model (đã loại bỏ các FC layers ở cuối). head_model: là các FC layers được thêm vào làm nhiệm vụ phân loại dựa theo tập dữ liệu mới.  Trong bài toán này, ta sử dụng pre-trained model của mạng InceptionNetV3 trên tập dữ liệu imagenet. Head_model bao gồm 2 FC layers xen kẽ DO layer ở giữa.\nHàm vẽ đồ thị:\ndef plot_chart(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10, 6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Traing and Validation, Accuracy and Loss\u0026#39;) plt.legend(loc=0) plt.show() Gộp tất cả lại và tiến hành train model:\nmodel = create_model() training_generator, validation_generator = data_gen() callbacks = create_callbacks() history = model.fit( training_generator, epochs=30, callbacks=[callback_1, callback_2], validation_data=validation_generator, verbose=1 ) Training output:\nEpoch 1/30 65/65 [==============================] - ETA: 0s - loss: 0.1342 - acc: 0.9786 Epoch 00001: val_acc improved from -inf to 0.91797, saving model to horse-humand_model_checkpoint/weights-improvement-01-0.92.hdf5 65/65 [==============================] - 12s 180ms/step - loss: 0.1342 - acc: 0.9786 - val_loss: 0.4456 - val_acc: 0.9180 Epoch 2/30 65/65 [==============================] - ETA: 0s - loss: 0.0992 - acc: 0.9708 Epoch 00002: val_acc did not improve from 0.91797 65/65 [==============================] - 12s 189ms/step - loss: 0.0992 - acc: 0.9708 - val_loss: 0.9824 - val_acc: 0.8594 Epoch 3/30 65/65 [==============================] - ETA: 0s - loss: 0.0501 - acc: 0.9903 Epoch 00003: val_acc improved from 0.91797 to 0.95312, saving model to horse-humand_model_checkpoint/weights-improvement-03-0.95.hdf5 ............ Epoch 8/30 65/65 [==============================] - ETA: 0s - loss: 0.1031 - acc: 0.9834 Epoch 00008: val_acc did not improve from 0.99219 65/65 [==============================] - 13s 201ms/step - loss: 0.1031 - acc: 0.9834 - val_loss: 0.5211 - val_acc: 0.9219 Epoch 9/30 65/65 [==============================] - ETA: 0s - loss: 0.0893 - acc: 0.9786 Epoch 00009: val_acc did not improve from 0.99219 65/65 [==============================] - 13s 203ms/step - loss: 0.0893 - acc: 0.9786 - val_loss: 0.5921 - val_acc: 0.9180 Epoch 10/30 65/65 [==============================] - ETA: 0s - loss: 0.1015 - acc: 0.9815 Epoch 00010: val_acc did not improve from 0.99219 65/65 [==============================] - 13s 204ms/step - loss: 0.1015 - acc: 0.9815 - val_loss: 0.4264 - val_acc: 0.9453 Model dừng train sau 10 epochs, độ chính xác cao nhất đạt được trên tập validation là 94.53%, một kết quả khá cao với số lượng epochs \u0026ldquo;khiêm tốn\u0026rdquo; như vậy.\nQuan sát thư mục horse-humand_model_checkpoint ta cũng thấy model được lưu tại một số điểm checkpoint. Model có độ chính xác cao nhất tại epoch thứ 3.\n├── weights-improvement-01-0.92.hdf5 ├── weights-improvement-03-0.95.hdf5 Kiểm tra quá trình huấn luyện bằng cách thể hiện giá trị loss và accuracy lên đồ thị:\nplot_chart(history)   Mặc dù giá trị của val_loss dao động 1 chút nhưng acc, val_acc và loss đều khá lý tưởng, chứng tỏ sự hiệu quả của kỹ thuật Transfer Learning trong bài toàn này.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nCác bài viết từ trước đến giờ đều chỉ phân loại 2 lớp. Đối với bài toán phân loại nhiều lớp thì sẽ như thế nào? Câu trả lời sẽ có ở bài tiếp theo . Mời các bạn đón đọc!\nTham khảo\n Coursera MachineLearningCoban  ","permalink":"https://tiensu.github.io/blog/34_transfer_learning/","tags":["Convolution Neural Network","Image Classification"],"title":"Sử dụng kỹ thuật Transfer Learning khi huấn luyện CNN model"},{"categories":["Convolution Neural Network","Image Classification"],"contents":"Tiếp theo bài trước, trong bài này chúng ta sẽ áp dụng thêm 2 kỹ thuật mới vào CNN model Cat\u0026amp;Dog classification:\n  Data Augmentation: Đây là kỹ thuật tăng cường dữ liệu huấn luyện cho model. Nó đặc biệt hữu ích khi chúng ta có ít dữ liệu vì từ một ảnh gốc ban đầu, thông qua các phép biến đổi hình thái học (xoay, lật, phóng to, thu nhỏ, thay đổi độ sáng, độ tương phải, \u0026hellip;) ta có thêm được nhiều ảnh mới. Kỹ thuật này không chỉ giới hạn trong các bài toán liên quan đến ảnh, mà các bài toán Data Science và NLP cũng có thể sử dụng được.    Model Checkpoint: Đây thực chất là một hàm callback, được gọi sau mỗi epoch trong quá trình huấn luyện model. Nó sẽ lưu lại model nếu giá trị loss hoặc accuracy được cải thiện sau mỗi epoch.\n  Cùng với EarlyStopping thì 2 kỹ thuật này được cũng được sử dụng rất thường xuyên để hạn chế hiện tượng overfitting của model.\nBây giờ, ta sẽ sử dụng chúng trong bài toán xây dựng CNN model phân loại Cat\u0026amp;Dog.\nImport thư viện:\nimport os import random import tensorflow as tf import shutil import matplotlib.pyplot as plt from tensorflow import keras from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint from tensorflow.keras.preprocessing.image import ImageDataGenerator config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Hai kỹ thuật Data augmentation và Model checkpoint sẽ được sử dụng thông qua lớp ImageDataGenerator và ModelCheckpoint, tương ứng.\nLớp ImageDataGenerator cho phép ta biến đổi ảnh gốc thành nhiều ảnh khác nhau thông quá các tham số truyền vào. Như trong hàm gen_data() dưới đây, ta sinh ra được 6 ảnh mới thông qua các phép biến đổi:\n Quay 40 độ Dịch theo chiều rộng 0.2 pixcel Dịch theo chiều cao 0.2 pixcel Cắt (xén) 0.2 pixcel Phóng to 0.2 pixcel Lật ngang  Chú ý rằng, các ảnh mới sinh ra không được lưu vào ổ cứng máy tính, mà chỉ được sinh ra tại thời điểm huấn luyện model và lưu tạm thời trong RAM. Khi kết thức quá trình training thì các ảnh đó cũng sẽ mất.\nTham số fill_mode='nearest' chỉ ra phương pháp bù lại giá trị cho những pixcel tại các vị trí bị mất mát do quá trình biến đổi. Nearest tức là dựa vào giá trị của các pixcel xung quanh, gần nó nhất (theo một tiêu chuẩn nào đó).\ndef gen_data(): training_datagen = ImageDataGenerator( rescale=1/255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\u0026#39;nearest\u0026#39; ) validation_datagen = ImageDataGenerator( rescale=1/255 ) training_generator = training_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/train\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) validation_generator = validation_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/val\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) return training_generator, validation_generator Tiếp theo, ta khai báo một instance của ModelCheckpoint:\ncallback_1 = ModelCheckpoint( \u0026#39;cat_dog_model_checkpoint/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\u0026#39;, # Tên model tại mỗi điểm checkpoint monitor=\u0026#39;val_acc\u0026#39;, # Giá trị cần theo dõi save_best_only=True, # Chỉ lưu những model tốt nhất đến thời điểm checkpoint save_weights_only=True, # Chỉ lưu weights của model (để giảm kích thước) save_freq=\u0026#39;epoch\u0026#39;, # Checkpoint sau mỗi epoch mode=\u0026#39;auto\u0026#39;, # Val_acc phải tăng mới tính là model được cải thiện. Nếu monitor=\u0026#39;loss/val_loss\u0026#39; thì nó phải giảm mới tính là model được cải thiện verbose=1 # Hiển thị thông tin model lúc checkpoint ) Chi tiết từng tham số của ModelCheckpoint instance được giải thích chi tiết theo các comments trong code khai báo.\nTa vẫn sử dụng thêm EarlyStopping để tiết kiệm thời gian training:\ncallback_2 = EarlyStopping(monitor=\u0026#39;val_acc\u0026#39;, patience=5) Kiến trúc CNN model vẫn giữ nguyên như bài trước:\ndef create_model(): model = keras.models.Sequential([ keras.layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150, 150, 3)), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Flatten(), keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=RMSprop(lr=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Hàm plot_chart để thể hiện kết quả training lên đồ thị:\ndef plot_chart(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10, 6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Traing and Validation, Accuracy and Loss\u0026#39;) plt.legend(loc=0) plt.show() Cuối cùng, gộp tất cả lại và train model:\ntraining_generator, validation_generator = gen_data() model = create_model() history = model.fit( training_generator, epochs=30, validation_data=validation_generator, callbacks=[callback_1, callback_2], verbose=1 ) Output:\nFound 20000 images belonging to 2 classes. Found 5000 images belonging to 2 classes. Epoch 1/30 2/625 [..............................] - ETA: 17s - loss: 1.6500 - acc: 0.5469WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0223s vs `on_train_batch_end` time: 0.0339s). Check your callbacks. 625/625 [==============================] - ETA: 0s - loss: 0.6804 - acc: 0.5882 Epoch 00001: val_acc improved from -inf to 0.69340, saving model to horse-humand_model_checkpoint/weights-improvement-01-0.69.hdf5 625/625 [==============================] - 115s 184ms/step - loss: 0.6804 - acc: 0.5882 - val_loss: 0.6017 - val_acc: 0.6934 Epoch 2/30 625/625 [==============================] - ETA: 0s - loss: 0.6239 - acc: 0.6591 Epoch 00002: val_acc improved from 0.69340 to 0.73060, saving model to horse-humand_model_checkpoint/weights-improvement-02-0.73.hdf5 625/625 [==============================] - 112s 179ms/step - loss: 0.6239 - acc: 0.6591 - val_loss: 0.5446 - val_acc: 0.7306 Epoch 3/30 625/625 [==============================] - ETA: 0s - loss: 0.5922 - acc: 0.6922 Epoch 00003: val_acc improved from 0.73060 to 0.76340, saving model to horse-humand_model_checkpoint/weights-improvement-03-0.76.hdf5 625/625 [==============================] - 113s 181ms/step - loss: 0.5922 - acc: 0.6922 - val_loss: 0.5106 - val_acc: 0.7634 ............ Epoch 30/30 625/625 [==============================] - ETA: 0s - loss: 0.4392 - acc: 0.8173 Epoch 00030: val_acc did not improve from 0.86600 625/625 [==============================] - 112s 179ms/step - loss: 0.4392 - acc: 0.8173 - val_loss: 0.4723 - val_acc: 0.7768 Model được train đầy đủ 30 epochs, không bị dừng giữa chừng do không thỏa mãn điều kiện của EarlyStopping.\nQuan sát thư mục cat_dog_model_checkpoint ta cũng thấy model được lưu tại một số điểm checkpoint. Model có độ chính xác cao nhất tại epoch thứ 29.\n├── weights-improvement-01-0.69.hdf5 ├── weights-improvement-01-0.98.hdf5 ├── weights-improvement-02-0.73.hdf5 ├── weights-improvement-02-1.00.hdf5 ├── weights-improvement-03-0.76.hdf5 ├── weights-improvement-05-0.80.hdf5 ├── weights-improvement-07-0.81.hdf5 ├── weights-improvement-10-0.83.hdf5 ├── weights-improvement-12-0.83.hdf5 ├── weights-improvement-17-0.83.hdf5 ├── weights-improvement-20-0.85.hdf5 ├── weights-improvement-24-0.86.hdf5 └── weights-improvement-29-0.87.hdf5 Kiểm tra quá trình huấn luyện bằng cách thể hiện giá trị loss và accuracy lên đồ thị:\nplot_chart(history)   Các giá trị loss và accuracy tuy có sự dao động nhưng kết quả cuối cùng vẫn khá tốt. Model không bị overfit quá nhiều, có thể chấp nhận được.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, ta sẽ học thêm một kỹ thuật rất thú vị nữa, giúp chúng ta giảm rất nhiều công sức trong việc huấn luyện model, đó là Tranfer Learning. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/33_dataaugmentation_modelcheckpoint_cnn/","tags":["Convolution Neural Network","Image Classification"],"title":"Sử dụng kỹ thuật Data Augmentation và Model Checkpoint khi huấn luyện CNN model"},{"categories":["Convolution Neural Network","Image Classification"],"contents":"Nếu như bài trước, toàn bộ dữ liệu được đưa vào training, thì bài này, ta sẽ chia tập dữ liệu thành 2 phần:\n Train set: Dùng để huấn luyện model. Validation set: Dùng đễ đánh giá model trong suốt quá trình training.  OK, hãy bắt đầu!\nTrước tiên, download bộ dataset cat-and-dog và giải nén về máy tính của bạn tại thư mục làm việc. Ta chỉ cần download file train.zip.\nBộ dataset này bao gồm 25.000 bức ảnh, chia thành 2 lớp chó và mèo. Mỗi lớp có 12.500 ảnh, kích thước 150x150.\nTa tiếp tục sử dụng lớp ImageDataGenerator để chuẩn bị dữ liệu cho training model. Lớp ImageDataGenerator yêu cầu cấu trúc thư mục dataset có dạng như sau:\n- training - class 1 - image 1 - image 2 - ... - class 2 - image 3 - image 4 - ... - ... - validation - class 1 - image 5 - image 6 - ... - class 2 - image 7 - image 8 Để chuẩn hóa cấu trúc thư mục như yêu cầu, ta có thể tự viết code để copy các ảnh vào đúng thư mục mong muốn. Hoặc có 1 cách đơn giản hơn là sử dụng thư viện split-folders.\nCài đặt thư viện:\npip install split-folders Sử dụng lệnh sau để tạo dữ liệu theo cấu trúc mong muốn:\nsplitfolders cats-and-dogs --ratio .8 .2 --output cat-dog-dataset Ta được thư mục output cat-dog-dataset:\ncat-dog-dataset ├── train │ ├── cats │ └── dogs └── val ├── cats └── dogs Có dữ liệu chuẩn chỉ rồi, giờ ta sẽ bắt tay vào viết code để train model.\nĐầu tiên, như thường lệ vẫn là import các thư viện sử dụng:\nimport os import random import tensorflow as tf import shutil import matplotlib.pyplot as plt from tensorflow import keras from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.preprocessing.image import ImageDataGenerator config = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Ở đây, ngoài các thư viện, các lớp quen thuộc, ta có sử dụng thêm hàm EarlyStopping của lớp callback. Hàm này cho phép model dừng training khi nó thoả mãn một tiêu chí về độ chính xác mà người dùng có thể định nghĩa được. So với hàm callback tự viết như trong các bài trước thì việc sử dụng EarlyStopping linh động hơn rất nhiều. Ta sẽ đi chi tiết ở phần sau.\nTiếp theo, ta sử dụng lớp ImageDataGenerator để chuẩn bị dữ liệu cho việc huấn luyện model.\ndef gen_data(): training_datagen = ImageDataGenerator( rescale=1/255 ) validation_datagen = ImageDataGenerator( rescale=1/255 ) training_generator = training_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/train\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) validation_generator = validation_datagen.flow_from_directory( \u0026#39;cat-dog-dataset/val\u0026#39;, target_size=(150, 150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) return training_generator, validation_generator Vì dataset đã được chia thành 2 phần, train set và validation set, nên ở đây ta cũng có 2 instances của lớp ImageDataGenerator tương ứng. Một cái dành cho train model, 1 cái dành cho validate model. Hai instances này chỉ khác nhau đường dẫn đến nơi chứa data, còn lại các thông số khác đều giống nhau. Cần lưu ý đến giá trị của tham số batch_size, vì lần này chúng ta sử dụng dữ liệu thật nên nếu bạn set giá trị của nó cao quá có thể dẫn đến hiện tượng out of memory. Thực tế, ban đầu mình set batch_size=64 nhưng bị lỗi nên phải giảm xuống còn 32.\nCNN model được tạo ra giống như bài trước:\ndef create_model(): model = keras.models.Sequential([ keras.layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150, 150, 3)), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Flatten(), keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=RMSprop(lr=0.001), loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Optimizer vẫn là RMSprop nhưng được learning_rate được khởi tạo với giá trị 0.001 thay vì sử dụng giá trị mặc đinh. Loss Function là binary_crossentroy vì có 2 classes cần phân biệt.\nTa định nghĩa thêm hàm plot_chart để thể hiện kết quả training lên đồ thị:\ndef plot_chart(history): acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(len(acc)) plt.figure(figsize=(10, 6)) plt.plot(epochs, acc, \u0026#39;r\u0026#39;, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(epochs, val_acc, \u0026#39;b\u0026#39;, label=\u0026#39;Validation Accuracy\u0026#39;) plt.plot(epochs, loss, \u0026#39;g\u0026#39;, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;y\u0026#39;, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Traing and Validation, Accuracy and Loss\u0026#39;) plt.legend(loc=0) plt.show() Bên trên ta đã nói về hàm callback sử dụng EarlyStopping. Ta định nghĩa nó như sau:\ncallback = EarlyStopping(monitor=\u0026#39;loss\u0026#39;, patience=5) Mục đích của hàm này là buộc model dừng quá trình training nếu sau 5 epochs liên tiếp mà giá trị của loss không giảm. Bạn cũng có thể thay loss bằng acc, khi đó, model sẽ dừng training nếu giá trị accuracy không tăng sau 5 epochs liên tiếp.\nGộp tất cả lại và train model:\ntraining_generator, validation_generator = gen_data() model = create_model() history = model.fit( training_generator, epochs=30, validation_data=validation_generator, callbacks=[callback], verbose=1 ) Dòng `validation_data=validation_generator` trong hàm fit chỉ ra dữ liệu được dùng để validate model trong suốt quá trình training. Output: ```python Found 20000 images belonging to 2 classes. Found 5000 images belonging to 2 classes. Epoch 1/30 1/625 [..............................] - ETA: 0s - loss: 0.6942 - acc: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0141s vs `on_train_batch_end` time: 0.0326s). Check your callbacks. 625/625 [==============================] - 116s 186ms/step - loss: 0.6851 - acc: 0.5663 - val_loss: 0.6292 - val_acc: 0.6240 Epoch 2/30 625/625 [==============================] - 121s 194ms/step - loss: 0.6250 - acc: 0.6571 - val_loss: 0.5594 - val_acc: 0.7356 Epoch 3/30 625/625 [==============================] - 117s 188ms/step - loss: 0.5928 - acc: 0.6921 - val_loss: 0.5248 - val_acc: 0.7652 ... Epoch 14/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4828 - acc: 0.7799 - val_loss: 0.4186 - val_acc: 0.8050 Epoch 15/30 625/625 [==============================] - 123s 197ms/step - loss: 0.4877 - acc: 0.7753 - val_loss: 0.5091 - val_acc: 0.7568 Epoch 16/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4787 - acc: 0.7796 - val_loss: 0.5068 - val_acc: 0.8126 Epoch 17/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4844 - acc: 0.7826 - val_loss: 0.3928 - val_acc: 0.8374 Epoch 18/30 625/625 [==============================] - 124s 198ms/step - loss: 0.4876 - acc: 0.7832 - val_loss: 0.4588 - val_acc: 0.8388 Ta thấy rằng quá trình training model dừng lại tại epoch thứ 18, vì từ epoch 13 đến epoch 18, giá trị của loss không giảm đi chút nào, thậm chí còn tăng lên.\nThử vẽ đồ thị loss và accurcy:\nplot_chart(history)   Từ đồ thị ta có thể nhận xét rằng model được training khá tốt, có một chút overfitting nhưng không đáng kể.\nCuối cùng, model nên được lưu lại để sử dụng cho việc dự đoán về sau:\nmodel.save(\u0026#39;cats-dogs-model.h5\u0026#39;) Source code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, ta sẽ áp dụng thêm 2 kỹ thuật quan trọng nữa cho bài toán phân loại cat-dog, đó là data augmentation và ModelCheckpoint. Cũng giống như EarlyStopping, mục đích của 2 kỹ thuật này không gì hơn là ngăn chặn hiện tượng Overfitting của model trong quá trình training. Mời các bạn đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/32_earlystopping_cnn/","tags":["Convolution Neural Network","Image Classification"],"title":"Sử dụng kỹ thuật EarlyStopping khi huấn luyện CNN model"},{"categories":["Convolution Neural Network","Image Classification"],"contents":"Như đã hứa ở bài trước, ở bài này chúng ta sẽ sử dụng một bộ dữ liệu \u0026ldquo;thực tế\u0026rdquo; hơn để huấn luyện một DL model phân loại hình ành, đó là happy-or-sad dataset. Tập dữ liệu này gồm 80 ảnh, được chia thành 2 lớp happy và sad.\nNhư thường lệ, đầu tiên ta sẽ import các thư viện sử dụng:\nimport PIL import pathlib import tensorflow as tf import numpy as np from tensorflow import keras from tensorflow.keras.preprocessing.image import ImageDataGenerator Cũng giống như đã đề cập trong bài trước, ta cần thêm 3 dòng sau để tránh sự xung đột giữa 2 phiên bản của Tensorflow.\nconfig = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Tiếp theo, download và giải nén tập dữ liệu này về máy tính theo đường dẫn bên trên. Ở đây, mình giả sử bạn đặt thư mục happy-or-sad trong cùng thư mục dự án.\nDataset gồm 80 ảnh, chia thành 2 lớp: happy và sad, mỗi lớp có 40 ảnh.\nĐịnh nghĩa hàm callback như các bài trước:\nclass MyCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if logs.get(\u0026#39;acc\u0026#39;) \u0026gt; 0.99: print(\u0026#39;Reach to 99%, stop training!\u0026#39;) self.model.stop_training = True Định nghĩa hàm load dữ liệu huấn luyện:\ndef load_data(): train_datagen = ImageDataGenerator( rescale=1/255 ) train_generator = train_datagen.flow_from_directory( \u0026#39;happy-or-sad\u0026#39;, target_size=(150,150), batch_size=32, class_mode=\u0026#39;binary\u0026#39; ) return train_generator Trong bài này, vì dataset là ảnh thực tế được lưu trong ổ cứng của máy tính nên ta sử dụng lớp ImageDataGenerator để chuẩn bị dữ liệu cho model training. Bản chất của lớp này là không load toàn bộ dataset một lần (*điều này có thể sẽ không khả thi nếu *) Ngoài chức năng cơ bản đó, nó còn giúp tăng cường dữ liệu (data augmentation), một trong những kỹ thuật hạn chế hiện tượng Overfitting khi huấn luyện model. Chúng ta sẽ sử dụng lớp này thường xuyên trong các bài tiếp theo.\nỞ đây, kích thước ảnh đầu vào là (150,150), các giá trị pixels của ảnh được rescale xuống 255 lần, và sẽ có 32 ảnh được load mỗi lần.\nCNN model được tạo bởi hàm sau:\ndef create_model(): model = keras.models.Sequential([ tf.keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150,150,3)), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(16, (3,3), activation=\u0026#39;relu\u0026#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), tf.keras.layers.Dense(1, activation=\u0026#39;sigmoid\u0026#39;) ]) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model Model được tạo theo kiến trúc: [CONV =\u0026gt; POOL]x3 =\u0026gt; FCx3. Compiler là rmsprop, Loss Function là binary_crossentropy vì chúng ta chỉ có 2 lớp cần phân biệt.\nCuối cùng, gọi các hàm định nghĩa bên trên để huấn luyện model:\nx_train, y_train = load_data() model = create_model() history = model.fit(x_train, y_train, epochs=100, callbacks=[MyCallback()], verbose=1) Output:\nEpoch 1/100 3/3 [==============================] - 0s 58ms/step - loss: 1.2190 - acc: 0.5500 Epoch 2/100 3/3 [==============================] - 0s 88ms/step - loss: 0.6919 - acc: 0.5000 Epoch 3/100 3/3 [==============================] - 0s 64ms/step - loss: 0.6957 - acc: 0.5375 ................... Epoch 98/100 3/3 [==============================] - 0s 65ms/step - loss: 0.1934 - acc: 0.9000 Epoch 99/100 3/3 [==============================] - 0s 89ms/step - loss: 0.1710 - acc: 0.9250 Epoch 100/100 3/3 [==============================] - 0s 68ms/step - loss: 0.2187 - acc: 0.8875 Như ta quan sát thấy, model không thể đạt đến được độ chính xác 99% như điều kiện dừng ở hàm callback. Độ chính xác cuối cùng là 88,75%.\nQua bài này, ta đã biết cách sử dụng dữ liệu thực tế, lưu trong ổ đĩa cứng để huấn luyện một CNN model, bằng cách sử dụng lớp ImageDataGenerator của thư viện Tensorflow.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, ta sẽ huấn luyện CNN model để phân loại 2 đối tượng trong ảnh: chó và mèo. Khác biệt so với bài này là ta sẽ sử dụng dữ liệu validation trong quá trình huấn luyện model, giúp cho model học tốt hơn.\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/31_happy_and_sad_classification_cnn/","tags":["Convolution Neural Network","Image Classification"],"title":"Xây dựng CNN model với tập dữ liệu Happy\u0026Sad"},{"categories":["Convolution Neural Network","Image Classification"],"contents":"Trong bài trước, chúng ta đã sử dụng các lớp FC để xây dựng model phân loại các sản phẩm trong tập MNIST thành 10 nhóm khác nhau.\nViệc phân loại hình ảnh (trong thực tế)là một nhìệm vụ tương đối phức tạp, nó yêu cầu phải xây dựng các NN model với nhiều lớp. Nếu chỉ sử dụng hoàn toàn FC layer thì sẽ không hiệu quả về cả độ chính xác cũng như hiệu năng của model. Thay vào đó, các lớp CONV, POOL, \u0026hellip; được sử dụng thường xuyên hơn.\nYêu cầu cần giải quyết ở bài này vẫn giống như bài trước, chỉ có điều ta sẽ sử dụng các lớp CONV, POOL, \u0026hellip; trong thư viện tensorflow xây dựng model phân loại các sản phẩm trong tập MNIST. Môi trường thực hành vẫn giống như các bài trước.\nĐầu tiên, như thường lệ ta sẽ import thư viện tensorflow:\nimport tensorflow as tf Tiếp đến là hàm callback:\nclass MyCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if logs.get(\u0026#39;acc\u0026#39;) \u0026gt; 0.99: print(\u0026#39;Reached to 99%, stop training!\u0026#39;) self.model.stop_training = True Nhắc lại là hàm này sẽ được gọi tại thời điểm kết thúc mỗi epoch trong quá trình train model. Nó làm nhiệm vụ kiểm tra độ chính xác của model tại thời điểm đó. Nếu độ chính xác đạt đến 99% thì kết thúc quá trình train.\nHàm load dữ liệu MNIST từ trong tensorflow:\ndef load_data(): (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = x_train.reshape(-1, 28, 28, 1) x_train = x_train/255 return x_train, y_train Giờ đến lúc quan trọng nhất của bài này, đó là tạo CNN model:\ndef create_model(): model = keras.models.Sequential([ keras.layers.Conv2D(128, (3,3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, input_shape=(28,28,1)), keras.layers.MaxPooling2D(2,2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2,2), keras.layers.Flatten(), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) ]) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) return model CNN model được tạo thành từ các layers: CONV2D, MaxPooling2D, Flatten, Dense, \u0026hellip; Tùy thuộc mức độ phức tạp của dữ liệu và yêu cầu của bài toán mà ta sử dụng số lượng và cách thức kết hợp các layers này theo những cách khác nhau. Chi tiết về các common pattern, các rules và các tham số sử dụng khi xây dựng mạng CNN, các bạn có thể đọc thêm tại bài viết trước của mình tại đây.\nỞ bài này, chúng tạo một mạng CNN đơn giản theo kiến trúc: [CONV2D =\u0026gt; MaxPooling2D]x2 =\u0026gt; FLATTEN =\u0026gt; DENSEx2.\nCNN model sau đó được compile sử dụng thuật toán RMSprop 9cùng với SGD, Adam và RMSprop là 2 thuật toán tối ưu cũng thường hay được sử dụng khi train DL model. Mình dự định sẽ có các bài viết chi tiết về 2 thuật toán này. Mời các bạn đón đọc), hàm loss là sparse_categorical_crossentropy, và metric là accuracy trên tập train.\nCó model rồi, đã đến lúc tiến hành train model:\nx_train, y_train = load_data() model = create_model() history = model.fit(x_train, y_train, epochs=100, callbacks=[MyCallback()], verbose=1) Model sẽ được train tối đa 100 epochs, hàm callback được truyền vào như 1 tham số để kểm tra điều kiện dừng train của model sau mỗi epoch.\nOutput:\nEpoch 1/100 1875/1875 [==============================] - 14s 7ms/step - loss: 0.1141 - acc: 0.9652 Epoch 2/100 1875/1875 [==============================] - 14s 7ms/step - loss: 0.0406 - acc: 0.9877 Epoch 3/100 1875/1875 [==============================] - ETA: 0s - loss: 0.0318 - acc: 0.9913Reached to 99%, stop training! 1875/1875 [==============================] - 14s 7ms/step - loss: 0.0318 - acc: 0.9913 Rất nhanh, model đạt đến độ chính xác 99% chỉ sau 3 epochs, so với 7 epochs nếu sử dụng hoàn toàn FC layer như bài trước. Với bộ dữ liêu đơn giản như MNIST, 3 epochs hay 7 epochs không có sự khác biệt nhiều về thời gian cũng như độ chính xác. Nhưng nếu dữ liệu rất lớn thì sự khác nhau đó sẽ trở nên rất rõ rệt.\nNếu khi gọi hàm fit() để train model mà gặp lỗi:\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [[node sequential/conv2d/Conv2D (defined at \u0026lt;ipython-input-6-04cbaae553c1\u0026gt;:1) ]] [Op:__inference_train_function_856] thì bạn hãy thêm 3 dòng bên dưới ngay sau khi import tensorflow.\nconfig = tf.compat.v1.ConfigProto() config.gpu_options.allow_growth = True session = tf.compat.v1.InteractiveSession(config=config) Nguyên nhân lỗi ở đây là do xung đột giữa 2 phiên bản 1.x và 2.x của tensorflow.\nOk, như vậy là chúng ta đã hoàn thành việc xây dựng CNN model để phân loại hình ảnh trong tập dữ liệu MNIST. Tuy đơn giản nhưng đây sẽ là bước đầu để dần dần bạn có thể tự tin tạo ra các CNN model phức tạp hơn, với các tập dữ liệu lớn hơn.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTrong bài sau, chúng ta sẽ xây dựng một CNN model khác với dữ liệu thực tế hơn. Hãy đón đọc!\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/30_fashion_mnist_classification_advanced/","tags":["Convolution Neural Network","Image Classification"],"title":"Xây dựng CNN model phân loại với tập dữ liệu MNIST"},{"categories":["Neural Network","Image Classification"],"contents":"Bài này, ta sẽ nâng độ khó hơn 1 chút so với bài trước. Yêu cầu đề bài như sau:\n Xây dựng một NN model phân loại các hình ảnh trong tập dữ liệu MNIST Trong quá trình train, khi độ chính xác của model đạt đến 99% thì dừng train.  Mục đích của bài này là giúp ta làm quen với dữ liệu \u0026ldquo;thực tế\u0026rdquo; hơn 1 chút so với bài trước và cách sử dụng hàm callback để điều khiển quá trình train model.\nMôi trường thực hành của bài này giống hệt bài dự đoán giá nhà trước đó.\nOk, hãy cùng bắt đầu!\nĐầu tiên, import tensorflow:\n1 import tensorflow as tf Tiếp theo, ta định nghĩa hàm callback. Hàm này sẽ được gọi mỗi khi model train xong một epoch.\n2 class CustomCallback(keras.callbacks.Callback): 3 def on_epoch_end(self, epoch, logs={}): 4 if logs.get(\u0026#39;acc\u0026#39;) \u0026gt; 0.99: 5 print(\u0026#39;Reached to 99%, stop training!\u0026#39;) 6 self.model.stop_training = True Ở đây, ta sẽ cho model dừng train khi độ chính xác đạt đến 99% như yêu cầu đề bài.\nDataset được load như code của hàm sau:\n7 def load_data(): 8 (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() 9 x_train = x_train/255 10 x_test = x_test/255 11 return x_train, y_train, x_test, y_test MNIST có thể coi là bộ dataset kinh điển mà hầu như bất kỳ ai cũng sử dụng khi mới học AI. Có lẽ vì thế mà nó được tích hợp sẵn trong thư viện tensorflow.\nCó một chú ý ở hàm load_data() là ta cũng scale down giá trị của x_train, x_test bằng cách chia cho 255. Mục đích của việc làm này cũng giống như mình đã trình bày trong bài trước.\nPhần chính của chúng ta là định nghĩa model:\n12 def create_model(): 13 model = tf.keras.models.Sequential([ 14 tf.keras.layers.Flatten(), 15 tf.keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;, input_shape=(28,28)), 16 tf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) 17 ]) 18 model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) 19 return model Model của chúng ta hôm nay gồm 2 lớp: 1 lớp input (128 nodes) và 1 lớp output (10 nodes), không có lớp ẩn (hidden layer).\nKích thước của dữ liệu đầu vào là (28,28), bằng với kích thước của mỗi bức ảnh trong tập MNIST.\nSố node của lớp output là 10, bằng với số lớp của tập MNIST mà ta cần phân loại. Hàm kích hoạt Softmax sử dụng ở lớp này sẽ cho ta biết chính xác xác suất của hình ảnh thuộc về mỗi lớp. Lớp nào có xác suất lớn nhất sẽ được lấy làm kết quả cuối cùng.\nModel được compile với thuật toán tối ưu SGD, hàm loss là sparse_categorical_crossentropy, và metric là accuracy trên tập train.\nBây giờ ta sẽ tiến hành train model:\n20 x_train, y_train, x_test, y_test = load_data() 21 model = create_model() 22 23 history = model.fit(x_train, y_train, epochs=100, verbose=1, callbacks=[CustomCallback()]) Model được train tối đa 100 epochs, hàm callback mà ta định nghĩa bên trên được truyền vào như 1 tham số.\nOutput:\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 1s 0us/step Epoch 1/100 1875/1875 - 1s - loss: 0.2573 - acc: 0.9266 Epoch 2/100 1875/1875 - 1s - loss: 0.1128 - acc: 0.9666 Epoch 3/100 1875/1875 - 1s - loss: 0.0771 - acc: 0.9765 Epoch 4/100 1875/1875 - 1s - loss: 0.0573 - acc: 0.9825 Epoch 5/100 1875/1875 - 1s - loss: 0.0445 - acc: 0.9856 Epoch 6/100 1875/1875 - 1s - loss: 0.0345 - acc: 0.9895 Epoch 7/100 Reached to 99%, stop training! 1875/1875 - 1s - loss: 0.0283 - acc: 0.9913 Đầu tiên, tập MNIST sẽ được download về local, sau đó model sẽ được train. Quá trình train dừng lại sau 7 epochs vì độ chính xác đã đạt đến 99% như định nghĩa ở hàm callback.\nNhư vậy là chúng ta đã giải quyết xong yêu cầu đặt ra lúc đầu. Qua bài này ta đã biết:\n Cách load dataset được tích hợp trong tensorflow. Cách xây dựng và sử dụng hàm callback khi train model. Các tạo và train model với 2 lớp NN sử dụng tensorflow.  Source code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nBài tiếp theo, chúng ta sẽ xây dựng model sử dụng lớp CONV trong tensorflow để nâng cao độ chính xác cũng như hiệu năng của model. Mời các bạn đón đọc.\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/29_fashion_mnist_classification/","tags":["Neural Network","Image Classification"],"title":"Xây dựng NN model phân loại với tập dữ liệu MNIST"},{"categories":["Neural Network"],"contents":"Sau một số bài về lý thuyết thì hôm nay chúng ta sẽ bắt tay vào thực hành code một bài toán mẫu giáo, áp dụng những lý thuyết mà ta đã tìm hiểu xem sao nhé!\nThông tin về bài toán và yêu cầu đặt ra như sau:\n Quy tắc tính giá nhà như sau: 50k + 50k/bedroom. Ví dụ: nhà có 1 bedroom thì giá sẽ là 100k, nhà có 2 bedrooms thì giá sẽ là 150k, \u0026hellip; Xây dựng một NN model để dự đoán giá tiền của một căn nhà có 7 phòng ngủ. So sánh giá của căn nhà đó theo 2 cách: tính theo các thông thường và theo kết quả dự đoán của NN model.  Mình giả sử là các bạn đã cài sẵn môi trường trên máy tính của các bạn (hoặc các bạn có thể sử dụng colab cũng được.). Chúng ta sẽ sử dụng tensorflow 2.3.0 và numpy 1.18.5\nĐầu tiên, import và kiểm tra thư viện:\n1 import tensorflow as tf 2 import numpy as np 3 import os 4 from tensorflow import keras 5 6 print(tf.__version__) 7 print(np.__version__) 8 print(tf.config.list_physical_devices(\u0026#39;GPU\u0026#39;)) Kết quả:\n2.3.0 1.18.5 [PhysicalDevice(name=\u0026#39;/physical_device:GPU:0\u0026#39;, device_type=\u0026#39;GPU\u0026#39;)] Máy của các bạn có thể không có GPU cũng không sao, vì bài này rất đơn giản nên cũng không cần đến GPU. Các bài sau thì nên có vì như thế thời gian train model sẽ nhanh hơn rất nhiều.\nTiếp theo, chúng ta sẽ tạo ra dữ liệu huấn luyện dựa theo quy tắc tính giá nhà như trong đề bài:\n9 x_train = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], dtype=float) 10 y_train = np.array([100.0, 150.0, 200.0, 250.0, 300.0, 350.0], dtype=float) Mỗi phần tử của x_train là số phòng ngủ của căn nhà, còn mỗi phần tử của y_train là giá nhà tương ứng với số phòng ngủ đó.\nBây giờ là lúc chúng ta xây dựng NN model:\n11 model = keras.Sequential([ 12 keras.layers.Dense(units=1, input_shape=[1]) 13 ]) 14 model.compile(optimizer=\u0026#39;sgd\u0026#39;, lost=\u0026#39;mse\u0026#39;) Dòng 11-13 định nghĩ một NN model với chỉ một layer, một input có kích thước là 1. Dòng 14, model được compiled, sử thuật toán tối ưu SGD và Lost Function là MSE (Mean Square Error).\nĐến đây, model đã sẵn sàng để train:\n15 model.fit(x_train, y_train/255, epochs=1000) Model được train với dữ liệu train đã tạo ở bên trên, số lượng epoch là 1000. Chú ý rằng ở đây, y_train được chia cho 255. Mục đích của việc scale down này là để model không bị quá bias vào giá trị của y_train khi mà giá trị của y_train lớn hơn giá trị của x_train rất nhiều.\nOutput:\nEpoch 1/1000 1/1 [==============================] - 0s 1ms/step - loss: 37.2759 Epoch 2/1000 1/1 [==============================] - 0s 920us/step - loss: 17.2548 Epoch 3/1000 1/1 [==============================] - 0s 525us/step - loss: 7.9884 .................. Epoch 998/1000 1/1 [==============================] - 0s 535us/step - loss: 3.0594e-06 Epoch 999/1000 1/1 [==============================] - 0s 938us/step - loss: 3.0372e-06 Epoch 1000/1000 1/1 [==============================] - 0s 1ms/step - loss: 3.0151e-06 Ta thấy giá trị loss giảm từ 37.2759 đến 3.0151e-06. Một con số khá thấp.\nCuối cùng, ta sẽ dùng model vừa trained để dự đoán giá của ngôi nhà có 7 phòng ngủ.\n16 prediction = model.predict([7.0]) 17 print(prdiction * 100) Kết quả:\n[[400.25043]] Giá của ngôi nhà nếu tính theo cách thông thường sẽ là 400$. So sánh 2 kết quả ta thấy chúng khá gần nhau. Như vậy là model đã làm việc khá tốt. Bạn có thể tăng số epochs lên và train lại model, sau đó kiểm trả lại kết quả dự đoán xem nó có được cải thiện hay không!\nOK, như vậy là chúng ta đã giải quyết xong yêu cầu của bài toán. Đây chỉ là 1 bài tập trình độ mẫu giáo để chúng ta làm quen với việc sử dụng tensorflow để xây dựng NN model. Trong các bài sau, chúng ta sẽ giải quyết các bài toán phức tạp hơn.\nSource code của bài này, các bạn có thể tham khảo trên github cá nhân của mình tại đây.\nTham khảo\n Coursera  ","permalink":"https://tiensu.github.io/blog/28_hourse_prices_prediction/","tags":["Neural Network"],"title":"Xây dựng NN model đơn giản dự đoán giá nhà"},{"categories":["Algorithm Optimization"],"contents":"\u0026ldquo;Nearly all of deep learning is powered by one very important algorithm: Stochastic Gradient Descent (SGD)\u0026rdquo; – Goodfellow et al.\nTừ bài trước chúng ta đã biết rằng để model có thể dự đoán đúng thì phải tìm được giá trị phù hợp cho $W$ và $b$. Nếu chúng ta chỉ dựa hoàn toàn vào việc chọn ngẫu nhiên thì gẫn như không bao giờ có thể tìm được giá trị mong muốn. Thay vì thế, chúng ta cần định nghĩa một thuật toán tối ưu (optimization) và sử dụng nó để cải thiện $W$ và $b$. Trong bài này, chúng ta sẽ tìm hiểu một thuật toán tối ưu được sử dụng rất rất phổ biến trong NN and DL model - Gradient Descent (GD) và các biến thể của nó. Ý tưởng chung của họ các thuật toán GD là đánh giá các tham số, tính toán loss, sau đó thực hiện một bước nhỏ theo hướng giảm loss. Cả 3 bước này được thực hiện trong các vòng lặp cho đến khi gặp một điều kiện dừng nào đó.\n1. The Loss Landscape và Optimization Surface\nGradient descent là thuật toán hoạt động theo kiểu tối ưu qua từng vòng lặp thông qua một mặt tối ưu(optimization surface / loss landscape), như minh họa ở hình bên dưới.\n Phía bên trái biểu diễn trong không gian 2 chiều để chúng ta dễ hình dùng, còn bên phải biểu diễn một cách thực tế hơn trong không gian nhiều chiều. Mục đích sử dụng gradient descent là tìm ra điểm global minumum (đáy của cái bát ở bên phải).\nChúng ta có thể thấy, optimization surface có rất nhiều đỉnh (peaks) và thung lũng (valleys*). Mỗi valley có một điểm đáy mà tại đó giá trị loss đạt giá trị cực tiểu, gọi là local minimum. Trong số các điểm local minimum, có 1 điểm mà giá trị loss đạt giá trị nhỏ nhất được gọi là gloabal minimum. Đây chính là điểm mà chính ta muốn tìm trong quá trình training AI model thông qua việc cập nhật các tham số.\nHãy tưởng tượng, việc dò tìm điểm global minimum trên optimization surface giống như việc đặt 1 viên bi (*chính là * $W$) trên mặt đó, nhiệ vụ của viên bi là dò tìm đường để đi đến điểm đích (global minimum).\nNếu chỉ nhìn vào hình trên, mọi người có thể thắc mắc: Nếu muốn đến điểm global minimum, tại sao không nhảy thẳng một phát đến đó?\nNhưng mọi việc không đơn giản như vậy, bởi vì trên thực tế, chúng ta không biết hình dạng của optimization surface như thế nào, chúng ta như một ngươi mù trên đường, không biết phương hướng. Và các thuật toán tối ưu (gradient descent là một trong số đó) chính là cây gậy trong tay, giúp chúng ta dò đường.\nCụ thể hơn 1 chút thì mỗi một điểm trên optimization surface tương ứng với một giá trị loss $L$ - chính là output của loss funtion khi đưa vào cặp giá trị ($W$, $b$). Ý tưởng của thuật toán tối ưu là cố gắng thử sử dụng các cặp giá trị ($W$, $b$) khác nhau, tính toán loss, cập nhật ($W$, $b$) sao cho giá trị loss thấp hơn \u0026hellip; Lý tưởng nhất là chúng ta có thể đạt được giá trị loss nhỏ nhất tại điểm global minimum, nhưng điều này thường khó xảy ra trong thực tế.\n2. Gradient Descent cho hàm 1 biến\nGiả sử Loss Function của chúng ta là hàm bậc 1, $f(x)$. Điểm global minimum là điểm mà tại đó $x = x^*$.\nĐạo hàm của của $f(x)$ là $f\u0026rsquo;(x)$. Nếu bạn còn nhớ, trong chương trình toán THPT, khi học về đạo hàm ta có các nhận xét:\n Nếu đạo hàm của hàm số tại thời điểm $t$, $f\u0026rsquo;(x_t) \u0026gt; 0$ thì $x_t$ nằm về phía bên phải so với $x^*$, và ngược lại. $x_t$ càng xa $x^*$ về phía bên phải thì $f\u0026rsquo;(x_t)$ càng lơn hơn 0, và ngược lại.  Từ nhận xét số 1 có thể suy ra, để điểm tiếp theo $x_{t+1}$ tiến gần về $x^*$ hơn thì cần di chuyển $x_t$ về phía bên trái, tức là phía âm, hay phía ngược dấu với đạo hàm:\n$x_{t+1} = x_t + \\Delta$ ($\\Delta$ là một đại lượng ngược dấu với đạo hàm $f'(x)$)  Từ nhận xét số 2 có thể suy ra lượng di chuyển $\\Delta$ tỉ lệ thuận với $-f\u0026rsquo;(x)$.\nTổng hợp hai nhận xét trên, ta có công thức cập nhật $x_t$ một cách đơn giản là:\n$x_{t+1} = x_t - \\eta f'(x_t)$ Hoặc viết dưới dạng đơn giản:\n$x = x - \\eta f'(x)$ Trong $\\eta$ là một số \u0026gt; 0, gọi là learning rate. Dấu trừ thể hiện viêc đi ngược chiều với đạo hàm (descent nghĩa là đi ngược).\n3. Gradient Descent cho hàm nhiều biến\nGiả sử Loss Function của chúng ta, $f(\\theta)$ là hàm nhiều biến, trong đó $\\theta$ là tập hợp các vector các tham số của model cần tối ưu. Đạo hàm của $f(\\theta)$ tại thời điểm $\\theta$ là $\\nabla_\\theta f(\\theta)$.\nTương tự hàm 1 biến, quy tắc cập nhật $\\theta$ là:\n$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta f(\\theta_t)$ Hoặc viết dưới dạng đơn giản: $\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta_t)$ Tóm lại, thuật toán GD hoạt động như sau:\n Dự đoán một điểm khởi tạo $\\nabla = \\nabla_0$. Cập nhật $\\nabla$ đến khi đạt được kết quả chấp nhận được (hoặc một điều kiện dừng nào đó). $\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta)$   với $\\nabla_\\theta f(\\theta)$ là đạo hàm của Loss Function tại $\\theta$.\n4. Stochastic Gradient Descent (SGD)\nThuật toán GD nguyên thủy có một nhược điểm to lớn là hội tụ rất chậm và yêu cầu tài nguyên tính toán rất lớn. Nguyên nhân gốc rễ của vấn đề này là do GD tính toán gradient trên toàn bộ training set. Điều này thật khó để chấp nhận nếu áp dụng với một tập dữ liệu lớn.\nMột biến thể của GD, gọi là Stochastic Gradient Descent (SGD) ra đời, khắc phục những hạn chế của GD. Thay vì tính toán và cập nhật weight matrix $W$ trên toàn bộ tập dữ liệu như cách làm của GD (cập nhật theo epoch), SGD chia nhỏ tập training thành các batchs (dữ liệu thường được xáo trộn ngẫu nhiên trước khi chia), tính toán và cập nhật $W$ theo từng batch đó (cập nhật theo batch).\nBiểu diễn theo toán học, công thức cập nhật của SGD như sau:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta;x_i;y_i)$ trong đó, $f(\\theta;x_i;y_i)$ là Loss Function với chỉ 1 cặp điểm dữ liệu (input, label) là ($x_i, y_i$).\nMặc dù ra đời từ rất lâu (1960), SGD vẫn là một thuật toán quan trọng, được sử dụng rộng rãi trong các kiến trúc DL hiện đại. Vì thế, viêc hiểu cặn cẽ về nó là một điều cần thiết khi học AI/ML.\n4.1 Mini-batch SGD\nMột câu hỏi đặt ra khi sử dụng SGD là kích thước của batch (batch_size) là bao nhiêu thì hợp lý? Theo như cách diễn giải bên trên thì có vẻ như batch_size càng nhỏ càng tốt? Và tốt nhất là batch_size = 1?\nTuy nhiên, điều này không đúng. Sử dụng batch_size \u0026gt; 1 mang lại cho chúng ta một số lợi ích nhất định. Nó giúp giảm phương sai khi cập nhật $W$, và đặc biệt hơn, nếu giá trị của batch_size là lũy thừa của 2 thì chúng ta còn hưởng lợi về tốc độ thực thi của các thư viện tối ưu trong đại đố tuyến tính. Trong các bài toán thực tế, batch_size thường nhận các giá trị 32, 64, 128, 256, tùy thuộc vào tài nguyên tính toán của bạn.\nLúc này, công thức cập nhật sẽ trở thành:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta;x_{i:i+n};y_{i:i+n})$ trong đó, $x_{i:i+n}, y_{i:i+n}$ là các cặp điểm dữ liệu (*input, label*) có vị trí từ $i$ đến $i + n -1$.\n4.2 Mở rộng của SGD\nTrong quá trình sử dụng SGD, ta có thể bắt gặp 2 kỹ thuật hỗ trợ tăng tốc độ hội tụ cho SGD. Đố là momentum và nesterov accelerated gradient (NAG).\n4.2.1 Momentum\nMomentum, hiểu theo nghĩa tiếng việt là đà, lấy đà hay quán tính. Mục tiêu của nó là đẩy nhanh tốc độ cập nhật $W$ tại những nơi mà các gradients có cùng hướng, và ngược lại. Quan sát lại hình bên trên, có thể tưởng tượng rằng nếu không có momentum, viên bi của chúng ta rất dễ bị mắc kẹt ở các local minimum, mà không sao thoát ra để tìm đến global minimum được.\nỞ phần trên, ta đã biết công thức cập nhật các tham số như sau:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta_t)$ Thêm vào momentum $V$, với:\n$V_t = \\gamma V_{t-1} - \\eta \\nabla_\\theta f(\\theta)$ ta được:\n$\\theta = \\theta - V_t$ Trong đó, $\\gamma$ là đại lượng thường được chọn giá trị 0.9, hoặc ban đầu chọn là 0.5, sau khi quá trình học diễn ra ổn định thì tăng lên 0.9. Nó hầu như không bao giờ \u0026lt; 0.5. Khi khai báo sử dụng momentum (trong tensorflow chẳng hạn), ta thường truyền vào giá trị của đại lương này.\n4.2.2 Nesterov Accelerated Gradient (NAG)\nMomentum tuy giúp ta vượt qua được các local minimum, nhưng khi tới gần global minimum, do có đà nên viên bi vẫn tiếp tục dao động thêm một khoảng thời gian nữa mới có thể dừng lại đúng điểm cần dừng. NAG ra đời để khắc phục nhược điểm này.\nÝ tưởng chính của NAG là sử dụng gradient ở thời tiếp theo, thay vì gradient ở thời điểm hiện tại khi tính lượng thay đổi của $\\theta$.\nÝ tưởng của Nesterov accelerated gradient.\nNguồn: CS231n Stanford: Convolutional Neural Networks for Visual Recognition  Công thức cập nhật sẽ như sau:\n$V_t = \\gamma V_{t-1} - \\eta \\nabla_\\theta f(\\theta - \\gamma V_{t-1}) \\theta$ $\\theta = \\theta - V_t$ Momentum là một kỹ thuật quan trọng và hiệu quả, gần như luôn luôn được sử dụng cùng với SGD. Còn đối với NAG, chúng ta ít gặp hơn. Trong khi về mặt lý thuyết, nó mang lại hiệu quả hơn momentum, nhưng trong thực tế các kiến trúc nổi tiếng như AlexNet, VGGNet, ResNet, Inception, \u0026hellip; khi train trên tập dữ liệu ImageNet, chỉ sử dụng SGD với momentum. Có lẽ NAG chỉ phù hợp với các tập dữ liệu nhỏ.\n5. Các thuật toán tối ưu khác\nNgoài SGD, hai thuật toán khác cũng rất hay được sử dụng trong các kiến trúc DL hiện đại là Adam và RMSprop. Mình sẽ có bài viết riêng về các thuật toán này. Mời các bạn đón đọc.\nTham khảo\n Pyimagesearch Dive into Deep Learning machinelearningcoban blog  ","permalink":"https://tiensu.github.io/blog/27_optimization_methods_gradient-descent/","tags":["Algorithm Optimization"],"title":"Các phương pháp Optimization - Gradient Descent"},{"categories":["Machine Learning","Deep Learning"],"contents":"Bạn có biết thuật toán kNN - một trong những thuật toán đơn giản nhất của ML? Về bản chất, nó không \u0026ldquo;học\u0026rdquo; bất cứ điều gì từ dữ liệu mà chỉ đơn giản là lưu dữ liệu bên trong model, và tại thời điểm dự đoán, nó so sánh dữ liệu cần dự đoán với dữ liệu trong tập training. Rõ ràng với cách làm việc như vậy thì ưu điểm lớn nhất của kNN là không mất thời gian training model. Không không cần quá quan tâm về độ chính xác thì ưu điểm này chính là lý do mà kNN vẫn còn được sử dụng trong một số trường hợp. Hạn chế của kNN chỉ xuất hiên khi gặp bài toán mà sử dụng lượng dữ liệu lớn. Lúc này thời gian dự đoán của kNN sẽ rất lâu, và đôi khi không thể sử dụng được trong thực tế.\nMột các tiếp cận khác của ML model mà ở đó nó có thể học được các patterns từ dữ liệu trong suốt quá trình training. Sau đó, ở giai đoạn dự đoán, ML model đó có thể thực hiện rất nhanh chóng để đưa ra kết quả. Dạng này của ML được gọi là parameterized learning.\nDưới đây là định nghĩa chính thống từ tác giả:\n“A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at the parametric model, it won’t change its mind about how many parameters it needs.” – Russell and Norvig (2009).\nCó thể nói parameterized learning là nền tằng của các thuật toán ML và DL hiện đại ngày nay. Trong bài này, chúng ta sẽ cùng xem xét chi tiết một vài khía cạnh về nó!\n1. Bốn thành phần cơ bản của Parameterized Learning\nHiểu một cách đơn giản, parameterization là quá trình định nghĩa các tham số cần thiết cho một model. Nó bao gồm 4 thành phần: Data, Score Function, Loss Function, Weight-Bias.\n1.1 Data\nData ở đây chính là input data đưa vào để model học, nó bao gồm Data Points (VD: Các raw pixel của bức anh, các trích xuất đặc trưng của đối tượng, \u0026hellip;) và nhãn kết hợp với Data Points.\nData thường được biểu diễn dưới dạng ma trận (gọi là desing matrix). Mỗi hàng của matrix đại diện cho một Data Point, trong khi mỗi cột của nó thể hiện moojtoj feature.\nVí dụ, xem xét một dataset gồm 1000 bức ảnh màu, mỗi ảnh có kích thước 96x96x3 pixels. Design Matrix cho dataset này sẽ là: $X \\subseteq R^{100 \\times (32 \\times 32 \\times 3)}$, với $X_i$ là ảnh thức $i$ trong $R$. Cùng với Design Matrix, ta cũng định nghĩa vector $y$ mà $y_i$ là nhãn cho ảnh thứ $i$ trong dataset.\n1.2 Scoring Function\nHàm này chấp nhận một input data, sau đó ánh xạ nó sang nhãn tương ứng.\nINPUT_IMAGES =\u0026gt; F(INPUT_IMAGES) =\u0026gt; OUTPUT_CLASS_LABELS\n1.3 Loss Function\nLoss Function đánh giá mức độ phù hợp giữa nhãn dự đoán (predicted label) và nhãn thực tế (ground-truth label). Giá trị của hàm này tỉ lệ nghịch với mức độ phù hợp hay độ chính xác của model. Mục tiêu của chúng ta khi training AI model là tối thiểu hóa loss funtion.\n1.4 Weights and Biases\nWeight Matrix ($W$) và vector bias ($b$) là những cái được điều chỉnh, cập nhật trong quá trình training AI model, dựa trên output của Score Function và Loss Function. Mục đích cuối cùng cũng vẫn là tăng độ chính xác phân loại của AI model.\nTiếp theo, chúng ta sẽ sử dụng 4 thành phần này để xây dựng một linear classification.\n2. Ưu điểm của Parameterized Learning\nCó 2 ưu điểm chính để sử dụng Parameterized Learning:\n Một khi đã hoàn thành viêc training model, ta có thể bỏ qua input data, chỉ giữ lại Weight Matrix $W$ và bias vector $b$. Điều này giúp giảm đáng kể kích thước của model. Tốc độ của việc phân loại sử dụng model đã trained nhanh hơn rất nhiều so với thuật toán kNN. Ta chỉ cần nhân 2 matrix $W$ và $x_i$, rồi cộng với $b$.  3. Linear Classification\nPhần này, chúng ta sẽ đi sâu hơn 1 chút về toán, tìm hiểu cách mà parameterized learning áp dụng vào machine learning.\nGiả sử chúng ta có một bộ dataset gồm $N$ ảnh kích thước ($W_{input}, H_{input}, 3$), ký hiệu là $x_i, i = 1, 2, \u0026hellip;, N$. Mỗi $x_i$ có một nhãn tương ứng $y_j, j = 1, 2, \u0026hellip;, K$. Nói một cách khác, chúng ta có $N$ Data Points, mỗi Data Point có $D = (W_{input} \\times H_{input}\\times3)$ chiều, và được chia thành $K$ nhóm phân biệt.\nTa định nghĩa Score Function thông qua một hàm tuyến tính đơn giản như sau:\n$f(x_i, W, b) = Wx_i + b$ Giả sử mỗi $x_i$ được đại diện bởi một vector cột với kích thước $[D\\times1]$ (đối với images, ta duỗi thẳng theo cả 3 chiều thành $D = (W_{input} \\times H_{input}\\times3)$ giá trị nguyên của các pixcels). Weight Matrix $W$ sẽ có kích thước $[K \\times D]$ và bias $b$ sẽ có kích thước $[K \\times 1]$. Vector bias cho phép ta dịch chuyển Score Function trên toàn bộ trên đồ thị, đóng vai trò quan trọng đối với sự thành công của AI model.\nVí dụ cụ thể đối một dataset có 3000 bức ảnh, mỗi ảnh có kích thước $[32 \\times 32 \\times 3]$ và được phân chia vào 1 trong 3 nhóm. Khi đó mỗi $x_i$ được đại diện bởi D = 32323 = 3072 pixels và có kích thước $[3072 \\times 1]$. Weight Matrix $W$ có kích thước $[3 \\times 3072]$ và bias vector $b$ có kích thước $[3 \\times 1]$.\nHình dưới minh họa linear classification Score Function $f$. Bên trái là ảnh đầu vào kích thuớc $[32 \\times 32 \\times 3]$ được \u0026ldquo;duỗi thẳng\u0026rdquo; thành 3072 pixcels (bằng cách reshape 3D array thành 1D list).\n Weight Matrix $W$ chứa 3 hàng (mỗi hàng tương ứng với 1 nhãn) và 3072 cột (mỗi cột tương ứng với 1 pixel của ảnh). Sau khi nhân 2 matrix $W$ và $x_i$, ta cộng thêm bias vector $b$ sẽ thu được output của Score Function, như bên trái của hình trên. Có 3 giá trị tương ứng với 3 nhãn: Cat, Dog và Panda.\n4. Thực hành Linear Classification với Python\nChúng ta đã có hình dung cơ bản về lý thuyết của Parameterized Learning, giờ ta sẽ bắt tay vào thực hành code để có thể hiểu hơn.\nMục đích của phần này không phải hướng dẫn việc training model, chỉ đơn giản là cài đạt Score Function bằng python mà thôi. Khi training model thì $W$ và $b$ sẽ được khởi tạo và cập nhật dần dần trong quá trình training, còn ở đây, chúng chỉ được khởi tạo 1 lần và sử dụng luôn để tính output của Score Function.\nMục tiêu của ta ở đây là nhận diện xem ảnh dưới đây là con gì trong số 3 con vật: Cat, Dog và Panda.\n Chúng ta sẽ code như sau (xem giải thích trong comment code):\n# import the necessary packages import numpy as np import cv2 # initialize the class labels and set the seed of the pseudorandom number generator so we can reproduce our results labels = [\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;panda\u0026#34;] np.random.seed(1) # randomly initialize our Weight Matrix and bias vector -- in a *real* training and classification task, these parameters would be  # *learned* by our model, but for the sake of this example, let\u0026#39;s use random values W = np.random.randn(3, 3072) b = np.random.randn(3) # load our example image, resize it, and then flatten it into our \u0026#34;feature vector\u0026#34; representation orig = cv2.imread(\u0026#34;dog.png\u0026#34;) image = cv2.resize(orig, (32, 32)).flatten() # compute the output scores by taking the dot product between the Weight Matrix and image pixels, followed by adding in the bias scores = W.dot(image) + b # loop over the scores + labels and display them Chạy code trên ta thu được kết quả trên terminal:\n[INFO] dog: 8058.57 [INFO] cat: -2926.35 [INFO] panda: 3531.41 và ảnh hiển thị:  Nhắc lại lần nữa là khi thực hiện bài toán AI trong thực tế từ đầu, chúng ta cần phải cập nhật $W$ và $b$ thông qua các thuật toán tối ưu. Thuật toán tối ưu kinh điển là Gradient Descent sẽ được tìm hiểu chi tiết trong bài viết tiếp theo.\n5. Loss Function\n5.1 Loss Function là gì?\nLoss Function, tên tiếng việt là hàm mất mát, thể hiện sai số giữa giá trị dự đoán của model và giá trị thực tế. Sai số càng nhỏ thì model dự đoán càng chính xác và ngược lại.\nMục tiêu của việc training model là cập nhật $W$ và $b$ để tối thiểu hóa giá trị của Loss Function, qua đó nâng cao độ chính xác của model.\nMột cách lý tưởng, Loss Function nên giảm dần theo thời gian trong quá trình training như hình bên dưới:\n 5.2 Multi-class SVM Loss\nMulti-class SVM Loss là sự mở rộng của Linear SVM, xuất hiện trong bài toán khi cần phân biệt nhiều labels (\u0026gt; 2). Nó sử dụng Score Function $f$ để ánh xạ mỗi Data Point thành các Scores cho mỗi nhãn.\nScore Function $f$ có dạng như sau:\n$f(x_i, W, b) = Wx_i + b$ Để phán định một model là \u0026ldquo;good\u0026rdquo; hay \u0026ldquo;bad\u0026rdquo;, ta cần thêm một Loss Function.\nTa đã biết khi tạo ra một ML model, chúng ta có Design Matrix $X$, ở đó, mỗi hàng của $X$ là một Data Point ($x_i$) mà chúng ta muốn phân loại (tìm nhãn cho Data Point đó). Ground-truth Label cho $x_i$, ký hiệu $y_i$ là vector mà chúng ta hi vọng Score Function sẽ dự đoán đúng.\nViết lại Score Function như sau:\n$s = f(x_i, W)$ Predicted Score của class j-th tại i-th Data Point sẽ là:\n$s_j = f(x_i, W)_j$ Ta định nghĩa Hinge Loss Function như sau:\n$L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} +1)$ Hàm này tính tổng các sai số giữa các Score của nhãn dự đoán so với nhãn thực tế. Ở đây, sử dụng hàm $max()$ để bỏ qua những giá trị âm của độ lệch. $x_i$ được dự đoán chính xác khi $L_i$ = 0.\nÁp dụng cho toàn bộ tập training, ta lấy trung bình của mỗi $L_i$:\n$L = \\frac{1}{N} \\sum_{i=1}^N L_i$ Một dạng khác của Loss Function mà ta có thể gặp trong một số tài liệu là Squared Hinge Loss:\n$L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} + 1)^2$ Dạng này sẽ \u0026ldquo;trừng phạt\u0026rdquo; (penalize) năng hơn Loss. Việc sử dụng dạng nào còn tùy thuộc vào dataset của bạn. Thực tế thì dạng chuẩn (không bình phương) có vẻ phổ biến hơn 1 chút. Nhưng trong nhiều bài toán, sử dụng dạng bình phương lại cho kết quả tốt hơn. Nói chung, đây sẽ là một hyperparameter mà chúng ta cần phải tuning trong quá trình xây dựng model.\nVí dụ:\nĐể hiểu rõ hơn, hãy cùng xem xét ví dụ sau:\n Chúng ta có 3 training examples cho 3 nhãn: Dogs, Cats và Pandas. Giả sử ta đã biết giá trị của $W$ và $b$, từ đó sẽ tính được giá trị của Score Fcuntion, như trong hình.\nTính các $L_i$ cho từng example:\n$L_i(Image 1) = max(0, 1.33 - 4.26 + 1) + max(0, -1.01 - 4.46 + 1) = 0$ $L_i(Image 2) = max(0, 3.76 - (-1.20) + 1) + max(0, -3.81 - (-1.20) + 1) = 5.96$ $L_i(Image 3) = max(0, -2.37 - (-2.27) + 1) + max(0, 1.03 - (-2.27) + 1) = 5.199$\nTa thấy, $L_i(Image1) = 0$, tức là Image #1 được dự đoán đoán chính xác là Dogs. Điều này cũng hợp lý vì Score của Image #1 đối với nhãn Dogs lớn hơn nhiều so với 2 nhãn còn lại.\nĐối với Image #2 và Image #3, đang được dự đoán là Dogs và Cats, tương ứng. Rõ ràng, đây là dự đoán sai. Nhìn vào $L_i(cats) và L_i(pandas)$ đều \u0026gt; 0, ta thấy khá hợp lý với những dự đoán này.\nTrong bài sau, ta sẽ học các phương pháp tối ưu để tìm ra giá trị của $W$ và $b$ để việc dự đoán sẽ đúng cho cả 3 bức ảnh.\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/26_parameterized_learning/","tags":["Machine Learning","Deep Learning"],"title":"Parameterized Learning"},{"categories":["Ebook"],"contents":"Bạn có là người thích đọc sách nhưng ngân quỹ có giới hạn, không đủ tiền để mua sách \u0026ldquo;xịn\u0026rdquo; trên amazon, hay trên các tạp chí khoa học, \u0026hellip;? Bạn đã từng trải qua cảm giác gặp một cuốn sách/bài báo rất hay. Bạn rất muốn tìm bản \u0026ldquo;full không che\u0026rdquo; để đọc nhưng không thể tìm được bản free trên mạng? (Mình đã từng mất cả ngày seach google, đăng ký tài khoản ở cả những trang web nước ngoài nhưng vẫn không tìm được. Có chăng thì chỉ là bản sample, hoặc không có code đi kèm) Lên amazon hoặc trang bán sách trực tiếp của tác giả xem thì giá bán là XXX đô la. Bạn đắn đo, không biết có nên bỏ ra số tiền như vậy để mua hay không? Nhỡ sách không hay như kỳ vọng thì sao? Rồi bạn lan man nghĩ ngợi, sáng nay bạn còn không còn đủ 10k ăn sáng, \u0026hellip; Cuối cùng là bạn \u0026hellip; thôi, ko mua nữa.\n\u0026ldquo;May mắn\u0026rdquo; là mình tìm được 2 cách lách luật để có thể sở hữu những cuốn sách như vậy. Cũng phải nhấn mạnh với mọi người là mình không phải là người chuyên đọc sách lậu, và cố tình cỗ súy cho việc này. Đối với một số sách mà mình thấy nó thực sự hay và thực sự có ích đối với mình thì mặc dù có thể download được free nhưng mình vẫn trả tiền cho tác giả. Vì mình nghĩ đơn giản, trả tiền cho tác giả thì họ mới có động lực viết ra những quyển sách hay như thế nữa cho mình đọc. Tuy nhiên, có 2 lý do mà mình quyết định viết bài này:\n Tri thức là của chung của mọi người, chia sẻ kiến thức là việc mà mọi người đều nên làm, không chỉ giúp người mà còn giúp chính mình. Những quyển sách đó có thể cũng đã được chia sẻ ở đâu đó rồi. Mình không nói thì có thể bạn cũng sẽ biết được từ một nguồn nào đó ở nơi khác.  Trong thời đại CNTT ngày nay thì mọi thông tin đều có thể tìm kiếm được từ Internet, vấn đề chủ yếu ở đây là bạn hành động như thế nào thôi. Bạn có thể phản đối hoàn toàn việc đọc sách \u0026ldquo;lậu\u0026rdquo;, làm như mình (trả tiền cho một số cuốn) hoặc dùng hoàn toàn hàng free mà không trả đồng nào. Mình không đánh giá bạn qua việc đó, mình chỉ hi vọng các bạn đọc sách một cách hiệu quả, thực sự yêu quý và trân trọng quyển sách đó, chứ đừng download về đầy máy tính rồi cả năm không động vào. Nếu mình là tác giả của những cuốn sách mà được mọi người yêu thích thì mình cũng cảm thấy được an ủi phần nào. (phần còn lại là nhận được tiền từ các bạn, :D).\nOk, lan man vậy thôi, mình sẽ đi vào vấn đề chính luôn.\n1. Download các bài báo khoa học\nKỹ năng đọc hiểu và implement theo các public papers là một kỹ năng cần thiết đối các kỹ sư AI. Thực tế, dự án mà mình vừa trải qua tại VTI, mình cần đọc hiểu bài báo sau Inception Single Shot MultiBox Detector for object detection. Thực ra thì cũng có khá nhiều các bài viết về thuật toán này rồi, nhưng mình muốn đọc paper gốc để hiểu rõ hơn ý định của tác giả nên mình đã vào IEEE để tìm. Nhưng mà trên đó, họ chỉ cho đọc mỗi phần Abstract. Nếu muốn đọc full thì phải đăng ký thành viên và phải trả phí.\n Số tiền tuy không lớn nhưng cũng không nhỏ đối với những người mà tiền trong ví lúc nào cũng chỉ đổ xăng, ăn sáng và uống trà đá như mình, :D.\nTrong trường hợp này, bạn chỉ cần truy cập vào trang web https://sci-hub.st/.\n Tại trang chủ của nó, bạn có thể nhìn thấy ngay slogan của họ: ... to remove all barriers in the way of science. Việc bạn cần làm là dán đường link bài báo mà bạn muốn tải về vào ô textbox rồi click vào nút open.\nVà đây là kết quả:\n Đến đây thì bạn có thể tải về máy tính của bạn một các dễ dàng rồi.\nĐây là cách mà mình được một GS giới thiệu cho khi học môn \u0026ldquo;Văn phong khoa học kỹ thuật\u0026rdquo; ở lớp Master. Thầy cũng nhấn mạnh rằng, đây là cách mà trong giới khoa học ngầm hiểu với nhau chứ không công khai trong bất kì tài liệu chính thức nào. Bởi vì có một thực tế rằng các nhà khoa học khi làm nghiên cứu, họ phải đọc hàng chục, thậm chí hằng trăm bài báo khoa học để tìm ý tưởng, lấy dẫn chứng, \u0026hellip; Giả sử mỗi bài báo có giá 1$ tải về thôi thì cũng là một vấn đề không nhỏ rồi.\nLiệu bạn có thắc mắc, tại sao một trang web như này lại có thể công khai tồn tại một cách hiển nhiên như thế này? Đó cũng là câu hỏi mà mình từng nghĩ đến nhưng vẫn chưa tìm được câu trả lời chính xác. Nhưng có lẽ, nên nhìn nhận vấn đề này giống như cách chúng ta sử dụng hệ điều hành Windows hay bộ công cụ Office của Microsoft. Mặc dù Microsoft biết là có rất nhiều người dùng \u0026ldquo;chùa\u0026rdquo; nhưng họ không chặn, cứ \u0026ldquo;giả ngơ\u0026rdquo; để cho mọi người dùng thoải mái. Không phải Microsoft không thể chặn, mà có lẽ là họ không muốn chặn. Phí để mua bản quyền hệ điều hành Windows khá cao so với mức thu nhập của rất nhiều người, nếu Microsoft ngăn chạn triệt để thì có thể người dùng sẽ quay lưng, chuyển sang một hệ điều hành mở khác (Ubuntu chẳng hạn). Khi người dùng đã trở nên quen với Windows rồi, Microsoft mới bắt đầu tính phí bản quyền của các doanh nghiệp, công ty. Nếu công ty, doanh nghiệp nào không mua bản quyền, thì rất dễ bị kiện bởi Microsoft, hoặc bị chính khách hàng của họ chấm dứt mối quan hệ làm ăn với công ty đó. Nếu tổ chức (trường học, viện nghiên cứu, \u0026hellip;) sử dụng các bài báo không có bản quyền để phục vụ các mục đích thuơng mại, hay công bố quốc tế thì rất có thể cũng sẽ bị kiện bởi tác giả hoặc nhà xuất bản.\nNgoài IEEE, các bài báo ở các nhà xuất bản khác như Sciencedirect, Springer, Researchgate, \u0026hellip; cũng có thể download được theo cách này.\nCó một lưu ý nhỏ là, trang https://sci-hub.st/ này có thể bị chặn, không truy cập được đối với một số mạng nội bộ ở công ty, doanh nghiệp. Khi đó bạn chỉ cần đổi qua một mạng Internet free open nào đó (4G chẳng hạn) là có thể truy cập được.\n2. Download sách\nMột trong những cuốn sách mình rất thích, đó là:\n Trên Amazon, quyển này có giá khá cao, $37.49 cho bản sách điện tử.\n  Và đây là cách mà mình đã làm để download cuốn sách này với giá $0.\nVào trang https://b-ok.asia/.\n Gõ tên sách cần tìm ở ô textbox rồi click Search. Kết quả:\n Woo, có đẩy đủ các Edition luôn. Mình chọn 2nd Editon cho cập nhật.\nTrang này mình vô tình biết được trong một lần lang thang lên mạng tìm sách free. Có khá nhiều trang cho download sách, nhưng cá nhân mình thâý trang này dễ dàng và đầy đủ nhất (trong phạm vi nhu cầu của mình).\n3. Đọc các bài viết trên https://medium.com/\nTrang web này có lẽ đã khá quen thuộc với nhiều người. Các bài viết trên đó đều đến từ các chuyên gia, và người có kinh nghiệm trong từng lĩnh vực cụ thể, và được cập nhật hàng ngày. Mình cảm thấy các bài viết trên đó rất chất lượng, rất đáng để đọc. Mình đang cố gắng tập thói quen chuyển bớt thời gian lướt Facebook sang đọc Medium. :D\nTuy nhiên, https://medium.com/ chỉ cho đọc free tối đa 5 bài. Muốn đọc nhiều hơn bạn phải đăng ký thành viên, trả phí 5$/tháng. 5$ thực sự không phải là một con số lớn so với lượng kiến thức hay ho như vậy. Thế nên mình quyết định mua, không xài \u0026ldquo;chùa\u0026rdquo; nữa. :D\nDù vậy, nếu bạn vẫn muốn đọc free thì dưới đây là cách cho bạn:\nGiả sử khi bạn vào https://medium.com/, click vào 1 bài để đọc thì nhận được thông báo:\n Hãy làm theo các bước sau:\nClick vào biểu tượng Cookie trên trình duyệt:\n Chọn Cookies:\n Chọn medium.com rồi click vào nút Remove.\nQuay trở lại trình duyệt, reload lại trang. Bạn sẽ thấy bài viết đã được hiển thị đầy đủ.\nNgoài nội dung kiến thức hay và phong phú, Medium còn có tính năng recommend các bài biết hay, phù hợp với lĩnh vực bạn quan tâm, dựa trên cookies của bạn. Vì thế, nếu bạn chọn cách đọc free thì đôi khi bạn sẽ bỏ lỡ các bài viết hay và mới nhất. Hãy cân nhắc khi quyết định hành động của bạn!\nVậy là mình đã giới thiệu đến mọi người 3 cách để tiếp cận với nguồn tri thức vô tận trên Internet. Nhấn mạnh lại lần nữa là mình không ủng hộ hoàn toàn các cách làm này. Nếu bạn có điều kiện, hãy mua sách, mua tài khoản và trả tiền một cách đầy đủ cho tác giả. Nếu bạn như mình, không quá dư giả nhưng sách thì vẫn muốn đọc thì có thể cân nhắc như cách mình đã làm. Ngoài ra, nếu bạn download một cuốn sách về đọc mà trong lòng cảm thấy \u0026ldquo;áy náy\u0026rdquo; thì hãy làm một việc tốt gì đó bù lại. Chẳng hạn, tối đi làm về trên đường, gặp người ăn xin, bạn có thể cho họ chút tiền lẻ của bạn. Như vậy thì ít nhiều bạn cũng đỡ \u0026ldquo;áy náy\u0026rdquo; hơn phần nào! :)\nCuối cùng, chúc mọi người tìm đuợc những cuốn sách hay, bài báo thú vị và bổ ích cho mình!\n","permalink":"https://tiensu.github.io/blog/25_for_book_lover/","tags":["Ebook"],"title":"Dành cho người yêu sách"},{"categories":["Deep Learning","Convolution Neural Network"],"contents":"Tiếp tục chuỗi các bài viết về CNN, trong bài này mình sẽ chia sẻ với các bạn một số \u0026ldquo;common patterns 7 rules\u0026rdquo; trong việc xây dựng kiến trúc CNN. Nắm rõ những \u0026ldquo;patterns \u0026amp; rules\u0026rdquo; này sẽ giúp các bạn giảm thiếu thời gian và công sức khá nhiều trong các dự án của các bạn!\n3. Common Architectures \u0026amp; Training Patterns\nQua 2 bài viết trước, chúng ta đã biết, CNN được tạo thành từ 4 loại layers chủ yếu, bao gồm: CONV, POOL, RELU, và FC. Sắp xếp các layers này với nhau theo một thứ tự nhất định ta sẽ một CNN (gọi tên đầy đủ là kiến trúc CNN).\n3.1 Layers Patterns\nNói chung, hầu hết các kiến trúc CNN đều có mộ vài lớp CONV và RELU liên tiếp nhau, theo sau bởi lớp POOL. Lặp lại như thế đến khi kích thước của input volumn đủ nhỏ, rồi thêm vào một hoặc nhiều FC layers. Pattern tổng quát như sau:\nINPUT =\u0026gt; [[CONV =\u0026gt; RELU]xM =\u0026gt; POOL?]xN =\u0026gt; [FC =\u0026gt; RELU]xK =\u0026gt; FC\nKý hiệu x ở đây tức là lặp lại 1 hoặc nhiều lần, còn ? nghĩa là tùy chọn, có thể có hoặc không.\nM, N, K thường chọn theo các rules sau:\n 0 \u0026lt;= N \u0026lt;= 3 M \u0026gt;= 0 0 \u0026lt;= K \u0026lt;= 2  Ví dụ một số kiến trúc CNN áp dụng pattern tổng quát bên trên như sau:\n INPUT =\u0026gt; FC INPUT =\u0026gt; [CONV =\u0026gt; RELU =\u0026gt; POOL]x2 =\u0026gt; FC =\u0026gt; RELU =\u0026gt; FC INPUT =\u0026gt; [[CONV =\u0026gt; RELU]x2 =\u0026gt; POOL]x3 =\u0026gt; [FC =\u0026gt; RELU]x2 =\u0026gt; FC  Các kiến trúc CNN kinh điển cũng dựa trên pattern tổng quát này:\n AlexNet: INPUT =\u0026gt; [CONV =\u0026gt; RELU =\u0026gt; POOL]x2 =\u0026gt; [CONV =\u0026gt; RELU]x3 =\u0026gt; POOL =\u0026gt; [FC =\u0026gt; RELU =\u0026gt; DO]x2 =\u0026gt; SOFTMAX VGGNet: INPUT =\u0026gt; [CONV =\u0026gt; RELU]x2 =\u0026gt; POOL =\u0026gt; [CONV =\u0026gt; RELU]x2 =\u0026gt; POOL =\u0026gt; [CONV =\u0026gt; RELU]x3 =\u0026gt; POOL =\u0026gt; [CONV =\u0026gt; RELU]x3 =\u0026gt; POOL =\u0026gt; [FC =\u0026gt; RELU =\u0026gt; DO]x2 =\u0026gt; SOFTMAX  Một cách khái quát, chúng ta sẽ áp dụng các kiến trúc CNN sâu khi gặp bài toán phức tạp, nhiều labels, các đối tượng thay đổi không có quy luật. Sử dụng nhiều CONV layers trước khi áp dụng POOL layer cho phép các CONV layers học được các complex features trước khi áp dụng POOL layer để giảm kích thước của input volumn.\nNhư đã đề cập ở bài trước, một số kiến trúc CNN đã loại bỏ hoàn toàn các POOL layers phía sau CONV layers, chỉ sử dụng CONV layers để giảm kích thước của input volumn. Hơn nữa, các FC layers ở cuối cũng không còn được sử dụng, thay vào đó là average pooling. GoogLeNet, ResNet, SqueezeNet là những kiến trúc sử dụng cách này. Kết quả là giảm số lượng tham số của CNN và thời gian train cũng ngắn hơn.\nĐặc biệt hơn, GoogLeNet còn áp dụng đồng thời 3 loại filters có kích thước khác nhau (1x1, 3x3, 5x5) tại cùng 1 vị trí trong kiến trúc để học multi-level features. Những kiến trúc kiểu như này được coi là công nghệ tiên tiến trong lĩnh vực DL.\n3.2 Quy tắc ngón tay cái\nTrong phần này, chúng ta sẽ cùng xem xét một số rules khi xây dựng CNN model.\n Rule 1  Đầu tiên, images đưa vào CNN nên có chiều rộng và chiều cao bằng nhau (square) ($W_{input} = H_{input}$). Sử dụng squere images cho phép chúng ta tận dụng các lợi ích của các thư viện tối ưu trong đại số tuyến tính. Kích thước thường hay sử dụng là: 32x32, 64x64, 96x96, 224x224, 227x227, 229x229.\n Rule 2  Thứ 2, sau khi đi qua CONV layer đầu tiên, kích thước của images nên có thể chia hết cho 2. Điều này, cho phép POOL layer tiếp sau đó hoạt động theo cách hiệu quả hơn. Để áp dụng rule này, có thể điêu chỉnh kích thước của filters và stride. Nói chung, CONV layers nên có kích thước nhỏ (3x3 hoặc 5x5). Tiny filter (1x1) có thể được sử dụng để học các local features, nhưng chỉ nên áp dụng trong các kiến trúc hiện đại và phức tạp. Kích thước lớn hơn của filters (7x7 hoặc 11x11) cũng có thể xuất hiện ở CONV layer đầu tiên trong kiế trúc để giảm nhanh kích thước không gian của input volumn có kích thước \u0026gt; 200x200 pixels. Nhấn mạnh là chỉ áp dung filers có kích thước lớn ở CONV layer đầu tiên, ngược lại, input volumn sẽ giảm rất nhanh làm mất mát các features quan trọng.\n  Rule 3 Stride của CONV, S = 1 cũng nên được sử dụng cho các CONV layers đối với các input volumns có kích thước trung bình nhỏ (\u0026lt; 200x200 pixels). Sử dụng S = 2 cho các input volumns có kích thước lớn hơn, nhưng cũng chỉ nên áp dụng ở CONV layer đầu tiên. Khi S = 1 thì CONV layers làm nhiệm vụ học các features của images, trong khi POOL layers chịu trách nhiệm giảm kích thước input volumns. Tuy nhiên, nhắc lại lần nữa rằng trong các kiến trúc CNN tiên tiến, POOL layers đang dần dần được thay thể bởi CONV layers với S \u0026gt;= 2.\n  Rule 4 Cá nhân mình thường áp dụng zero-padding trong CONV layer để đảm bảo kich thước của input volumns không đổi khi đi qua CONV layer và sử dụng POOL layer để giảm kích thước input volumn. Thực nghiệm của mình cho thấy classification accuracy thường cao hơn khi sử dụng rule này. Khi làm viêc với Keras framework, bạn có thể làm điều này một cách dễ dàng bằng cách setting padding=same khi tạo CONV layer. Bạn chỉ nên sử thay POOL layer bằng CONV layer khi đã thành thạo ở mức chuyên gia trong việc thiết kế kiến trúc của CNN.\n  Rule 5\n  Đối với POOL layer, kích thước thông thường của nó trong kiến trúc CNN là 2x2, cộng với stride S = 2. Kích thước 3x3 cũng có thể sử dụng ở các layers đầu trong CNN để giảm nhanh kích thước của input volumn. Kích thước \u0026gt; 3x3 chưa từng thấy xuất hiện trong bất cứ mạng CNN nào từ trước đến giờ.\n Rule 6  Về phần Batch Normalization, như trong bài trước đã đề cập, mặc dù nó làm tăng lên đáng kể thời gian training, nhưng chúng ta vẫn nên sử dụng nó trong hầu hết các trường hợp vì những lợi ích mà nó mang lại. BN layer được đặt sau ACT layer như trong các ví dụ sau:\n  INPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; FC\n  INPUT =\u0026gt; [CONV =\u0026gt; RELU =\u0026gt; BN =\u0026gt; POOL]x2 =\u0026gt; FC =\u0026gt; RELU =\u0026gt; BN =\u0026gt; FC\n  INPUT =\u0026gt; [[CONV =\u0026gt; RELU =\u0026gt; BN]x2 =\u0026gt; POOL]x3 =\u0026gt; [FC =\u0026gt; RELU =\u0026gt; BN]x2 =\u0026gt; FC\n  Rule 7\n  Droput (DO) được đặt giữa các FC layers với xác suất ngắt kết nối các nodes là 50%. Nó cũng được khuyên sử dụng DO trong mọi kiến trúc CNN của bạn. Cá nhân mình, đôi khi cũng đặt DO ở giữa CONV và POOL layers, và điều này đôi khi cũng tỏ ra hiệu quả trong việc giảm bớt Overfitting. Bạn có thể thử-sai trong các bài toán của bạn.\nOk, đó là 7 rules mình muốn giới thiệu đến các bạn. Bằng viêc ghi nhớ những rules này, bạn sẽ bớt đau đầu hơn khi xây dựng kiến trúc CNN của riêng mình. Một khi bạn đã trở thành chuyên gian xây dựng mạng CNN theo cách truyền thống như thế này, hãy thử bỏ qua max pooling, chỉ sử dụng CONV layer để giảm kích thước không gian của input volumns và sử dụng average pooling thay thế cho FC layer để giảm độ phức tạp tính toán của CNN. Mình sẽ đề cập chi tiết hơn những kỹ thuật advances này trong các bài viết về sau.\n4. Tổng kết\nVậy là mình đã kết thúc 3 bài viết về CNN. Hi vọng với những kiến thức chia sẻ ở đây sẽ giúp ích được cho các bạn, đặc biệt là các patterns và rules ở bài số 3 này. Các bạn có thể áp dụng luôn vào trong bài toán của mình và kiểm tra sự khác biệt.\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/24_convolutional_neural_network_3/","tags":["Deep Learning","Convolution Neural Network"],"title":"Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 3"},{"categories":["Deep Learning","Convolution Neural Network"],"contents":"Vì sử dụng trực tiếp raw pixel của image nên so với CNN, FCN (Fully Connected Network) có 2 nhược điểm kích thước của image tăng lên:\n Hiệu năng giảm mạnh. Kích thước của mạng tăng nhanh. Kết quả thực nghiệm cho thấy, khi áp dụng Fully Connected Network vào bộ dataset CIFAR-10, độ chính xác chỉ đạt được 52%.  CNN, theo một cách khác, sắp xếp các layers theo dạng 3D volume với 3 chiều: Width, Height, Depth. Các neurons trong mỗi layer chỉ kết nối tới 1 small region của layer trước đó - gọi là local connectivity. Điều này giúp giảm bớt rất nhiều kích thước của mạng.\n2. Các loại layers\nCó khá nhiều các dạng layers khác nhau để xây dựng nên CNNs. Các loại dưới đây được sử dụng phổ biến:\n Convolutional (CONV) Activation (ACT, RELU, SOFTMAX) Pooling (POOL) Fully-connected (FC) Batch normalization (BN) Dropout (DO)  Sắp xếp các dạng layers trên liên tiếp nhau theo thứ tự nào đó sẽ cho ta một CNN.\nVí dụ: INPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; FC =\u0026gt; SOFTMAX\nỞ đây, ta định nghĩa một CNN đơn giản, nhận input, áp dụng \u0026ldquo;convolution layer\u0026rdquo;, sau đó là 1 \u0026ldquo;activation layer\u0026rdquo; (RELU), tiếp theo là 1 \u0026ldquo;fully-connected layer\u0026rdquo;. Cuối cùng là một \u0026ldquo;activation layer\u0026rdquo; nữa (SOFTMAX) để đạt được xác suất của output theo các nhãn cần phân loại.\nTrong số các lớp trên, chỉ có CONV và FC là chứa các tham số được cập nhật trong quá trình training. POOL có tác dụng thay đổi kích thước không gian của image khi nó di chuyển qua các lớp của CNN.\nCONV, POOL, RELU và FC là 4 layers quan trọng nhất, gần như không thể thiếu khi xây dựng CNNs.\n2.1 CONV\nCONV chứa một tập K learnable filters (ví dụ: Kernel), mỗi filter có kích thước width x height. Mặc định,width luôn luôn bằng height, trừ khi có lý do đặc biệt. Hai giá trị này có thường nhỏ (1, 3, 5, 7) nhưng K thì có thể rất lớn (4, 8, 16, 32, 64, 128, \u0026hellip;). K cũng được gọi là độ sâu (depth) của CONV layer.\nCùng xem xét forward-pass của một CNN. CONV có K filters, áp dụng vào một input volumn có kích thước WxH. Tưởng tượng rẵng, mỗi filters sẽ trượt ngang qua toàn bộ input volumn, tính toán convolution, sau đó lưu kết quả ra một mảng 2 chiều, gọi là activation map. Xem hình bên dưới:\n Sau khi áp dùng K filters lên input volumn, chúng ta thu được K, 2-dimensional activation maps. Xếp chồng (Stack) những activation maps này theo chiều sâu sẽ thu được kết qủa cuối cùng (output volumn).\n Xét về kích thước của output volumn, có 3 tham số tác động. Chúng là depth, stride và zero-padding size.\n Depth  Như đã nói ở trên, depth chính là số lượng filters trong CONV layer, có giá trị là K.\n Stride  Stride là kích thước của step khi các filters trượt qua input volumn. Giá trị của stride thường là 1 hoặc 2 (S=1 hoặc S=2), tương ứng với step là 1 hoặc 2 pixel. S nhỏ sẽ sinh ra ouput volumn lớn, và có nhiều vùng được bị trùng lặp trong quá trình trượt và tính covolution của các filters. Đối với S lớn, kết quả sẽ ngược lại.\nXem ví dụ sau:\nTrong hình bên dưới, ma trận bên trái là input volumn, ma trận bên phải là filter.\n Sử dụng S = 1 và S = 2 cho convolution, thu được kết quả tương ứng bên trái, phải trong hình sau:\n Cùng với pooling (xem phần bên dưới), stride có thể được sử dụng để giảm kích thước của input volumn.\n Zero-padding  Sử dụng stride làm giảm kích thước của input volumn. Vậy nếu muốn giữ nguyên kích thước của input volumn thì sao? Zero-padding chính là câu trả lời.\nZero-padding tức là gắn thêm (pad) viền (border) cho input volumn. Các giá trị được gắn thêm đều là 0, vì thế mà có tên zero-padding.\nXem ví dụ sau:\n Bên trái là 3x3 ouput khi áp dụng 3x3 convolution tới 5x5 input.\nBên phải là khi áp dụng zero-padding vào input, thu được input mới có kích thước 7x7. Giá trị của zero-padding trong trường hợp này là P = 1.\nBên dưới là 5x5 ouput khi áp dụng 3x3 convolution tới 7x7 input. Ta thấy kích thước 5x5 của input ban đầu được duy trì trong output.\nNếu không sử dụng zero-padding, kích thước của input volumn sẽ giảm rất nhanh, do đó không thể xây dựng CNN nhiều layers.\nCông thức tính kích thước của ouput volumn như sau: O = ((W - F + 2P)/S) + 1\nTrong đó:\n 0: kích thước của output volumn. W: kích thước input volumn F: kích thước của filter P: kích thước zero-padding S: kích thước stride  Nếu O không phải là số nguyên, cần thay đổi lai giá trị của S.\nThử áp dụng công thức này vào ví dụ bên trên:\nO = ((5 - 3 + 2*1)/1) = 5\n2.2 Activation Layers\nActivation layer thường được áp dụng sau mỗi CONV layer trong một mạng CNN. Các activation layer hay dùng là các hàm phi tuyến, giống như: ReLU, ELU, Leaky ReLU. Trong các public paper, ReLU được sử dụng rất phổ biến, gần như là mặc định. Khi viết ACT, ta ngầm hiểu đó là ReLU.\nTrên thực tế, activation layer không được coi là một layer theo đúng nghĩa. Bởi vì nó không có parameters nào được học trong quá trình huấn luyện mô hình. Trong một số diagram kiến trúc của mạng, nó có thể không xuất hiện và được ngầm hiểu rằng nó nằm ngay sau CONV layer.\nVí dụ với kiến trúc sau:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; FC\nCó thể được viết gọn thành:\nINPUT =\u0026gt; CONV =\u0026gt; FC\nMột activation layer nhận input volumn có kích thước là $W_{input}$x$H_{input}$x$D_{input}$, áp dụng activation function theo kiểu element-wise nên kích thước của output volumn đúng bằng kích thước của input volumn: $W_{input}$=$W_{output}$, $H_{input}$=$H_{output}$, $D_{input}$=$D_{output}$.\n2.3 Pooling Layers\nCó 2 phương pháp để giảm kích thước của input volumn: CONV layer (với stride \u0026gt; 1) và POOL layer.\nThông thường, POOL layer được đặt ngay sau ACT layer và trước CONV layer. Trong trường hợp không có ACT layer thì nó nằm giữa 2 CONV layer.\nVí dụ kiến trúc mạng sau:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; POOL =\u0026gt; CONV =\u0026gt; POOL =\u0026gt; FC\nChức năng đầu tiên của POOL layer là giảm kích thước không gian của input volumn một các từ từ. Điều này dẫn đến việc giảm số lượng tham số và sự phức tạp tính toán của mạng. Vì thế mà nó là một trong những phương pháp hiệu của để tránh hiện tượng Overfitting cho mạng DL.\nPOOL layer hoạt động độc lập trên mỗi depth slice của input volumn, sử dụng hàm max hoặc average. Hai cái tên có lẽ cũng đã nói lên cách thức hoạt động của chúng. Max pooling chỉ giữ lại giá trị lớn nhất trong phạm vi kích thước của nó còn average pooling thì lấy giá trị trung bình của các giá trị trong phạm vi kích thước của POOL layer. Về vị trí trong kiến trúc, trong khi max pooling thường được đặt ở giữa của kiến trúc mạng DL để giảm kích thước, còn average pooling lại hay được đặt ở các layers cuối (hoặc gần cuối) (VD: GoogLeNet, SqueezeNet, ResNet) để thay thế cho các FC layers, giúp giảm độ phức tạp cả model.\nKích thước của POOL layer hay được sử dụng là 2x2. Mặc dù vậy, với input volumn có kích thước \u0026gt; 200x200, ta có thể sử dụng kích thước 3x3 của POOL ở những layer đầu.\nBước nhảy (stride, S) của mỗi POOL layer thường là 1 hoặc 2. Hình bên dưới minh họa kết quả hoạt dộng của max pooling với S =1,2, tương ứng.\n Công thức tính kích thước của output volumn sau khi qua POOL layer như sau:\n $W_{output}$ = (($W_{input}$ - $F$)/$S$) + 1 $H_{output}$ = (($H_{input}$ - $F$)/$S$) + 1 $D_{output}$ = $D_{input}$  Trong đó:\n $W_{input}$x$H_{input}$x$D_{input}$: kích thước của input volumn. $W_{output}$x$H_{output}$x$D_{output}$: kích thước của output volumn. F: kích thước của POOL layer (cũng gọi là pool size). S: stride  Trong các bài toán thực tế, có 3 dạng max pooling thường hay được sử dụng.\n Dạng 1: (F = 3, S = 2), gọi là overlapping pooling, thường áp dụng đối với các images/input volumn có kích thước lớn (\u0026gt; 200x200 pixels) Dạng 2: (F = 2, S = 2), gọi là non-overlapping pooling, thường được áp dụng với các images/input volumn có kích thước trung bình (\u0026gt; 64x64 pixels và \u0026lt; 200x200 pixels) Dạng 3: (F = 2, S = 1), gọi là small pooling, áp dụng với các images/input volumn nhỏ (\u0026lt; 64x64 pixels)  Đến đây, có thể các bạn sẽ thắc mắc, khi nào thì dùng CONV layer, khi nào thì dùng POOL layer để giảm kích thước của input volumn?\nSpringenberg et al, trong paper Striving for Simplicity: The All Convolutional Net xuất bản năm 2014 của họ đã đề xuất loại bỏ hoàn toàn POOL layer, chỉ sử dụng CONV layer (với S\u0026gt;1). Họ đã chứng minh tính hiệu của cách tiếp cận này trên một số tập dữ liệu, bao gồm cả CIFAR-10 (small images, low number of class) và ImageNet (large input images, 1000 classes). Xu hướng này cũng xuất hiện trong kiến trúc của mạng Resnet năm 2015 và đang dần dần trở nên phổ biến hơn. Có lẽ trong tương lai không xa, chúng ta sẽ không sử dụng POOL layer (cụ thể là max pooling) trong phần giữa các kiến trúc mạng DL hiện đại nữa mà chỉ sử dụng average pooling tại các layer cuối để thay thế cho FC layer vốn cồng kềnh và phức tạp. Tuy nhiên, trước mắt thì max pooling vẫn chưa thể biến mất hoàn toàn được, nên chúng ta vẫn cần phải học, hiểu và áp dụng chúng trong việc xây dựng kiến trúc mạng DL của riêng mình, cũng như đọc hiểu các kiến trúc mạng DL kinh điển khác.\n2.4 Full-Connected (FC) Layers\nFC Layer chính là Feedforward Neural Network mà chúng ta đã tìm hiểu trong bài \u0026hellip;. Nó luôn được đặt ở cuối trong các kiến trúc mạng DL.\nVí dụ kiến trúc mạng sau,\nINPUT =\u0026gt; (CONV =\u0026gt; RELU =\u0026gt; POOL)x2 =\u0026gt; FC =\u0026gt; FC =\u0026gt; SOFTMAX.\nta đã sử dụng 2 FC layers ở gần cuối mạng, theo sau là ACT layer (SOFTMAX) để phân loại (tính toán xác suất của mỗi classes).\n2.5 Batch Normalization (BN)\nĐược giới thiệu lần đầu vào năm 2015 trong paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift của Ioffe và Szegedy, BN đã nhanh chóng trở nên phổ biến trong các kiến trúc mạng DL. Đúng như tên của nó, BN layer có tác dụng normalize input volumn trước khi đi vào layer tiếp theo.\nGiả sử, ta có input tại thời đểm i (mini-batch i) là $x_i$, sau khi đi qua BN layer, ta thu được giá trị $\\widehat{x_i}$ theo công thức:\n$\\widehat{x_i} = \\frac{x_i - \\mu_ \\beta}{\\sqrt{\\sigma^2_ \\beta + \\varepsilon}}$   Trong đó:   $\\mu_ \\beta = \\frac{1}{M}\\sum_{i=1}^m x_i$    $\\sigma^2_ = \\frac{1}{M}\\sum_{i=1}^m (x_i - \\mu_\\beta)^2$      Giá trị của $\\varepsilon$ được chọn là giá trị dương đủ nhỏ (VD: 1e-7) để tránh việc chia cho 0. Sau khi áp dụng BN, input volumn sẽ có trung bình (mean) xấp xỉ 0 và độ lệch chuẩn (variance) xấp xỉ 1 (còn gọi là zero-centered).\nKhi sử dụng model để test, ta thay thế $\\mu_\\beta$ và $\\sigma_\\beta$ bằng giá trị trung bình của chúng trong suốt quá trình training. Điều này đảm bảo cho ta có thể pass input volumn xuyên qua mạng DL mà không bị biased bởi giá trị $\\mu_\\beta$ và $\\sigma_\\beta$ tại thời điểm cuối cùng ($x_m$).\nBN layer tỏ ra hiệu quả cao trong việc làm cho quá trình training một NN ổn định hơn, giảm số lượng epochs cần thiêt để train model, và quan trọng nhất là hạn chế tình trạng Overfitting. Khi sử dụng BN, việc tuning các tham số khác của model cũng trở nên đơn giản hơn bởi vì BN đã thu hẹp đáng kế phạm vi giá trị của các weights trong mạng.\nHạn chế lớn nhất của BN có lẽ là nó làm tăng thời gian training của bạn do phải tính toán normalization và statistic tại mỗi nơi mà nó xuất hiện trong kiến trúc mạng. Thường là tăng gấp 2 đến 3 so với không sử dụng BN.\nTuy nhiên, có lẽ hạn chế trên không đáng kể so với những ưu điểm mà BN mang lại. Vì vậy, lời khuyên ở đây là nên sử dụng BN thường xuyên trong bài toán của bạn.\nCuối cùng là về vị trí đạt BN layer trong kiến trúc DL. Mặc dù trong paper gốc của tác giả BN layer được đặt trước ACT layer, nhưng điều này lại không hợp lý vê mặt thống kê. Bởi vì output của BN là zero-centered, khi đi qua ACT layer (ReLU), phần giá trị âm sẽ bị triệt tiêu. Điều này vô tình làm mất đi bản chất của BN. Thực nghiệm rất nhiều cũng đã chỉ ra rằng, đặt BN layer ở sau ACT layer cho kết quả tốt hơn (higher accuracy và lower loss) trong hầu hết mọi trường hợp. Vì thế, mặc định, hãy đặt BN layer sau ACT layer, trừ khi bạn có lý do đặc biệt nào khác.\nVí dụ về việc đặt BN layer trong kiến trúc DL:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; BN \u0026hellip;\n2.6 Dropout (DO) layer\nDO thực chất là một dạng của regularization, mục đích là để hạn chế hiện tượng Overfitting. Tại mỗi mini-batch trong quá trình train, DO layer sẽ ngẫu nhiên ngắt kết nối các inputs giữa 2 layer liên tiếp, với xác suất p.\nVí dụ về DO với p = 5 giữa 2 FC layers như hình bên dưới:\n DO chỉ hoạt động theo 1 chiều forwarding, chiều ngược lại (backwarding), các dropped connections sẽ được kết nối lại để tính toán.\nDO giúp giảm Overfitting theo cách như trên bởi vì khi đó, vai trò của các nodes trong mạng sẽ được phân phối đều hơn, không có nodes nào chịu trách nhiệm chính, nhiều hơn các nodes khác. Điều này sẽ giúp model generalize tốt hơn.\nVề vị trí trong kiến trúc mạng DL, DO layer thường được set với p = 0.5 và đặt xen kẽ 2 FC layers ở cuối.\nVí dụ:\nINPUT =\u0026gt; CONV =\u0026gt; RELU =\u0026gt; POOL =\u0026gt; FC =\u0026gt; DO =\u0026gt; FC =\u0026gt; DO =\u0026gt; SOFTMAX\nBài thứ 2 về CNN xin được kết thúc tại đây. Trong bài tiếp theo (cũng là bài cuối cùng về CNN), mình sẽ chia sẻ một số patterns và một số rules trong việc xây dựng kiến trúc CNN. Mời các bạn đón đọc!\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/23_convolutional_neural_network_2/","tags":["Deep Learning","Convolution Neural Network"],"title":"Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 2"},{"categories":["Deep Learning","Convolution Neural Network"],"contents":"Sau khi đã tìm hiểu cơ bản về Neural Network, chúng ta sẽ đi tìm hiểu về CNN. CNN là một dạng kiến trúc Neural Network đóng vai trò vô cùng quan trọng trong Deep Learning.\nTrong Feedfoward Neural Network, mỗi neural trong một layer được kết nối đến tất cả các nodes của layer tiếp theo. Ta gọi điều này là Fully Connected (FC) layer. Tuy nhiên, trong CNNs, FC layers chỉ được sử dụng ở 1 vài layers cuối. Các layers còn lại được gọi là convolutional layers.\nMột hàm kích hoạt (activation function) (thường là ReLU) được áp dụng tới output của các convolutional layers. Kết hợp với các dạng layers khác nhau để giảm kích thước của input. Các FC layers ở cuối có nhiệm vụ phân loại output thành các classes khác nhau.\nMỗi layer trong CNN áp dụng một tập các bộ lọc (filters) (có thể lên đến hàng trăm hoặc hàng nghìn), kết hợp kết quả lại, cho qua layer tiếp theo. Trong suốt quá trình training, giá trị của các filters được cập nhật (tương tự như trọng số weight trong Neural Network).\nTrong lĩnh vực xử lý ảnh, CNN có thể học để:\n Phát hiện biên (edges) từ raw pixel data ở layer đầu tiên. Sử dụng edges đã phát hiện để phát hiện hình dạng (shapes) đối tượng ở layer thứ 2. Sử dụng shapes để phát hiện heigher-level features trong các layers tiếp theo.  1. Hiểu rõ về Convolutions\nChúng ta sẽ cùng nhau trả lời một số câu hỏi sau:\n Convolutions là gì? Chúng lamf được những việc gì? Tại sao lại sử dụng chúng? Áp dụng chúng vào xử lý ảnh như thế nào?  Từ convolution, dịch sang tiếng việt là tích chập, nghe có vẻ phức tạp. Bạn chắc chắn đã nghe đến từ này nếu bạn học qua môn Xử lý tín hiệu sô. Tuy nhiên, convolution trong lĩnh vực xử lý lý ảnh hơi khác một chút. Không phải khi có Deep Learning, chúng ta mới sử dụng convolution, các phương pháp xử lý ảnh truyền thống đều sử dụng convolution: Edges detection, Sharpen images, Blurring and Smoothing images, \u0026hellip; Vì thế mới nói, convolution là xương sống của xử lý ảnh. Hiểu rõ convolution là điều kiện tiên quyết để bước chân vào lĩnh vựa xử lý ảnh (theo cả phương pháp truyền thống và sử dụng Deep Learning).\nNghe thì có vẻ \u0026ldquo;đao to búa lớn\u0026rdquo; vậy, nhưng thực sự không phải vậy. Convolution đơn giản chỉ là tổng của các tích đôi một của từng phần tử tron 2 ma trận. Chia nhỏ các bước ra cho dễ hiểu:\n Lấy 2 ma trận có cùng kích thước Nhân 2 ma trận đôi một (element-by-element) (không phải phép nhân ma trận trong đại số tuyến tính). Cộng kết quả của các tích lại.  Yup, đó là convolution.\n1.1 Kernel\nMột image là một ma trận nhiều chiều. Thường là 3 chiều (w, h, c) với width là số cột, height là số hàng và depth là số kênh màu. \u0026ldquo;Image matrix\u0026rdquo; thường được gọi với cái tên \u0026ldquo;big matrix\u0026rdquo;. Một ma trận khác gọi là kernel (hoặc \u0026ldquo;convolution matrix\u0026rdquo;, \u0026ldquo;tiny matrix\u0026rdquo;, filter) đặt bên trên \u0026ldquo;big matrix\u0026rdquo;, trượt từ trái sang phải, từ trên xuống dưới. Trong quá trình di chuyển, các phép toán (convolution, \u0026hellip;) được áp dụng đối với 2 ma trận đó. Sử dụng các kernel khác nhau, ta có thể đạt được các mục đích mong muốn: Blurring (average smoothing, Gaussian smoothing, \u0026hellip;), Edge detection (Laplacian, Sobel, ...), \u0026hellip;\nĐể hiểu rõ hơn, chúng ta sẽ làm thử 1 ví dụ cụ thể.\nGiả sử có \u0026ldquo;image matrix\u0026rdquo; kernel như sau:\n   Theo lý thuyết bên trên, kernel sẽ được trượt qua \u0026ldquo;image matrix\u0026rdquo; từ trái qua phải, từ trên xuống dưới. Số bước trượt thường là 1 hoặc 2. Tại bước, sau khi trượt xong, ta sẽ dừng lại, thực hiện phép convolution giữa kernel và phần \u0026ldquo;image matrix\u0026rdquo; bị che bởi kernel. Giá trị ouput được lưu trong ma trận kết quả tại vị trí trung tâm của kernel tại bước đó.\nChi tiết các bước:\n Chọn tọa độ ($x,y$) từ \u0026ldquo;image matrix\u0026rdquo;. Đặt center của kernel tại ($x,y$). Thực hiện convolution giữa kernel và phần \u0026ldquo;image matrix\u0026rdquo; bị che phủ bởi kernel. Lưu kết quả tại ($x,y$) của ma trận kết quả.  Ví dụ, với $(x,y) = (3,3)$:   Sau khi tính toán xong, ta sẽ gán giá trị 132 cho pixel tại vị trí (3,3) của ma trận kết quả. $O_{i,j}$ = 132.\n1.2 Implement Convolutions bằng python.\nGiờ hãy bắt tay vào code thôi. Việc thực hiện convolution bằng code sẽ giúp bạn hiểu sâu sắc hơn cách áp dụng convolution trong xử lý ảnh.\nTạo file convolutions.py và code như sau:\n# USAGE # python convolutions.py --image mai-ngoc.jpg # import the necessary packages from skimage.exposure import rescale_intensity import numpy as np import argparse import cv2 def convolve(image, K): # grab the spatial dimensions of the image and kernel (iH, iW) = image.shape[:2] (kH, kW) = K.shape[:2] # allocate memory for the output image, taking care to \u0026#34;pad\u0026#34; the orders of the input image so the spatial size (i.e., width and height) are not reduced pad = (kW - 1) // 2 image = cv2.copyMakeBorder(image, pad, pad, pad, pad, cv2.BORDER_REPLICATE) output = np.zeros((iH, iW), dtype=\u0026#34;float\u0026#34;) # loop over the input image, \u0026#34;sliding\u0026#34; the kernel across each (x, y)-coordinate from left-to-right and top-to-bottom for y in np.arange(pad, iH + pad): for x in np.arange(pad, iW + pad): # extract the ROI of the image by extracting the *center* region of the current (x, y)-coordinates dimensions roi = image[y - pad:y + pad + 1, x - pad:x + pad + 1] # perform the actual convolution by taking the element-wise multiplication between the ROI and the kernel, the summing the matrix k = (roi * K).sum() # store the convolved value in the output (x, y)- coordinate of the output image output[y - pad, x - pad] = k # rescale the output image to be in the range [0, 255] output = rescale_intensity(output, in_range=(0, 255)) output = (output * 255).astype(\u0026#34;uint8\u0026#34;) # return the output image return output # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(\u0026#34;-i\u0026#34;, \u0026#34;--image\u0026#34;, required=True, help=\u0026#34;path to the input image\u0026#34;) args = vars(ap.parse_args()) # construct average blurring kernels used to smooth an image smallBlur = np.ones((7, 7), dtype=\u0026#34;float\u0026#34;) * (1.0 / (7 * 7)) largeBlur = np.ones((21, 21), dtype=\u0026#34;float\u0026#34;) * (1.0 / (21 * 21)) # construct a sharpening filter sharpen = np.array(( [0, -1, 0], [-1, 5, -1], [0, -1, 0]), dtype=\u0026#34;int\u0026#34;) # construct the Laplacian kernel used to detect edge-like regions of an image laplacian = np.array(( [0, 1, 0], [1, -4, 1], [0, 1, 0]), dtype=\u0026#34;int\u0026#34;) # construct the Sobel x-axis kernel sobelX = np.array(( [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]), dtype=\u0026#34;int\u0026#34;) # construct the Sobel y-axis kernel sobelY = np.array(( [-1, -2, -1], [0, 0, 0], [1, 2, 1]), dtype=\u0026#34;int\u0026#34;) # construct an emboss kernel emboss = np.array(( [-2, -1, 0], [-1, 1, 1], [0, 1, 2]), dtype=\u0026#34;int\u0026#34;) # construct the kernel bank, a list of kernels we\u0026#39;re going to apply using both our custom `convole` function and OpenCV\u0026#39;s `filter2D` function kernelBank = ( (\u0026#34;small_blur\u0026#34;, smallBlur), (\u0026#34;large_blur\u0026#34;, largeBlur), (\u0026#34;sharpen\u0026#34;, sharpen), (\u0026#34;laplacian\u0026#34;, laplacian), (\u0026#34;sobel_x\u0026#34;, sobelX), (\u0026#34;sobel_y\u0026#34;, sobelY), (\u0026#34;emboss\u0026#34;, emboss)) # load the input image and convert it to grayscale image = cv2.imread(args[\u0026#34;image\u0026#34;]) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # loop over the kernels for (kernelName, K) in kernelBank: # apply the kernel to the grayscale image using both our custom `convolve` function and OpenCV\u0026#39;s `filter2D` function print(\u0026#34;[INFO] applying {} kernel\u0026#34;.format(kernelName)) convolveOutput = convolve(gray, K) opencvOutput = cv2.filter2D(gray, -1, K) # show the output images cv2.imshow(\u0026#34;Original\u0026#34;, gray) cv2.imshow(\u0026#34;{} - convole\u0026#34;.format(kernelName), convolveOutput) cv2.imshow(\u0026#34;{} - opencv\u0026#34;.format(kernelName), opencvOutput) cv2.waitKey(0) cv2.destroyAllWindows() Kết quả:\nTừ trái sang phải: Ảnh gốc, ảnh áp dụng \"average blur\" sử dụng 7x7 kernel convolution, và ảnh áp dụng \"average blur\" sử dụng OpenCV’s cv2.filter2D.\n 1.3 Vai trò của Convolutions trong Deep Learning\nNhư các bạn đã thấy từ phần trước, chúng ta phải tự định nghĩa (manually hand-define) các kernel cho mỗi nhiệm vụ xử lý ảnh khác nhau.\nLiệu có cách nào \u0026ldquo;tự động hóa\u0026rdquo; việc này?\nCNN chính là câu trả lời. Bằng cách sắp xếp nhiều lớp convolutions, kết hợp với \u0026ldquo;activation function\u0026rdquo;, pooling, backpropagation, \u0026hellip; CNNs có khả năng học để cập nhật giá trị của kernel, từ đó trích xuất được các đặc trưng của đối tượng trong ảnh.\nTrong bài tiếp theo, mình sẽ tìm hiểu kỹ hơn về các dạng layers khác nhau, sau đó sẽ đưa ra một số \u0026ldquo;common layer stacking patterns\u0026rdquo; được sử dụng rộng rãi trong lĩnh vực xử lý ảnh.\nSource code sử dụng trong bài này, các bạn có thể tham khảo tại github cá nhân của mình tại github\nTham khảo\n Pyimagesearch Dive into Deep Learning CS231  ","permalink":"https://tiensu.github.io/blog/22_convolutional_neural_network_1/","tags":["Deep Learning","Convolution Neural Network"],"title":"Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 1"},{"categories":["Deep Learning","Neural Network"],"contents":"Trong quá trình tìm hiểu về mạng NN, mình thấy khá là khó hiểu, đặc biệt với các bạn không mạnh về toán. Bài này, mình sẽ diễn giải cách thức làm việc của NN một cách trực quan, dễ hiểu cho các bạn thông qua một ví dụ cụ thể.\n1. Nhắc lại lý thuyết\nGiả sử ta có mạng NN như sau:\n Quá trình training model bao gồm 2 phases:\n1.1 Forward Path\nPhase này tính toán (dự đoán) đầu ra $o_1, o_2$, tính loss.\nGiả sử activation là hàm sigmoid:  Ta sẽ tính lần lượt các đại lượng trung gian:\n $in_{h_1}$: input của $h_1$ $in_{h_2}$: input của $h_2$ $out_{h_1}$: output của $h_1$ $out_{h_2}$: output của $h_2$ $in_{o_1}$: input của $o_1$ $in_{o_2}$: input của $o_2$ $out_{o_1}$: output của $o_1$ $out_{o_2}: output của $o_2$$  Công thức tính của từng đại lượng như sau:\n$in_{h_1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1$\n$in_{h_2} = w_3 * i_1 + w_4 * i_2 + b_1 * 1$\n$out_{h_1} = sigmoid(in_{h_1}) =$$\\frac{1}{1 + e^{-in_{h_1}}}$\n$out_{h_2} = sigmoid(in_{h_2}) =$$\\frac{1}{1 + e^{-in_{h_2}}}$\n$in_{o_1} = w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1$\n$in_{o_2} = w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1$\n$out_{o_1} = sigmoid(in_{o_1}) =$$\\frac{1}{1 + e^{-in_{o_1}}}$\n$out_{o_2} = sigmoid(in_{o_2}) =$$\\frac{1}{1 + e^{-in_{o_2}}}$\n Tiếp theo là tính loss bằng cách so sánh đầu ra của mạng NN với các giá trị thực tế:\n $target_{o_1}$ $target_{o_2}$:  Công thức tính loss như sau:\n$E_{total} = \\sum_{i=1}^2 E_{o_i} = \\sum_{i=1}^2 \\frac{1}{2} (target_{o_i} - out_{o_i})^2 = E_{o_1} + E_{o_2}$\n$E_{o_1} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2$\n$E_{o_2} = \\frac{1}{2} (target_{o_2} - out_{o_2})^2$\n 1.2 Backward Path\nMục đích của phase này là cập nhật trọng số $w$ sao cho tối thiểu hóa loss.\nTa sẽ sử dụng thuật toán tối ưu Stochastic Gradient Descent (SGD) để cập nhật $w$.\nCông thức cập nhật như sau:\n$\\theta = \\theta - \\eta \\nabla_\\theta f(\\theta)$ với:\n $\\nabla_\\theta f(\\theta)$ là đạo hàm của Loss Function tại $\\theta$ (đạo hàm từng phần theo $\\nabla$). $\\eta$ là một số \u0026gt; 0, gọi là learning rate. $\\theta$ là tập hợp các vector các tham số của model cần tối ưu. Trong trường hợp này là các trọng số $w$.  Đạo hàm từng phần của các $w$ tại output layer được tính theo quy tắc chain rule như sau:\n$\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_5}$\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_7} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_7}$/p $\\frac{\\partial E_{total}}{\\partial w_8} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_8}$\n Đạo hàm từng phần của các $w$ tại hidden layer được tính như sau:\n$\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_1}$\n$\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_2}$\n$\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_3}$\n$\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_4}$\n Sau khi tính được đạo hàm từng phần của mỗi $w$, ta áp dụng công thức phía trên để cập nhật $w$.\n2. Ví dụ áp dụng\nVẫn với kiến trúc mạng như trên, ta sẽ gán các giá trị khởi tạo cho các tham số như hình bên dưới:\n Ok, bây giờ ta sẽ bắt đầu đi tính toán.\n2.1 Fordward Path\nInput của $h_1$:\n$in_{h_1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1$\n$in_{h_1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1$\n$in_{h_1} = 0.3775$\n Input của $h_2$:\n$in_{h_2} = w_3 * i_1 + w_4 * i_2 + b_1 * 1$\n$in_{h_2} = 0.25 * 0.05 + 0.3 * 0.1 + 0.35 * 1$\n$in_{h_2} = 0.3925$\n Ouput của $h_1$:\n$out_{h_1} = $$\\frac{1}{1 + e^{-in_{h_1}}}$\n$out_{h_1} = \\frac{1}{1 + e^{-0.3775}}$\n$out_{h_1} = 0.593269992$\n Output của $h_2$:\n$out_{h_2} = $$\\frac{1}{1 + e^{-in_{h_2}}}$\n$out_{h_2} = \\frac{1}{1 + e^{-0.3925}}$\n$out_{h_2} = 0.596884378$\n Input của $o_1$:\n$in_{o_1} = w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1$\n$in_{o_1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1$\n$in_{o_1} = 1.105905967$\n Input của $o_2$:\n$in_{o_2} = w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1$\n$in_{o_2} = 0.5 * 0.593269992 + 0.55 * 0.596884378 + 0.6 * 1$\n$in_{o_2} = 1.224921404$\n Output của $o_1$:\n$out_{o_1} = $$\\frac{1}{1 + e^{-in_{o_1}}}$\n$out_{o_1} = \\frac{1}{1 + e^{-1.105905967}}$\n$out_{o_1} = 0.75136507$\n Output cuat $o_2$:\n$out_{o_2} = $$\\frac{1}{1 + e^{-in_{o_2}}}$\n$out_{o_2} = \\frac{1}{1 + e^{-1.224921404}}$\n$out_{o_2} = 0.772928465$\n Tổng lỗi:\n$E_{o_1} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2$\n$E_{o_2} = \\frac{1}{2} (0.01 - 0.75136507)^2$\n$E_{o_1} = 0.274811083$\n $E_{o_2} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2$\n$E_{o_2} = \\frac{1}{2} (0.01 - 0.772928465)^2$\n$E_{o_2} = 0.023560026$\n $E_{total} = \\sum_{i=1}^2 E_{o_i}$\n$E_{total} = 0.274811083 + 0.023560026$\n$E_{total} = 0.298371109$\n 2.2 Backward Path\nTính đạo hàm từng phần của Loss Function theo mỗi $w$.\nCác $w$ của output layer ($w_5, w_6, w_7, w_8$) có cách tính giống nhau:\n $w_5$:  $\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_5}$\n Ta biết:\n$E_{total} = \\sum_{i=1}^2 \\frac{1}{2} (target_{o_i} - out_{o_i})^2$\n$E_{total} = \\frac{1}{2} (target_{o_1} - out_{o_1})^2 + \\frac{1}{2} (target_{o_2} - out_{o_2})^2$\n Nên:\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1})^{2-1} * (-1) + 0$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(target_{o_1} - out_{o_1})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(0.01 - 0.75136507) = 0.74136507$\n Tiếp theo, vì:\n$out_{o_1} = sigmoid(in_{o_1}) =$ $\\frac{1}{1 + e^{-in_{01}}}$\n Nên:\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.186815602$\n Và, $in_{o_1} = w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1$\n Nên:\n$\\frac{\\partial in_{o_1}}{\\partial w_5}$ $ = out_{h_1}$\n$\\frac{\\partial in_{o_1}}{\\partial w_5}$ $ = 0.593269992$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_5}$$\n$\\frac{\\partial E_{total}}{\\partial w_5}$ $ = 0.74136507 * 0.186815602 * 0.593269992$\n$\\frac{\\partial E_{total}}{\\partial w_5}$ $ = 0.082167041$\n  $w_6$:  $\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n $\\frac{\\partial E_{total}}{\\partial out_{o_1}} = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1})^{2-1} * (-1) + 0$$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(target_{o_1} - out_{o_1})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_1}}$ $ = -(0.01 - 0.75136507) = 0.74136507$\n $\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$ $ = 0.186815602$\n $\\frac{\\partial in_{o_1}}{\\partial w_6} $ $= out_{h_2}$\n$\\frac{\\partial in_{o_1}}{\\partial w_6}$ $ = 0.596884378$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_6}$ $ = 0.74136507 * 0.186815602 * 0.596884378$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= 0.082667628$\n  $w_7$:  $\\frac{\\partial E_{total}}{\\partial w_7} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_7}$\n $\\frac{\\partial E_{total}}{\\partial out_{o_2}} = 0 + 2 * \\frac{1}{2} (target_{o_2} - out_{o_2})^{2-1} * (-1)$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}}$ $ = -(target_{o_2} - out_{o_2})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}}$ $ = -(0.99 - 0.772928465) = -0.217071535$\n $\\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$ $ = out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.175510053$\n $\\frac{\\partial in_{o_2}}{\\partial w_7}$ $ = out_{h_1}$\n$\\frac{\\partial in_{o_2}}{\\partial w_7} $ $= 0.593269992$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.217071535 * 0.175510053 * 0.593269992$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.022602541$\n  $w_8$:  $\\frac{\\partial E_{total}}{\\partial w_8} = \\frac{\\partial E_{total}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial w_8}$\n $\\frac{\\partial E_{total}}{\\partial out_{o_2}} = 0 + 2 * \\frac{1}{2} (target_{o_2} - out_{o_2})^{2-1} * (-1)$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}} $ $= -(target_{o_2} - out_{o_2})$\n$\\frac{\\partial E_{total}}{\\partial out_{o_2}} $ $= -(0.99 - 0.772928465) = -0.217071535$\n $\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial out_{o_2}}{\\partial in_{o_2}} $ $= 0.175510053$\n $\\frac{\\partial in_{o_2}}{\\partial w_8} $ $= out_{h_2}$\n$\\frac{\\partial in_{o_2}}{\\partial w_8} $ $= 0.596884378$\n Tổng hợp lại ta được:\n$\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial w_6}$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.217071535 * 0.175510053 * 0.596884378$\n$\\frac{\\partial E_{total}}{\\partial w_6} $ $= -0.022740242$\n Các $w$ của hidden layer ($w_1, w_2, w_3, w_4$) có cách tính giống nhau:\n $w_1$:  $\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_1}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = w_5$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = 0.4$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}}$ $ = 0.138498562 * 0.4$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} $ $= 0.055399425$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$ $ = w_7$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} $ $= 0.5$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.038098237 * 0.5$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.019049118$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_1}} $$= 0.055399425 + (-0.019049118) = 0,036350307$\n ----------------------\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_1}}})}{\\partial in_{h_1}}$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} $ $= out_{h_1}(1 - out_{h_1})$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}}$ $ = 0.59326999(1 - 0.59326999) = 0.241300709$\n ----------------------\n$\\frac{\\partial in_{h_1}}{\\partial w_1} = \\frac{\\partial (w_1 * i_1 + w_2 * i_2 + b_1 * 1)}{\\partial w_1}$\n$\\frac{\\partial in_{h_1}}{\\partial w_1}$ $ = i_1$\n$\\frac{\\partial in_{h_1}}{\\partial w_1} $ $= 0.05$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_1}$\n$\\frac{\\partial E_{total}}{\\partial w_1} $ $= 0.036350306 * 0.241300709 * 0.05$\n$\\frac{\\partial E_{total}}{\\partial w_1} $ $= 0.000438568$\n  $w_2$:  $\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_2}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = w_5$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$ $ = 0.4$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}}$ $ = 0.138498562 * 0.4$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_1}} $ $= 0.055399425$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_1}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_1}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}}$ $ = w_7$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_1}} $ $= 0.5$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.038098237 * 0.5$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$ $ = -0.019049118$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_1}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_1}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_1}} $$= 0.055399425 + (-0.019049118) = 0,036350307$\n ----------------------\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_1}}})}{\\partial in_{h_1}}$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}} $ $= out_{h_1}(1 - out_{h_1})$\n$\\frac{\\partial out_{h_1}}{\\partial in_{h_1}}$ $ = 0.59326999(1 - 0.59326999) = 0.241300709$\n ----------------------\n$\\frac{\\partial in_{h_1}}{\\partial w_2} = \\frac{\\partial (w_1 * i_1 + w_2 * i_2 + b_1 * 1)}{\\partial w_2}$\n$\\frac{\\partial in_{h_1}}{\\partial w_2}$ $ = i_2$\n$\\frac{\\partial in_{h_1}}{\\partial w_2} $ $= 0.1$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial out_{h_1}} * \\frac{\\partial out_{h_1}}{\\partial in_{h_1}} * \\frac{\\partial in_{h_1}}{\\partial w_2}$\n$\\frac{\\partial E_{total}}{\\partial w_2} $ $= 0.036350306 * 0.241300709 * 0.1$\n$\\frac{\\partial E_{total}}{\\partial w_2} $ $= 0.000877135$\n  $w_3$:  $\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_3}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = w_6$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = 0.45$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}}$ $ = 0.138498562 * 0.45$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} $ $= 0.062324353$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$ $ = w_8$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} $ $= 0.55$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.038098237 * 0.55$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.02095403$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_2}} $$= 0.062324353 + (-0.02095403) = 0.041370323$\n ----------------------\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_2}}})}{\\partial in_{h_2}}$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} $ $= out_{h_2}(1 - out_{h_2})$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}}$ $ = 0.596884378(1 - 0.596884378) = 0.240613417$\n ----------------------\n$\\frac{\\partial in_{h_2}}{\\partial w_3} = \\frac{\\partial (w_3 * i_1 + w_4 * i_2 + b_1 * 1)}{\\partial w_3}$\n$\\frac{\\partial in_{h_2}}{\\partial w_3}$ $ = i_1$\n$\\frac{\\partial in_{h_2}}{\\partial w_3} $ $= 0.05$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_3}$\n$\\frac{\\partial E_{total}}{\\partial w_3} $ $= 0.041370323 * 0.240613417 * 0.05$\n$\\frac{\\partial E_{total}}{\\partial w_3} $ $= 0.000497713$\n  $w_4$:  $\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_4}$\n--------------------------------------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n----------------------------------------------------\n $\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n---------------------------------\n  $\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial E_{o_1}}{\\partial out_{o_1}} * \\frac{\\partial out_{o_1}}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} = \\frac{\\partial (\\frac{1}{2}(target_{o_1} - out_{o_1})^2)}{\\partial out_{o_1}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_1}}})}{\\partial in_{o_1}}$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $$ = 2 * \\frac{1}{2} (target_{o_1} - out_{o_1}) * (-1) * out_{o_1}(1 - out_{o_1})$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}}$ $ = (0.01 - 0.75136507) * (-1) * 0.75136507(1 - 0.75136507)$\n$\\frac{\\partial E_{o_1}}{\\partial in_{o_1}} $ $= 0.138498562$\n-------------------------\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial (w_5 * out_{h_1} + w_6 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = w_6$\n$\\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$ $ = 0.45$\nGộp lại:\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}}$ $ = 0.138498562 * 0.45$\n$\\frac{\\partial E_{o_1}}{\\partial out_{h_2}} $ $= 0.062324353$\n----------------------------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_2}} * \\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$\n---------------------------------\n $\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial E_{o_2}}{\\partial out_{o_2}} * \\frac{\\partial out_{o_2}}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} = \\frac{\\partial (\\frac{1}{2}(target_{o_2} - out_{o_2})^2)}{\\partial out_{o_2}} * \\frac{\\partial (\\frac{1}{1 + e^{-in_{o_2}}})}{\\partial in_{o_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $$ = 2 * \\frac{1}{2} (target_{o_2} - out_{o_2}) * (-1) * out_{o_2}(1 - out_{o_2})$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}}$ $ = (0.99 - 0.772928465) * (-1) * 0.772928465(1 - 0.772928465)$\n$\\frac{\\partial E_{o_2}}{\\partial in_{o_2}} $ $= -0.038098237$\n-------------------------\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial (w_7 * out_{h_1} + w_8 * out_{h_2} + b_2 * 1)}{\\partial out_{h_2}}$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}}$ $ = w_8$\n$\\frac{\\partial in_{o_2}}{\\partial out_{h_2}} $ $= 0.55$\nGộp lại:\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_2}}{\\partial in_{o_1}} * \\frac{\\partial in_{o_1}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.038098237 * 0.55$\n$\\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$ $ = -0.02095403$\n---------------------------------\n $\\frac{\\partial E_{total}}{\\partial out_{h_2}} = \\frac{\\partial E_{o_1}}{\\partial out_{h_2}} + \\frac{\\partial E_{o_2}}{\\partial out_{h_2}}$\n$\\frac{\\partial E_{total}}{\\partial out_{h_2}} $$= 0.062324353 + (-0.02095403) = 0.041370323$\n ----------------------\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} = \\frac{\\partial (\\frac{1}{1 + e^{-in_{h_2}}})}{\\partial in_{h_2}}$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}} $ $= out_{h_2}(1 - out_{h_2})$\n$\\frac{\\partial out_{h_2}}{\\partial in_{h_2}}$ $ = 0.596884378(1 - 0.596884378) = 0.240613417$\n ----------------------\n$\\frac{\\partial in_{h_2}}{\\partial w_4} = \\frac{\\partial (w_3 * i_1 + w_4 * i_2 + b_1 * 1)}{\\partial w_3}$\n$\\frac{\\partial in_{h_2}}{\\partial w_4}$ $ = i_2$\n$\\frac{\\partial in_{h_2}}{\\partial w_4} $ $= 0.1$\n --------------------------------------------------------------\n$\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial out_{h_2}} * \\frac{\\partial out_{h_2}}{\\partial in_{h_2}} * \\frac{\\partial in_{h_2}}{\\partial w_4}$\n$\\frac{\\partial E_{total}}{\\partial w_4} $ $= 0.041370323 * 0.240613417 * 0.1$\n$\\frac{\\partial E_{total}}{\\partial w_4} $ $= 0.000995425$\n Đến đây ta đã tính xong các đạo hàm từng phần theo các $w$. Áp dụng SGD để cập nhật các $w$ ta được (chọn $\\eta = 0.9$):\n$w_5^+ = w_5 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_5}$\n$w_5^+ = 0.4 - 0.9 * 0.082167041$\n$w_5^+ = 0.326049663$\n-------------------------\n $w_6^+ = w_6 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_6}$\n$w_6^+ = 0.45 - 0.9 * 0.082667628$\n$w_6^+ = 0.375599135$\n-------------------------\n $w_7^+ = w_7 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_7}$\n$w_7^+ = 0.5 - 0.9 * (-0.022602541)$\n$w_7^+ = 0.520342287$\n-------------------------\n $w_8^+ = w_8 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_8}$\n$w_8^+ = 0.55 - 0.9 * (-0.022740242)$\n$w_8^+ = 0.570466218$\n-------------------------\n $w_1^+ = w_1 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_1}$\n$w_1^+ = 0.15 - 0.9 * 0.000438568$\n$w_1^+ = 0.149605289$\n-------------------------\n $w_2^+ = w_2 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_2}$\n$w_2^+ = 0.2 - 0.9 * 0.0080877135$\n$w_2^+ = 0.192721058$\n-------------------------\n $w_3^+ = w_3 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_3}$\n$w_3^+ = 0.25 - 0.9 * 0.000497713$\n$w_3^+ = 0.249552058$\n-------------------------\n $w_4^+ = w_4 - \\eta * $$\\frac{\\partial E_{total}}{\\partial w_4}$\n$w_4^+ = 0.3 - 0.9 * 0.000995425$\n$w_4^+ = 0.299104118$\n-------------------------\n Phù, như vậy là chúng ta đã cập nhật xong giá trị mới cho các trọng số $w$. Đây là những phép toán xảy ra trong mỗi lần cập nhật khi training model. Hi vọng, thông qua ví dụ trong bài này, các bạn đã có thể hiểu rõ hơn bản chất của mạng NN. Hẹn gặp lại các bạn trong các bài tiếp theo!\n3. Tham khảo\n Mattmazur Dive into Deep Learning Wikipedia  ","permalink":"https://tiensu.github.io/blog/21_neural_network_fundamentals_2/","tags":["Deep Learning","Neural Network"],"title":"Neural Network cơ bản (Phần 2)"},{"categories":["Deep Learning","Neural Network"],"contents":"Trong bài này chúng ta sẽ cùng nhau tìm hiểu lys thuyết cơ bản về mạng thần kinh nhân tạo (neural network):\n Cấu trúc của neural network. Thuật toán lan truyền (propagation) và lan truyền ngược (backpropagation).  Những kiến thức trong bài này sẽ là tiền đề để các bạn tiến xa hơn trong thế giới của Deep Learning.\n1. Neural Network là gì?\nNeural Networks là các khối (blocks) để xây dựng lên các hệ thống Deep Learning. Chúng ta sẽ bắt đầu với việc xem xét ở mức \u0026quot;high-level\u0026quot; của Neural Network, bao gồm cả mối liên hệ của chúng với não bộ của con người.\nTrong thực tế, có rất nhiều những công việc rất khó để có thể thực hiện tự động hóa bởi máy móc nhưng lại rất dễ dàng đối với các loài động vật (bao gồm cả con người). Những công việc đó thường liên quan đến việc nhận diện, phân loại đối tượng.\nVí dụ:\n Con chó của gia đình bạn có thể phân biệt được người quen (người trong gia đình bạn) và người lạ (không phải trong gia đình bạn)? Một đứa trẻ có thể nhận biết được sự khác nhau giữa xe oto con và xe oto tải.  Tại sao con chó và đứa trẻ có thể làm được những việc đó?\nCâu trả lời nằm ở cấu tạo bên trong não bộ của chúng. Não bộ của cả 2 đều chứa một mạng thần kinh sinh học kết nối đến hệ thần kinh trung tâm. Mạng này được tạo ra bởi rất nhiều các neurons kết nối với nhau.\nTừ neural là dạng tính từ của neuron, và network ngầm chỉ kiến trúc \u0026quot;graph\u0026quot; của hệ thần kinh. Do vậy, một Artificial Neural Network (ANN) là một hệ thống tính toán, cố gắng mô phỏng (bắt chước) mạng thần kinh sinh học của các loài động vật. ANN là một graph có định hướng, nó bao gồm các nodes và các connections (kết nối giữa các nodes). Mỗi node thực hiện một tính toán đơn giản nào đó, mỗi connection mang một tín hiệu từ node này đến node khác. Những tín hiệu này đi kèm theo một trọng số (weight) chỉ ra mức độ khuyếch đại hoặc giảm bớt cường độ tín hiệu đó. Giá trị weight càng lớn chứng tỏ tín hiệu đi kèm càng quan trọng đối với kết quả đầu ra và ngược lại.\nHình dưới đây là một ANN đơn giản, bao gồm một lớp input ở đầu, 2 lớp ở giữa (hidden layers) và một lớp output ở cuối. Mỗi connection mang theo một tín hiệu xuyên qua hai hidden layers. Kết quả cuối cùng được tính toán tại lớp output.\n 2. Artificial Models\nHãy xem thử một ANN cơ bản như hình bên dưới:\n ANN này thực hiện tính tổng có trọng số ($w_i$) của các input vectors($x_i$). Trong thực tế, các input vectors có thể là pixcel của images, hay các rows của một dataset dạng tabular.\nMỗi $x_i$ kết nối với một neuron thông qua vector trọng số $w_i$.\nDiễn giải bằng công thức toán học thì output của ANN này sẽ là:\n $y = f(w_1x_1 + w_2x_2 + ... + w_nx_n)$ $y = f(\\sum_{i=1}^n w_ix_i)$ $y = f(net)$. Với net = $\\sum_{i=1}^n w_ix_i$  Nói chung, dù diễn đạt theo cách nào đi nữa thì ý tưởng chung vẫn là áp dụng hàm activate (f) vào tổng có trọng số của các input vectors.\n3. Activation Functions\n a) Step function  Hàm activation đơn giản nhất có lẽ là Step function. Hàm này được sử dụng bởi thuật toán Perceptron (sẽ đề cập ở phần sau).\n Hàm này luôn nhận giá trị 1 nếu $\\sum_{i=1}^n$ $w_i$$x_i$ \u0026gt;= 0 và nhận giá trị 0 trong trường hợp còn lại.\n Vấn đề của step function là nó giá trị của nó không có sự khác biệt khi net \u0026gt;=0 hoặc net \u0026lt; 0. Điều này có thể dẫn đến một số vấn đề khi huấn luyện neural network.\n b) Sigmoid function   y = tf.nn.sigmoid(x) d2l.plot(x.numpy(), y.numpy(), \u0026#39;x\u0026#39;, \u0026#39;sigmoid(x)\u0026#39;, figsize=(5, 2.5))  So với step function, sigmoid function có một số ưu điểm sau:\n Giá trị của nó liên tục và phân biệt nhau tại một nơi. Đồ thị của nó đối xứng qua trục y.  Tuy nhiên, có 2 nhược điểm lớn nhất của sigmoid function là:\n Output của nó không tập trung quanh điểm gốc tọa độ. Càng xa gốc tọa độ, giá trị của nó tiệm cận với giá trị bão hòa. Điề u này vô tình triệt tiêu gradient, vì delta của gradient vô cùng nhỏ.  Đạo hàm của sigmoid function như sau:\n with tf.GradientTape() as t: y = tf.nn.sigmoid(x) d2l.plot(x.numpy(), t.gradient(y, x).numpy(), \u0026#39;x\u0026#39;, \u0026#39;grad of sigmoid\u0026#39;, figsize=(5, 2.5))   c) Tanh function  Hàm này giải quyết được nhược điểm thứ nhất của sigmoid function.\n y = tf.nn.tanh(x) d2l.plot(x.numpy(), y.numpy(), \u0026#39;x\u0026#39;, \u0026#39;tanh(x)\u0026#39;, figsize=(5, 2.5))   d) ReLU (Rectified Linear Unit) funtion   Hàm này nhận giá trị 0 khi inputs \u0026lt; 0, nhưng sẽ tăng tuyến tính khi inputs \u0026gt;= 0.\nThực tế chứng minh, ReLU function hoạt động tốt hơn hẳn so với các hàm kể trên. Bắt đầu từ năm 2015, nó được sử dụng thường xuyên trong Deep Learning.\nx = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32) y = tf.nn.relu(x) d2l.plot(x.numpy(), y.numpy(), \u0026#39;x\u0026#39;, \u0026#39;relu(x)\u0026#39;, figsize=(5, 2.5))  ReLU function vẫn có nhược điểm. Khi inputs \u0026lt; 0, nó nhận giá trị 0. Như vậy thì không thể tính được gradient tại những điể m đó. Thực tế thì cũng hiếm khi có trường hợp nào mà inputs lại có giá trị \u0026lt; 0. Tuy nhiên, để giải quyết triệt để vấn đề thì lại sinh ra một hàm mới:\n e) Leaky ReLU function   Đây là một biến thể của ReLU funtion, nó nhận một giá trị khác 0 (thường rất nhỏ) khi inputs \u0026lt; 0. Giá trị của $\\alpha$ rất nhỏ và được cập nhật trong quá trình huấn luyện neural network.\n  f) ELU (Exponential Linear Units) function   Khác với ReLU function, giá trị của $\\alpha$ trong ELU function được cố định từ đầu (lúc xây đựng kiến trúc mạng). Giá trị thông thường của $\\alpha$ là 1.\n Nên sử dụng activation function nào?\nViệc có nhiều hơn 1 activation function đôi khi làm cho bạn bối rối khi lựa chọn sử dụng cái nào, không sử dụng cái nào?\nLời khuyên của mình như sau:\n Bắt đầu với ReLU để đặt được một baseline accuracy. (Hầu hết các public papers đều làm như vậy) Thử chuyển qua sử dụng các biến thể của ReLU: Leaky ReLU, ELU.  Trong các dự án thực tế thì mình thường làm theo các bước:\n Sử dụng ReLU Tune các hyper-parameters khác: architecture, learning rate, regularization strength, ... Ghi lại các giá trị accuracy. Một khi đã tương đối thoả mãn về accuracy, chuyển qua ELU. Độ chính xác thường sẽ tăng khoảng 1-5% tùy thuộc dataset.  Cách này chỉ là kinh nghiệm cá nhân của mình, và không có gì đảm bảo đúng trong mọi trường hợp. Bạn có thể tham khảo hoặc không. Hãy luôn nhớ thử-sai mọi khả năng có thể cho bài toán của bạn.\n4. Feedfoward Network Architecture\nKiến trúc ANN thì có rất nhiều, nhưng phổ biến nhất là dạng feedfoward network.\n Trong kiến trúc này, một connection giữa 2 nodes chỉ được phép đi theo chiều từ layer $i$ tới layer $i+1$ (vì thế mà có tên feedfoward). Không có chiều từ layer $i+1$ đến layer $i$ hoặc bất kỳ chiều nào khác. Khi có thêm chiều từ layer $i+1$ đến layer $i$ (feedback connection) thì ta được kiến trúc RNN (Recurrent Neural Network). Feedfoward network được sử dụng chủ yếu trong các bài toán về Computer Vision (mạng CNN là một ví dụ điển hình) , trong khi feedback network lại được sử dụng chủ yếu trong các bài toán về Natural Language Processing.\nANN được sử dụng cho cả 3 dạng bài toán: supervised, unsupervised, and semi-supervised. Một số ví dụ điển hình là classification, regression, clustering, vector quantization, pattern association, ...\nTrong bài tiếp theo, mình sẽ minh họa cách thức cập nhật trong số của mạng ANN thông qua một ví dụ rất dễ hiểu. Mời các bạn đón đọc.\n3. Tham khảo\n Pyimagesearch Dive into Deep Learning Wikipedia  ","permalink":"https://tiensu.github.io/blog/20_neural_network_fundamentals_1/","tags":["Deep Learning","Neural Network"],"title":"Neural Network cơ bản (Phần 1)"},{"categories":["Deep Learning"],"contents":"Bài viết này nhằm mục đích tổng hợp, tóm tắt lại các thuật toán của Deep Learning, giúp bạn đọc có cái nhìn toàn cảnh và hiểu rõ hơn về Deep Learning.\n1. Deep Learning (DL) là gì?\nTheo Wikipedia: \u0026ldquo;Deep learning (also known as deep structured learning or differential programming) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised\u0026rdquo;.\nHiểu theo một cách khác thì DL là một tập hợp các thuật toán mô phỏng lại cách thức hoạt động của bộ não của con người trong việc phân tích dữ liệu, nhằm tạo ra các models cái mà được sử dụng cho việc đưa ra quyết định dựa trên dữ liệu đầu vào. Về bản chất, DL bao gồm nhiều layers, mỗi layer có thể coi là một mạng Neural Network (NN).\nGiống như não bộ của con người, NN có chứa các neurons. Mỗi neurons nhận các tín hiệu ở đầu vào, nhân chúng với các trọng số (weights), tổng hợp chúng lại, và sau cùng là áp dụng các hàm kích hoạt (thường là non-linear) lên chúng. Các neurons được sắp xếp thành các layers liên tiếp nhau (stack).\nCác kiến trúc mạng DL đều sử dụng thuật toán Backpropagation để tính toán và cập nhật các trọng số của nó thông qua quá trình huấn luyện. Về mặt ý tưởng, dữ liệu được đưa vào mạng DL, sinh ra output, so sánh output với giá trị thực tế (sử dụng loss function) rồi điều chỉnh trọng số theo kết quả so sánh đó.\nViệc điều chỉnh trọng số là một quá trình tối ưu, thường sử dụng thuật toán SGD. Ngoài SGD, một số thuật toán khác cũng hay được sử dụng. Đó là Adam, Radam, SRMprop.\n2. Các thuật toán DL\n 2.1 Convolution Neural Network (CNN)\nCNN sử dụng phép toán tích chập để tạo liên kết giữa các layers trong mạng NN. Mỗi neuron, thay vì kết nối đến tất cả các neurons khác thì sẽ chỉ kết nối đến một vài neurons đại diện. Điều này giúp cho CNN học được các mối liên hệ không gian của dữ liệu tốt hơn.\nĐó là lý do mà vì sao CNN lại được sử dụng rất phổ biến trong các bài toán về Computer Vision như phân loại hình ảnh, phát hiện đối tượng trong ảnh/video, \u0026hellip;\nMột vài kiến trúc mạng CNN kinh điển có thể kể đến như: VGG, ResNet, MobiNet, InceptionNet, Yolo, SSD, \u0026hellip;\n 2.2 Recurrent Neural Network (RNN)\nRNN là thuật toán rất phù hợp với các loại dữ liệu có mối liên hệ về thời gian như time serial forecasting hay trong các bài toán về xử lý ngôn ngữ tự nhiên (NLP), bởi vì một phần output của nó ở thời điể m này được đưa trở lại thành input ở thời điểm tiếp theo. Do cách thức làm việc đặc biệt như vậy mà nó có có năng ghi nhớ được thông tin trong quá khứ và sử dụng những thông tin đó vào việc dự đoán kết quả.\nMột số kiến trúc mạng kinh điển của RNN có thể kể đến như: GRU, LSTM, \u0026hellip;\n 2.3 AutoEncoders\nAutoEncoders là kiến trúc mạng bao gồm 2 thành phần: Encoder và Decoder. Encoder nhận input và mã hóa nó thành các vector trong một không gian có số chiều ít hơn. Decoder sử dụng các vertors đó để giải mã (xây dựng lại) nó thành dữ liệu ban đầu.\nAutoEncoders sử dụng chủ yếu trong các bài toán về giảm chiều dữ liệu, nén dữ liệu, \u0026hellip;\n 2.4 Generative Adversarial Networks (GAN)\nGAN cũng bao gồm 2 thành phần: Generator và Discriminator. Generator sinh ra dữ liệu giả và Discriminator sẽ cố gắng phân biệt dữ liệu mới sinh ra đó là giả hay thật. Hai thành phần này được huấn luyện cùng với nhau, chúng sẽ cạnh tranh nhau để cuối cùng tạo ra được model GAN tốt nhất.\nGAN được dùng để sinh ra dữ liệu mới từ dữ liệu ban đầu, ví dụ: sinh ra các mẫu thời trang mới, các mẫu nhân vật hoạt hình mới, \u0026hellip;\n 2.5 Transformer\nTransformer là kiến trúc mạng mới được phát triển và được sử dụng rộng rãi gần đây trong các bài toán NLP khi nó chứng minh được sự vượt trội của nó so với RNN. Có được này là nhờ vào một cơ chế, gọi là Attention, tức là chỉ tập trung và một số vị trí cụ thể thay vì toàn bộ vị trí trong dữ liệu.\nTransformer cũng bao gồm một số bộ encoders và một số bộ decoders được đặt cạnh nhau (stacked) cùng với các lớp attentions.\n Hiện nay, BERT và GPT-3 là 2 pre-trained model của transformer được sử dụng rất nhiều trong lĩnh vực NLP.\n2.6 Graph Neural Network (GNN)\nDL nói chung thường làm việc với dữ liệu có cấu trúc. Tuy nhiên, trong thực tế có rất nhiều dữ liệu ở dạng phi cấu trúc (unstructed data) và được sắp xếp dưới dạng graph. Ví dụ như là mạng xã hội, cấu tạo phân tử trong hóa học, \u0026hellip;\nGNN chính là mạng NN mô hình hóa graph data. Chúng sẽ nhận diện các mối liên kết của các nodes trong graph và output ra các vector đặc trưng của dữ liệu đó, giống như embedding. Output này được sử dụng làm input cho các NN khác.\n3. Các thuật toán DL trong các bài toán Computer Vision \u0026amp; Natural Language Processing\n3.1 DL trong CV\n  Image Classification  Đây là bài toán nhận diện đối tượng trong ảnh thuộc về class nào. Số lượng class có thể là 2 (binary) hoặc nhiều hơn (multiple).\nCác kiến trúc mạng kinh điển như VGG (VGG16, VGG19), ResNet, InceptionNet, MobiNet, AlexNet \u0026hellip; giải quyết rất tốt bài toán này.\n Object Detection  Đây là bài toán mở rộng của image classification vì không chỉ phân loại mà còn phải định vị được đối tượng trong ảnh.\nCác kiến trúc NN giải quyết vấn đề này bao gồm: Họ R-CNN (R-CNN, Fast R-CNN, Faster R-CNN), Yolo (YOLOv2, YOLOv3, YOLOv4), SSD, EfficientNet, \u0026hellip;\n  Semantic Segmentation  Nếu như Image Classification là bài toán gán nhãn cho toàn bộ bức ảnh thì Semantic Segmentation có thể coi là bài toán gán nhãn cho từng pixcel trong ảnh đó. Tất cả các pixel thuộc cùng một class (hay các objects thuộc cùng một class) được coi như là một thực thể (thể hiện bằng màu sắc giống nhau).\n Một số kiến trúc mạng như Full Connected Networks FCN, UNET, \u0026hellip; giải quyết tốt bài toán này.\n Instance Segmentation  Cũng giống như Semantic Segmentation là gán nhãn cho từng pixel trong ảnh, tuy nhiên Instance Segmentation coi mỗi đối tượng (cùng hoặc khác class) là các thực thể riêng biệt (thể hiện bằng màu sắc khác nhau).\n Kiến trúc Mask_RCNN nổi tiếng nhất cho viêc giải quyết bài toán này.\n Face Recognition  Đây là bài toán nhận diện khuôn mặt của người trong video/ảnh. Nó bao gồm 2 công đoạn: - Đầu tiên là phát hiện vùng chứa khuôn mặt (bài toán Object Detection), - Sau đó nhận diện xem khuôn mặt đó là của ai (bài toán classification)\nMột số thuật toán nổi tiếng giải quyết cho công đoạn 2 là: FaceNet, VGGFace, MTCNN, \u0026hellip; Còn đối với công đoạn 1 có thể sử dụng OpenCV.\n  Optical Character Recogniton (OCR)  Đây là bài toán nhận diện ký tự trong hình ảnh. Tương tự như bài toán Face Detection, nó cũng bao gồm 2 công đoạn: - Phát hiện vùng chứa ký tự trong ảnh (bài toán object detection) - Nhận diện từng ký tự trong vùng chứa đó.\nMột số thuật toán nổi bật là: - Công đoạn 1: Craft, Yolo, \u0026hellip; - Công đoạn 2: CRNN, \u0026hellip;\nMột số thư viện hỗ trợ bài toán OCR: Tesseract, EasyOCR\n  Humand Pose  Humand Pose hay Pose Estimation là bài toán xác định vị trí của các khớp nối của con người trong ảnh/video.\n PostNet là thuật toán nối tiếng cho bài toán này.\n3.2 DL trong NPL\n Các thuật toán Word Embedding  Word Embedding là quá trình biến đổi các từ thành các vector đại diện, làm đầu vào cho các DL model. Việc biến đổi này có tính đến hoàn cảnh và ngữ nghĩa của từ đó trong câu. Một số thuật toán phổ biến:\na. Word2Vec: Nó hoạt động theo theo 1 trong 2 cách, dự đoán từ dựa vào những tù xung quanh của nó (CBOW) hoăc dự đoán các từ xung quanh của 1 từ (Skip-Gram). Các từ được đưa vào Word2Vec theo dạng One-hot-encoding.\n b. Glove: Là một mô hình khác mở rộng ý tưởng của Word2Vec bằng cách kết hợp nó với các kỹ thuật phân tích thừa số ma trận.\n c. Contextual Word Embeddings: Sử dụng 2 layers bi-directional LSTM để embedding các từ, cho phép tận dụng sự phụ thuộc của nó với các từ trước đó.\nd. Transformer: Đây là phương pháp tiên tiến nhất hiện nay, cho phép tập trung vào một vài vị trí cụ thể của từ trong câu thay vì toàn bộ như khi sử dụng LSTM.\n  Sequence Modeling  Mô hình này giải quyết hầu hết các bài toán trong NLP như Machine Translation, Speech Recognition, Autocompletion và Sentiment Classification. Nó có khả năng xử lý cả một chuỗi đầu vào thay vì từng từ một.\n Còn rất nhiều kiến trúc, thuật toán, model nữa mà trong bài này chưa thể kể hết được. Nhưng hi vọng qua đây, các bạn cũng có được cái nhìn tổng quát về các thuật toán, model, kiến trúc phổ biến trong các bài toán DL.\nTrong các bài viết tiếp theo đi chi tiết vào một số thụât toán với các ứng dụng cụ thể. Mời các bạn đón đọc!\n4. Tham khảo\n Dive Into Deep Learning AI Summer  ","permalink":"https://tiensu.github.io/blog/19_deep_learning_algorithms_summary/","tags":["Deep Learning"],"title":"Tổng hợp các thuật toán Deep Learning"},{"categories":["Machine Learning","Data Science"],"contents":"Bạn thường nghe nói Data Scientist là nghê sexy nhất thế kỷ 21, với mức lương cao ngất ngưởng, tạo ra những sản phầm có tầm ảnh hưởng lớn, được mọi người ngưỡng mộ, blabla. Và thế là bạn quyết định chuyển hướng sang học làm data scientist. Bạn lao vào học toán (đại số tuyến tính, xác suất thống kê, đạo hàm, tích phân, \u0026hellip;), học lập trình (python, R, \u0026hellip;), học các thuật toán ML, cách sử dụng các thư viện ML. Thậm chí có bạn còn chơi lớn, học luôn cao học về nghành này vì \u0026ldquo;hình như\u0026rdquo; ngành này yêu cầu phải có trình độ cao học trở lên (bản thân mình chính là 1 ví dụ của trường hợp này, :D). OK, mình không có ý kiến gì về lựa chọn của bạn cả. Trong bài viết này, mình chỉ muốn chia sẻ một số điều mà mình cảm nhận được sau một thời gian làm trong nghành với danh xưng data scientist này. Những điều này là sự khác biệt giữa lý thuyết các bạn học được và thực tế công việc mà các bạn sẽ trải qua. Bạn sẽ tìm được cho mình câu trả lời cho câu hỏi \u0026ldquo;Thế nào là một data scientist giỏi? Và thế nào là một data scientist xuất sắc?\u0026quot;.\n1. \u0026ldquo;Tóm lại, câu chuyện bạn muốn kể là gì?\u0026quot;\nMột trong những câu hỏi tôi thường nghe sau mỗi lần kết thúc buổi seminar của CEO (hoặc một người nào đó ở vị trí tương đương) là:\n\u0026ldquo;Tóm lại, câu chuyện bạn muốn kể là gì?\u0026quot;\nThú thực, lần đầu tiên khi nghe câu hỏi này, tôi có phần bối rối. Tôi không hiểu tại sao họ lại nhấn mạnh vào \u0026ldquo;câu chuyện\u0026rdquo;? Tại sao họ không hỏi:\n Những gì tôi đã trình bày trong slide. Kết quả tôi đạt được là gì. Tôi đã làm như thế nào. \u0026hellip;  Trước khi thực sự hiểu được sự quan trọng của kỹ năng kể chuyện (telling story), tôi đã trải qua khá nhiều sai lầm \u0026hellip;\nHoặc là stackeholders (thường là những người non-techical) không hiểu những gì bạn đang nói, đang trình bày, hoặc là nội dung bạn đang truyền tải chưa đủ để thuyết phục họ, thúc đẩy họ đưa ra quyết định hay hành động cụ thể nào (take actions) \u0026hellip;\nVà tôi quyết định cải thiện kỹ năng kể chuyện của mình \u0026hellip;\nKhi tôi đã đạt được sự tiến bộ nhất đinh, rất nhiều thứ đã thay đổi \u0026hellip;\nStackeholders đã bắt đầu hiểu những gì mà tôi đang diễn giải. Và họ đã có những hành động cụ thể \u0026hellip;\nNếu bạn muốn trở thành môt data scientist giỏi, hãy tập trung vào kiến thức kỹ thuật.\nCòn nếu bạn muốn trở thành một data scientist xuất sắc, hãy tập trung vào kỹ năng kể chuyện.\nVậy \u0026hellip; làm thế nào để học kỹ năng kể chuyện? Lời khuyên của tôi là hãy học từ Vox. Họ là những bậc thầy về kỹ năng này, họ luôn luôn có thể giải thích bất kì vấn đề phức tạp nào một cách rất đơn giản và dễ hiểu. Bạn có thể xem thử video bên dưới của họ. Hãy quan sát cách họ giải thích các hiện tượng và vấn đề xã hội theo cách kể chuyện trực quan nhất để ai ai cũng có thể hiểu được.\n  Tóm tắt lại một vài lời khuyên rất hay cho chúng ta để nâng cao kỹ năng kể chuyện. Những điều này không chỉ đúng trong phạm vi data science mà nó còn đúng cho tất cả các lĩnh vực khác.\n𝟏) Lãnh đạo có ít thời gian hơn chúng ta:\nVới vai trò lãnh đạo cấp cao, họ phải xử lý nhiều việc quan trọng hơn.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Hãy đi thẳng vào vấn đề chính Trình bày ngắn gọn, có cấu trúc rõ ràng Trình bày xong, hãy đưa ra lời kiến nghị hành động với họ.  𝟐) Lãnh đạo dễ mất kiên nhẫn\nLãnh đạo cấp cao có quá nhiều việc cần phải xử lý một lúc và vì vậy họ dễ mất kiên nhẫn.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Cho họ thông tin tổng quát là bạn sẽ trình bày trong bao lâu. Nếu bạn trình bày trong 30 phút, thì bạn hãy rút ngắn thời gian nói xuống 15 phút, 15 còn lại để hỏi đáp. Đó là nguyên tắc 50:50.  𝟑) Chào đón sự cắt ngang\nNhững khán giả cấp cao thường xuyên ngắt ngang giữa chừng những bài thuyết trình.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Chuẩn bị bài với cấu trúc rõ ràng để khi “được chen ngang”, bạn nắm rõ là đang nói phần nào.  𝟒) Luôn có các slide phụ lục Nhà lãnh đạo cấp cao cần nghe một bức tranh tổng thể.\n𝑳𝒐̛̀𝒊 𝒌𝒉𝒖𝒚𝒆̂𝒏:\n Thuyết trình tổng thể về ý tưởng theo một cấu trúc rõ ràng có chủ đích. Chuẩn bị sẵn các số liệu và phần “phụ lục”. Khi trình bày xong, chia sẻ thêm các số liệu này, hoặc mở lên khi có câu hỏi.  2. Data sạch ư? Mơ đi nhé!\nBạn tự hào rằng mình đã từng tham gia rất nhiều cuộc thi trên Kaggle, thâm chí còn leo lên top 10, top 5. Hãy quên điều đó đi, vì các dự án trong thực tế không \u0026ldquo;màu hồng\u0026rdquo; như vậy đâu. Trên Kaggle, bạn đuợc cung cấp sẵn bộ dữ liệu tương đối đầy đủ và sạch sẽ, còn trên thực tế thì hầu như bạn chỉ được đưa cho một mớ bòng bong hỗ độn. Khách hàng bảo, \u0026ldquo;đấy, bọn tao chỉ có như này, mày dùng như nào thì dùng\u0026rdquo;. Trường hợp xấu hơn, chúng ta bắt đầu dự án chả có tý data nào cả. Xấu hơn nữa là chúng ta không biết tìm kiếm data ở đâu, tìm kiếm như thế nào \u0026hellip; Quả thực rất khó khăn.\nThế mới nói, data collection và data integrity là hai trong số những bước quan trọng nhất của bất kỳ dự án data science nào. Garbage in, garbage out (đầu vào là rác thì đầu ra cũng là rác), câu nói này chắc ai làm trong lĩnh vực này đều đã từng nghe. Nhưng có lẽ không phải ai cũng thực sự hiểu được ý nghĩa của nó.\n Data collection: tức là thu thập dữ liệu. Để thu thập được dữ liệu thì cần phải hiểu rõ yêu cầu bài toán là gì, dữ liệu nào đã có, dữ liệu nào còn thiếu, cấu trúc của dữ liệu ra sao, số lượng tối thiểu như thế nào, cần chuẩn bị những công cụ dụng cụ gì để lấy dữ liệu (ví dụ, phải có camera để quay video, chụp ảnh)\u0026hellip; Ngoài ra, không phải lúc nào cũng có thể thu thập đầy đủ dữ liệu ngay từ ban đầu. Dữ liệu có thể đến trong suốt quá trình phát triển dự án hoặc vận hành sản phẩm. Vì vậy, phải có chiển lựợc quản lý dữ liệu hợp lý để dễ dàng cho viêc sử dụng, báo cáo thống kê về sau. Data integrity: tức là tính toàn vẹn của dữ liệu. Bạn cần phải kiểm tra kỹ lưỡng xem dữ liệu thu được có ý nghĩa hay không, có đầy đủ hay không bằng cách tự đặt câu hỏi và trả lời, hoặc nhờ sự trợ giúp từ các bên có liên quan và hiểu biết.  Nếu hai bước này làm không tốt thì cho dù bạn có áp dung bao nhiêu kỹ thuật cao siêu của data clearning, EDA, và tuning model thì kết qủa cũng sẽ không thể tốt đẹp như bạn mong đơi.\n3. Soft skills \u0026gt; Technical skills\nBạn đã từng tự đặt ra câu hỏi:\nNhững kỹ năng gì cần học để có thể làm việc được trong lĩnh vực data science?\nTheo ý kiến cá nhân của tôi, technical skills (lập trình, thống kê, \u0026hellip;) là kỹ năng bắt buộc và nên được ưu tiên học trước tiên khi bắt đầu bước chân vào thế giới data science.\nMột khi đã có đủ tự tin về technical skills của mình, chúng ta nên tập trung vào xây dựng và cải thiện soft skills (giao tiếp, làm việc nhóm, kể chuyện, \u0026hellip;). Khi học, bạn thường học một mình, rất ít khi giao tiếp hoặc làm cùng người khác. Nhưng dự án trong thực tế thường rất lớn, gồm nhiều phần, nhiều công đoạn. Và bạn thường chỉ làm một trong số những công đoạn đó. Nếu bạn là team-leader hay PM, bạn phải giao tiếp với các phòng ban bộ phận khác để lấy thông tin, trình bày kết quả với lãnh đạo, phân chia nhiệm vụ giữa các thành viên, \u0026hellip; Nếu bạn là nhân viên bình thường thì bạn vẫn cần phải trao đổi với các thành viên khác trong dự án, trình bày giải pháp kỹ thuật của mình. Rất may là những soft skills này khá giống với những dự án thuộc lĩnh vực khác, nên nếu bạn là người đã có kinh nghiệm làm việc ở những lĩnh vực khác chuyển sang data science thì bạn không cần quá lo lắng. Ngược lại, đối với các bạn sinh viên mới ra trường thì cần đặc biệt lưu ý hơn về việc phát triển soft skills của mình.\nCó một câu nói khá nổi tiếng của W.Edwards Deming:\n\u0026ldquo;Without data you\u0026rsquo;re just another person with an opinion\u0026rdquo;\nTạm dich: Nếu không có số liệu chứng minh cụ thể, thì tất cả chỉ là ý kiến, suy đoán của bạn mà thôi. (giống như trong các vụ điều tra, phá án. Lập luận, suy đoán có thuyết phục, hợp lý đi bao nhiêu chăng nữa mà không có bằng chứng cụ thể thì cũng vô nghĩa, không thể bắt được hung thủ).\nÁp dụng vào lĩnh vực data science này thì, có data chỉ là bước đầu tiên. Điều quan trọng là làm thế nào để sử dụng data đó để sinh ra những phán quyết, hành động (business decisions) mà mang lại lợi ích cho tổ chức.\nThử thay đổi câu nói trên một chút cho phù hợp với ngữ cảnh:\n\u0026ldquo;Without storytelling skills you\u0026rsquo;re just another persion with data\u0026rdquo;\nBạn có thể phân tích dữ liệu một cách xuất sắc.\nBạn có thể tạo ra một model tốt nhất trên thế giới.\nBạn thuộc làu làu cuốn Code Complete và viết code đẹp như trong phim.\nNhưng nếu bạn không thể sử dụng những kết quả đó để thuyết phục stackeholders để đưa ra business decisions, thì những nỗ lực của bạn vẫn chỉ nằm trên những slides PowerPoints mà thôi.\nHơi buồn, nhưng đó là sự thật!\n4. \u0026ldquo;What pattern observed behind?\u0026quot;\nĐối với hầu hết các bài toán, trừ khi bạn đang làm việc trong một công ty lớn về công nghệ (cutting-edge technology company), những models phức tạp thường không phải lựa chọn đầu tiên để phân tích và dự đoán.\nBạn cần phải thực sự hiểu những gì diễn ra đằng sau model và kết quả của chúng ta để thuyết phục các stackeholders, bởi vì họ có thể hỏi bạn:\n Tại sao lại không phát hiện ra lỗi này? Tại sao lỗi này lại bị nhầm sang lỗi kia? Tại sao độ chính xác lại thấp như vậy? Nếu xảy ra trường hợp này thì phải làm như thế nào? \u0026hellip;  Stackeholders không thể \u0026ldquo;nhắm mắt\u0026rdquo; cho sử dụng một model trong các sản phẩm thực tế khi mà họ chưa hiểu rõ (blackbox model), vì như thế nguy cơ thất thoát tiền bạc là rất lớn.\nĐây có lẽ cũng là một trong những lý do mà các models đơn giản như decision tree, logistic regression hay kNN vần còn còn được sử dụng khá nhiều trong các bài toán công nghiệp.\n5. Luôn luôn nhìn bức tranh tổng quát\nĐây có lẽ là một trong những lỗi phổ biến nhất khi bắt đầu với data science. Mọi người thường quá chú trọng vào việc làm sao train được model tốt, viết code cho đẹp mà không để ý đến việc model sẽ chạy trong thực tế như thế nào, tốc độ đáp ứng ra sao, \u0026hellip;\nNói chung ngay từ lúc bắt đầu dự án data science, bạn nên nhìn một cách tổng quát bài toán, từ việc hiểu rõ yêu cầu (cả function và non-function), chuẩn bị dữ liệu, huấn luyện model, triển khai model trong thực tế và cập nhật model. Có được cái nhìn rõ ràng về bức tranh lớn, mạch suy nghĩ và làm việc của bạn sẽ được xuyên suốt từ đầu đến cuối và không bị \u0026ldquo;khớp\u0026rdquo; khi chuyển tiếp giữa các giai đoạn với nhau.\n 6. Kết luận\nTrong bất kỳ lình vực nào, lý thuyết và thực tế luôn luôn có sự khác biệt. Sự khác biệt này càng rõ hơn trong phạm vi data science nói riêng. Hiễu được những điều này chính là bạn đã có được cái nhìn về bức tranh lớn của data science.\nHi vọng những điều chia sẽ trong bài viết này sẽ giúp ích cho các bạn trên con đường chinh phục sexy job. Hẹn gặp lại trong những bài viết sau!\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/18_data_scientist_theory_and_real/","tags":["Machine Learning","Data Science"],"title":"Nghề Data Scientis - Lý thuyết và thực tế - Sự khác biêt"},{"categories":["Machine Learning","XGBoost"],"contents":"Trong quá trình training, XGBoost thường xuyên phải thực hiện công việc chọn lựa ngẫu nhiên tập dữ liệu con (subsamples) từ tập dữ liệu gốc ban đầu. Các kỹ thuật để làm việc này được gọi bằng cái tên Stochastic Gradient Boosting (SGB).\nTrong bài này chúng ta sẽ cùng tìm hiểu về SGB và tuning SGB để tìm ra kỹ thuật phù hợp với bài toán.\n1. Tuning Row Subsampling\nRow subsampling liên quan đến việc chọn ngẫu nhiên các samples từ tập train set. Trong XGBoost, giá trị của row subsampling được chỉ ra bởi tham số subsample. Giá trị mặc định là 1, nghĩa là sử dụng toàn bộ tập train set, không subsampling.\nTiếp tục sử dụng tập Otto dataset, chúng ta sẽ grid-search tham số subsample với các giá trị như sau: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0].\nCó 9 giá trị của subsample, mỗi model sẽ được đánh giá sử dụng 10-fold cross-validation. Như vậy, có 9x10=90 models cần phải trained.\nCode đầy đủ như sau:\n# XGBoost on Otto dataset, tune subsample from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] param_grid = dict(subsample=subsample) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(subsample, means, yerr=stds) pyplot.title(\u0026#34;XGBoost subsample vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;subsample\u0026#39;) pyplot.ylabel(\u0026#39;Accuracy\u0026#39;) pyplot.savefig(\u0026#39;subsample.png\u0026#39;) Kết quả chạy:\nFitting 10 folds for each of 9 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 3.3min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 12.3min finished Best: 0.999919 using {\u0026#39;subsample\u0026#39;: 0.2} 0.999887 (0.000126) with: {\u0026#39;subsample\u0026#39;: 0.1} 0.999919 (0.000081) with: {\u0026#39;subsample\u0026#39;: 0.2} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.3} 0.999919 (0.000108) with: {\u0026#39;subsample\u0026#39;: 0.4} 0.999919 (0.000108) with: {\u0026#39;subsample\u0026#39;: 0.5} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.6} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.7} 0.999903 (0.000107) with: {\u0026#39;subsample\u0026#39;: 0.8} 0.999887 (0.000103) with: {\u0026#39;subsample\u0026#39;: 1.0} Độ chính xác của model đạt được bằng 0.999919 tại điểm subsample = 0.2, hay subset của mỗi model = 30% train set.\nĐồ thị bên dưới thể hiện mối quan hệ giữa subsample và accuracy.\n 2. Tuning Column Subsampling by Tree\nChúng ta cũng có thể tạo ra một tập ngẫu nhiên các input features để sử dụng cho mỗi decision tree. Trong XGBoost, điều này được cấu hình thông qua tham số colsample_tree. Giá trị mặc định của nó là 1, tức là tất cả các input features đều được sử dụng cho mỗi tree.\nTa sẽ thử tune tham số này với tập giá trị như sau: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\nCode đầy đủ:\n# XGBoost on Otto dataset, tune colsample_bytree from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] param_grid = dict(colsample_bytree=colsample_bytree) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(colsample_bytree, means, yerr=stds) pyplot.title(\u0026#34;XGBoost colsample_bytree vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;colsample_bytree\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;colsample_bytree.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 9 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 3.0min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 10.6min finished Best: 0.999887 using {\u0026#39;colsample_bytree\u0026#39;: 1.0} 0.998578 (0.000484) with: {\u0026#39;colsample_bytree\u0026#39;: 0.1} 0.999855 (0.000152) with: {\u0026#39;colsample_bytree\u0026#39;: 0.2} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.3} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.4} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.5} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.6} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.7} 0.999871 (0.000121) with: {\u0026#39;colsample_bytree\u0026#39;: 0.8} 0.999887 (0.000103) with: {\u0026#39;colsample_bytree\u0026#39;: 1.0} Độ chính xác của XGBoost đạt được là 0.999887 tại colsample_bytree = 1.0. Điều này có nghĩa rằng trong trường hợp này, subsampling column không mang lại giá trị nào.\nĐồ thị thể hiện mối quan hệ giữa subsampling column và accuracy.\n 3. Tuning Column Subsampling by Split\nThay vì subsampling column cho mỗi tree, ta có thể subsampling column ở mức node (hay Split). Tức là tại mỗi node, ta sẽ subsampling column để tìm ra 1 tập ngẫu nhiên các input features để quyết định hướng đi tiếp theo. Đây chính là điểm khác biệt giữa Random Forest và Bagging meta-data mà ta đã đề cập đến trong bài 2 của chuỗi các bài viết về XGBoost.\nSubsampling column ở mức node được cấu hình thông qua tham số colsample_bylevel. Ta sẽ tiến hành tune tham số này với giá trị thay đổi từ 10% đến giá trị mặc định ban đầu của nó (100%).\nCode đầy đủ như bên dưới:\n# XGBoost on Otto dataset, tune colsample_bylevel from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() colsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] param_grid = dict(colsample_bylevel=colsample_bylevel) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(colsample_bylevel, means, yerr=stds) pyplot.title(\u0026#34;XGBoost colsample_bylevel vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;colsample_bylevel\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;colsample_bylevel.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 9 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 2.6min [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 9.3min finished Best: 0.999919 using {\u0026#39;colsample_bylevel\u0026#39;: 0.3} 0.999903 (0.000129) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.1} 0.999903 (0.000079) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.2} 0.999919 (0.000081) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.3} 0.999903 (0.000107) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.4} 0.999887 (0.000103) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.5} 0.999903 (0.000107) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.6} 0.999903 (0.000107) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.7} 0.999871 (0.000121) with: {\u0026#39;colsample_bylevel\u0026#39;: 0.8} 0.999887 (0.000103) with: {\u0026#39;colsample_bylevel\u0026#39;: 1.0} colsample_bylevel = 0.3 cho ta model với độ chính xác cao nhất, 0.999919.\nĐồ thị thể hiện mối quan hệ giữa colsample_bylevel và accuracy.\n 6. Kết luận\nNhư vậy là chúng ta đã biêt cách tuning các kỹ thuật subsample hay stochastic gradient boosting của XGBoost. Nếu có phần cứng đủ mạnh, các bạn có thể tune tất cả các tham số đồng thời với nhau. Khi đó, độ chính xác của model có thể tăng lên 1 chút. Nhưng cái giá phải trả là thời gian train sẽ rất lâu. :D\nĐây cũng là bài cuối cùng trong loạt bài viết về XGBoost model. Hi vọng qua những bài viết của mình, các bạn có thể hiểu hơn về XGBoost và tự tin hơn khi làm viêc với nó. Hẹn mọi người ở những chủ để tiếp theo! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/17_tuning_subsampling/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 14: Tuning Subsample"},{"categories":["Machine Learning","XGBoost"],"contents":"Một vấn đề còn tồn tại của XGBoost là khả năng học trên tập dữ liệu huấn luyện một cách rất nhanh chóng. Điều này đôi khi dễ dẫn đến hiện tượng Overfitting, mặc dù XGBoost đã sử dụng regularization. Một cách hiệu quả để điều khiển quá trình học của XGBoost là sử dụng learning_rate (hay eta).\nTrong bài này, chúng ta sẽ cùng nhau tune learning_rate, learning_rate kết hợp với số lượng trees để tìm ra giá trị tối ưu của hai tham số này.\n1. Tuning Learning_Rate\nChúng ta tiếp tục sử dụng Otto dataset trong bài này. Sử dụng giá trị mặc định của số lượng trees là 100, ta sẽ đánh giá sự phù hợp của mỗi giá trị learning_rate trong tập sau: [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nCó 6 giá trị của learning_rate, kết hợp với 10-fold cross-validation \u0026ndash;\u0026gt; Có 60 models được trained.\nCode tuning như sau:\n# XGBoost on Otto dataset, Tune learning_rate from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] param_grid = dict(learning_rate=learning_rate) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(learning_rate, means, yerr=stds) pyplot.title(\u0026#34;XGBoost learning_rate vs Log Loss\u0026#34;) pyplot.xlabel(\u0026#39;learning_rate\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;learning_rate.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 6 candidates, totalling 60 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 26 tasks | elapsed: 9.1min [Parallel(n_jobs=-1)]: Done 60 out of 60 | elapsed: 13.8min finished Best: 0.999887 using {\u0026#39;learning_rate\u0026#39;: 0.001} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.2} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.3} Giá trị learning_rate tối ưu tìm được là 0.001.\nĐồ thị bên dưới thể hiện mối qua hệ giữa learning_rate và độ chính xác của model.\n 2. Tuning Learning_Rate và số lượng decision tree\nNói chung, khi có nhiều trees được thêm vào XGBoost, những trees thêm vào sau nên sử dụng giá trị learning_rate nhỏ. Ta sẽ kiểm tra nhận định này thông qua quá trình tuning như sau:\n Số lượng trees (n_estimators) = [100, 200, 300, 400, 500] learning_rate = [0.0001, 0.001, 0.01, 0.1]  Có 5 giá trị của n_estimators và 4 giá trị của learning_rate, kết hợp với 10-fold cross-validation ta có 200 models cần train.\nCode đầy đủ như dưới đây:\n# XGBoost on Otto dataset, Tune learning_rate and n_estimators from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot import numpy # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() n_estimators = [100, 200, 300, 400, 500] learning_rate = [0.0001, 0.001, 0.01, 0.1] param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot results scores = numpy.array(means).reshape(len(learning_rate), len(n_estimators)) for i, value in enumerate(learning_rate): pyplot.plot(n_estimators, scores[i], label=\u0026#39;learning_rate: \u0026#39; + str(value)) pyplot.legend() pyplot.xlabel(\u0026#39;n_estimators\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;n_estimators_vs_learning_rate.png\u0026#39;) Sau khoảng 2 tiếng chờ đơi thì chúng ta cũng thu được kết quả:\nFitting 10 folds for each of 20 candidates, totalling 200 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers. [Parallel(n_jobs=-1)]: Done 18 tasks | elapsed: 6.2min [Parallel(n_jobs=-1)]: Done 168 tasks | elapsed: 58.2min [Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 67.6min finished Best: 0.999887 using {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 100} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 100} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 200} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 300} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 400} 0.999838 (0.000102) with: {\u0026#39;learning_rate\u0026#39;: 0.0001, \u0026#39;n_estimators\u0026#39;: 500} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 100} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 200} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 300} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 400} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;n_estimators\u0026#39;: 500} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 100} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 200} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 300} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 400} 0.999887 (0.000103) with: {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 500} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 100} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 200} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 300} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 400} 0.999871 (0.000121) with: {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 500} Ta có thể thấy, kết quả tốt nhất của model đạt được tại learning_rate=0.001 và n_estimators=100. Tuy nhiên, kết quả này cũng không có sự khác biệt đáng kể so với những trường hợp khác. Bạn có thể thử nghiệm với các metrics đánh giá khác (F1-score, precition, recall, log_loss) để nhìn thấy sự khác biệt rõ hơn.\nBên dưới là đồ thị thể hiện mối quan hệ của mỗi learning_rate với các giá trị khác nhau của n_estimators.\n 3. Kết luận\nỞ bài viết này, chúng ta đã tiến hành tuning XGBoost model với 2 hyper-parameters là learning_rate và n_estimators.\nBài viết tiếp theo chúng ta sẽ tiếp tục tune thêm một tham số khác là subsample. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/16_tuning_learning_rate_and_number_decition_tree/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 13: Tuning Learning_Rate và số lượng của Decision Tree"},{"categories":["Machine Learning","XGBoost"],"contents":"Ý tưởng cơ bản của thuật toán Gradient Boosting là lần lượt thêm các decision trees nối tiếp nhau. Tree thêm vào sau sẽ cố gắng giải quyết những sai sót của tree trước đó. Câu hỏi đặt ra là bao nhiêu trees (weak learner hay estimators) là đủ?\nTrong bài nãy, hãy cùng nhau tìm hiều cách lựa chọn số lượng và kích thước của các trees phù hợp với từng bài toán của các bạn.\n1. Tune số lượng của decision tree\nThông thường khi sử dụng GBM, ta thường chọn số lượng trees tương đối nhỏ. Có thể là vài chục, vài trăm, hoặc vài nghìn. Nguyên nhân có lẽ là vì tăng số lượng trees lên nhiều hơn, hiệu năng của model cũng không tăng, thậm chí còn giảm đi so với khi sử dụng số lượng trees ít hơn.\nMình sẽ sử dụng Otto dataset để minh họa việc tuning số lượng trees. Ở đây mình sử dụng 10-fold cross-validation, số lượng trees trong khoảng [50, 400, 50] -\u0026gt; Có 80 models được train.\nSố lượng của trees được chỉ ra bởi giá trị của tham số n_estimators.\n# XGBoost on Otto dataset, Tune n_estimators from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() n_estimators = range(50, 400, 50) param_grid = dict(n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(n_estimators, means, yerr=stds) pyplot.title(\u0026#34;XGBoost n_estimators vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;n_estimators\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;_estimators.png\u0026#39;) Kết quả:\nBest: -0.001155 using {\u0026#39;n_estimators\u0026#39;: 100} -0.001160 (0.001059) with: {\u0026#39;n_estimators\u0026#39;: 50} -0.001155 (0.001053) with: {\u0026#39;n_estimators\u0026#39;: 100} -0.001156 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 150} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 200} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 250} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 300} -0.001155 (0.001054) with: {\u0026#39;n_estimators\u0026#39;: 350} Số lượng trees phù hợp nhất là 100, neg_log_loss đạt được tại đó là -0.001055. Hiệu năng của model không được cải thiện khi tăng số lượng trees từ 100 lên 350.\nĐồ thị bên dưới thể hiện mối quan hệ giữa số lượng trees và inverted logarihmic:\n 2. Tune kích thước của decision tree\nKích thước của tree hay còn gọi là số lớp (layers) hay độ sâu (depth) của tree đó. Nếu tree quá nông (shallow) sẽ dẫn đến underfitting vì model chỉ học được rất ít chi tiết từ dữ liệu. Ngược lại, tree quá sâu (deep) thì model lại học quá nhiều chi tiết từ dữ liệu -\u0026gt; overfitting.\nKích thước của tree được chỉ ra bởi giá trị của tham số max_depth. Ta sẽ thử grid-seach tham số này theo phạm vi [1, 11, 2].\nCode đầy đủ như bê dưới:\n# XGBoost on Otto dataset, Tune max_depth from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(✬Agg✬) from matplotlib import pyplot # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() max_depth = range(1, 11, 2) print(max_depth) param_grid = dict(max_depth=max_depth) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot pyplot.errorbar(max_depth, means, yerr=stds) pyplot.title(\u0026#34;XGBoost max_depth vs accuracy\u0026#34;) pyplot.xlabel(\u0026#39;max_depth\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;max_depth.png\u0026#39;) Kết quả:\nFitting 10 folds for each of 5 candidates, totalling 50 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 26 tasks | elapsed: 5.0min [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 7.8min finished Best: -0.001136 using {\u0026#39;max_depth\u0026#39;: 5} -0.001319 (0.001100) with: {\u0026#39;max_depth\u0026#39;: 1} -0.001153 (0.001066) with: {\u0026#39;max_depth\u0026#39;: 3} -0.001136 (0.001077) with: {\u0026#39;max_depth\u0026#39;: 5} -0.001150 (0.001063) with: {\u0026#39;max_depth\u0026#39;: 7} -0.001150 (0.001063) with: {\u0026#39;max_depth\u0026#39;: 9} Quan sát ouput, ta thấy rằng max_depth = 5 cho kết quả tốt nhất. Tăng giá trị này lên 7 hoặc 9, hiệu năng của model không những không được cải thiện mà còn kém đi.\nĐồ thị thể hiện mối quan hệ của kích thước tree và neg_log_loss.\n 3. Tune đồng thời số lượng và kích thước của decision tree\nCó một mối liên hệ giữa số lượng và kích thước của mỗi tree. Nhiều tree hơn thì kích thước của mỗi tree sẽ nhỏ hơn. Ngược lại, ít tree hơn thì kích thước của mỗi tree sẽ lớn hơn.\nĐể tìm ra cặp giá trị (n_estimators, max_depth) phù hợp, ta sẽ thử grid-search như sau:\n n_estimators: (50, 100, 150, 200) max_depth: (2, 4, 6, 8) 10-fold cross-validation -\u0026gt; 4x4x10 = 160 models  Code đầy đủ bên dưới:\n# XGBoost on Otto dataset, Tune n_estimators and max_depth from pandas import read_csv from xgboost import XGBClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder import matplotlib matplotlib.use(\u0026#39;Agg\u0026#39;) from matplotlib import pyplot import numpy # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # grid search model = XGBClassifier() n_estimators = [50, 100, 150, 200] max_depth = [2, 4, 6, 8] print(max_depth) param_grid = dict(max_depth=max_depth, n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) grid_search = GridSearchCV(model, param_grid, scoring=\u0026#34;accuracy\u0026#34;, n_jobs=-1, cv=kfold, verbose=1) grid_result = grid_search.fit(X, label_encoded_y) # summarize results print(\u0026#34;Best: %fusing %s\u0026#34; % (grid_result.best_score_, grid_result.best_params_)) means = grid_result.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = grid_result.cv_results_[\u0026#39;std_test_score\u0026#39;] params = grid_result.cv_results_[\u0026#39;params\u0026#39;] for mean, stdev, param in zip(means, stds, params): print(\u0026#34;%f(%f) with: %r\u0026#34; % (mean, stdev, param)) # plot results scores = numpy.array(means).reshape(len(max_depth), len(n_estimators)) for i, value in enumerate(max_depth): pyplot.plot(n_estimators, scores[i], label=\u0026#39;depth: \u0026#39; + str(value)) pyplot.legend() pyplot.xlabel(\u0026#39;n_estimators\u0026#39;) pyplot.ylabel(\u0026#39;accuracy\u0026#39;) pyplot.savefig(\u0026#39;n_estimators_vs_max_depth.png\u0026#39;) Kết quả:\nBest: -0.001131 using {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 100} -0.001266 (0.001112) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 50} -0.001249 (0.001101) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 100} -0.001248 (0.001101) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 150} -0.001247 (0.001100) with: {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;n_estimators\u0026#39;: 200} -0.001141 (0.001094) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 50} -0.001131 (0.001088) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 100} -0.001132 (0.001089) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 150} -0.001132 (0.001089) with: {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 200} -0.001160 (0.001059) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 50} -0.001155 (0.001053) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 100} -0.001156 (0.001054) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 150} -0.001155 (0.001054) with: {\u0026#39;max_depth\u0026#39;: 6, \u0026#39;n_estimators\u0026#39;: 200} -0.001155 (0.001068) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 50} -0.001150 (0.001063) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 100} -0.001150 (0.001064) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 150} -0.001150 (0.001064) with: {\u0026#39;max_depth\u0026#39;: 8, \u0026#39;n_estimators\u0026#39;: 200} Từ kết quả ta thấy kết quả tốt nhất đạt được tại max_depth=4 và n_estimators=100, tương tự như kết quả của 2 lần tuning riêng rẽ 2 tham số ở bên trên.\nĐồ thị thể hiện mối quan hê của mỗi max_depth với các giá trị của n_estimators.\n Kết quả thể hiện trên đồ thị cũng minh họa cho nhận định về mối quan hệ giữa số lượng và kích thước của tree mà ta đã nói bên trên.\n6. Kết luận\nQua bài viết này, chúng ta đã biết cách tuning XGBoost model, sử dụng phương pháp grid-search (hỗ trợ bởi thư viện scikit-learn) để tìm được số lượng và kích thước của trees phù hợp với bài toán đặt ra ban đầu. Ngoài phương pháp này, còn có 1 phương pháp khác cũng rất hiệu quả là bayes (sử dụng định luật bayes). Phương pháp này thước được các ông lớn AWS, Google, \u0026hellip; sử dụng trong các dịch vụ về AI của họ.\nBài viết tiếp theo chúng ta sẽ tiếp tục tune learning_rate đồng thời với số lượng của tree trong XGboost model. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/15_tuning_number_and_size_decision_tree/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 12: Tuning số lượng và kích thước của Decision Tree"},{"categories":["Machine Learning","XGBoost"],"contents":"Thư viện XGBoost được thiết kế để tận dụng tối đa sức mạnh của phần cứng hệ thống, bao gồm tất cả CPU cores và bộ nhớ. Trong bài viết này, ta sẽ cùng nhau tìm hiểu cách thiết lập một server trên AWS để train XGBoost model, sao cho vừa nhanh, vừa rẻ! :D\nBài viết gồm 4 phần:\n Tạo tài khoản AWS Chạy AWS EC2 Instance Kết nối đến EC2 Instance và chạy code train XGBoost model Đóng AWS EC2 Instance  Chú ý quan trọng: Sẽ mất khoảng 1-2$ chi phí để sử dụng các dịch vụ của AWS trong bài viết này.\n1. Tạo tài khoản AWS\n(Nếu bạn đã có tài khoản AWS, hãy bỏ qua bước này!)\n Truy cập vào màn hình console của AWS. Tại đây ta có thể đăng nếu đã có tài khoản hoặc đăng ký tài khoản mới nếu chưa có.    Bạn cần cung cấp một số thông tin cần thiết, và đặc biệt là phải có một thẻ credit còn hiệu lực để có thể tiến hành tạo tài khoản. Các công đoạn khác, hãy làm theo chỉ dần trên màn hình.  2. Chạy AWS EC2 Instance\nChúng ta sẽ sử dụng dịch vụ EC2 để chạy XGBoost.\n Đăng nhập vào AWS console. Sau khi đăng nhập thành công, danh sách các dịch vụ của AWS sẽ hiển thị. Chọn EC2.    Click vào nút Launch EC2 Instance. Click vào Community AMIs    Nhập ami-1c40bf7d vào Search community AMIs và chọn Select.    Chọn EC2 Instance type là r4.8xlarge (32 cores CPU). Click Review and Launch. Click Launch. Chọn Create a new key pair, điền tên của key là xgboost-key và click Download Key Pair`.   Lưu file key vào máy tính ở local, sau đó click Launch EC2 Instances.\n Click View EC2 Instances. Chờ khoảng 3 phút và kiểm tra trạng thái của EC2 Instance.   Nếu trạng thái là Running thì tức là đã tạo EC2 Instance thành công. Ta cũng để ý thấy địa chỉ public IP của EC2 Instance là: 54.92.106.10. Tiếp theo ta sẽ sử dụng địa chỉ này để kết nối đến EC2 Instance từ localhost thông qua giao thức SSH.\n3. Kết nối đến EC2 Instance và chạy code\n3.1 Kết nối đến EC2 Instance qua giao thức ssh\n Trên máy tính local (mình dùng Ubuntu), mở cửa sổ Terminal và gõ lệnh:  $ cd Documents #Thư mục chứa key file $ chmod 600 xgboost-key.pem $ ssh -i xgboost.pem fedora@54.92.106.10 Nếu đây là lần đầu kết nối đến EC2 Instance, sẽ có 1 cảnh báo xuất hiện. Gõ yes.\nNếu kết nối thành công, màn hình Terminal sẽ xuất hiện như sau:\n  Kiểm ra số lượng CPU cores  $cat /proc/cpuinfo | grep processor | wc -l Kết quả:\n32 3.2 Cài đặt các thư viện cần thiết\n Cài đặt GCC, Python và SciPy  sudo dnf install gcc gcc-c++ make git unzip python python3-numpy python3-scipy python3-scikit-learn python3-pandas python3-matplotlib  Cài đặt Cmake  XGBoost yêu cầu cmake \u0026gt;= 3.13. Nếu bạn cài bằng bằng lệnh dnf install cmake thì phiên bản của make la 3.9. Để cài cmake \u0026gt;= 3.13, bạn phải cài build từ source.\n$ wget https://github.com/Kitware/CMake/releases/download/v3.15.2/cmake-3.15.2.tar.gz $ tar -zxvf cmake-3.15.2.tar.gz $ cd cmake-3.15.2 $ ./bootstrap $ make $ sudo make install Kiểm tra GCC:\n$gcc --version Kết quả:\n[fedora@ip-172-31-37-253 ~]$ gcc --version gcc (GCC) 6.3.1 20161221 (Red Hat 6.3.1-1) Kiểm tra Python:\n$ python3 --version Kết quả:\nPython 3.5.1 Kiểm tra SciPy:\n$ python3 -c \u0026quot;import scipy;print(scipy.__version__)\u0026quot; $ python3 -c \u0026quot;import numpy;print(numpy.__version__)\u0026quot; $ python3 -c \u0026quot;import pandas;print(pandas.__version__)\u0026quot; $ python3 -c \u0026quot;import sklearn;print(sklearn.__version__)\u0026quot; Kết quả:\n0.16.1 1.11.0 0.18.0 0.17.1 Kiểm tra Cmake\n$ cmake --version Kết quả:\n3.15.1 3.3. Cài đặt thư viện XGBoost\n$ pip3 install xgboost==1.1 Tại thời điểm viết bài, phiên bản mới nhất của xgboost là 1.2. Nhưng vì phiên bản của python=3.5 nên bạn chỉ có thể sử dụng được phiên bản 1.1 của xgboost.\nKiểm tra:\n$ python3 -c \u0026quot;import xgboost;print(xgboost.__version__)\u0026quot; Kết quả:\n1.1.0 4. Train XGBoost model\nTương tự như ở bài 9, chúng ta cũng sẽ sử dụng Otto dataset để kiểm tra khả năng của XGBoost model theo số lượng cores của CPU.\n Tạo thư mục xgboost trên máy local, tạo file check_num_threads.py với code như sau:  # Otto multi-core test from pandas import read_csv from xgboost import XGBClassifier from sklearn.preprocessing import LabelEncoder from time import time # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # evaluate the effect of the number of threads results = [] num_threads = [1, 16, 32] for n in num_threads: start = time() model = XGBClassifier(nthread=n) model.fit(X, label_encoded_y) elapsed = time() - start print(n, elapsed) results.append(elapsed) Copy file train.csv ở bài trước vào thư mục xgboost.\n Tại cửa sổ Terminal của máy local, copy thư mục xgboost lên EC2 Instance:  $ scp -i xgboost-key.pem -r xgboost fedora@54.92.106.10:/home/fedora  Tại của sổ Terminal kết nôi tới EC2 Instance, tiến hành chạy code:  $ cd xgboost python3 check_num_threads.py Kết quả thu được:\n1 70.75178146362305 16 6.106862545013428 32 5.045598745346069 Sử dụng 32 cores, mất 5s để train XGBoost model với tập dữ liệu tương đối lớn. Đây quả là một kết quả ấn tượng. :D\nBonus:: Trong trường hợp việc train model mất nhiều thời gian hơn mà chẳng may bạn bị mất kết nối đến EC2 Instance giữa chừng thì thế nào? Bạn phải chạy train lại từ đầu ư? Quả là mất thời gian phải không? Giải pháp để ngăn chặn tình huống này, hoặc là bạn chạy lệnh train ở chế độ background process và ghi kết quả ra một file như lệnh sau:\n$ nohup python script.py \u0026gt;script.py.out 2\u0026gt;\u0026amp;1 \u0026amp; hoặc bạn cũng có thể sử dụng Tmux.\n5. Tắt EC2 EC2 Instance\nTiết kiệm là quốc sách hàng đầu, hãy luôn nhớ tắt EC2 EC2 Instance mỗi khi sử dụng xong. Bản thân mình đã từng một lần quên không tắt trong vài ngày. Kết quả là con số trên hóa đơn AWS tháng đó làm mình buồn mất cả tuần, :).\nĐể tắt EC2 EC2 Instance, đơn giản là làm theo như hình vẽ sau:\n  Chọn 1-\u0026gt;2-\u0026gt;3 nếu bạn muốn tắt tạm thời (khi nào muốn dùng thì khởi động lên). Chọn 1-\u0026gt;2-\u0026gt;4 nếu bạn muốn xóa hẳn EC2 EC2 Instance này.  6. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách cài đặt vào cấu hình EC2 Instance để train XGBoost model trên AWS.\nBài viết tiếp theo sẽ thiên về lý thuyết một chút, chúng ta sẽ tìm hiểu cách cấu hình hyper-parameters cho gradient boosting model. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/14_train_xgboost_models_on_aws/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 11: Train XGBoost model trên AWS"},{"categories":["Machine Learning","XGBoost"],"contents":"Thư viện XGBoost được thiết kế để làm việc h iệu quả vớicơ chế xử lý song song trên nhiều core (multithreading) của phần cứng, cả trong quá trình train và dự đoán. Hãy cùng nhau tìm hiểu cơ chế đó thông qua bài viết này.\n1. Chuẩn bị dataset\nChúng ta sẽ sử dụng Otto Group Product Classification Challenge dataset để minh họa cơ chế multithreading của thư viện XGBoost. Để download dataset này, bạn cần đăng nhập vào Kaggle. có 2 file là train.csv và test.csv. Vì chỉ có file train.csv là có nhãn nên ta sẽ sử dụng file này. Download file train.csv về máy tính (dạng train.csv.zip). Giải nén nó ra và đặt trong thư mục làm việc của bạn.\nDataset này bao gồm khoảng 94.000 sản phẩm và 93 input featuresđược chia thành 10 nhóm (ví dụ: thời trang, điện tử, \u0026hellip;). Mục tiêu là xây dựng một model để phân loại một sản phẩm mới vào các nhóm này. Cuộc thi này đã kết thúc vào 05/2015 và người chiến thắng cũng sử dụng XGBoost để tạo model.\n2. Ảnh hưởng của số lượng threads đến thời gian train model\nXGBoost được viết bằng C++, sử dụng OpenMP API để phát triển cơ chế xử lý song song. Trong một số trường hợp, bạn có thể phải compile mại XGBoost mới có thể sử dụng cơ chế song song này. Chi tiết về việc cài đặt XGBoost, có thể xem tại đây.\nHai lớp XGBClassifier và XGBRegressor trong thư viện XGBoost cung cấp tham số nthread để chỉ ra số lượng threads mà XGBoost sử dụng trong quá trình train. Mặc định thì giá trị của tham số này là -1, tức là sử dụng tất cả các core của hệ thống.\nmodel = XGBClassifier(nthread=-1) Bây giờ chúng ta sẽ xây dựng một số XGBoost models khác nhau theo số lượng threads sử dụng và đo đặc thời gian train của mỗi model.\nCode snippet:\n# evaluate the effect of the number of threads results = [] num_threads = [1, 2, 3, 4, 5, 6, 7, 8] for n in num_threads: start = time.time() model = XGBClassifier(nthread=n) model.fit(X_train, y_train) elapsed = time.time() - start print(n, elapsed) results.append(elapsed) Áp dụng vào bộ dữ liệu Otto (bạn có thể thay đổi giá trị của mảng num_threads theo máy tính của bạn):\nfrom matplotlib import pyplot # load data data = read_csv(\u0026#39;test.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # evaluate the effect of the number of threads results = [] num_threads = [1, 2, 3, 4] for n in num_threads: start = time() model = XGBClassifier(nthread=n) model.fit(X, label_encoded_y) elapsed = time() - start print(n, elapsed) results.append(elapsed) # plot results pyplot.plot(num_threads, results) pyplot.ylabel(\u0026#39;Speed (seconds)\u0026#39;) pyplot.xlabel(\u0026#39;Number of Threads\u0026#39;) pyplot.title(\u0026#39;XGBoost Training Speed vs Number of Threads\u0026#39;) pyplot.show() Chạy code trên thu được kết quả:\n1 50.13007640838623 2 26.999273538589478 3 18.629448890686035 4 15.561982154846191 5 13.940500497817993 6 12.550707578659058 7 13.075348854064941 8 12.36113166809082 và đồ thị thể hiện mối quan hệ giữa số lượng threads và thời gian train model.\n Có thể thấy rõ xu hướng giảm của thời gian train model khi số lượng threads tăng lên. Nếu kết quả chạy trên máy của bạn không giống như vậy, bạn cần phải xem xét lại cách enable cơ chế song song của XGBoost như link tham khảo mình đề cập bên trên.\n3. Ảnh hưởng của số lượng threads đến thời gian cross-validation\nk-fold cross-validation hỗ trợ bởi thư viện scikit-learn cũng có cơ chế xử lý song song tương tự như XGBoost. Tham số n_jobs của hàm cross_val_crore() chỉ ra số lượng threads sử dụng. Mặc định, n_jobs=1 tức là chỉ sử dụng 1 core (1 thread). Ta có thể gán giá trị -1 cho nó để sử dụng tất cả cores của hệ thống.\nresults = cross_val_score(model, X, label_encoded_y, cv=kfold,scoring=\u0026#39;neg_log_loss\u0026#39;, n_jobs=-1, verbose=1) Đến đây lại xuất hiện một câu hỏi, chúng ta nên chọn phương án nào trong 3 phương án sau:\n Disable multithreading của XGBoost, enable multithreading của cross-validation. Enable multithreading của XGBoost, disable multithreading của cross-validation. Enable multithreading của XGBoost, enable multithreading của cross-validation.  Để trả lời câu hỏi này, không gì chính xác hơn là chúng ta sẽ code cho cả 3 cách và so sánh thời gian thực thi của mỗi cách:\n# Otto, parallel cross validation from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import cross_val_score from sklearn.preprocessing import LabelEncoder import time # load data data = read_csv(\u0026#39;train.csv\u0026#39;) dataset = data.values # split data into X and y X = dataset[:,0:94] y = dataset[:,94] # encode string class values as integers label_encoded_y = LabelEncoder().fit_transform(y) # prepare cross validation kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) # Single Thread XGBoost, Parallel Thread CV start = time.time() model = XGBClassifier(nthread=1) results = cross_val_score(model, X, label_encoded_y, cv=kfold, coring=\u0026#39;neg_log_loss\u0026#39;, n_jobs=-1) elapsed = time.time() - start print(\u0026#34;Single Thread XGBoost, Parallel Thread CV: %f\u0026#34; % (elapsed)) # Parallel Thread XGBoost, Single Thread CV start = time.time() model = XGBClassifier(nthread=-1) results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=\u0026#39;neg_log_loss, n_jobs=1) elapsed = time.time() - start print(\u0026#34;Parallel Thread XGBoost, Single Thread CV: %f\u0026#34; % (elapsed)) # Parallel Thread XGBoost and CV start = time.time() model = XGBClassifier(nthread=-1) results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring=\u0026#39;neg_log_loss\u0026#39;, n_jobs=-1) elapsed = time.time() - start print(\u0026#34;Parallel Thread XGBoost and CV: %f\u0026#34; % (elapsed)) Kết quả cuối cùng:\nSingle Thread XGBoost, Parallel Thread CV: 101.820478 Parallel Thread XGBoost, Single Thread CV: 455.847770 Parallel Thread XGBoost and CV: 98.794466 Rõ ràng, phương án thứ 3 sử dụng thời gian ít nhất. Bây giờ bạn đã biết câu trả lời rồi phải không? :D\n4. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách cấu hình cơ chế multithreading của XGBoost model. Chúng ta cũng nhận thức được ảnh hưởng của số lượng threads (số lượng cores) đến thời gian train model, từ đó biết cách kiểm tra xem hệ thống có hỗ trợ cơ chế xử lý song song của XGBoost hay không? Cuối cùng, cách cấu hình tốt nhất cho cả XGBoost và cross-validation để giảm thời gian thực thi cũng đã được tìm ra.\nỞ bài viết tiếp theo chúng ta sẽ tìm hiểu cách scale-up XGBoost model để sử dụng nhiều cores của hệ thống hơn trên AWS cloud. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/13_multithreading-xgboost/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 10: Cấu hình Multithreading cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Overfitting vẫn luôn là một vấn đề làm đau đầu những kỹ sư AI. Trong bài viết này chúng ta sẽ cùng tìm hiểu cách thức monitor (giám sát) performance (hiệu năng) của XGBoost model trong suốt quá trình train. Từ đó cấu hình early stopping để quyết định khi nào thì nên dừng lại quá trình này để tránh hiện tượng overfitting. Bài viết gồm 2 phần:\n Monitor hiệu năng của XGBoost model thông qua learning curve (đường cong học tập). Cấu hình early stopping.  1. Giám sát hiệu năng của XGBoost model\nĐể monitor porformance của XGBoost model, ta cần cung cấp cả train set, test set và một metric (chỉ tiêu đánh giá) khi train model (gọi hàm model.fit()). Ví dụ, để tính toán error metric trên tập test set, sử dụng code snippet sau:\neval_set = [(X_test, y_test)] model.fit(X_train, y_train, eval_metric=\u0026#34;error\u0026#34;, eval_set=eval_set, verbose=True) XGBoost model hỗ trợ một số metric như sau:\n rmse: root mean squared error. mae: mean absolute error. logloss: binary logarithmic loss. mlogloss: multiclass log loss (cross entropy). error: classification error. auc: area under ROC curve. Danh sách đầy đủ các metrics, các bạn có thể xem tại đây.  Code dưới đây minh hoạ việc monitor performance trong quá trình train một XGBoost model trên tập dữ liệu Pima Indians onset of diabetes.\n# monitor training performance from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on training data model = XGBClassifier() eval_set = [(X_test, y_test)] model.fit(X_train, y_train, eval_metric=\u0026#34;error\u0026#34;, eval_set=eval_set, verbose=True) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Ở đây, ta sử dụng 67% dữ liệu cho việc train, 33% còn lại cho viêc đánh giá model. Error metric được tính toán tại cuối mỗi vòng lặp (sau khi mỗi boosted tree được thêm vào model). Cuối cùng, độ chính xác của model được in ra.\nKết quả hiển thị trên màn hình như sau:\n[0]\tvalidation_0-error:0.28347 [1]\tvalidation_0-error:0.25984 [2]\tvalidation_0-error:0.25591 [3]\tvalidation_0-error:0.24803 [4]\tvalidation_0-error:0.24409 [5]\tvalidation_0-error:0.24803 [6]\tvalidation_0-error:0.25591 [7]\tvalidation_0-error:0.24803 [8]\tvalidation_0-error:0.25591 [9]\tvalidation_0-error:0.24409 ... [89]\tvalidation_0-error:0.26378 [90]\tvalidation_0-error:0.27165 [91]\tvalidation_0-error:0.26772 [92]\tvalidation_0-error:0.27165 [93]\tvalidation_0-error:0.26378 [94]\tvalidation_0-error:0.27165 [95]\tvalidation_0-error:0.26378 [96]\tvalidation_0-error:0.25984 [97]\tvalidation_0-error:0.26378 [98]\tvalidation_0-error:0.25984 [99]\tvalidation_0-error:0.25984 Accuracy: 74.02% Quan sát kết quả ta thấy performance của model không thay dổi quá nhiều trong suốt quá trình train. Thậm chí đến cuối quá trình, performance còn kém hơn so với nửa đầu.\nĐể có cái nhìn tường minh hơn, hãy thể hiện performance của model trên đồ thị. Ta sẽ monitor performace của model trên cả train set và test set:\neval_set = [(X_train, y_train), (X_test, y_test)] model.fit(X_train, y_train, eval_metric=\u0026#34;error\u0026#34;, eval_set=eval_set, verbose=True) Performance của model trên mỗi tập evaluation set được lưu bởi model sau khi train kết thúc. Để truy cập giá trị performace này, sử dụng hàm model.evals_result():\nresults = model.evals_result() print(results) Kết quả in ra sẽ giống như sau:\n{ \u0026#39;validation_0\u0026#39;: {\u0026#39;error\u0026#39;: [0.259843, 0.26378, 0.26378, ...]}, \u0026#39;validation_1\u0026#39;: {\u0026#39;error\u0026#39;: [0.22179, 0.202335, 0.196498, ...]} } validation_0 và validation_1 theo thứ tự tương ứng với hai tập validation set mà ta đã định nghĩa trong tham số eval_set khi gọi hàm fit().\nError metric được truy cập như sau:\nresults[\u0026#39;validation_0\u0026#39;][\u0026#39;error\u0026#39;] results[\u0026#39;validation_1\u0026#39;][\u0026#39;error\u0026#39;] Thêm nữa, bạn có thể lựa chọn nhiều metrics để đánh giá model bằng cách cung cấp một mảng các giá trị metric tới tham số eval_metric của hàm fit(). Giá trị của các metric thu được sau đó đươc thể hiện trên đồ thị, gọi là learning curve.\nCode đầy đủ dưới đây minh họa việc thu thập giá tị của các metrics và thể hiện trên learning curve:\n# plot learning curve from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on training data model = XGBClassifier() eval_set = [(X_train, y_train), (X_test, y_test)] model.fit(X_train, y_train, eval_metric=[\u0026#34;error\u0026#34;, \u0026#34;logloss\u0026#34;], eval_set=eval_set, verbose=True) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) # retrieve performance metrics results = model.evals_result() epochs = len(results[\u0026#39;validation_0\u0026#39;][\u0026#39;error\u0026#39;]) x_axis = range(0, epochs) # plot log loss fig, ax = pyplot.subplots() ax.plot(x_axis, results[\u0026#39;validation_0\u0026#39;][\u0026#39;logloss\u0026#39;], label=\u0026#39;Train\u0026#39;) ax.plot(x_axis, results[\u0026#39;validation_1\u0026#39;][\u0026#39;logloss\u0026#39;], label=\u0026#39;Test\u0026#39;) ax.legend() pyplot.ylabel(\u0026#39;Log Loss\u0026#39;) pyplot.title(\u0026#39;XGBoost Log Loss\u0026#39;) pyplot.show() # plot classification error fig, ax = pyplot.subplots() ax.plot(x_axis, results[\u0026#39;validation_0\u0026#39;][\u0026#39;error\u0026#39;], label=\u0026#39;Train\u0026#39;) ax.plot(x_axis, results[\u0026#39;validation_1\u0026#39;][\u0026#39;error\u0026#39;], label=\u0026#39;Test\u0026#39;) ax.legend() pyplot.ylabel(\u0026#39;Classification Error\u0026#39;) pyplot.title(\u0026#39;XGBoost Classification Error\u0026#39;) pyplot.show() Chạy code trên, error và logloss metric trên cả 2 tập train set và test set được in ra. Ta có thể bỏ qua điều này bằng cách truyền giá trị False (giá trị mặc định) cho tham sô verbose khi gọi hàm fit(). Hai đồ thị được tạo ra. Đồ thị đầu tiên thể hiện logloss của XGBoost model đối với mỗi epoch (iteration) trong quá trình train.\n Đồ thị thứ 2 hiển thị error metric của mỗi epoch.\n Quan sát cả 2 đồ thị trên ta có một nhận xét rằng, nếu dừng train sớm hơn tại epoch \u0026lt; 100 thì performace của model sẽ tốt hơn. Đây chính là tiền đề của kỹ thuật early stopping mà chúng ta sẽ tìm hiểu ngay sau đây.\n2. Cấu hình early stopping cho XGBoost model\nEarly stopping là một kỹ thuật khá phổ biến áp dụng cho các ML model phức tạp để tránh hiện tượng overfitting. Nó làm việc bằng cách monitor performance của model trên tập test set trong suốt quá trình train và buộc quá trình này dừng lại một khi performance của model không được cải thiện sau một số epochs nhất định.\nTrên đồ thị learning curve, điểm bắt đầu overfitting là điể mà tại đó performace của model trên tập test set bắt đầu giảm trong khi performance của model trên tập train set vẫn tiếp tục tăng.\nĐể cấu hình early stopping cho XGBoost model, cần cung cấp thêm giá trị cho tham số early_stopping_rounds khi gọi hàm fit(). Ý nghĩa của nó là chỉ ra số lượng epochs mà quá trình húân luyện trải qua mà performance không có sự cải thiện nào.\nVí dụ:\neval_set = [(X_test, y_test)] model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\u0026#34;logloss\u0026#34;, eval_set=eval_set, verbose=True) Nếu có nhiều evaluation sets hoặc nhiều metrics được cung cấp, early stopping sẽ sử dụng cái cuối cùng trong danh sách.\nCode đầy đủ cấu hình early stopping như bên dưới:\n# early stopping from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() eval_set = [(X_test, y_test)] model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\u0026#34;logloss\u0026#34;, eval_set=eval_set, verbose=True) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Kết quả chạy code:\n[0]\tvalidation_0-logloss:0.60491 Will train until validation_0-logloss hasn\u0026#39;t improved in 10 rounds. [1]\tvalidation_0-logloss:0.55934 [2]\tvalidation_0-logloss:0.53068 [3]\tvalidation_0-logloss:0.51795 [4]\tvalidation_0-logloss:0.51153 [5]\tvalidation_0-logloss:0.50935 [6]\tvalidation_0-logloss:0.50818 [7]\tvalidation_0-logloss:0.51097 [8]\tvalidation_0-logloss:0.51760 [9]\tvalidation_0-logloss:0.51912 [10]\tvalidation_0-logloss:0.52503 [11]\tvalidation_0-logloss:0.52697 [12]\tvalidation_0-logloss:0.53335 [13]\tvalidation_0-logloss:0.53905 [14]\tvalidation_0-logloss:0.54546 [15]\tvalidation_0-logloss:0.54613 [16]\tvalidation_0-logloss:0.54982 Stopping. Best iteration: [6]\tvalidation_0-logloss:0.50818 Accuracy: 74.41% Quá trình train model dừng lại ở epoch 16 (gần với những gì mà chúng ta phán đoán dựa trên đồ thị learning curve) và model đạt được metric thấp nhất tại epoch 6. Việc chọn giá trị của tham số early_stopping_rounds thường dựa vào quan sát trên đồ thị learning curve. Nếu bạn không biết thì có thể chọn giá trị mặc định là 10.\n3. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách monitor performance của XGBoost model trong quá trình train và cấu hình early stopping để hạn chế hiện tượng overfitting của model.\nỞ bài viết tiếp theo chúng ta sẽ tìm hiểu cách cấu hình XGBoost model để tận dụng hết tài nguyên của phần cứng khi train model và khi sử model để dự đoán. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/12_early_stopping/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 9: Cấu hình Early_Stopping cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Feature selection hay lựa chọn features là một bước tương đối quan trọng trước khi train XGBoost model. Lựa chọn đúng các features sẽ giúp model khái quát hóa vấn đề tốt hơn (low variance) -\u0026gt; đạt độ chính xác cao hơn.\nTrong bài viết này, hãy cùng xem xét về cách dùng thư viện XGBoost để tính importance scores và thể hiện nó trên đồ thị, sau đó lựa chọn các features để train XGBoost model dựa trên importance scores đó.\n1. Tính và hiển thị importance score trên đồ thị\n1.1 Cách 1\nModel XGBoost đã train sẽ tự động tính toán mức độ quan trọng của các features. Các giá trị này được lưu trong biến feature_importances_ của model đã train. Kiểm tra bằng cách:\nprint(model.feature_importances_) Thể hiện các features importance lên đồ thị:\n# plot pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_) pyplot.show() Code dưới đây minh họa đầy đủ việc train XGBoost model trên tập dữ liệu Pima Indians onset of diabetes và hiển thị các features importances lên đồ thị:\n# plot feature importance manually from numpy import loadtxt from XGBoost import XGBClassifier from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] y = dataset[:,8] # fit model on training data model = XGBClassifier() model.fit(X, y) # feature importance print(model.feature_importances_) # plot pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_) pyplot.show() Chạy code trên, importance score được in ra:\n[0.10621197 0.2424023 0.08803366 0.07818192 0.10381887 0.1486732 0.10059207 0.13208601] và đồ thị:\n 1.2 Cách 2\nNhược điểm của cách này là các importance scores được sắp xếp theo thứ tự của các features trong tập dataset. Điều này làm cho chúng ta khó quan sát trong trường hợp số lượng features lớn. Liệu có thể sắp thứ tự các importance scores này theo giá trị của chúng được hay không? Câu trả lời là có thể. Thư viện XGBoost có một hàm gọi là plot_importance() giúp chúng ta thực hiện việc này.\n# plot feature importance plot_importance(model) pyplot.show() Code dưới đây minh họa đầy đủ việc train XGBoost model trên tập dữ liệu Pima Indians onset of diabetes và hiển thị các features importances lên đồ thị:\n# plot feature importance using built-in function from numpy import loadtxt from XGBoost import XGBClassifier from XGBoost import plot_importance from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] y = dataset[:,8] # fit model on training data model = XGBClassifier() model.fit(X, y) # plot feature importance plot_importance(model) pyplot.show() Chạy code ví dụ bên trên thu được kết quả:\n Quan sát đồ thị ta thấy, các features được tự động đặt tên từ f0 đến f7 theo thứ tự của chúng trong mảng dữ liệu input X. Từ đồ thị có thể kết lụân rằng:\n f6 có importance score cao nhất - 333 f4 có importance score thấp nhất - 124  Nếu có bảng mô tả dữ liệu, ta có thể ánh xạ f4, f6 thành tên các features tương ứng.\n2. Lựa chọn features (feature selection) theo importance scores\nThư viện scikit-learn cung cấp lớp SelectFromModel cho phép lựa chọn các features để train model. Lớp này yêu cầu 2 tham số bắt buộc:\n model: model đã được train trên toàn bộ dataset. threshold: ngưỡng để lựa chọn features. Chỉ những features có importance score không nhỏ hơn ngưỡng mới được lựa chọn. Sau khi gọi hàm transform() thì lớp SelectFromModel sẽ chuyển đổi tập dữ liệu ban đầu thành tập dữ liệu nhỏ hơn chỉ bao gồm các features được chọn.  # select features using threshold selection = SelectFromModel(model, threshold=thresh, prefit=True) select_X_train = selection.transform(X_train) Sau khi có tập dữ liệu mới, ta tiến hành train và đánh giá model mới tạo ra như bình thường.\n# train model selection_model = XGBClassifier() selection_model.fit(select_X_train, y_train) # eval model select_X_test = selection.transform(X_test) y_pred = selection_model.predict(select_X_test) Trong các bài toán thực tế, ta thường không biết chính xác giá trị nào của threshold là phù hợp. Vì vậy mà ta sẽ tuning giá trị này bằng phương pháp grid-seach (mình sẽ có 1 bài viết riêng giải thích chi tiết về các phương pháp tuning hyper-parameters. Ở đây, bạn chỉ cần hiểu một cách đơn giản là kiểm tra với nhiều giá trị của threshold để chọn ra giá trị tốt nhất). Chúng ta sẽ bắt đầu kiểm tra với tất cả features, kết thúc với feature quan trọng nhất.\nCode hoàn chỉnh như bên dưới:\n# use feature importance for feature selection from numpy import loadtxt from numpy import sort from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.feature_selection import SelectFromModel # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on all training data model = XGBClassifier() model.fit(X_train, y_train) # make predictions for test data and evaluate predictions = model.predict(X_test) accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) # Fit model using each importance as a threshold thresholds = sort(model.feature_importances_) for thresh in thresholds: # select features using threshold selection = SelectFromModel(model, threshold=thresh, prefit=True) select_X_train = selection.transform(X_train) # train model selection_model = XGBClassifier() selection_model.fit(select_X_train, y_train) # eval model select_X_test = selection.transform(X_test) predictions = selection_model.predict(select_X_test) accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Thresh=%.3f, n=%d, Accuracy: %.2f%%\u0026#34; % (thresh, select_X_train.shape[1], accuracy*100.0)) Chạy code trên thu được kết quả như sau:\nAccuracy: 74.02% Thresh=0.088, n=8, Accuracy: 74.02% Thresh=0.089, n=7, Accuracy: 71.65% Thresh=0.098, n=6, Accuracy: 71.26% Thresh=0.098, n=5, Accuracy: 74.41% Thresh=0.100, n=4, Accuracy: 74.80% Thresh=0.136, n=3, Accuracy: 71.26% Thresh=0.152, n=2, Accuracy: 71.26% Thresh=0.240, n=1, Accuracy: 67.32% Có thể thấy rằng độ chính xác của model cao nhất trên tập dữ liệu gồm 4 features quan trọng nhất và thấp nhất trên tập dữ liệu chỉ gồm một feature.\nTuning theo kiểu grid-seach như này đặc biệt hiệu quả trong trường hợp bộ dữ liệu lớn.\n3. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách thể hiện importance score của các features trên đồ thị và sử dụng importance score để lựa chọn các features sao cho model đạt được độ chính xác cao nhất.\nBài viết tiếp theo ta sẽ tìm hiểu cách giám sát (monitor) hiệu năng của model trong quá trình train và cấu hình early stop (dừng train khi model đáp ứng một tiêu chí nào đó). Hai kỹ thuật này rất cần thiết để train một XGBoost model tốt. Hãy cùng đón đọc! :)\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/11_feature-selection/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 8: Lựa chọn features cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Giả sử bạn đã train xong một XGBoost model đạt được độ chính xác rất cao. Câu hỏi đặt ra là làm sao lưu lại model đó để sử dụng về sau (không phải mất công train lại model mỗi khi cần sử dụng)?\nTrong bài viết này, chúng ta hãy cùng tìm hiểu cách thức lưu một XGBoost model thành 1 file sử dụng Python pickle API. Nội dung bài viết gồm 2 phần chính:\n Lưu và sử dụng XGBoost model bằng thư viện pickle. Lưu và sử dụng XGBoost model bằng thư viện joblib.  1. Lưu và sử dụng XGBoost model bằng thư viện pickle.\nPickle là một cách chuẩn chỉ để lưu một dối tượng trong Python thành một file. Cách sử dụng tương đối đơn giản.\n Lưu model thành file  # save model to file pickle.dump(model, open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;wb\u0026#34;))  Gọi model đã lưu để sử dụng  # load model from file loaded_model = pickle.load(open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;rb\u0026#34;)) Ví dụ dưới đây mình họa việc train một XGBoost model trên tập dữ liệu Pima Indians onset of diabetes, lưu model thành file và gọi model đã lưu để dự đoán.\n# Train XGBoost model, save to file using pickle, load and make predictions from numpy import loadtxt from XGBoost import XGBClassifier import pickle from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) # save model to file pickle.dump(model, open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;wb\u0026#34;)) print(\u0026#34;Saved model to: pima.pickle.dat\u0026#34;) # some time later... # load model from file loaded_model = pickle.load(open(\u0026#34;pima.pickle.dat\u0026#34;, \u0026#34;rb\u0026#34;)) print(\u0026#34;Loaded model from: pima.pickle.dat\u0026#34;) # make predictions for test data predictions = loaded_model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Kết quả:\nSaved model to: pima.pickle.dat Loaded model from: pima.pickle.dat Accuracy: 74.02% 2. Lưu và sử dụng XGBoost model bằng thư viện joblib.\nJoblib là một phần của hệ sinh thái SciPy, nó cũng hỗ trợ việc lưu ML model thành file rât dễ dàng, sử dụng cấu trúc dữ liệu của NumPy. Ưu điểm của viêc sử dụng joblib so với pickle là nó hoạt động khá nhanh, đặc biệt với những model có kích thước lớn. Cách sử dụng:\n Lưu model thành file  # save model to file joblib.dump(model, \u0026#34;pima.joblib.dat\u0026#34;)  Sử dụng model đã lưu  # load model from file loaded_model = joblib.load(\u0026#34;pima.joblib.dat\u0026#34;) Ví dụ dưới đây mình họa việc train một XGBoost model trên tập dữ liệu Pima Indians onset of diabetes, lưu model thành file và gọi model đã lưu để dự đoán.\n# Train XGBoost model, save to file using joblib, load and make predictions from numpy import loadtxt from XGBoost import XGBClassifier from joblib import dump from joblib import load from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) # save model to file dump(model, \u0026#34;pima.joblib.dat\u0026#34;) print(\u0026#34;Saved model to: pima.joblib.dat\u0026#34;) # some time later... # load model from file loaded_model = load(\u0026#34;pima.joblib.dat\u0026#34;) print(\u0026#34;Loaded model from: pima.joblib.dat\u0026#34;) # make predictions for test data predictions = loaded_model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Output của đoạn code trên:\nSaved model to: pima.joblib.dat Loaded model from: pima.joblib.dat Accuracy: 74.02% 3. Kết luận\nTrong bài viết này, chúng ta đã tìm hiểu cách thức lưu XGBoost model thành file sử dụng pickle và joblib, sau đó gọi lại model đã lưu từ file để dự đoán.\nBài viết tiếp theo sẽ tìm hiểu cách tính toán và lựa chọn các features tốt nhất cho việc train XGBoost model.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo.\n","permalink":"https://tiensu.github.io/blog/10_save-load-xgboost-model/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 7: Lưu và sử dụng XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Ta đã biết, XGBoost thực chất là tập hợp gồm nhiều decision tree. Việc thể hiện mỗi decision tree đó trên đồ thì sẽ giúp chúng ta hiểu sâu sắc hơn quá trình boosting khi đưa vào một tập dữ liệu. Trong bài này, hãy cùng tìm hiểu cách thức thể hiện đó từ một XGBoost model đã được train.\n1. Vẽ một decision tree đơn lẻ\nXGBoost Python API cung cấp một hàm cho việc vẽ các decision tree của một XGBoost model đã train, đó là plot_tree(). Hàm này nhận một tham số đầu tiên chính là model cần thể hiện.\nplot_tree(model) Đồ thị vẽ ra bởi hàm này có thể được lưu dưới dạng file hoặc hiển thị trên màn hình bằng cách sử dụng hàm pyplot.show() của thư viện matplotlib. Yêu cầu là thư viện graphviz đã được cài đặt.\nĐể minh họa cho việc này, hãy cùng tạo một một XGBoost model và train nó trên tập dữ liệu Pima Indians onset of diabetes dataset. Code đầy đủ như bên dưới:\n# plot decision tree from numpy import loadtxt from XGBoost import XGBClassifier from XGBoost import plot_tree from matplotlib import pyplot # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] y = dataset[:,8] # fit model on training data model = XGBClassifier() model.fit(X, y) # plot single tree plot_tree(model) pyplot.show() Đoạn code bên trên sẽ tạo ra một đồ thị của decision tree đầu tiên trong model (index 0). Các feature và feature value được thể hiện trên đồ thị.\n Một vài quan sát:\n Các features được đặt tên tự động từ f1 đến f5 tương ứng với các feature indices trong dataset. Trong mỗi node, hai hướng trái phải được phân biệt bằng màu sắc. Bên trái là màu xanh, trong khi bên phải là màu đỏ.  2. Một số tùy chọn\nNgoài tham số model cần vẽ là bắt buộc, hàm plot_tree() còn nhận vào một vài tham số tùy chọn khác:\n num_trees: Chỉ số tree muốn vẽ. Giá trị mặc định là 0. Ví dụ:  plot_tree(model, num_trees=4) sẽ vẽ boosted tree thứ 5.\n rankdir: Hướng của đồ thị. Ví dụ: LR là left-to-right. Mặc định là UT - top-to-bottom.  Ví dụ:\nplot_tree(model, num_trees=0, rankdir=\u0026#39;LR\u0026#39;) sẽ cho kết quả như sau:\n 3. Kết luận\nTrong bài này, chúng ta đã tìm hiểu cách vẽ các decision tree của một XGBoost model đã train. Đây là cách rất hay giúp chúng ta có cái nhiều sâu hơn vào bên trong của model, hiểu rõ hơn cách thức mà model hoạt động.\nTrong bài tiếp theo, chúng ta sẽ tìm hiểu cách lưu lại XGBoost model để train và sử dụng model đã lưu để dự đoán trên một mẫu data mới.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/09_visualize-xgboost-model/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 6: Trực quan hóa XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"Mục đích của việc phát triển mô hình dự đoán là tạo ra một mô hình có độ chính xác cao khi kiểm tra trên bộ dữ liệu độc lập với dữ liệu train (gọi là unseen data). Trong bài viết này, chúng ta cùng tìm hiểu hai phương pháp đánh giá một XGBoost model:\n Sử dụng train và test dataset. Sử dụng k-fold cross-validation. Bạn hoàn toàn có thể áp dụng những phương pháp trong bài này cho những ML models khác. Tại vì dạo này mình đang tìm hiểu vê XGBoost model nên mình lấy XGBoost model làm ví dụ thôi.  1. Phương pháp 1: Sử dụng train-test set\nĐây là phương pháp đơn giản nhất để đánh giá một ML model. Từ tập dữ liệu ban đầu, ta chia thành 2 phần, gọi là train set và test set theo tỉ lệ nhất định (thường là 7:3, 8:2 hoặc thậm chí 9:1 tùy theo kích thước của tập và đặc trưng của tập dữ liệu). Sau đó, tiến hành train model trên train set rồi sử dụng model đã train đó để dự đoán trên tập test set. Dựa trên kết quả của dự đoán để đưa ra đánh giá chất lượng của model.\nƯu điểm của phương pháp này là nhanh. Nó sẽ phù hợp để áp dụng khi bài toán của bạn đáp ứng ít nhất 1 trong 2 tiêu chí sau:\n Tập dữ liệu có kích thước lớn (hàng triệu mẫu) và có cơ sở để tin rằng cả 2 phần dữ liệu đều đại diện đầy đủ cho tất cả các khía cạnh của vấn đề cần dự đoán (để chắc chắn hơn về điều này, ta có thể xáo trộn ngẫu nhiên tập dữ liệu trước khi chia) Thuật toán train của model rất lâu để hội tụ.  Nếu điều kiện thứ 2 không thỏa mãn mà ta vẫn sử dụng phương pháp này thì sẽ gặp phải vấn đề high variance. Tức là khi 2 tập train set và test set chứa những đại diện khác nhau của vấn đề cần dự đoán thì kết quả đánh giá trên tập test set không thể hiện đúng chất lượng của model.\nThư viện scikit-learn cung cấp hàm train_test_split() giúp chúng ta thực hiện việc chia dữ liệu:\n# split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) Source code dưới đây sử dụng Pima Indians onset of diabetes dataset để train XGBoost model và đánh giá model theo phương pháp này:\n# train-test split evaluation of XGBoost model from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Chạy code trên thu được kết quả:\nAccuracy: 74.02% 2. Phương pháp 2: k-fold cross-validation\nCross-validation là phương pháp mở rộng của phương pháp bên trên để hạn chế được vấn đề high variance. Các bước tiến hành của nó như sau:\n Xáo trộn dữ liệu một cách ngẫu nhiên. Chia tập dữ liệu ban đầu thành k phần (k=5,10,\u0026hellip;), mỗi phần gọi là một fold. - train model trên k-1 fold và đánh giá trên fold còn lại. Lặp lại k lần bước bên trên để mỗi fold trong tập dữ liệu đều có cơ hội trở thành test set. Sau khi toàn bộ quá trình kết thúc ta sẽ có k kết quả đánh giá khác nhau, kêt quả cuối cùng sẽ được tổng hợp dựa vào trung bình (mean) và độ lệch chuẩn (standard deviation) của k kết quả đó.  Phương pháp này cho kết quả đánh giá tin cậy hơn so với phương pháp sử dụng train-test set bởi vì nó được train và đánh giá nhiều lần trên các tập dữ liệu khác nhau. Việc lựa chọn k cũng cần phải xem xét sao cho kích thước của mỗi fold đủ lớn để dữ liệu trong mỗi fold mang tính đại diện cao về mặt thống kê của toàn bộ dữ liệu. Thực nghiệm cho thấy k=5 hoặc k=10 là lựa chọn tốt nhất cho hầu hết các trường hợp. Hãy sử dụng 2 giá trị này trước khi thử nghiệm với các giá trị khác.\nThư viện scikit-learn cung cấp lớp KFold để sử dụng phương pháp này. Đầu tiên, khai báo đối tượng KFold và chỉ ra giá trị của k. Sau đó sử dụng hàm cross_val_score() để bắt đầu đánh giá model.\nkfold = KFold(n_splits=10, random_state=7) results = cross_val_score(model, X, Y, cv=kfold) Code đầy đủ như bên dưới:\n# k-fold cross validation evaluation of XGBoost model from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # CV model model = XGBClassifier() kfold = KFold(n_splits=10, random_state=7) results = cross_val_score(model, X, Y, cv=kfold) print(\u0026#34;Accuracy: %.2f%%(%.2f%%)\u0026#34; % (results.mean()*100, results.std()*100)) Chạy code trên thu được kết quả:\nAccuracy: 73.96% (4.77%) Nếu bài toán là multi-classification hoặc dữ liệu bị mất cân bằng (imbalanced) giữa các lớp (số lượng mẫu giữa các lớp chênh lệch nhau lớn) thì ta có thể sử dụng lớp  StratifiedKFold thay vì KFold của thư viện scikit-learn. Việc làm này có tác dụng làm cho sự phân phối dữ liệu trong mỗi fold giống nhau hơn, nâng cao hiệu quả của model.\n# stratified k-fold cross validation evaluation of XGBoost model from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import cross_val_score # load data dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;) # split data into X and y X = dataset[:,0:8] Y = dataset[:,8] # CV model model = XGBClassifier() kfold = StratifiedKFold(n_splits=10, random_state=7) results = cross_val_score(model, X, Y, cv=kfold) print(\u0026#34;Accuracy: %.2f%%(%.2f%%)\u0026#34; % (results.mean()*100, results.std()*100)) Chạy code trên được output là:\nAccuracy: 73.57% (4.39%) Có thể thấy ràng variance có sự giảm nhẹ trong kết quả.\n3. Lựa chọn phương pháp đánh giá nào?\n Nói chung, k-fold cross-validation là phương pháp tốt nhất cho việc đánh giá hiệu năng của một ML model trong hầu hết mọi trường hợp. Sử dụng stratified cross-validation để đảm bảo sự thống nhất về mặt phân phối dữ liệu khi dữ liệu có nhiều lớp cần dự đoán và bị mất cân bằng giữa các lớp. Sử dụng train-test set trong trường hợp thuật toán train mất nhiều thời gian để hội tụ và số lượng mẫu của dữ liệu rất lớn.  Lời khuyên hợp lý nhất là hãy thử nghiệm nhiều lần và tìm ra phương pháp phù hợp với bài toán của bạn sao cho nhanh nhất có thể. Phương pháp được coi là phù hợp khi nó kết quả đánh giá của nó đáp ứng đúng (hoặc gần đúng) yêu cầu bài toán đặt ra ban đầu. Lời khuyên cuối cùng (từ kinh nghiệm thực tế của tác giả):\n Sử dụng 10-fold cross-validation cho bài toán regression. Sử dụng stratified 10-fold-validation cho bài toán classification.  4. Kết luận\nTrong bài viết này, chúng ta đã cùng tìm 2 phương pháp phổ biến đánh gía hiệu quả của một ML model nói chung, XGBoost mode nói riêng:\n Sử dụng train-test set Sử dụng k-fold cross-validation Ngoài ra, mình cũng đưa vài lời khuyên cho các bạn trong viêc lựa chọn phương pháp nào để áp dụng trong bài toán của các bạn!  Trong bài tiếp theo, chúng ta sẽ tìm hiểu cách thức visualize XGBoost model để hiểu sâu hơn bản chất của nó.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/08_evaluate-xgbosst-models/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 5: Đánh giá hiệu năng của XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"XGBoost là một thuật toán thuộc họ Gradient Boosting. Những ưu điểm vượt trội của nó đã được chứng minh qua các cuộc thi trên kaggle. Dữ liệu đầu vào cho XGBoost model phải ở dạng số. Nếu dữ liệu không ở dạng số thì phải được chuyển qua dạng số (numeric) trước khi đưa vào XGBoost model để train. Có một vài phương pháp để thực hiện việc này, hãy cùng nhau điểm qua trong phần còn lại của bài viết.\nSau khi xem hết bài viết này, bạn sẽ biết:\n Làm thế nào để mã hóa chuỗi (string) đầu ra cho việc phân loại? Làm thế nào để mã hóa dữ liệu đầu vào kiểu \u0026ldquo;categorical\u0026rdquo; sử dụng mã hóa one-hot (one hot encoding). Làm thế nào để tự động xử lý vấn đề thiếu dữ liệu trong dataset (missing data)?  1. Mã hóa chuỗi đầu ra\nChúng ta sẽ sử dụng bài toán phân loại hoa Iris để minh họa cho vấn đề này. Đưa vào các số liệu đo đặc các thành phần của hoa Iris, cần dự đoán hoa Iris đó thuộc loại nào (đầu ra dự đoán của model là các chuỗi ký tự).\nHãy xem ví dụ của dataset:\n5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa XGBoost sẽ không thể mô hình hóa bài toán này bởi vì nó yêu cầu dữ liệu đầu vào phải ở dạng số. Chúng ta có thể dễ dang chuyển đổi từ kiểủ chuổi (string) sang kiểu số (integer) sử dụng lớp LabelEncoder của thư viện sklearn. Ba giá trị đầu ra kiểu string (Iris-setosa, Iris-versicolor, Iris-virginica) sẽ được ánh xạ thành các giá trị số tương ứng (0, 1, 2):\n# encode string class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) Chúng ta cần lưu lại bộ mã hóa này để chuyển đổi ngược lại từ số sang chuỗi trong quá trình dự đoán.\nDưới đây là toàn bộ code minh họa việc việc đọc dataset, mã hóa dữ liệu đầu ra, train XGBoost model và đánh giá độ chính xác của model đó:\n# multiclass classification from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder # load data data = read_csv(\u0026#39;iris.csv\u0026#39;, header=None) dataset = data.values # split data into X and y X = dataset[:,0:4] Y = dataset[:,4] # encode string class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y,test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Chạy đoạn code trên, được kết quả như sau:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, objective='multi:softprob', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 92.00% 2. Mã hóa one-code dữ liệu kiểu Categorical\nRất nhiều bộ dataset chứa các dữ liệu kiểu categorical. Trong phần này, chúng ta sử dụng bộ dataset breast cancer để làm việc. Bộ dataset này miêu tả thông tin của y tế của các bệnh nhân, nhãn của nó chỉ ra bệnh nhân đó có bị ung thư hay không.\nVí dụ của bộ dữ liệu này như bên dưới:\n'40-49','premeno','15-19','0-2','yes','3','right','left_up','no','recurrence-events' '50-59','ge40','15-19','0-2','no','1','right','central','no','no-recurrence-events' '50-59','ge40','35-39','0-2','no','2','left','left_low','no','recurrence-events' '40-49','premeno','35-39','0-2','yes','3','right','left_low','yes','no-recurrence-events' '40-49','premeno','30-34','3-5','yes','2','left','right_up','no','recurrence-events' Chúng ta nhìn thấy rằng tất cả 9 giá trị input đều ở là kiểu categorical và được thể hiện ở dạng string. Đây cũng là bài toán phân lớp nhị phân và nhãn cần dự đoán cũng đang ở dạng string. Vì vậy, ta có thể sử dụng lại cách tiếp cận ở phần trước, chuyển các giá trị dạng string sang integer sử dụng LabelEncoder.\n# encode string input values as integers features = [] for i in range(0, X.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(X[:,i]) features.append(feature) encoded_x = numpy.array(features) encoded_x = encoded_x.reshape(X.shape[0], X.shape[1]) Khi sử dụng LabelEncoder, XGBoost có thể hiểu rằng các giá trị encoded integer của mỗi input feature có mối quan hệ thứ tự. Ví dụ, đối với input feature breast-quad, giá trị left-up được mã hóa là 0, left-low được mã hóa là 1. Điều này thực tế là không đúng trong trường hợp này. Để tránh điều này, ta phải ánh xạ các giá trị integer thành 1 giá trị kiểu binary. Ví dụ, các giá trị của biến đầu vào breast-quad là:\nleft-up left-low right-up right-low central được ánh xạ thành 5 giá trị binary tương ứng:\n1,0,0,0,0 0,1,0,0,0 0,0,1,0,0 0,0,0,1,0 0,0,0,0,1 Điều này được gọi là OneHotEncoder. Ta có thể mã hóa OneHot tất cả các input features kiểu categorical sử dụng lớp OneHotEncoder của thư viện scikit-leaen:\nonehot_encoder = OneHotEncoder(sparse=False, categories=✬auto✬) feature = onehot_encoder.fit_transform(feature) OneHot Encoder cho tất cả input features của dữ liệu:\n# encode string input values as integers columns = [] for i in range(0, X.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(X[:,i]) feature = feature.reshape(X.shape[0], 1) onehot_encoder = OneHotEncoder(sparse=False, categories=✬auto✬) feature = onehot_encoder.fit_transform(feature) columns.append(feature) # collapse columns into array encoded_x = numpy.column_stack(columns) Nếu biết chắc chắn răng một input feature nào đó có mối quan hệ về thứ tự, có thể bỏ qua OneHot Encoder mà chỉ cần LabelEncoder cho feature đó.\nToàn bộ source code của phần này:\n# binary classification, breast cancer dataset, label and one hot encoded from numpy import column_stack from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder # load data data = read_csv(✬datasets-uci-breast-cancer.csv✬, header=None) dataset = data.values # split data into X and y X = dataset[:,0:9] X = X.astype(str) Y = dataset[:,9] # encode string input values as integers columns = [] for i in range(0, X.shape[1]): label_encoder = LabelEncoder() feature = label_encoder.fit_transform(X[:,i]) feature = feature.reshape(X.shape[0], 1) onehot_encoder = OneHotEncoder(sparse=False, categories=✬auto✬) feature = onehot_encoder.fit_transform(feature) columns.append(feature) # collapse columns into array encoded_x = column_stack(columns) print(\u0026#34;X shape: : \u0026#34;, encoded_x.shape) # encode string class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(encoded_x, label_encoded_y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Chạy code trên ta được kết quả:\n('X shape: : ', (286, 43)) XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 68.42% 3. Giải quyết vấn đề missing data\nXGBoost có thể tự động học cách để đưa ra cách giải quyết tốt nhất cho vấn đề missing data. Trên thực tế, XGBoost được thiết kế để làm việc với sparse data, giống như one hot encoded data ở phần trước. Chi tiết hơn về các kỹ thuật xử lý missing data của XGBoost, có thể tham khảo Section 3.4 Sparsity-aware Split Finding trong bài báo XGBoost: A Scalable Tree Boosting System.\nTrong phần này, ta sẽ sử dụng Horse Colic dataset để minh họa khả năng xử lý missing data của XGBoost. Trong dataset này, tỷ lệ mising data tương dối lớn, rơi vào khoảng 30%.\nDễ dàng đọc được dataset này với thư viện Pandas:\ndataframe = read_csv(\u0026#34;horse-colic.csv\u0026#34;, delim_whitespace=True, header=None) dataframe.head() Ta có thể quan sát thấy rằng, missing data được đánh dấu bằng dấu ?.\n 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 0 2 1 530101 38.50 66 28 3 3 ? 2 5 4 4 ? ? ? 3 5 45.00 8.40 ? ? 2 2 11300 0 0 2 1 1 1 534817 39.2 88 20 ? ? 4 1 3 4 2 ? ? ? 4 2 50 85 2 2 3 2 2208 0 0 2 2 2 1 530334 38.30 40 24 1 1 3 1 3 3 1 ? ? ? 1 1 33.00 6.70 ? ? 1 2 0 0 0 1 3 1 9 5290409 39.10 164 84 4 1 6 2 2 4 4 1 2 5.00 3 ? 48.00 7.20 3 5.30 2 1 2208 0 0 1 4 2 1 530255 37.30 104 35 ? ? 6 2 ? ? ? ? ? ? ? ? 74.00 7.40 ? ? 2 2 4300 0 0 2 Thay ? bởi giá trị 0:\n# set missing values to 0 X[X == \u0026#39;?\u0026#39;] = 0 # convert to numeric X = X.astype(\u0026#39;float32\u0026#39;) Bài toán đối với dataset này là Binary Classification, nhãn bao gồm 2 giá trị là 1 và 2. Ta dễ dàng chuyển sang 0 và 1 sử dụng LabelEncoder:\n# encode Y class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) Code đầy đủ:\n# binary classification, missing data from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder # load data dataframe = read_csv(\u0026#34;horse-colic.csv\u0026#34;, delim_whitespace=True, header=None) dataset = dataframe.values # split data into X and y X = dataset[:,0:27] Y = dataset[:,27] # set missing values to 0 X[X == \u0026#39;?\u0026#39;] = 0 # convert to numeric X = X.astype(\u0026#39;float32\u0026#39;) # encode Y class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Output:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 82.83% Hãy kiểm tra khả năng của XGBoost bằng cách thử thay missing value với các giá trị khác nhau:\n Thay missing value bởi 1  X[X == \u0026#39;?`] = 1 Kết quả:\nAccuracy: 81.82%  Thay missing value bởi NaN  X[X == \u0026#39;?`] = 1 Kết quả:\nAccuracy: 83.84%  Thay thế missing value bằng giá trị trung bình (mean) của toàn bộ feature đó  # impute missing values as the mean imputer = SimpleImputer() imputed_x = imputer.fit_transform(X) Source đầy đủ:\n# binary classification, missing data, impute with mean import numpy from pandas import read_csv from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import Imputer # load data dataframe = read_csv(\u0026#34;horse-colic.csv\u0026#34;, delim_whitespace=True, header=None) dataset = dataframe.values # split data into X and y X = dataset[:,0:27] Y = dataset[:,27] # set missing values to NaN X[X == \u0026#39;?\u0026#39;] = numpy.nan # convert to numeric X = X.astype(✬float32✬) # impute missing values as the mean. imputer = Imputer() imputed_x = imputer.fit_transform(X) # encode Y class values as integers label_encoder = LabelEncoder() label_encoder = label_encoder.fit(Y) label_encoded_y = label_encoder.transform(Y) # split data into train and test sets seed = 7 test_size = 0.33 X_train, X_test, y_train, y_test = train_test_split(imputed_x, label_encoded_y, test_size=test_size, random_state=seed) # fit model on training data model = XGBClassifier() model.fit(X_train, y_train) print(model) # make predictions for test data predictions = model.predict(X_test) # evaluate predictions accuracy = accuracy_score(y_test, predictions) print(\u0026#34;Accuracy: %.2f%%\u0026#34; % (accuracy * 100.0)) Kết quả:\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) Accuracy: 81.82% Có thể thấý rằng, đối với bài toán này, giải pháp thay thế missing value bằng NaN mang lại kết quả tốt nhất. Trong các bài toán thực tế, chúng ta cũng cần phải thử-sai nhiều cách khác nhau để chọn được phương pháp tối ưu cho bài toán đó.\n4. Kết luận\nTrong bài viết này chúng ta đã cùng nhau tìm hiểu các cách để chuẩn bị dữ liệu cho việc train XGBoost model. Cụ thể:\n Mã hóa dữ liệu kiểu string bằng LabelEncoder Mã hóa dữ liệu kiểu categorical bằng OneHot Encoder Xử lý missing data  Trong bài tiếp theo, chúng ta sẽ tìm hiểu các phương pháp đánh giá hiệu năng của XGBoost model.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/07_data-preparation-for-gradient-boosting/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 4: Chuẩn bị dữ liệu cho XGBoost model"},{"categories":["Machine Learning","XGBoost"],"contents":"XGBoost là một thuật toán rất mạnh mẽ, tối ưu hóa về tốc độ và hiệu năng cho việc xây dựng các mô hình dự đoán. Một thống kê chỉ ra rằng, hầu hết những người chiến thắng trong các cuộc thi trên Kaggle đều sử dụng thuật toán này. Trong bài viết này, hãy cùng nhau xây dựng một mô hình XGBoost đơn giản để có thể hiểu được cách thức làm việc của nó.\nNội dung bài viết chia thành các phần:\n Cài đặt thư viện XGBoost Chuẩn bị dữ liệu Train XGBoost model Đánh giá XGBoost model Nguồn tham khảo  1. Cài đặt thư viện XGBoost\nCó 2 cách để cài đặt thư viện XGBoost. Sử dụng pip hoặc biên dịch từ mã nguồn:\n1.1 Sử dụng pip để cài đặt:\npip install XGBoost Để cập nhật thư viện, sử dụng lệnh sau:\npip install --upgrade XGBoost 1.2 Biên dịch từ mã nguồn\nSử dụng cách này nếu muốn cài đặt phiên bản mới nhất của XGBoost.\ngit clone --recursive https://github.com/dmlc/XGBoost cd XGBoost cp make/minimum.mk ./config.mk make -j8 cd python-package sudo python setup.py install Tại thời điểm viết bài, phiên bản của XGBoost là 1.2\nChuẩn bị dữ liệu  Trong bài viết này, chúng ta sẽ sử dụng dataset về bênh tiểu đường của Ấn Độ. Dataset bao gồm 8 features, miêu tả chi tiết tình trạng của mỗi bệnh nhân và một feature tương ứng chỉ ra bênh nhân có bị tiểu đường hay không. Chi tiết về dataset này, bạn có thể tham khảo trên UCI Machine Learning Repository website\nĐây là một dataset khá đơn giản bởi vì tất cả các features của nó đều đã ở dạng số và vấn đề chỉ là \u0026ldquo;binary classification\u0026rdquo;.\n6,148,72,35,0,33.6,0.627,50,1 1,85,66,29,0,26.6,0.351,31,0 8,183,64,0,0,23.3,0.672,32,1 1,89,66,23,94,28.1,0.167,21,0 0,137,40,35,168,43.1,2.288,33,1 Tải dataset và đặt nó trong thư mục làm việc hiện tại của bạn với tên là pima-indians-diabetes.csv.\nTiếp theo, load dataset từ file vừa tải về để chuẩn bị cho training và evaluating XGBoost model.\n Import các thư viện sử dụng:  from numpy import loadtxt from XGBoost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score  Load csv file  dataset = loadtxt(\u0026#39;pima-indians-diabetes.csv\u0026#39;, delimiter=\u0026#34;,\u0026#34;)  Chia dataset thành dữ liệu input (X) và output (Y)  X = dataset[:, 0:8] y = dataset[:, 8]  Chia X và y thành data training và data testing  Training data được sử dụng để train XGBoost model, trong khi testing data được sử dụng để đánh giá độ chính xác của model đó. Để làm điều này, ta có thể sử dụng hàm train_test_split() trong thư viện scikit-learn.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_seed=42) Đến đây, dữ liệu đã được chuẩn bị sẵn sàng cho việc train XGBoost model.\n2. train XGBoost model\nThư viện XGBoost cung cấp một \u0026ldquo;Wrapper class\u0026rdquo; cho phép sử dụng XGBoost model tương tự như như làm việc với thư viện scikit-learn. XGBoost model trong thư viện XGBoost là XGBClassifier.\nTạo XGBoost model và thực hiện train:\nmodel = XGBClassifier() model.fit(X_train, y_train) Ở đây, chúng ta đang sử dụng giá trị mặc định của các tham số. Mình sẽ có các bài việc về việc *tuning papameters\u0026rdquo; cho XGBoost model, mời các bạn đón đọc.\nBạn có thể quan sát các tham số sử dụng trong model bằng lệnh sau:\nprint(model) 3. Đánh giá XGBoost model\nĐể sử dụng model đã train để dự đoán, sử dụng hàm model.predict():\npredictions = model.predict(X_test) Ta có thể đánh giá độ chính xác của model bằng cách so sánh kết quả dự đoán của model với kêt quả thực tế. Hàm accuracy_score() giúp chúng ta thực hiện việc này:\naccuracy = accuracy_score(y_test, predictions) print(\u0026#39;Accuracy: %.2f%%\u0026#39; % (accuracy*100)) Kết quả cuối cùng:\nAccuracy: 77.95% Kết quả khá tốt đối với bài toán này.\n5. Tổng kết\nTrong bài viết này, chúng ta đã xây dựng XGBoost model sử dụng thư viện XGBoost. Cụ thể, chúng ta đã học:\n Cách cài đặt thư viện XGBoost Chuẩn bị dữ liệu train model Đánh giá model  Trong bài tiếp theo, chúng ta sẽ bàn luận về một số phương pháp chuẩn bị dữ liệu train cho XGBoost model.\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại tham khảo\n","permalink":"https://tiensu.github.io/blog/06_build-xgboost-model/","tags":["Machine Learning","XGBoost"],"title":"XGBoost - Bài 3: Xây dựng XGBoost model"},{"categories":["Machine Learning","Ensemble Learning","XGBoost"],"contents":"Tiếp tục phần 2 của loạt bài tìm hiểu toàn cảnh về Ensemble Learning, trong phần này ta sẽ đi qua một số thuât toán thuộc nhóm Bagging và Boosting.\n Các thuật toán thuộc nhóm Bagging bao gồm:  Bagging meta-estimator Random forest   Các thuật toán thuộc họ Boosting bao gồm:  AdaBoost Gradient Boosting (GBM) XGBoost (XGBM) Light GBM CatBoost    Để minh họa cho các thuật toán kể trên, mình sẽ sử dụng bộ dữ liệu Loan Prediction Problem.\n1. Bagging techniques\n1.1 Bagging meta-estimator\nBagging meta-estimator là thuật toán sử dụng cho cả 2 loại bài toán classification (BaggingClassifier) và regression (BaggingRegressor).\nCác bước thực hiện của thuật toán như sau:\n Bước 1: Tạo ngẫu nhiên các N bags từ tập train set. Bước 2: Tạo N objects của lớp BaggingClassifier và train trên mỗi bag, độc lập với nhau. Bước 3: Sử dụng các objects đã trained để dự đoán trên tập test set.  Code cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.ensemble import BaggingClassifier from sklearn import tree from sklearn.preprocessing import LabelEncoder #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1)) model.fit(x_train, y_train) accuracy = model.score(x_test,y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Kết quả:\nAccuracy: 77.83% Đối với bài toán regression, thay BaggingClassifier bằng BaggingRegressor.\nMột số tham số:\n base_estimator: Định nghĩa thuật toán mà base model sử dụng. Mặc định là decision tree. n_estimators: Định nghĩa số lượng base models. Mặc định là 10. max_samples: Định nghĩa số lượng mẫu data tối đa trong mỗi bag. Mặc định là 1. max_features: Định nghĩa số lượng features tối đa sử dụng trong mỗi bag. Mặc định là 1. n_jobs: Số lượng jobs chạy song song cho cả quá trình train và predict. Mặc định là 1. Nếu giá trị bằng -1 thì số jobs bằng số cores của hệ thống. random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi BaggingClassifier thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh các models với nhau.  1.2 Random Forest\nCác thức hoạt động của Random Forest gần giống Bagging meta-estimator, chỉ khác một điều duy nhất là tại mỗi node của tree trong Decision Tree, nó tạo ra một tập ngẫu nhiên các features và sử dụng tập này đê chọn hướng đi tiếp theo. Trong khi đó, Bagging meta-estimator sử dụng tất cả features để chọn đường.\nCode ví dụ:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = RandomForestClassifier() model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Output:\nAccuracy: 79.86% Một số tham số:\n n_estimators: Số lượng decition trees (base models). Mặc định là 100 (đối với phiên bản scikit-learn từ 0.22) và 10 (đối với phiên bản \u0026lt; 0.22). criterion: Chỉ ra hàm được sử dụng để quyết định hướng đi tại mỗi node của tree. Tham số này có thể nhận 1 trong 2 giá trị {\u0026ldquo;gini\u0026rdquo;, \u0026ldquo;entropy\u0026rdquo;}. Giá trị mặc định là \u0026ldquo;gini\u0026rdquo;. max_features: Số lượng features được sử dụng tại mỗi node để tìm đường đi tiếp theo. Một số giá trị thường được sử dụng là:  auto/sqrt: max_features = sqrt(n_features). Đây là giá trị mặc định. log2: max_features = log2(n_features). None: max_features = n_features.   max_depth: Độ sâu của mỗi tree. Mặc định, các nodes sẽ được mở rộng tận khi tất cả các leaves chứa ít hơn min_samples_split mẫu (samples). min_sample_split: Số lượng mẫu tối thiểu tại mỗi leaf node để có thể tiếp tục mở rộng tree. Giá trị mặc định là 2. min_samples_leaf: Số lượng mẫu tối thiểu tại mỗi leaf node. Mặc định là 1. max_leaf_nodes: Số lượng leaf node tối đa của mỗi tree. Giá trị mặc định là không có giới hạn số lượng. n_jobs: Số lượng jobs chạy song song. Mặc định là 1. Gán giá trị -1 để sử dụng tất cả các cores của hệ thống. random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi RandomForestClassifier thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh các models với nhau.  2. Boosting techniques\n2.1 AdaBoost\nAdaBoost là thuật toán đơn giản nhất trong họ Boosting, nó cũng thường sử dụng decision tree để làm base model.\nThuật toán thực hiện như sau:\n Bước 1: Ban đầu, tất cả các mẫu dữ liệu được gán cho cùng một giá trị trọng số (weight). Bước 2: Lựa chọn ngẫu nhiên một tập dữ liệu con (tập S) từ tập dữ liệu ban đầu (tập D) và train decition tree model trên tập dữ liệu con này. Bước 3: Sử dụng model đã trained, tiến hành dự đoán trên toàn tập D. Bước 4: Tính toán lỗi (error) bằng cách so sánh giá trị dự đoán và giá trị thực tế. Bước 5: Gán giá trị weight cao hơn cho những mẫu dữ liệu có error cao hơn. Bước 6: Lặp lại bước 2,3,4,5 đến khi error không đổi hoặc số lượng tốí đa của weak learner đạt được.  Code mẫu cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn import tree from sklearn.ensemble import AdaBoostClassifier from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = AdaBoostClassifier(random_state=1) model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Kết quả:\nAccuracy: 72.22% Đối với bài toán regression, thay AdaBoostClassifier bằng AdaBoostRegressor.\nMột vài tham số quan trọng:\n base_estimator: Chỉ ra weak learner là gì. Mặc định sử dụng decition tree. n_estimators: Số lượng của weak learners. Mặc định là 50. learning_rate: Điều chỉnh mức độ đóng góp của mỗi weak learner đến kết quả cuối cùng. random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi AdaBoostClassifier thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh các models với nhau.  2.2 Gradient Boosting (GBM)\nĐể giúp mọi người dễ hình dung, mình sẽ trình bày ý tưởng của GBM thông qua ví dụ sau:\nCho bảng dữ liệu bên dưới:\n   ID Married Gender City Monthly Income Age (target)     1 Y F Hanoi 51.000 35   2 N M HCM 25.000 24   3 Y F Hanoi 70.000 38   4 Y M HCM 53.000 30   5 N M Hanoi 47.000 33    Bài toán đạt ra là cần dự đoán tuổi dựa trên các input features: Tình trạng hôn nhân, giới tính, thành phố sinh sống, thu nhập hàng tháng.\n Bước 1: Train decition tree model thứ nhất trên tập dữ liệu bên trên. Bước 2: Tính toán lỗi dựa theo sai số giữa giá trị thưc tế và giá trị dự đoán.     ID Married Gender City Monthly Income Age (target) Age (prediction 1) Error 1     1 Y F Hanoi 51.000 35 32 3   2 N M HCM 25.000 24 32 -8   3 Y F Hanoi 70.000 38 32 6   4 Y M HCM 53.000 30 32 -2   5 N M Hanoi 47.000 33 32 1     Bước 3: Một decition tree model thứ 2 được tạo, sử dụng cùng input features với model trước đó, nhưng target là Error 1. Bước 4: Giá trị dự đoán của model thứ 2 được cộng với giá trị dự đoán của model thứ nhất.     ID Age (target) Age (prediction 1) Error 1 (new target) Prediction 2 Combine (Pred1+Pred2)     1 35 32 3 3 35   2 24 32 -8 -5 27   3 38 32 6 3 35   4 30 32 -2 -5 27   5 33 32 1 3 35     Bước 5: Giá trị kết hợp bở bước 3 coi như là giá trị dự đoán mới. Ta tính lỗi (Error 2) dựa trên sai số giữa giá trị này và giá trị thực teses.     ID Age (target) Age (prediction 1) Error 1 (new target) Prediction 2 Combine (Pred1+Pred2) Error 2     1 35 32 3 3 35 0   2 24 32 -8 -5 27 -3   3 38 32 6 3 35 3   4 30 32 -2 -5 27 3   5 33 32 1 3 35 -3     Bước 6: Lặp lại bước 2-5 ho đến khi số lượng weak learner đạt được hoặc giá trị lỗi không đổi.  Code ví dụ cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree from sklearn.ensemble import GradientBoostingClassifier from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = GradientBoostingClassifier(learning_rate=0.01,random_state=1) model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Output:\nAccuracy: 78.47% Đối với bài toán regression, thay GradientBoostingClassifier thành GradientBoostingRegressor.\nMột số tham số quan trọng:\n min_sample_split: Số lượng mẫu tối thiểu tại mỗi leaf node để có thể tiếp tục mở rộng tree. Giá trị mặc định là 2. min_samples_leaf: Số lượng mẫu tối thiểu tại mỗi leaf node. Mặc định là 1. max_depth: Độ sâu của mỗi tree. Nên xem xét tham số này khi tuning model. Giá trị mặc định là 3. max_features: Số lượng tối đa features xem xét khi tìm đường mở rộng tree. Những features này được chọn ngẫu nhiên.  2.3 XGBoost\nXGBoost (extreme Gradient Boosting) là phiên bản cải tiến của Gradient Boosting. Ưu điểm vượt trội của nó được chứng minh ở các khía cạnh:\n  Tốc độ xử lý\n XGBoost thực hiện tinh toán song song nên tốc độ xử lý có thể tăng gấp 10 lần so với GBM. Ngoài ra, XGboost còn hỗ trợ tính toán trên Hadoop.    Overfitting\n XGBoost áp dụng cơ chế Regularization nên hạn chế đáng kể hiệ tượng Overfitting (GBM không có regularization).    Sự linh hoạt\n XGboost cho phép người dùng sử dụng hàm tối ưu và chỉ tiêu đánh giá của riêng họ, không hạn chế ở những hàm cung cấp sẵn.    Xử lý missing value\n XGBoost bao gồm cơ chế tự động xử lý missing value bên trong nó. Vì thế, có thể bỏ qua bước này khi chuẩn bị dữ liệu cho XGBoost.    Tự động cắt tỉa\n Tính năng tree pruning hộ trợ việc tự động bỏ qua những leaves, nodes không mang giá trị tích cực trong quá trình mở rộng tree.    Chính vì những ưu điểm đó mà hiệu năng của XGBoost tăng lên đáng kể so với các thuật toán ensemble learning khác. Nó được sử dụng ở hầu hết các cuộc thi trên Kaggle cũng như Hackathons.\nCode ví dụ cho bài toán classification:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree import xgboost as xgb from sklearn.preprocessing import LabelEncoder from sklearn.metrics import accuracy_score #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = xgb.XGBClassifier(random_state=1, eta=0.01) model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Kết quả:\nAccuracy: 82% Đối với vài toán regression, sử dụng XGBRegressor thay vì XGBClassifier.\nMột số tham số quan trọng:\n n_thread: Số lượng cores của hê thống được sử dụng để chạy model. Giá trị mặc định là -1, XGBoost sẽ tự động phát hiện và sử dụng tất cả các cores. eta: Tương tự learning_rate trong GBM. Giá trị mặc định là 0.3. max_depth: Độ sâu tối đa của decision tree. Giá trị mặc định là 6. colsample_bytree: Tương tự max_features của GBM. lambda: L2 regularization. Giá trị mặc định là 1. alpha: L1 regularization. Giá trị mặc định là 0.  2.4 Light GBM\nTại sao chúng ta vẫn cần thuật toán này khi mà ta đã có XGBoost rất mạnh mẽ rồi?\nSự khác nhau nằm ở kích thước của dữ liệu huấn luyện. Light GBM đánh bại tất cả các thuật toán khác khi tập dataset có kích thước cực lớn. Thực tế chứng minh, nó cần ít thời gian đê xử lý hơn trên tập dữ liệu này (Có lẽ vì thế mà có chứ light - ánh sáng). Nguyên nhân sâu xa của sự khác biệt này nằm ở cơ chế làm viêc của Light GBM. Trong khi các thuật toán khác sử dụng cơ chế level-wise thì nó lại sử dụng leaf-wise.\nHình dưới đây minh họa sự khác nhau giữa 2 cơ chế level-wise và leaf-wise:\n   Như chúng ta thấy, leaf-wise chỉ mở rộng tree theo 1 trong 2 hướng so với cả 2 hướng của level-wise, tức là số lượng tính toán của Light GBM chỉ bằng 1/2 so với XGBoost.\nCode ví dụ cho bài toán classifier:\n#importing important packages import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn import tree import lightgbm as lgb from sklearn.preprocessing import LabelEncoder #reading the dataset df = pd.read_csv(\u0026#34;train_ctrUa4K.csv\u0026#34;) # drop nan values df.dropna(inplace=True) # instantiate labelencoder object le = LabelEncoder() # Categorical boolean mask categorical_feature_mask = df.dtypes==object # Get list of categorical column names categorical_cols = df.columns[categorical_feature_mask].tolist() # apply le on categorical feature columns df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) #split dataset into train and test train, test = train_test_split(df, test_size=0.3, random_state=0) x_train = train.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_train = train[\u0026#39;Loan_Status\u0026#39;] x_test = test.drop(\u0026#39;Loan_Status\u0026#39;,axis=1) y_test = test[\u0026#39;Loan_Status\u0026#39;] model = lgb.LGBMClassifier() model.fit(x_train, y_train) accuracy = model.score(x_test, y_test) print(\u0026#34;Accuracy: {:.2f}%\u0026#34;.format(accuracy*100)) Trong trường hợp regression, sử dụng LGBMRegressor thay cho LGBMClassifier.\nMột số tham số quan trọng:\n num_leaves: Số lượng leaves tối đa trên mỗi node. Giá trị mặc định là 31 max_depth: Độ sâu tối đa của mỗi tree. Mặc định là không có giới hạn. learing_rate: learning rate của mỗi tree. Mặc định là 0.1. n_estimators: Số lượng weak learners. Mặc định là 100. n_jobs: Số lượng cores của hê thống được sử dụng để chạy model. Giá trị mặc định là -1, XGBoost sẽ tự động phát hiện và sử dụng tất cả các cores.  2.5 CatBoost\nKhi làm việc với tập dữ liệu mà có số lượng lớn input features kiểu categorical, nếu chúng ta áp dụng one-hot encoding thì số chiều dữ liệu sẽ tăng lên rất nhanh (theo hàm mũ e).\nCatBoost ra đời chính là để gánh vác sứ mệnh giải quyết những bài toán như vậy (CatBoost = Categories + Boosting). Khi làm việc với CatBoost, chúng ta không cần thực hiện one-hot encoding.\nCode ví dụ cho classification:\n# importing required libraries import pandas as pd import numpy as np from catboost import CatBoostClassifier from sklearn.metrics import accuracy_score # read the train and test dataset train_data = pd.read_csv(\u0026#39;train-data.csv\u0026#39;) test_data = pd.read_csv(\u0026#39;test-data.csv\u0026#39;) # Now, we have used a dataset which has more categorical variables # hr-employee attrition data where target variable is Attrition  # seperate the independent and target variable on training data train_x = train_data.drop(columns=[\u0026#39;Attrition\u0026#39;],axis=1) train_y = train_data[\u0026#39;Attrition\u0026#39;] # seperate the independent and target variable on testing data test_x = test_data.drop(columns=[\u0026#39;Attrition\u0026#39;],axis=1) test_y = test_data[\u0026#39;Attrition\u0026#39;] # find out the indices of categorical variables categorical_var = np.where(train_x.dtypes != np.float)[0] model = CatBoostClassifier(iterations=50) # fit the model with the training data model.fit(train_x,train_y,cat_features = categorical_var,plot=False) # predict the target on the train dataset predict_train = model.predict(train_x) # Accuray Score on train dataset accuracy_train = accuracy_score(train_y,predict_train) print(\u0026#39;\\naccuracy_score on train dataset : {:.2f}%\u0026#39;.format(accuracy_train*100)) # predict the target on the test dataset predict_test = model.predict(test_x) # Accuracy Score on test dataset accuracy_test = accuracy_score(test_y,predict_test) print(\u0026#39;\\naccuracy_score on test dataset : {:.2f}%\u0026#39;.format(accuracy_test*100)) Kết quả:\naccuracy_score on train dataset : 91.41% accuracy_score on test dataset : 86.05% Thay CatBoostRegressor cho CatBoostClassifier trong bài toán regression.\nMột số tham số quan trọng:\n loss_function: Định nghĩa loss_function sử dụng để training model. iterations: Số lượng weak learner. learning_rate: Learning rate của mỗi tree. depth: Độ sâu của mỗi tree.  3. Kết luận\nChúng ta đã cùng nhau đi qua 2 phần khá dài để tìm hiểu về Ensemble Learning. Rất nhiều khía cạnh đã được bàn bạc và kèm theo code ví dụ. Hi vọng các bạn đã có cái nhìn rõ hơn về Ensemble Learning. Trong các bài tiếp theo, mình sẽ đi sâu hơn về XGBoost, một thuật toán mạnh mẽ, chiến thắng trong hầu như mọi cuộc thi Kaggle. Hãy đón đọc!\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/05_comprehensive_guide_to_ensemble_model_2/","tags":["Machine Learning","Ensemble Learning","XGBoost"],"title":"XGBoost - Bài 2: Toàn cảnh về Ensemble Learning - Phần 2"},{"categories":["Machine Learning","Ensemble Learning","XGBoost"],"contents":"1. Giới thiệu về Ensemble Learning\nGiả sử chúng ta có một bài toán phân loại sản phẩm sử dụng ML. Team của bạn chia thành 3 nhóm, mỗi nhóm sử dụng một thuật toán khác nhau và đánh giá độ chính xác trên tập validation set:\n Nhóm 1: Sử dụng thuật toán Linear Regression. Nhóm 2: Sử dụng thuật toán k-Nearest Neighbour. Nhóm 3: Sử dụng thuật toán Decision Tree. Độ chính xác của mỗi nhóm lần lượt là 70%, 67% và 76%. Điều này hoàn toàn dễ hiểu bởi vì 3 models làm việc theo những các khác nhau. Ví dụ, Linear Regression cố gắng tìm ra mối quan hệ tuyến tính giữa các điểm dữ liệu, trong khi Decision Tree thì lại dựa vào mỗi quan hệ phi tuyến để liên kết dữ liệu.  Có cách nào kết hợp kết quả cả 3 models để tạo ra kết quả cuối cùng hay không?\n Câu hỏi này là tiền đề cho một phương pháp, một họ các thuật toán hoạt động rất hiệu quả trong các bài toán ML. Đó là Ensemble Learning hay Ensemble Models.\nHình dưới đây thể hiện bức tranh tổng quát về Ensemble Learning.\n 2. Basic Ensemble Techniques\nỞ mức độ cơ bản, có 3 kỹ thuật là:\n Max Voting Averaging Weighted Averaging Mặc dù đơn giản nhưng những kỹ thuật này lại tỏ ra hiệu quả trong một số trường hợp nhất định. Hãy cùng tìm hiểu kỹ hơn về chúng.  2.1 Max Voting\nKỹ thuật này hay được sử dụng cho bài toán phân lớp, ở đó, nhiều models được sử dụng để dự đoán cho mỗi mẫu dữ liệu. Kết quả dự đoán của mỗi model được xem như là một vote. Cái nào có số vote cao nhất thì sẽ là kết quả dự đoán cuối cùng. Nói cách khác, đây là kiểu bầu chọn theo số đông, được áp dụng rất nhiều trong cuộc sống, chính trị, \u0026hellip;\nLấy ví dụ, đợt vừa rồi, công ty của bạn tổ chức khám sức khỏe cho nhân viên tại bệnh viện X. Sau khi khám xong, phòng tổ chức nhân sự (TCNS) lấy ý kiến mọi người về chất lượng khám bệnh để xem năm sau có tiếp tục khám ở bênh viên X đó nữa không. Bảng dưới là ý kiến của 5 người được chọn ngẫu nhiên trong số toàn bộ nhân viên.\n   Người 1 Người 2 Người 3 Người 4 Người 5     Có Không Không Có Có    Có 3 ý kiến muốn tiêp tục khám ở bệnh viện X vào năm sau, và 2 ý kiến muốn đổi bênh viện khác. Căn cứ theo max voting thì phòng TCNS sẽ tiếp tục chọn bệnh viên Xlà nơi khám bệnh cho nhân viên cho năm tiếp theo.\nCode minh họa:\nx_train, y_train, x_test, y_test = get_data() model_1 = DecisionTreeClassifier() model_2 = KNeighborsClassifier() model_3= LogisticRegression() model_1.fit(x_train,y_train) model_2.fit(x_train,y_train) model_3.fit(x_train,y_train) pred_1=model_1.predict(x_test) pred_2=model_2.predict(x_test) pred_3=model_3.predict(x_test) final_pred = np.array([]) for i in range(0,len(x_test)): final_pred = np.append(final_pred, mode([pred_1[i], pred_2[i], pred_3[i]])) Thư viện scikit-learn có module VotingClassifier giúp chúng ta đơn giản hóa việc này:\nfrom sklearn.ensemble import VotingClassifier x_train, y_train, x_test, y_test = get_data() model_1 = LogisticRegression(random_state=1) model_2 = DecisionTreeClassifier(random_state=1) model = VotingClassifier(estimators=[(\u0026#39;lr\u0026#39;, model_1), (\u0026#39;dt\u0026#39;, model_2)], voting=\u0026#39;hard\u0026#39;) model.fit(x_train,y_train) model.score(x_test,y_test) 2.2 Averaging\nTương tự như kỹ thuật Voting, Averaging cũng sử dụng kết quả dự đoán của nhiều models. Tuy nhiên, ở bước quyết định kết quả cuối cùng, giá trị trung bình của tất cả kêt quả của các models được lựa chọn.\n Tiếp tục với ví dụ ở trên, một đề nghị khác của phòng TCNS là yêu cầu nhân viên chấm điểm chất lượng khám bệnh của bênh viện X, theo thang điểm từ 1 đến 5.\nBảng kết quả trả lời của 5 người ngẫu nhiên:\n   Người 1 Người 2 Người 3 Người 4 Người 5     2 4 3 5 4    Điểm đánh giá cuối cùng sẽ là: (2+4+3+5+4)/5 = 3.6\nCode ví dụ:\nx_train, y_train, x_test, y_test = get_data() model_1 = tree.DecisionTreeClassifier() model_2 = KNeighborsClassifier() model_3 = LogisticRegression() model_1.fit(x_train,y_train) model_2.fit(x_train,y_train) model_3.fit(x_train,y_train) pred_1 = model1.predict_proba(x_test) pred_2 = model2.predict_proba(x_test) pred_3 = model3.predict_proba(x_test) final_pred =(pred1+pred2+pred3)/3 2.3 Weighted Average\nĐây là kỹ thuật mở rộng của averaging. Mỗi model được gắn kèm với một trọng số tỷ lệ với mức độ quan trọng của model đó. Kết quả cuối cùng là trung bình có trọng số của tất cả kết quả của các models.\n Vẫn với ví dụ ở mục 2.2, nhưng trong số 5 người được hỏi thì người thứ nhất có vợ là bác sĩ, người thứ 2 có mẹ là y tá, người thứ 3 có người yêu là sinh viên trường y. Vì vậy, ý kiến của 3 người này rõ ràng có giá trị hơn so với 2 người còn lại. Ta đánh trọng số cho mỗi người như bảng dưới (hàng thứ 2 là trọng số, hàng thứ 3 là điểm đánh giá):\n   Người 1 Người 2 Người 3 Người 4 Người 5     1 0.8 0.5 0.3 0.3   2 4 3 5 4    Điểm đánh giá cuối cùng sẽ là: (21 + 40.8 + 30.5 + 50.3 + 4*0.3)/5 = 1.88\nCode minh họa:\nx_train, y_train, x_test, y_test = get_data() model_1 = DecisionTreeClassifier() model_2 = KNeighborsClassifier() model_3 = LogisticRegression() model_1.fit(x_train,y_train) model_2.fit(x_train,y_train) model_3.fit(x_train,y_train) pred_1 = model1.predict_proba(x_test) pred_2 = model2.predict_proba(x_test) pred_3 = model3.predict_proba(x_test) final_pred=(pred_1*0.3 + pred_2*0.3 + pred_3*0.4) 3. Advanced Ensemble techniques\nĐã có basic thì chắc chắn phải có advanced, phải không mọi người. :D\nCó 4 kỹ thuật của Ensemble Learning được xếp vào nhóm advanced:\n Stacking Blending Bagging Boosting  Chúng ta tiếp tục đi qua lần lượt từng kỹ thuật này:\n3.1 Stacking\nHãy xem các bước thực hiện của kỹ thuật này:\n Bước 1: Train model A (base model) theo kiểu cross-validation với k=10. Bước 2: Tiếp tuc train model A trên toàn bộ train set. Bước 3: Sử dụng model A để dự đoán trên test set. Bước 4: Lặp lại bước 1,2,3 cho các base model khác. Bước 5:  Kết quả dự đoán trên train set của các base models được sử dụng như là input features (ensemble train set) để train stacking model. Kết quả dự đoán trên test set của các base models được sử dụng như là test set (ensemble test set) của stacking model.   Bước 6: Train và đánh giá stacking model sử dụng ensemble train set và ensemble test set.  Code minh họa ý tưởng:\n# We first define a function to make predictions on n-folds of train and test dataset. This function returns the predictions for train and test for each model. def Stacking(model,train,y,test,n_fold): folds=StratifiedKFold(n_splits=n_fold,random_state=1) test_pred = np.empty((test.shape[0],1),float) train_pred = np.empty((0,1),float) for train_indices,val_indices in folds.split(train,y.values): x_train,x_val = train.iloc[train_indices],train.iloc[val_indices] y_train,y_val = y.iloc[train_indices],y.iloc[val_indices] model.fit(X=x_train,y=y_train) train_pred = np.append(train_pred,model.predict(x_val)) test_pred = np.append(test_pred,model.predict(test)) return test_pred.reshape(-1,1),train_pred # Now we’ll create two base models – decision tree and knn. model_1 = DecisionTreeClassifier(random_state=1) test_pred_1 ,train_pred_1 = Stacking(model=model_1, n_fold=10, train=x_train, test=x_test, y=y_train) train_pred_1 = pd.DataFrame(train_pred_1) test_pred_1 = pd.DataFrame(test_pred_1) model_2 = KNeighborsClassifier() test_pred_2, train_pred_2 = Stacking(model=model_2, n_fold=10, train=x_train,test=x_test, y=y_train) train_pred_2 = pd.DataFrame(train_pred_2) test_pred_2 = pd.DataFrame(test_pred_2) # Create a final model, logistic regression, on the predictions of the decision tree and knn models. df = pd.concat([train_pred_1, train_pred_2], axis=1) df_test = pd.concat([test_pred_1, test_pred_2], axis=1) model = LogisticRegression(random_state=1) model.fit(df,y_train) model.score(df_test, y_test) Đoạn code trên chỉ minh họa stack model với 2 levels. Decision Tree và kNN là level 0, còn Logistic Regression là level 1. Bạn hoàn toàn có thể thử nghiệm với nhiều levels hơn.\n3.2 Blending\nCác bước thực hiện phương pháp này như sau:\n Buớc 1: Chia dataset thành train set, validation set và test set. Bước 2: Base model được train trên train set. Bước 3: Sử dụng base model để dự đoán trên validation set và test set. Bước 4: Lặp lại bước 2,3 cho các base models khác. Bước 5:  Validation set và các kết quả dự đoán trên validation set của các base models được sử dụng như là input features (ensemble train set) của blending model. Test set và các kết quả dự đoán trên test set của các base models được sử dụng như là test set (ensemble test set) của blending model.   Bước 6: Train và đánh giá blending model sử dụng ensemble train set và ensemble test set.  Code minh họa ý tưởng:\n# build two models, decision tree and knn, on the train set in order to make predictions on the validation set. model_1 = DecisionTreeClassifier() model_1.fit(x_train, y_train) val_pred_1 = model1.predict(x_val) test_pred_1 = model1.predict(x_test) val_pred_1 = pd.DataFrame(val_pred_1) test_pred_1 = pd.DataFrame(test_pred_1) model_2 = KNeighborsClassifier() model_2.fit(x_train, y_train) val_pred_2 = model_2.predict(x_val) test_pred_2 = model2.predict(x_test) val_pred_2 = pd.DataFrame(val_pred_2) test_pred_2 = pd.DataFrame(test_pred_2) # Combining the meta-features and the validation set, a logistic regression model is built to make predictions on the test set. df_val = pd.concat([x_val, val_pred_1,val_pred_2], axis=1) df_test = pd.concat([x_test, test_pred1,test_pred_2], axis=1) model = LogisticRegression() model.fit(df_val, y_val) model.score(df_test, y_test) 3.3 Bagging\nBagging (Bootstrap Aggregating) khác với hai kỹ thuật trên ở chỗ, nó sử dụng chung 1 thuật toán cho tất cả các base models. Tập dataset sẽ được chia thành các phần khác nhau (bags) và mỗi base model sẽ được train trên mỗi bag đó.\nCác bước thực hiện của bagging như sau:\n Bước 1: Chia tập dữ liệu ban đầu thành nhiều phần khác nhau (bags). Bước 2: Tạo các base models (weak learner) và train chúng trên các bags. Các base model được train song song và độc lập với nhau. Bước 3: Kết quả dự đoán cuối cùng được quyết định bằng cách kết hợp kết quả từ các base models.   3.4 Boosting\nNếu như các base models được train độc lập với nhau trong phương pháp bagging, thì ở phương pháp boosting, chúng lại được train một cách tuần tự. Base model sau được train dựa theo kết quả của base model trước đó để cố gắng sửa những lỗi sai tồn tại ở model này.\nCác bước tiến hành như sau:\n Bước 1: Tạo một tập dữ liệu con (tập A) từ tập dữ liệu ban đầu (tập D). Bước 2: Gán cho mỗi điểm dữ liệu trong tập A một trọng số w có giá trị giống nhau. Bước 3: Tạo một base model X và train trên tập A. Bước 4: Sử dụng model X để dự đoán trên toàn bộ tập D. Bước 5: Tính toán sai số dự đoán dựa vào kết quả dự đoán và kết quả thực tế. Bước 6: Gán giá trị w cao hơn cho những điểm dữ liệu bị dự đoán sai. Bước 7: Lặp lại bước 1,2,3,4,5,6 đối với base model mới, Y. Bước 8: Model cuối cùng (boosting model) sẽ là trung bình có trọng số của tất cả các base models.  Mỗi base model được gọi là một weak learner. Chúng sẽ không hoạt động tốt trên toàn bộ tập D, nhưng khi kết hợp nhiều weak learners ta được một strong learner. Strong learner này chắc chắn sẽ hiệu quả trên tập D. Ta nói, các weak learners đã boost performance cho strong learner.\nBagging và Boosting là 2 kỹ thuật quan trọng, hiệu quả. Có một số thuật toán đã được phát triển dựa trên nền tảng của chúng. Đặc biệt là thuật toán XGBoost. Trong bài tiếp theo, chúng ta sẽ đi chi tiết hơn về các thuật toán này.\nMời các bạn đón đọc!\nToàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại github.\nBài viết có tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/04_comprehensive_guide_to_ensemble_model_1/","tags":["Machine Learning","Ensemble Learning","XGBoost"],"title":"XGBoost - Bài 1: Toàn cảnh về Ensemble Learning - Phần 1"},{"categories":["Machine Learning","Ensemble Learning","XGBoost"],"contents":"XGBoost là một thuật toán rất được quan tâm gần đây vì những ưu điểm vượt trội của nó so với các thuật toán khác. Vì vậy, mình quyết định sẽ viết một chuỗi các bài về chủ để này.\nNội dung các bài viết sẽ chủ yếu tập trung vào code thực hành, sẽ có (ít) lý thuyết toán để các bạn đỡ bị đau đầu. :D\nDanh sách các bài viết (sẽ cập nhật dần dần):\n Bài 1: Toàn cảnh về Ensemble models - Phần 1 Bài 2: Toàn cảnh về Ensemble models - Phần 2 Bài 3: Xây dựng model XGBoost đầu tiên Bài 4: Chuẩn bị dữ liệu cho XGBoost model Bài 5: Các phương pháp đánh giá độ chính xác của XGBoost model Bài 6: Trực quan hóa XGBoost model Bài 7: Lưu và sử dụng XGBoost model Bài 8: Lựa chọn Features cho XGBoost model Bài 9: Cấu hình Early_Stopping cho XGBoost model Bài 10: Cấu hình Multi-Threading cho XGBoost model Bài 11: Train XGBoost model trên AWS Bài 12: Tuning số lượng và kích thước của Decision Tree Bài 13: Tuning Learning_Rate và số lượng của Decision Tree Bài 14: Tuning Subsample  ","permalink":"https://tiensu.github.io/blog/03_xgboost-model-serial-introduction/","tags":["Machine Learning","Ensemble Learning","XGBoost"],"title":"XGBoost - Giới thiệu chuỗi bài viết về thuật toán XGBoost"},{"categories":["Machine Learning"],"contents":"Bài viết này nhằm mục đích tổng hợp, tóm tắt lại các thuật toán của Machine Learning, giúp bạn đọc có cái nhìn toàn cảnh và hiểu rõ hơn về Deep Learning.\nCác thuật toán ML, nhìn chung có thể phân loại theo một trong 2 cách:\n Theo cách thức \u0026ldquo;học\u0026rdquo; của thuật toán Theo cách thức làm việc của thuật toán  Cả 2 cách phân loại đều hợp lý, bạn có thể chọn tùy ý. Trong bài bài này, mình sẽ đi sâu hơn theo cách thứ 2. Cũng phải nói thêm rằng, cho dù phân loại theo cách nào thì cũng đều mang tính chất tương đối, vì một thuật toán có thể thuộc nhiều nhóm khác nhau, tùy thuộc vào dữ liệu đưa vào huấn luyện model.\n 1. Phân loại theo cách \u0026ldquo;học\u0026rdquo; (Learning Style)\n1.1 Học có giám sát (Supervised Learning)\nTrong cách học này, dữ liệu đưa vào huấn luyện model, gọi là input data, đi kèm với một nhãn đã biết trước (input data đã được dánh nhãn). Ví dụ như là spam/not-spam, giá cổ phiếu tại 1 thời điểm, \u0026hellip;\nTrong quá trình training, output của model được so sánh với nhãn. Nếu có sự sai khác, model sẽ cố gắng cập nhật các trọng số của nó để giảm sự sai khác đó đến một mức nào đó thỏa mãn yêu cầu bài toán.\nCác vấn đề có thể giải quyết theo cách này: Phân lớn, hồi quy.\nMột số thuật toán thuộc loại này: Logistic Regression, Backpropagation, \u0026hellip;\n 1.2 Học không giám sát (Unsupervised Learning)\nInput data không được đánh nhãn theo cách học này. Model được huấn luyện bằng cách giảm cấu trúc phức tạp của dữ liệu, tìm ra các đặc trưng, các mối liên hệ tương quan trong dữ liệu.\nCác vấn đề có thể giải quyết theo cách này: phân cụm, giảm chiều dữ liệu.\nMột số thuật toán thuộc loại này: Apriori, K-Means, \u0026hellip;\n 1.3 Học bán giám sát (Semi-supervised)\nInput data bao gồm cả 2 loại: đã đánh nhãn và không đánh nhãn.\nModel sẽ sử dụng kết hợp cả 2 cách học giám sát và không giám sát trong quá trình huấn luyện. Dựa vào kết quả dự đoán của model trên dữ liệu chưa đánh nhãn, nhà phát triển sẽ tốn ít công sức hơn trong việc đánh nhãn cho những dữ liệu đó. Độ chính xác của model sẽ được cải thiện dần dần khi có nhiều dữ liệu được đánh nhãn hơn.\nThực tế, tất cả các thuật toán đều có thể thuộc thể loại này vì không phải lúc nào cũng có đầy đủ dữ liệu được đánh nhãn ngay từ đầu.\n 2. Phân loại theo cách làm việc\n2.1 Regression Algorithms\nCác thùât toán được xếp vào nhóm này khi nhãn của dữ liệu là các giá trị liên tục. Ví dụ: nhiệt độ, giá tiền, diện tích, \u0026hellip;\nMột số thuật toán:\n Ordinary Least Squares Regression (OLSR) Linear Regression Logistic Regression Stepwise Regression Multivariate Adaptive Regression Splines (MARS) Locally Estimated Scatterplot Smoothing (LOESS)   2.2 Classification Algorithms\nCác thuật toán thụộc nhóm này khi nhãn của dữ liệu chỉ bao gồm một số lượng hữu hạn các giá trị. Ví dụ: Spam/not-spam, hình dạng (tròn, vuông, tam giác), \u0026hellip;\nMột số thuật toán:\n Linear Classifier Support Vector Machine (SVM) Kernel SVM Sparse Representation-based classification (SRC)   2.3 Instance-based Algorithms\nCác thuật toán thuộc nhóm này không \u0026ldquo;học\u0026rdquo; gì từ dữ liệu. Khi nào cần dự đoán nhãn cho dữ liệu mới, chúng sẽ quét toàn bộ dữ liệu ban đầu và tính toán tương quan với dữ liệu mới để quyết định nhãn.\nMột số thuật toán:\n k-Nearest Neighbor (kNN) Learning Vector Quantization (LVQ) Self-Organizing Map (SOM) Locally Weighted Learning (LWL)   2.4 Regularization Algorithms\nCác thuật toán có thể được mở rộng theo cách \u0026ldquo;trừng phạt\u0026rdquo; model dựa trên độ phức tạp của chúng, làm cho model trở nên đơn giản hơn, kết quả là \u0026ldquo;học\u0026rdquo; tốt hơn.\nMột số thuật toán:\n Ridge Regression Least Absolute Shrinkage and Selection Operator (LASSO) Elastic Net Least-Angle Regression (LARS)   2.5 Decision Tree\nĐây là phương pháp xây dựng model dựa vào trực tiếp giá trị thực tế của input data. Tùy theo các điều kiện cụ thể áp dụng vào input data mà model sẽ đưa ra các quyết định khác nhau. Trong ML, các thuật toán thuộc nhóm này được sử dụng khá phổ biến.\nMột số thuật toán: Classification and Regression Tree (CART)\n Iterative Dichotomiser 3 (ID3) C4.5 and C5.0 (different versions of a powerful approach) Chi-squared Automatic Interaction Detection (CHAID) Decision Stump M5 Conditional Decision Trees   2.6 Bayesian Algorithms\nĐây là họ các thụât toán áp dụng định luật Bayes trong xác suất thống kê.\nMột số thuật toán:\n Naive Bayes Gaussian Naive Bayes Multinomial Naive Bayes Averaged One-Dependence Estimators (AODE) Bayesian Belief Network (BBN) Bayesian Network (BN)   2.7 Clustering Algorithms\nDựa trên số lượng cụm (nhóm, lớp) cho trước, các thuật toán clusering sẽ phân bổ các điểm dữ liệu về từng lớp, dựa trên sự tương quan giữa các điểm dữ liệu đó với nhau.\nMột số thuật toán:\n k-Means k-Medians Expectation Maximisation (EM) Hierarchical Clustering   2.8 Association Rule Learning Algorithms\nCác thuật toán này tập trung vào việc tìm ra các quy tắc kết hợp giữa các điểm dữ liệu để sinh ra dữ liệu mới, hoặc dữ liệu tồn tại trong tập ban đầu.\nMột số thuật toán:\n Apriori algorithm Eclat algorithm   2.9 Artificial Neural Network Algorithms (ANN)\nĐược truyền cảm hứng từ cấu tạo não bộ của các loài động vật, các thuật toán này mô phỏng lại cách làm viêc của các bộ não đó. Chúng được cấu tạo gồm các layers và các nerurons liên kết với nhau.\nMột số thuật toán:\n Perceptron Multilayer Perceptrons (MLP) Back-Propagation Stochastic Gradient Descent Hopfield Network Radial Basis Function Network (RBFN)   2.10 Deep Learning (DL) Algorithms\nCác thuật toán DL là sự nâng cấp, mở rộng của thuật toán ANN. Chúng bao gồm các mạng ANN phức tạp hơn, giải quyết các bài toán với lượng dữ liệu lớn hơn.\nMột số thuật toán:\n Convolutional Neural Network (CNN) Recurrent Neural Networks (RNNs) Long Short-Term Memory Networks (LSTMs) Stacked Auto-Encoders Deep Boltzmann Machine (DBM) Deep Belief Networks (DBN)   Chi tiết hơn về các thuật toán ở nhóm này, mình sẽ đề cập trong bài tiếp theo. Mời các bạn đón đọc.\n2.11 Dimensionality Reduction Algorithms\nĐôi khi dữ liệu quá phức tạp sẽ làm giảm khả năng học của các ML model. Các thuật toán này sẽ giúp giải quyết vấn đề này bằng cách giảm bớt số chiều của dữ liệu (giảm độ phức tạp của dữ liệu).\nMột số thuật toán:\n Principal Component Analysis (PCA) Principal Component Regression (PCR) Partial Least Squares Regression (PLSR) Sammon Mapping Multidimensional Scaling (MDS) Projection Pursuit Linear Discriminant Analysis (LDA) Mixture Discriminant Analysis (MDA) Quadratic Discriminant Analysis (QDA) Flexible Discriminant Analysis (FDA)   2.12 Ensemble Algorithms\nEnsemble là phương pháp sử dụng kết hợp nhiều thuật toán khác nhau để tạo thành một thuật toán mới. Mỗi cách kết hợp khác nhau sẽ cho ra các thuật toán khác nhau. Trong ML, các thuật toán thuộc nhóm này được sử dụng rất phổ biến, đạt hiệu quả rất cao.\nMột số thuật toán:\n Boosting Bootstrapped Aggregation (Bagging) AdaBoost Weighted Average (Blending) Stacked Generalization (Stacking) Gradient Boosting Machines (GBM) Gradient Boosted Regression Trees (GBRT) Random Forest   2.13 Recommendation System Algorithms\nĐúng như tên gọi, đây là các thuật toán giải quyết bài toán khuyến nghị người dùng làm một việc gì đó bằng cách đưa cho họ những cái mà họ có thể quan tâm. Chúng thường được áp dụng trong các trang web thương mại điện tử, các ứng dụng xem phim trực tuyến, \u0026hellip;\nMột số thuật toán:\n Content based Collaborative filtering   2.14 Các thuật toán khác\nCòn rất nhiều thuật toán chưa được liệt kê bên trên, đó là những thuật toán giải quyết các bài toán cụ thể. Có thể kể ra một số như sau:\n Feature selection algorithms Algorithm accuracy evaluation Performance measures Optimization algorithms \u0026hellip;  Vậy là mình đã giới thiệu đến các bạn các thuật toán ML mà các bạn có thể gặp trong quá trình học ML. Hi vọng rằng các bạn đã có cái nhìn tổng quát về chúng, làm tiền đề để đi sâu hơn trong các bài toán ML về sau.\nTrong các bài viết tiếp theo, mình sẽ tổng hợp lại các thuật toán Deep Learning, sau đó sẽ đi chi tiết vào một số thụât toán với các ứng dụng cụ thể. Mời các bạn đón đọc!\n3. Tham khảo\n Machinelearningmastery Simplilearn  ","permalink":"https://tiensu.github.io/blog/02_machine_learning_algorithms_summary/","tags":["Machine Learning"],"title":"Tổng hợp các thuật toán Machine Learning"},{"categories":["Machine Learning","Project Management"],"contents":"Trong bất kỳ dự án nào, đứng ở góc độ của nhà đầu tư và người quản lý, họ đều muốn biết được mốc thời gian dự án có thể được hoàn thành trước khi thực sự bắt đầu dự án. Bởi vì thông tin này giúp cho họ đưa ra quyết định về ngân sách dành cho dự án, khả năng thu hồi vốn và sinh lời (ROI). Cuối cùng là kết luận xem dự án có đáng để đầu tư hay không?\nCác dự án trong lĩnh vực phát triển phần mềm cũng không ngoại lệ. Đối với các dự án phần mềm thông thường, hiện nay, có rất nhiều công cụ, kỹ thuật có thể giúp chúng ta lên kế hoạch, ước lượng khối lượng công việc ban đầu tuơng đối dễ dàng và đầy đủ. Tuy nhiên, các dự án AI sẽ có một vài điểm khác biệt cần lưu ý trong quá trình thực hiện các công đoạn như ước lượng chi phí, lập kế hoạch quản lý dự án. Trong bài viết này, chúng ta sẽ cùng bàn chiêt tiết về các vấn đề đó. Nội dung của mỗi phần được tham khảo từ nhiều nguồn, kết hợp với kinh nghiệm thực tế của bản thân.\n1. Ví dụ về 2 loại dự án phần mềm Loại dự án 1: Xây dựng một website bán hàng hoặc một ứng dụng mobile Ví dụ, dự án phát triên một website bán hàng. Yêu cầu của dự án là thiết kế các form, layout, button, database, và xử lý các hành vi của người dùng khi mua hàng. Những hành vi này thực chất là một chuỗi các bước được thực hiện tuần tự để đạt được một mục tiêu cụ thể nào đó. Sau đó, anh em developers chỉ cần code theo đúng các bước như vậy.\nLoại dự án 2: Phát triển một phần mềm nhận diện các giống mèo khác nhau trong ảnh Để thực hiện dự án này theo cách thông thường, cần phải hiểu rõ đặc tính sinh học, giải phẫu học của loài mèo, trích xuất những thông tin đó từ trong bức ảnh. Đây là một công việc không hề dễ dàng, và trong nhiều trường hợp không thể thực hiện được. Thay vì thế, ta có thể xây dựng một mô hình học máy (Machine Learning model - ML model) và cho nó \u0026ldquo;học\u0026rdquo; những đặc tính khác nhau của từng loài mèo. Từ đó, nó có thể dễ dàng phân loại các loài mèo khác nhau khi đưa cho nó một bức ảnh về mèo.\n2. Sơ lựơc về AI model Về bản chất, AI model là một tập hợp các công thức toán học, cùng với rất nhiều các tham số có thể điều chỉnh được trong suốt quá trình train (gọi là các parameters, phân biệt với hyper-parameters là các tham số không thay đổi thông qua quá trình train). Có thể hình dung một cách đơn giản, train một AI model giống như việc một người giáo viên đưa cho một học sinh bức ảnh và nói đó là con mèo. Người học sinh đóng vai trò là AI mode, sẽ cố gắng học những đặc điểm của bức ảnh để biết đó là con mèo. Còn người giáo viên chính là người train.\nCó rất nhiều thuật toán AI khác nhau, mỗi loại lại sử dụng những phép toán khác nhau, kiểu và số lượng các parameters cũng khác nhau. Điều này dẫn đến các AI model tương ứng cũng cho ra kết quả khác nhau trên cùng 1 tập dữ liệu. Và nhiệm vụ chính của người kỹ sư AI chính là làm sao xây dựng được AI model có độ chính xác cao nhất, thời gian xử lý nhanh nhất đối với 1 tập dữ liệu nhất định. Để xây dựng lên một AI model tốt, cần phải thực hiện rất nhiều xác thực nghiệm, thí nghiệm trên các thuật toán khác nhau, kết hợp với việc điều chỉnh các hyper-parameters hợp lý. Quá trình này gọi là \u0026ldquo;thử-sai\u0026rdquo; (try - error). Điều này nghe thì rất đơn giản nhưng thực tế thì không hề dễ dàng.\nVòng đời của một AI model có thể tóm tắt trong 6 bước:\n  Bussiness Understanding Hiểu yêu cầu bài toán, hiểu vấn đề của khách hàng. Data Understanding \u0026amp; Collection Hiểu dữ liệu, cần dữ liệu như thế nào để train được model tốt. Khách hàng có thể có rất nhiều dữ liệu dạng thô, nhưng họ không biết cách sử dụng, khai thác. Mình phải là người hướng dẫn họ. Data Preparation Dữ liệu thu thập được cần phải được chuẩn hóa trước khi đưa vào train AI model. Modeling Có dữ liệu rồi, lựa chọn thuật toán phù hợp và tiến hành train mô hình. Quá trình này có thể kéo dài vì chúng ta phải \u0026ldquo;thử-sai\u0026rdquo; rất nhiều lần. Evaluation Model sau khi train xong cần được đánh giá dựa theo 1 tiêu chí cụ thể nào đó. Nếu chưa thỏa mãn thì lại quay lại các bước trước đó. Deployment Sau khi model được train thoả mãn yêu cầu, nó sẽ được đem vào triển khai trong sản phẩm thực tế. Vòng đời của một AI model là tuần hoàn khép kín vì nó luôn phải được cập nhật theo dữ liệu mới. Để làm tốt các bước 1,2 thì cần phải trải qua kinh nghiệm thực tế để có cảm quan tốt về vấn đề, đồng thời có thể phải cần hiểu về quy trình nghiệp vụ của từng bài toán. Các bước còn lại, có khá nhiều kỹ thuật, phuơng pháp xử lý. Mình sẽ giới thiệu trong các bài sau.  3. Tại sao dự án AI khó ước lượng? Một người kỹ sư phần mềm với kinh nghiệm lâu năm của mình có thể dễ dàng ước lượng được thời gian cần thiết để có thể hoàn thành một công việc, bởi vì họ đã làm những công việc tương tự như thế rất nhiều lần rồi.\nMột người kỹ sư AI có kinh nghiệm, đã từng xây dựng nhiều AI model, độ chính xác lên đến 90%. Tuy nhiên, nếu được hỏi mất bao nhiêu thời gian để họ có thể phát triển AI model tương tự, đạt được độ chính xác như thế, họ sẽ rất khó để trả lời. Tại sao lại có sự khác biệt đó? Có một câu nói rất nổi tiếng trong giới \u0026ldquo;AI\u0026rdquo; rằng: No free launch. Điều này ngụ ý là không có một công thức, một cách tiếp cận hay một phuơng pháp nào chung cho các vấn đề trong AI. ML model của bạn có thể đạt độ chính xác cao đối với vấn đề A, nhưng nếu bạn sử dụng cách thức train ML model đó để tạo ra ML model cho vấn đề B, không có gì đảm bảo rằng ML mới cũng hoạt động tốt.\nMột vấn đề tối quan trọng nữa, đó là dữ liệu. Có thể nói dữ liệu là vấn đề sống còn của ML model. Thống kê chỉ ra rằng một người kỹ sư AI dành đến 80% thời gian của họ để làm việc với dữ liệu. 20% thời gian còn lại dành cho việc train và triển khai ML model. Nếu bạn chưa có chút hiểu biết gì về dữ liệu bạn cần sử dụng thì mọi ước lượng của bạn có thể coi như là vô nghĩa. \u0026ldquo;Garbage in, garbage out\u0026rdquo; - hãy nhớ điều này.\nNgoài ra, bạn cũng nên biết rằng mỗi lần điều chỉnh, cập nhật ML model đều phải dựa trên kết quả của lần train trước đó. Vì thế mà việc xây dựng một bản kế hoạch chi tiết trong thời gian dài cho 1 dự án AI là điều không thực tế.\n4. Rủi ro của dự án AI/ML Có 3 vấn đề cần lưu ý (risks) trong một dự án AI: 1. Chưa xác định được cách thức xây dựng ML model phù hợp: Chọn thuật toán, chọn tham số, \u0026hellip; 2. Thiếu dữ liệu train ML model 3. Khó lập kế hoạch chi tiết\nVấn đề đầu tiên có thể được giải quyết bằng cách thực hiện điều tra nghiên cứu tính khả thi của ML model trước khi thực sự bắt đầu dự án. Thậm chí, việc này có thể coi là một dự án PoC, làm tiền đề cho dự án thực sự phía sau. Trong giai đoạn PoC, chúng ta sẽ thử nghiệm các thuật toán, các bộ tham số, các cách thức tiếp cận khác nhau để giải quyết một vấn đề nhỏ nhưng tiêu biểu cho dự án lớn. PoC không chỉ giúp chúng ta xác định được các khả năng có thể và không thể của ML model đối với yêu cầu của dự án mà còn chỉ ra được những yêu cầu cụ thể đối với dữ liệu đầu vào. Thông qua PoC, người kỹ sư AI sẽ hiểu rõ hơn về nghiệp vụ, từ đó rút ngắn được danh sách các thuật toán PoC cần thử nghiệm. Và thậm chí là không cần dùng đến ML model cũng có thể giải quyết được yêu cầu của bài toán đặt ra. Nếu mà kết quả của PoC không được như mong đợi, không chọn ra được bất kỳ phuơng pháp nào đáp ứng yêu cầu dự án thì hoặc là cần phải dành thêm thời gian để nghiên cứu hoặc là quyết định dừng dự án lại, tránh lãng phí thời gian và tiền bạc. Sau khi đã giải quyết được vấn đề đầu tiên thì vấn đề thứ 2 đã sáng tỏ hơn. Chúng ta đã biết là cần dữ liệu như thế nào. Việc còn lại chỉ là làm thế nào để có được dữ liệu đó. Thông thường, khi đưa ra bài toán, khách hàng thường đã có dữ liệu (dạng thô) rồi. Người kỹ sư AI phải làm sao sử dụng được dữ liệu đó một cách hiệu quả và hợp lý để tăng được độ chính xác của ML model. Trong trường hợp, khách hàng không có sẵn dữ liệu thì chúng ta phải hướng dẫn họ cách thức thu thập dữ liệu, sao cho đơn giản, nhanh chóng và chính xác dữ liệu được mong đợi. Tuy nhiên, việc làm này có một hạn chế đó là việc thu thập dữ liệu trong một thời gian ngắn thường sẽ khó có thể bao quát hết các trường hợp xảy ra trong thực tế nghiệp vụ của khách hàng (vấn đề bias dữ liệu trong AI). Điều này chắc chắn sẽ làm giảm độ chính xác của ML model khi triển khai trong thực tế. Tất nhiên, theo thời gian, ML model sẽ được cập nhật theo dữ liệu thực tế thì độ chính xác cũng sẽ được tăng lên. Nhưng làm sao để khách hàng hiểu và chấp nhận điều này thì lại là một vấn đề không đơn giản. :D Vấn đề còn lại, giải quyết bằng cách áp dụng nguyên lý Agile-Scrum trong quản trị dự án. Chúng ta lập kế hoạch ngắn hạn cho từng Sprin. Kế hoạch của Sprin tiếp theo sẽ phụ thuộc vào kết quả của Sprin trước đó. Trong quá trình thực hiện dự án, cũng nên liên tục trao đổi, chia sẻ thông tin với khách hàng để điều chỉnh kế hoạch cho phù hợp, đảm bảo dự án đang đi đúng hướng.\n5. Kết luận Vấn đề về chất lượng dữ liệu và việc không có phương pháp chính xác ngay từ đầu yêu cầu phải có giai đoạn \u0026ldquo;research\u0026rdquo; - nghiên cứu tìm hiêu (PoC) trước khi chính thức bắt đầu một dự án AI. Tuy nhiên, những rủi ro này thường không đuợc xem xét trong quá trình ước lựong và lập kế hoạch cho một dự án AI. Cần nhấn mạnh một điều rằng, dự án AI thuộc thể loại \u0026ldquo;experiment-driven\u0026rdquo;, tức là dự án phải trải qua rất nhiều \u0026ldquo;experiments\u0026rdquo; - thực nghiệm, thí nghiệm. Và hành động tiếp theo phụ thuộc vào kết quả của hành động trước đó. Nếu bạn đã từng trải qua những dự án AI, hãy chia sẻ kinh nghiệm của bạn dưới phần bình luận! Rất vui nếu được tiếp thu những ý kiến đóng góp của mọi người!\nBài viết có tham khảo tại đây.\n","permalink":"https://tiensu.github.io/blog/01_ai-project-planing/","tags":["Machine Learning","Project Management"],"title":"Lưu ý khi lập kế hoạch cho một dự án AI"}]