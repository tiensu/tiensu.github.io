<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="format-detection" content="telephone=no" />

    <title>
        Neural Network cơ bản (Phần 1) | ML in Practical
    </title>


    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/manifest.json" />
    <meta name="theme-color" content="#ffffff" />


    <link rel="stylesheet" href="https://unpkg.com/modern-normalize@0.6.0/modern-normalize.css" />





    <link rel="stylesheet" href="https://tiensu.github.io/style.min.388cbd0ce358245ec0dfcee3b8889b3cc50e2bb8a5b2bcd40f8bd092ebefb81a.css" integrity="sha256-OIy9DONYJF7A387juIibPMUOK7ilsrzUD4vQkuvvuBo=" />




    <script type="application/javascript">
        var doNotTrack = false;
        if (!doNotTrack) {
            (function(i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function() {
                    (i[r].q = i[r].q || []).push(arguments)
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                    m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
            ga('create', 'UA-180180568-1', 'auto');
            ga('set', 'anonymizeIp', true);
            ga('send', 'pageview');
        }
    </script>


</head>

<body>
    <header id="header">

        <script type="application/javascript">
            var doNotTrack = false;
            if (!doNotTrack) {
                (function(i, s, o, g, r, a, m) {
                    i['GoogleAnalyticsObject'] = r;
                    i[r] = i[r] || function() {
                        (i[r].q = i[r].q || []).push(arguments)
                    }, i[r].l = 1 * new Date();
                    a = s.createElement(o),
                        m = s.getElementsByTagName(o)[0];
                    a.async = 1;
                    a.src = g;
                    m.parentNode.insertBefore(a, m)
                })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
                ga('create', 'UA-180180568-1', 'auto');
                ga('set', 'anonymizeIp', true);
                ga('send', 'pageview');
            }
        </script>

        <div class="header_container">
            <h1 class="sitetitle">
                <a href="https://tiensu.github.io/" title="ML in Practical">ML in Practical</a>
            </h1>
            <nav class="navbar">
                <ul>
                    <li><a href="https://tiensu.github.io/">Home</a></li>

                    <li>
                        <a href="/about/">

                            <span>About</span>
                        </a>
                    </li>

                    <li>
                        <a href="/tags/">

                            <span>Tags</span>
                        </a>
                    </li>

                    <li>
                        <a href="/archives/">

                            <span>Archives</span>
                        </a>
                    </li>

                    <li class="hide-sm"><a href="https://tiensu.github.io/index.xml" type="application/rss+xml">RSS</a></li>
                </ul>
            </nav>
        </div>
        <script>
            MathJax = {
                tex: {
                    inlineMath: [
                        ['$', '$'],
                        ['\\(', '\\)']
                    ],
                    displayMath: [
                        ['$$', '$$'],
                        ['\\[', '\\]']
                    ],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            };

            window.addEventListener('load', (event) => {
                document.querySelectorAll("mjx-container").forEach(function(x) {
                    x.parentElement.classList += 'has-jax'
                })
            });
        </script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    </header>


    <section id="main">
        <article class="post content">
            <h2 class="title">Neural Network cơ bản (Phần 1)</h2>
            <div class="post_content">
                <p>Trong bài này chúng ta sẽ cùng nhau tìm hiểu lys thuyết cơ bản về mạng thần kinh nhân tạo (<em>Neural Network</em>):</p>

                <ul>
                    <li>Cấu trúc của <code>Neural Network</code>.</li>
                    <li>Thuật toán lan truyền (<em>propagation</em>) và lan truyền ngược (<em>backpropagation</em>).</li>
                </ul>

                <p>Những kiến thức trong bài này sẽ là tiền đề để các bạn tiến xa hơn trong thế giới của Deep Learning.</p>

                <p><strong>1. Neural Network là gì?</strong></p>

                <p>Neural Networks là các khối (<em>blocks</em>) để xây dựng lên các hệ thống Deep Learning. Chúng ta sẽ bắt đầu với việc xem xét ở mức &quot;high-level&quot; của Neural Network, bao gồm cả mối liên hệ của chúng với não bộ của con người.</p>

                <p>Trong thực tế, có rất nhiều những công việc rất khó để có thể thực hiện tự động hóa bởi máy móc nhưng lại rất dễ dàng đối với các loài động vật (bao gồm cả con người). Những công việc đó thường liên quan đến việc nhận diện, phân loại đối
                    tượng.
                </p>

                <p>Ví dụ:</p>

                <ul>
                    <li>Con chó của gia đình bạn có thể phân biệt được người quen (người trong gia đình bạn) và người lạ (không phải trong gia đình bạn)?</li>
                    <li>Một đứa trẻ có thể nhận biết được sự khác nhau giữa xe oto con và xe oto tải.</li>
                </ul>

                <p>Tại sao con chó và đứa trẻ có thể làm được những việc đó?</p>

                <p>Câu trả lời nằm ở cấu tạo bên trong não bộ của chúng. Não bộ của cả 2 đều chứa một mạng thần kinh sinh học kết nối đến hệ thần kinh trung tâm. Mạng này được tạo ra bởi rất nhiều các <code>neurons</code> kết nối với nhau.</p>

                <p>Từ <code>neural</code> là dạng tính từ của <code>neuron</code>, và <code>network</code> ngầm chỉ kiến trúc &quot;graph&quot; của hệ thần kinh. Do vậy, một <code>Artificial Neural Network</code> (<em>ANN</em>) là một hệ thống tính toán,
                    cố gắng mô phỏng (<em>bắt chước</em>) mạng thần kinh sinh học của các loài động vật. ANN là một graph có định hướng, nó bao gồm các nodes và các connections (<em>kết nối giữa các nodes</em>). Mỗi node thực hiện một tính toán đơn giản
                    nào đó, mỗi connection mang một tín hiệu từ node này đến node khác. Những tín hiệu này đi kèm theo một trọng số (<em>weight</em>) chỉ ra mức độ khuyếch đại hoặc giảm bớt cường độ tín hiệu đó. Giá trị weight càng lớn chứng tỏ tín hiệu
                    đi kèm càng quan trọng đối với kết quả đầu ra và ngược lại.</p>

                <p>Hình dưới đây là một ANN đơn giản, bao gồm một lớp input ở đầu, 2 lớp ở giữa (<em>hidden layers</em>) và một lớp output ở cuối. Mỗi connection mang theo một tín hiệu xuyên qua hai hidden layers. Kết quả cuối cùng được tính toán tại lớp
                    output.
                </p>



                <div style="text-align:center">
                    <img src="/simple_ANN.png">
                </div>



                <p><strong>2. Artificial Models</strong></p>

                <p>Hãy xem thử một ANN cơ bản như hình bên dưới:</p>



                <div style="text-align:center">
                    <img src="/basic_ANN.png">
                </div>



                <p>ANN này thực hiện tính tổng có trọng số ($w_i$) của các input vectors($x_i$). Trong thực tế, các input vectors có thể là pixcel của images, hay các rows của một dataset dạng tabular.</p>

                <p>Mỗi $x_i$ kết nối với một neuron thông qua vector trọng số $w_i$.</p>

                <p>Diễn giải bằng công thức toán học thì output của ANN này sẽ là:</p>

                <ul>
                    <li>$y = f(w_1x_1 + w_2x_2 + ... + w_nx_n)$</li>
                    <li>$y = f(\sum_{i=1}^n w_ix_i)$</li>
                    <li>$y = f(net)$. Với net = $\sum_{i=1}^n w_ix_i$</li>
                </ul>

                <p>Nói chung, dù diễn đạt theo cách nào đi nữa thì ý tưởng chung vẫn là áp dụng hàm <code>activate</code> (<em>f</em>) vào tổng có trọng số của các input vectors.</p>

                <p><strong>3. Activation Functions</strong></p>

                <ul>
                    <li><strong><em>a) Step function</em></strong></li>
                </ul>

                <p>Hàm activation đơn giản nhất có lẽ là <code>Step function</code>. Hàm này được sử dụng bởi thuật toán <code>Perceptron</code> (<em>sẽ đề cập ở phần sau</em>).</p>



                <div style="text-align:center">
                    <img src="/step_func.png">
                </div>



                <p>Hàm này luôn nhận giá trị 1 nếu $\sum_{i=1}^n$ $w_i$$x_i$ &gt;= 0 và nhận giá trị 0 trong trường hợp còn lại.</p>



                <div style="text-align:center">
                    <img src="/step.png">
                </div>



                <p>Vấn đề của <code>step function</code> là nó giá trị của nó không có sự khác biệt khi <code>net</code> &gt;=0 hoặc <code>net</code> &lt; 0. Điều này có thể dẫn đến một số vấn đề khi huấn luyện <code>Neural Network</code>.</p>

                <ul>
                    <li><strong><em>b) Sigmoid function</em></strong></li>
                </ul>



                <div style="text-align:center">
                    <img src="/sigmoid.png">
                </div>


                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid(x)&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span></code></pre></div>


                <div style="text-align:center">
                    <img src="/sigmoid_plot.png">
                </div>



                <p>So với <code>step function</code>, <code>sigmoid function</code> có một số ưu điểm sau:</p>

                <ul>
                    <li>Giá trị của nó liên tục và phân biệt nhau tại một nơi.</li>
                    <li>Đồ thị của nó đối xứng qua trục y.</li>
                </ul>

                <p>Tuy nhiên, có 2 nhược điểm lớn nhất của <code>sigmoid function</code> là:</p>

                <ul>
                    <li>Output của nó không tập trung quanh điểm gốc tọa độ.</li>
                    <li>Càng xa gốc tọa độ, giá trị của nó tiệm cận với giá trị <code>bão hòa</code>. Điề u này vô tình triệt tiêu gradient, vì delta của gradient vô cùng nhỏ.</li>
                </ul>

                <p>Đạo hàm của <code>sigmoid function</code> như sau:</p>



                <div style="text-align:center">
                    <img src="\sigmoid_derivative.png">
                </div>


                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;grad of sigmoid&#39;</span><span class="p">,</span>
         <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span></code></pre></div>


                <div style="text-align:center">
                    <img src="\sigmoid_derivative_plot.png">
                </div>



                <ul>
                    <li><strong><em>c) Tanh function</em></strong></li>
                </ul>

                <p>Hàm này giải quyết được nhược điểm thứ nhất của <code>sigmoid function</code>.</p>



                <div style="text-align:center">
                    <img src="\tanh.png">
                </div>


                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;tanh(x)&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span></code></pre></div>


                <div style="text-align:center">
                    <img src="\tanh_plot.png">
                </div>



                <ul>
                    <li><strong><em>d) ReLU (Rectified Linear Unit) funtion</em></strong></li>
                </ul>



                <div style="text-align:center">
                    <img src="\relu.png">
                </div>



                <p>Hàm này nhận giá trị 0 khi inputs &lt; 0, nhưng sẽ tăng tuyến tính khi inputs &gt;= 0.</p>

                <p>Thực tế chứng minh, <code>ReLU function</code> hoạt động tốt hơn hẳn so với các hàm kể trên. Bắt đầu từ năm 2015, nó được sử dụng thường xuyên trong Deep Learning.</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="mf">8.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;relu(x)&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span></code></pre></div>


                <div style="text-align:center">
                    <img src="\relu_plot.png">
                </div>



                <p><code>ReLU function</code> vẫn có nhược điểm. Khi inputs &lt; 0, nó nhận giá trị 0. Như vậy thì không thể tính được gradient tại những điể m đó. Thực tế thì cũng hiếm khi có trường hợp nào mà inputs lại có giá trị &lt; 0. Tuy nhiên, để
                    giải quyết triệt để vấn đề thì lại sinh ra một hàm mới:</p>

                <ul>
                    <li><strong><em>e) Leaky ReLU function</em></strong></li>
                </ul>



                <div style="text-align:center">
                    <img src="\leaky_ReLU.png">
                </div>



                <p>Đây là một biến thể của <code>ReLU funtion</code>, nó nhận một giá trị khác 0 (thường rất nhỏ) khi inputs &lt; 0. Giá trị của $\alpha$ rất nhỏ và được cập nhật trong quá trình huấn luyện <code>Neural Network</code>.</p>



                <div style="text-align:center">
                    <img src="\leaky_relu_plot.png">
                </div>



                <ul>
                    <li><strong><em>f) ELU (Exponential Linear Units) function</em></strong></li>
                </ul>



                <div style="text-align:center">
                    <img src="\elu.png">
                </div>



                <p>Khác với <code>ReLU function</code>, giá trị của $\alpha$ trong <code>ELU function</code> được cố định từ đầu (<em>lúc xây đựng kiến trúc mạng</em>). Giá trị thông thường của $\alpha$ là 1.</p>



                <div style="text-align:center">
                    <img src="\elu_plot.png">
                </div>



                <p><strong>Nên sử dụng <code>activation function</code> nào?</strong></p>

                <p>Việc có nhiều hơn 1 <code>activation function</code> đôi khi làm cho bạn bối rối khi lựa chọn sử dụng cái nào, không sử dụng cái nào?</p>

                <p>Lời khuyên của mình như sau:</p>

                <ul>
                    <li>Bắt đầu với ReLU để đặt được một <code>baseline accuracy</code>. (Hầu hết các public papers đều làm như vậy)</li>
                    <li>Thử chuyển qua sử dụng các biến thể của ReLU: Leaky ReLU, ELU.</li>
                </ul>

                <p>Trong các dự án thực tế thì mình thường làm theo các bước:</p>

                <ul>
                    <li>Sử dụng ReLU</li>
                    <li>Tune các hyper-parameters khác: architecture, learning rate, regularization strength, ... Ghi lại các giá trị <code>accuracy</code>.</li>
                    <li>Một khi đã tương đối thoả mãn về <code>accuracy</code>, chuyển qua ELU. Độ chính xác thường sẽ tăng khoảng 1-5% tùy thuộc dataset.</li>
                </ul>

                <p>Cách này chỉ là kinh nghiệm cá nhân của mình, và không có gì đảm bảo đúng trong mọi trường hợp. Bạn có thể tham khảo hoặc không. Hãy luôn nhớ thử-sai mọi khả năng có thể cho bài toán của bạn.</p>

                <p><strong>4. Feedfoward Network Architecture</strong></p>

                <p>Kiến trúc ANN thì có rất nhiều, nhưng phổ biến nhất là dạng <code>feedfoward network</code>.</p>



                <div style="text-align:center">
                    <img src="\feedfoward_network.png">
                </div>



                <p>Trong kiến trúc này, một <code>connection</code> giữa 2 nodes chỉ được phép đi theo chiều từ layer $i$ tới layer $i+1$ (<em>vì thế mà có tên feedfoward</em>). Không có chiều từ layer $i+1$ đến layer $i$ hoặc bất kỳ chiều nào khác. Khi
                    có thêm chiều từ layer $i+1$ đến layer $i$ (<em>feedback connection</em>) thì ta được kiến trúc RNN (<strong>Recurrent Neural Network</strong>). <code>Feedfoward network</code> được sử dụng chủ yếu trong các bài toán về <code>Computer Vision</code>                    (<em>mạng CNN là một ví dụ điển hình</em>) , trong khi <code>feedback network</code> lại được sử dụng chủ yếu trong các bài toán về <code>Natural Language Processing</code>.</p>

                <p>ANN được sử dụng cho cả 3 dạng bài toán: supervised, unsupervised, and semi-supervised. Một số ví dụ điển hình là classification, regression, clustering, vector quantization, pattern association, ...</p>

                <p>Trong bài tiếp theo, mình sẽ minh họa cách thức cập nhật trong số của mạng ANN thông qua một ví dụ rất dễ hiểu. Mời các bạn đón đọc.</p>

                <p><strong>3. Tham khảo</strong></p>

                <ul>
                    <li><a href="https://www.pyimagesearch.com/">Pyimagesearch</a></li>
                    <li><a href="https://d2l.ai/chapter_multilayer-perceptrons/mlp.html">Dive into Deep Learning</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Activation_function">Wikipedia</a></li>
                </ul>

            </div>
            <div class="info post_meta">
                <time datetime=2020-10-21T00:00:00Z class="date">Wednesday, October 21, 2020</time>

                <ul class="tags">

                    <li> <a href="https://tiensu.github.io/tags/ai">AI</a> </li>

                    <li> <a href="https://tiensu.github.io/tags/deep-learning">Deep Learning</a> </li>

                    <li> <a href="https://tiensu.github.io/tags/neural-network">Neural Network</a> </li>

                </ul>


            </div>
            <div class="clearfix"></div>
        </article>

        <div class="other_posts">

            <a href="https://tiensu.github.io/posts/17_data_scientist_theory_and_real/" class="prev">Nghề Data Scientis - Lý thuyết và thực tế - Sự khác biêt</a>


            <a href="https://tiensu.github.io/posts/19_neural_network_fundamentals_2/" class="next">Neural Network cơ bản (Phần 2)</a>

        </div>
        <aside id="comments">
        </aside>


    </section>

    <a id="back_to_top" title="Go To Top" href="#">
        <span>
    <svg viewBox="0 0 24 24">
      <path fill="none" d="M0 0h24v24H0z"></path>
      <path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path>
    </svg>
  </span>
    </a>

    <footer id="footer">
        <p>
            <span>&copy; 2020 <a href="https://tiensu.github.io/" title="ML in Practical">ML in Practical</a> </span>
            <span>Built with <a rel="nofollow" target="_blank" href="https://gohugo.io">Hugo</a></span>
            <span>Theme by <a rel="nofollow" target="_blank" href="https://github.com/wayjam/hugo-theme-mixedpaper">WayJam</a></span>
        </p>
        <script src="https://tiensu.github.io/js/main.min.8b182175f5874aeed0acc0979345c98d4bde22208ec4f36cc1d6e3102acb4b10.js" integrity="sha256-ixghdfWHSu7QrMCXk0XJjUveIiCOxPNswdbjECrLSxA=" crossorigin="anonymous" async></script>
    </footer>

</body>

</html>