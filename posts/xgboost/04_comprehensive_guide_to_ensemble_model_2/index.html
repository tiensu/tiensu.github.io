<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="format-detection" content="telephone=no" />

    <title>
        XGBoost - Bài 2: Toàn cảnh về Ensemble Learning - Phần 2 | ML in Practical
    </title>


    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/manifest.json" />
    <meta name="theme-color" content="#ffffff" />


    <link rel="stylesheet" href="https://unpkg.com/modern-normalize@0.6.0/modern-normalize.css" />





    <link rel="stylesheet" href="https://tiensu.github.io/style.min.388cbd0ce358245ec0dfcee3b8889b3cc50e2bb8a5b2bcd40f8bd092ebefb81a.css" integrity="sha256-OIy9DONYJF7A387juIibPMUOK7ilsrzUD4vQkuvvuBo=" />




    <script type="application/javascript">
        var doNotTrack = false;
        if (!doNotTrack) {
            (function(i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function() {
                    (i[r].q = i[r].q || []).push(arguments)
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                    m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
            ga('create', 'UA-180180568-1', 'auto');
            ga('set', 'anonymizeIp', true);
            ga('send', 'pageview');
        }
    </script>


</head>

<body>
    <header id="header">

        <script type="application/javascript">
            var doNotTrack = false;
            if (!doNotTrack) {
                (function(i, s, o, g, r, a, m) {
                    i['GoogleAnalyticsObject'] = r;
                    i[r] = i[r] || function() {
                        (i[r].q = i[r].q || []).push(arguments)
                    }, i[r].l = 1 * new Date();
                    a = s.createElement(o),
                        m = s.getElementsByTagName(o)[0];
                    a.async = 1;
                    a.src = g;
                    m.parentNode.insertBefore(a, m)
                })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
                ga('create', 'UA-180180568-1', 'auto');
                ga('set', 'anonymizeIp', true);
                ga('send', 'pageview');
            }
        </script>

        <div class="header_container">
            <h1 class="sitetitle">
                <a href="https://tiensu.github.io/" title="ML in Practical">ML in Practical</a>
            </h1>
            <nav class="navbar">
                <ul>
                    <li><a href="https://tiensu.github.io/">Home</a></li>

                    <li>
                        <a href="/about/">

                            <span>About</span>
                        </a>
                    </li>

                    <li>
                        <a href="/tags/">

                            <span>Tags</span>
                        </a>
                    </li>

                    <li>
                        <a href="/archives/">

                            <span>Archives</span>
                        </a>
                    </li>

                    <li class="hide-sm"><a href="https://tiensu.github.io/index.xml" type="application/rss+xml">RSS</a></li>
                </ul>
            </nav>
        </div>
        <script>
            MathJax = {
                tex: {
                    inlineMath: [
                        ['$', '$'],
                        ['\\(', '\\)']
                    ],
                    displayMath: [
                        ['$$', '$$'],
                        ['\\[', '\\]']
                    ],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            };

            window.addEventListener('load', (event) => {
                document.querySelectorAll("mjx-container").forEach(function(x) {
                    x.parentElement.classList += 'has-jax'
                })
            });
        </script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    </header>


    <section id="main">
        <article class="post content">
            <h2 class="title">XGBoost - Bài 2: Toàn cảnh về Ensemble Learning - Phần 2</h2>
            <div class="post_content">
                <p>Tiếp tục phần 2 của loạt bài tìm hiểu toàn cảnh về <code>Ensemble Learning</code>, trong phần này ta sẽ đi qua một số thuât toán thuộc nhóm <code>Bagging</code> và <code>Boosting</code>.</p>
                <ul>
                    <li>Các thuật toán thuộc nhóm <code>Bagging</code> bao gồm:
                        <ul>
                            <li>Bagging meta-estimator</li>
                            <li>Random forest</li>
                        </ul>
                    </li>
                    <li>Các thuật toán thuộc họ <code>Boosting</code> bao gồm:
                        <ul>
                            <li>AdaBoost</li>
                            <li>Gradient Boosting (GBM)</li>
                            <li>XGBoost (XGBM)</li>
                            <li>Light GBM</li>
                            <li>CatBoost</li>
                        </ul>
                    </li>
                </ul>
                <p>Để minh họa cho các thuật toán kể trên, mình sẽ sử dụng bộ dữ liệu <a href="https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/">Loan Prediction Problem</a>.</p>
                <p><strong>1. Bagging techniques</strong></p>
                <p><em><strong>1.1 Bagging meta-estimator</strong></em></p>
                <p><code>Bagging meta-estimator</code> là thuật toán sử dụng cho cả 2 loại bài toán <code>classification</code> (<em>BaggingClassifier</em>) và <code>regression</code> (<em>BaggingRegressor</em>).</p>
                <p>Các bước thực hiện của thuật toán như sau:</p>
                <ul>
                    <li>Bước 1: Tạo ngẫu nhiên các N <code>bags</code> từ tập <code>train set</code>.</li>
                    <li>Bước 2: Tạo N objects của lớp <code>BaggingClassifier</code> và train trên mỗi bag, độc lập với nhau.</li>
                    <li>Bước 3: Sử dụng các objects đã trained để dự đoán trên tập <code>test set</code>.</li>
                </ul>
                <p>Code cho bài toán <code>classification</code>:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#importing important packages</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="c1">#reading the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;train_ctrUa4K.csv&#34;</span><span class="p">)</span>

<span class="c1"># drop nan values</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># instantiate labelencoder object</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="c1"># Categorical boolean mask</span>
<span class="n">categorical_feature_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="o">==</span><span class="nb">object</span>
<span class="c1"># Get list of categorical column names</span>
<span class="n">categorical_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># apply le on categorical feature columns</span>
<span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">col</span><span class="p">:</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>

<span class="c1">#split dataset into train and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Accuracy: {:.2f}%&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div>
                <p>Kết quả:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Accuracy</span><span class="p">:</span> <span class="mf">77.83</span><span class="o">%</span>
</code></pre></div>
                <p>Đối với bài toán <code>regression</code>, thay <code>BaggingClassifier</code> bằng <code>BaggingRegressor</code>.</p>
                <p>Một số tham số:</p>
                <ul>
                    <li>base_estimator: Định nghĩa thuật toán mà <code>base model</code> sử dụng. Mặc định là <code>decision tree</code>.</li>
                    <li>n_estimators: Định nghĩa số lượng <code>base models</code>. Mặc định là 10.</li>
                    <li>max_samples: Định nghĩa số lượng mẫu data tối đa trong mỗi bag. Mặc định là 1.</li>
                    <li>max_features: Định nghĩa số lượng features tối đa sử dụng trong mỗi bag. Mặc định là 1.</li>
                    <li>n_jobs: Số lượng jobs chạy song song cho cả quá trình train và predict. Mặc định là 1. Nếu giá trị bằng -1 thì số jobs bằng số cores của hệ thống.</li>
                    <li>random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi <code>BaggingClassifier</code> thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh
                        các models với nhau.</li>
                </ul>
                <p><em><strong>1.2 Random Forest</strong></em></p>
                <p>Các thức hoạt động của <code>Random Forest</code> gần giống <code>Bagging meta-estimator</code>, chỉ khác một điều duy nhất là tại mỗi node của <code>tree</code> trong <code>Decision Tree</code>, nó tạo ra một tập ngẫu nhiên các <code>features</code>                    và sử dụng tập này đê chọn hướng đi tiếp theo. Trong khi đó, <code>Bagging meta-estimator</code> sử dụng tất cả <code>features</code> để chọn đường.</p>
                <p>Code ví dụ:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#importing important packages</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1">#reading the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;train_ctrUa4K.csv&#34;</span><span class="p">)</span>

<span class="c1"># drop nan values</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># instantiate labelencoder object</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="c1"># Categorical boolean mask</span>
<span class="n">categorical_feature_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="o">==</span><span class="nb">object</span>
<span class="c1"># Get list of categorical column names</span>
<span class="n">categorical_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># apply le on categorical feature columns</span>
<span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">col</span><span class="p">:</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>

<span class="c1">#split dataset into train and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Accuracy: {:.2f}%&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div>
                <p>Output:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Accuracy</span><span class="p">:</span> <span class="mf">79.86</span><span class="o">%</span>
</code></pre></div>
                <p>Một số tham số:</p>
                <ul>
                    <li>n_estimators: Số lượng <code>decition trees</code> (<em>base models</em>). Mặc định là 100 (đối với phiên bản scikit-learn từ 0.22) và 10 (đối với phiên bản &lt; 0.22).</li>
                    <li>criterion: Chỉ ra hàm được sử dụng để quyết định hướng đi tại mỗi node của tree. Tham số này có thể nhận 1 trong 2 giá trị {&ldquo;gini&rdquo;, &ldquo;entropy&rdquo;}. Giá trị mặc định là &ldquo;gini&rdquo;.</li>
                    <li>max_features: Số lượng <code>features</code> được sử dụng tại mỗi node để tìm đường đi tiếp theo. Một số giá trị thường được sử dụng là:
                        <ul>
                            <li>auto/sqrt: max_features = sqrt(n_features). Đây là giá trị mặc định.</li>
                            <li>log2: max_features = log2(n_features).</li>
                            <li>None: max_features = n_features.</li>
                        </ul>
                    </li>
                    <li>max_depth: Độ sâu của mỗi tree. Mặc định, các nodes sẽ được mở rộng tận khi tất cả các leaves chứa ít hơn <code>min_samples_split</code> mẫu (<em>samples</em>).</li>
                    <li>min_sample_split: Số lượng mẫu tối thiểu tại mỗi <code>leaf node</code> để có thể tiếp tục mở rộng tree. Giá trị mặc định là 2.</li>
                    <li>min_samples_leaf: Số lượng mẫu tối thiểu tại mỗi <code>leaf node</code>. Mặc định là 1.</li>
                    <li>max_leaf_nodes: Số lượng <code>leaf node</code> tối đa của mỗi tree. Giá trị mặc định là không có giới hạn số lượng.</li>
                    <li>n_jobs: Số lượng jobs chạy song song. Mặc định là 1. Gán giá trị -1 để sử dụng tất cả các cores của hệ thống.</li>
                    <li>random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi <code>RandomForestClassifier</code> thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so
                        sánh các models với nhau.</li>
                </ul>
                <p><strong>2. Boosting techniques</strong></p>
                <p><em><strong>2.1 AdaBoost</strong></em></p>
                <p>AdaBoost là thuật toán đơn giản nhất trong họ <code>Boosting</code>, nó cũng thường sử dụng <code>decision tree</code> để làm <code>base model</code>.</p>
                <p>Thuật toán thực hiện như sau:</p>
                <ul>
                    <li>Bước 1: Ban đầu, tất cả các mẫu dữ liệu được gán cho cùng một giá trị trọng số (<em>weight</em>).</li>
                    <li>Bước 2: Lựa chọn ngẫu nhiên một tập dữ liệu con (tập S) từ tập dữ liệu ban đầu (tập D) và train <code>decition tree</code> model trên tập dữ liệu con này.</li>
                    <li>Bước 3: Sử dụng model đã trained, tiến hành dự đoán trên toàn tập D.</li>
                    <li>Bước 4: Tính toán lỗi (error) bằng cách so sánh giá trị dự đoán và giá trị thực tế.</li>
                    <li>Bước 5: Gán giá trị weight cao hơn cho những mẫu dữ liệu có error cao hơn.</li>
                    <li>Bước 6: Lặp lại bước 2,3,4,5 đến khi error không đổi hoặc số lượng tốí đa của <code>weak learner</code> đạt được.</li>
                </ul>
                <p>Code mẫu cho bài toán <code>classification</code>:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#importing important packages</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1">#reading the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;train_ctrUa4K.csv&#34;</span><span class="p">)</span>

<span class="c1"># drop nan values</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># instantiate labelencoder object</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="c1"># Categorical boolean mask</span>
<span class="n">categorical_feature_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="o">==</span><span class="nb">object</span>
<span class="c1"># Get list of categorical column names</span>
<span class="n">categorical_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># apply le on categorical feature columns</span>
<span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">col</span><span class="p">:</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>

<span class="c1">#split dataset into train and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Accuracy: {:.2f}%&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div>
                <p>Kết quả:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Accuracy</span><span class="p">:</span> <span class="mf">72.22</span><span class="o">%</span>
</code></pre></div>
                <p>Đối với bài toán <code>regression</code>, thay <code>AdaBoostClassifier</code> bằng <code>AdaBoostRegressor</code>.</p>
                <p>Một vài tham số quan trọng:</p>
                <ul>
                    <li>base_estimator: Chỉ ra <code>weak learner</code> là gì. Mặc định sử dụng <code>decition tree</code>.</li>
                    <li>n_estimators: Số lượng của <code>weak learners</code>. Mặc định là 50.</li>
                    <li>learning_rate: Điều chỉnh mức độ <code>đóng góp</code> của mỗi <code>weak learner</code> đến kết quả cuối cùng.</li>
                    <li>random_state: Nếu tham số này được gán giá trị giống nhau mỗi lần gọi <code>AdaBoostClassifier</code> thì các dữ tập dữ liệu con sinh ra (một cách ngẫu nhiên) từ tập dữ liệu ban đầu sẽ giống nhau. Tham số này hữu ích khi cần so sánh
                        các models với nhau.</li>
                </ul>
                <p><em><strong>2.2 Gradient Boosting (GBM)</strong></em></p>
                <p>Để giúp mọi người dễ hình dung, mình sẽ trình bày ý tưởng của GBM thông qua ví dụ sau:</p>
                <p>Cho bảng dữ liệu bên dưới:</p>
                <table>
                    <thead>
                        <tr>
                            <th align="center">ID</th>
                            <th align="center">Married</th>
                            <th align="center">Gender</th>
                            <th align="center">City</th>
                            <th align="center">Monthly Income</th>
                            <th align="center">Age (target)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td align="center">1</td>
                            <td align="center">Y</td>
                            <td align="center">F</td>
                            <td align="center">Hanoi</td>
                            <td align="center">51.000</td>
                            <td align="center">35</td>
                        </tr>
                        <tr>
                            <td align="center">2</td>
                            <td align="center">N</td>
                            <td align="center">M</td>
                            <td align="center">HCM</td>
                            <td align="center">25.000</td>
                            <td align="center">24</td>
                        </tr>
                        <tr>
                            <td align="center">3</td>
                            <td align="center">Y</td>
                            <td align="center">F</td>
                            <td align="center">Hanoi</td>
                            <td align="center">70.000</td>
                            <td align="center">38</td>
                        </tr>
                        <tr>
                            <td align="center">4</td>
                            <td align="center">Y</td>
                            <td align="center">M</td>
                            <td align="center">HCM</td>
                            <td align="center">53.000</td>
                            <td align="center">30</td>
                        </tr>
                        <tr>
                            <td align="center">5</td>
                            <td align="center">N</td>
                            <td align="center">M</td>
                            <td align="center">Hanoi</td>
                            <td align="center">47.000</td>
                            <td align="center">33</td>
                        </tr>
                    </tbody>
                </table>
                <p>Bài toán đạt ra là cần dự đoán tuổi dựa trên các <code>input features</code>: Tình trạng hôn nhân, giới tính, thành phố sinh sống, thu nhập hàng tháng.</p>
                <ul>
                    <li>Bước 1: Train <code>decition tree</code> model thứ nhất trên tập dữ liệu bên trên.</li>
                    <li>Bước 2: Tính toán lỗi dựa theo sai số giữa giá trị thưc tế và giá trị dự đoán.</li>
                </ul>
                <table>
                    <thead>
                        <tr>
                            <th align="center">ID</th>
                            <th align="center">Married</th>
                            <th align="center">Gender</th>
                            <th align="center">City</th>
                            <th align="center">Monthly Income</th>
                            <th align="center">Age (target)</th>
                            <th align="center">Age (prediction 1)</th>
                            <th align="center">Error 1</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td align="center">1</td>
                            <td align="center">Y</td>
                            <td align="center">F</td>
                            <td align="center">Hanoi</td>
                            <td align="center">51.000</td>
                            <td align="center">35</td>
                            <td align="center">32</td>
                            <td align="center">3</td>
                        </tr>
                        <tr>
                            <td align="center">2</td>
                            <td align="center">N</td>
                            <td align="center">M</td>
                            <td align="center">HCM</td>
                            <td align="center">25.000</td>
                            <td align="center">24</td>
                            <td align="center">32</td>
                            <td align="center">-8</td>
                        </tr>
                        <tr>
                            <td align="center">3</td>
                            <td align="center">Y</td>
                            <td align="center">F</td>
                            <td align="center">Hanoi</td>
                            <td align="center">70.000</td>
                            <td align="center">38</td>
                            <td align="center">32</td>
                            <td align="center">6</td>
                        </tr>
                        <tr>
                            <td align="center">4</td>
                            <td align="center">Y</td>
                            <td align="center">M</td>
                            <td align="center">HCM</td>
                            <td align="center">53.000</td>
                            <td align="center">30</td>
                            <td align="center">32</td>
                            <td align="center">-2</td>
                        </tr>
                        <tr>
                            <td align="center">5</td>
                            <td align="center">N</td>
                            <td align="center">M</td>
                            <td align="center">Hanoi</td>
                            <td align="center">47.000</td>
                            <td align="center">33</td>
                            <td align="center">32</td>
                            <td align="center">1</td>
                        </tr>
                    </tbody>
                </table>
                <ul>
                    <li>Bước 3: Một <code>decition tree</code> model thứ 2 được tạo, sử dụng cùng <code>input features</code> với model trước đó, nhưng <code>target</code> là <code>Error 1</code>.</li>
                    <li>Bước 4: Giá trị dự đoán của model thứ 2 được cộng với giá trị dự đoán của model thứ nhất.</li>
                </ul>
                <table>
                    <thead>
                        <tr>
                            <th align="center">ID</th>
                            <th align="center">Age (target)</th>
                            <th align="center">Age (prediction 1)</th>
                            <th align="center">Error 1 (new target)</th>
                            <th align="center">Prediction 2</th>
                            <th align="center">Combine (Pred1+Pred2)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td align="center">1</td>
                            <td align="center">35</td>
                            <td align="center">32</td>
                            <td align="center">3</td>
                            <td align="center">3</td>
                            <td align="center">35</td>
                        </tr>
                        <tr>
                            <td align="center">2</td>
                            <td align="center">24</td>
                            <td align="center">32</td>
                            <td align="center">-8</td>
                            <td align="center">-5</td>
                            <td align="center">27</td>
                        </tr>
                        <tr>
                            <td align="center">3</td>
                            <td align="center">38</td>
                            <td align="center">32</td>
                            <td align="center">6</td>
                            <td align="center">3</td>
                            <td align="center">35</td>
                        </tr>
                        <tr>
                            <td align="center">4</td>
                            <td align="center">30</td>
                            <td align="center">32</td>
                            <td align="center">-2</td>
                            <td align="center">-5</td>
                            <td align="center">27</td>
                        </tr>
                        <tr>
                            <td align="center">5</td>
                            <td align="center">33</td>
                            <td align="center">32</td>
                            <td align="center">1</td>
                            <td align="center">3</td>
                            <td align="center">35</td>
                        </tr>
                    </tbody>
                </table>
                <ul>
                    <li>Bước 5: Giá trị kết hợp bở bước 3 coi như là giá trị dự đoán mới. Ta tính lỗi (Error 2) dựa trên sai số giữa giá trị này và giá trị thực teses.</li>
                </ul>
                <table>
                    <thead>
                        <tr>
                            <th align="center">ID</th>
                            <th align="center">Age (target)</th>
                            <th align="center">Age (prediction 1)</th>
                            <th align="center">Error 1 (new target)</th>
                            <th align="center">Prediction 2</th>
                            <th align="center">Combine (Pred1+Pred2)</th>
                            <th align="center">Error 2</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td align="center">1</td>
                            <td align="center">35</td>
                            <td align="center">32</td>
                            <td align="center">3</td>
                            <td align="center">3</td>
                            <td align="center">35</td>
                            <td align="center">0</td>
                        </tr>
                        <tr>
                            <td align="center">2</td>
                            <td align="center">24</td>
                            <td align="center">32</td>
                            <td align="center">-8</td>
                            <td align="center">-5</td>
                            <td align="center">27</td>
                            <td align="center">-3</td>
                        </tr>
                        <tr>
                            <td align="center">3</td>
                            <td align="center">38</td>
                            <td align="center">32</td>
                            <td align="center">6</td>
                            <td align="center">3</td>
                            <td align="center">35</td>
                            <td align="center">3</td>
                        </tr>
                        <tr>
                            <td align="center">4</td>
                            <td align="center">30</td>
                            <td align="center">32</td>
                            <td align="center">-2</td>
                            <td align="center">-5</td>
                            <td align="center">27</td>
                            <td align="center">3</td>
                        </tr>
                        <tr>
                            <td align="center">5</td>
                            <td align="center">33</td>
                            <td align="center">32</td>
                            <td align="center">1</td>
                            <td align="center">3</td>
                            <td align="center">35</td>
                            <td align="center">-3</td>
                        </tr>
                    </tbody>
                </table>
                <ul>
                    <li>Bước 6: Lặp lại bước 2-5 ho đến khi số lượng <code>weak learner</code> đạt được hoặc giá trị lỗi không đổi.</li>
                </ul>
                <p>Code ví dụ cho bài toán <code>classification</code>:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#importing important packages</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1">#reading the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;train_ctrUa4K.csv&#34;</span><span class="p">)</span>

<span class="c1"># drop nan values</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># instantiate labelencoder object</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="c1"># Categorical boolean mask</span>
<span class="n">categorical_feature_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="o">==</span><span class="nb">object</span>
<span class="c1"># Get list of categorical column names</span>
<span class="n">categorical_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># apply le on categorical feature columns</span>
<span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">col</span><span class="p">:</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>

<span class="c1">#split dataset into train and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Accuracy: {:.2f}%&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div>
                <p>Output:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Accuracy</span><span class="p">:</span> <span class="mf">78.47</span><span class="o">%</span>
</code></pre></div>
                <p>Đối với bài toán <code>regression</code>, thay <code>GradientBoostingClassifier</code> thành <code>GradientBoostingRegressor</code>.</p>
                <p>Một số tham số quan trọng:</p>
                <ul>
                    <li>min_sample_split: Số lượng mẫu tối thiểu tại mỗi <code>leaf node</code> để có thể tiếp tục mở rộng tree. Giá trị mặc định là 2.</li>
                    <li>min_samples_leaf: Số lượng mẫu tối thiểu tại mỗi <code>leaf node</code>. Mặc định là 1.</li>
                    <li>max_depth: Độ sâu của mỗi tree. Nên xem xét tham số này khi tuning model. Giá trị mặc định là 3.</li>
                    <li>max_features: Số lượng tối đa <code>features</code> xem xét khi tìm đường mở rộng tree. Những features này được chọn ngẫu nhiên.</li>
                </ul>
                <p><em><strong>2.3 XGBoost</strong></em></p>
                <p>XGBoost (<em>extreme Gradient Boosting</em>) là phiên bản cải tiến của <code>Gradient Boosting</code>. Ưu điểm vượt trội của nó được chứng minh ở các khía cạnh:</p>
                <ul>
                    <li>
                        <p>Tốc độ xử lý</p>
                        <ul>
                            <li>XGBoost thực hiện tinh toán song song nên tốc độ xử lý có thể tăng gấp 10 lần so với GBM. Ngoài ra, XGboost còn hỗ trợ tính toán trên Hadoop.</li>
                        </ul>
                    </li>
                    <li>
                        <p>Overfitting</p>
                        <ul>
                            <li>XGBoost áp dụng cơ chế <code>Regularization</code> nên hạn chế đáng kể hiệ tượng Overfitting (GBM không có regularization).</li>
                        </ul>
                    </li>
                    <li>
                        <p>Sự linh hoạt</p>
                        <ul>
                            <li>XGboost cho phép người dùng sử dụng hàm tối ưu và chỉ tiêu đánh giá của riêng họ, không hạn chế ở những hàm cung cấp sẵn.</li>
                        </ul>
                    </li>
                    <li>
                        <p>Xử lý <code>missing value</code></p>
                        <ul>
                            <li>XGBoost bao gồm cơ chế tự động xử lý <code>missing value</code> bên trong nó. Vì thế, có thể bỏ qua bước này khi chuẩn bị dữ liệu cho XGBoost.</li>
                        </ul>
                    </li>
                    <li>
                        <p>Tự động cắt tỉa</p>
                        <ul>
                            <li>Tính năng <code>tree pruning</code> hộ trợ việc tự động <code>bỏ qua</code> những leaves, nodes không mang giá trị tích cực trong quá trình mở rộng tree.</li>
                        </ul>
                    </li>
                </ul>
                <p>Chính vì những ưu điểm đó mà hiệu năng của XGBoost tăng lên đáng kể so với các thuật toán <code>Ensemble Learning</code> khác. Nó được sử dụng ở hầu hết các cuộc thi trên Kaggle cũng như Hackathons.</p>
                <p>Code ví dụ cho bài toán <code>classification</code>:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#importing important packages</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="kn">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1">#reading the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;train_ctrUa4K.csv&#34;</span><span class="p">)</span>

<span class="c1"># drop nan values</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># instantiate labelencoder object</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="c1"># Categorical boolean mask</span>
<span class="n">categorical_feature_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="o">==</span><span class="nb">object</span>
<span class="c1"># Get list of categorical column names</span>
<span class="n">categorical_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># apply le on categorical feature columns</span>
<span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">col</span><span class="p">:</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>

<span class="c1">#split dataset into train and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Accuracy: {:.2f}%&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div>
                <p>Kết quả:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Accuracy</span><span class="p">:</span> <span class="mi">82</span><span class="o">%</span>
</code></pre></div>
                <p>Đối với vài toán <code>regression</code>, sử dụng <code>XGBRegressor</code> thay vì <code>XGBClassifier</code>.</p>
                <p>Một số tham số quan trọng:</p>
                <ul>
                    <li>n_thread: Số lượng cores của hê thống được sử dụng để chạy model. Giá trị mặc định là -1, XGBoost sẽ tự động phát hiện và sử dụng tất cả các cores.</li>
                    <li>eta: Tương tự <code>learning_rate</code> trong GBM. Giá trị mặc định là 0.3.</li>
                    <li>max_depth: Độ sâu tối đa của <code>decision tree</code>. Giá trị mặc định là 6.</li>
                    <li>colsample_bytree: Tương tự <code>max_features</code> của GBM.</li>
                    <li>lambda: L2 regularization. Giá trị mặc định là 1.</li>
                    <li>alpha: L1 regularization. Giá trị mặc định là 0.</li>
                </ul>
                <p><em><strong>2.4 Light GBM</strong></em></p>
                <p>Tại sao chúng ta vẫn cần thuật toán này khi mà ta đã có XGBoost rất mạnh mẽ rồi?</p>
                <p>Sự khác nhau nằm ở kích thước của dữ liệu huấn luyện. <code>Light GBM</code> đánh bại tất cả các thuật toán khác khi tập dataset có kích thước cực lớn. Thực tế chứng minh, nó cần ít thời gian đê xử lý hơn trên tập dữ liệu này (<em>Có lẽ vì thế mà có chứ <code>light - ánh sáng</code></em>).
                    Nguyên nhân sâu xa của sự khác biệt này nằm ở cơ chế làm viêc của <code>Light GBM</code>. Trong khi các thuật toán khác sử dụng cơ chế <code>level-wise</code> thì nó lại sử dụng <code>leaf-wise</code>.</p>
                <p>Hình dưới đây minh họa sự khác nhau giữa 2 cơ chế <code>level-wise</code> và <code>leaf-wise</code>:</p>
                <p>

                    <div style="text-align:center">
                        <img src="/level-wise.webp">
                    </div>




                    <div style="text-align:center">
                        <img src="/leaf-wise.webp">
                    </div>

                </p>
                <p>Như chúng ta thấy, <code>leaf-wise</code> chỉ mở rộng tree theo 1 trong 2 hướng so với cả 2 hướng của <code>level-wise</code>, tức là số lượng tính toán của <code>Light GBM</code> chỉ bằng 1/2 so với XGBoost.</p>
                <p>Code ví dụ cho bài toán <code>classifier</code>:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#importing important packages</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">import</span> <span class="nn">lightgbm</span> <span class="kn">as</span> <span class="nn">lgb</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="c1">#reading the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;train_ctrUa4K.csv&#34;</span><span class="p">)</span>

<span class="c1"># drop nan values</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># instantiate labelencoder object</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="c1"># Categorical boolean mask</span>
<span class="n">categorical_feature_mask</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dtypes</span><span class="o">==</span><span class="nb">object</span>
<span class="c1"># Get list of categorical column names</span>
<span class="n">categorical_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># apply le on categorical feature columns</span>
<span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">col</span><span class="p">:</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>

<span class="c1">#split dataset into train and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;Loan_Status&#39;</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Accuracy: {:.2f}%&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div>
                <p>Trong trường hợp <code>regression</code>, sử dụng <code>LGBMRegressor</code> thay cho <code>LGBMClassifier</code>.</p>
                <p>Một số tham số quan trọng:</p>
                <ul>
                    <li>num_leaves: Số lượng leaves tối đa trên mỗi node. Giá trị mặc định là 31</li>
                    <li>max_depth: Độ sâu tối đa của mỗi tree. Mặc định là không có giới hạn.</li>
                    <li>learing_rate: <code>learning rate</code> của mỗi tree. Mặc định là 0.1.</li>
                    <li>n_estimators: Số lượng <code>weak learners</code>. Mặc định là 100.</li>
                    <li>n_jobs: Số lượng cores của hê thống được sử dụng để chạy model. Giá trị mặc định là -1, XGBoost sẽ tự động phát hiện và sử dụng tất cả các cores.</li>
                </ul>
                <p><em><strong>2.5 CatBoost</strong></em></p>
                <p>Khi làm việc với tập dữ liệu mà có số lượng lớn <code>input features</code> kiểu <code>categorical</code>, nếu chúng ta áp dụng <code>one-hot encoding</code> thì số chiều dữ liệu sẽ tăng lên rất nhanh (theo hàm mũ <code>e</code>).</p>
                <p><code>CatBoost</code> ra đời chính là để gánh vác sứ mệnh giải quyết những bài toán như vậy (<code>CatBoost</code> = <code>Categories</code> + <code>Boosting</code>). Khi làm việc với CatBoost, chúng ta không cần thực hiện <code>one-hot encoding</code>.</p>
                <p>Code ví dụ cho <code>classification</code>:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># importing required libraries</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># read the train and test dataset</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;train-data.csv&#39;</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test-data.csv&#39;</span><span class="p">)</span>

<span class="c1"># Now, we have used a dataset which has more categorical variables</span>
<span class="c1"># hr-employee attrition data where target variable is Attrition </span>

<span class="c1"># seperate the independent and target variable on training data</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Attrition&#39;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;Attrition&#39;</span><span class="p">]</span>

<span class="c1"># seperate the independent and target variable on testing data</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Attrition&#39;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="s1">&#39;Attrition&#39;</span><span class="p">]</span>

<span class="c1"># find out the indices of categorical variables</span>
<span class="n">categorical_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span><span class="n">iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># fit the model with the training data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span><span class="n">train_y</span><span class="p">,</span><span class="n">cat_features</span> <span class="o">=</span> <span class="n">categorical_var</span><span class="p">,</span><span class="n">plot</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># predict the target on the train dataset</span>
<span class="n">predict_train</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>

<span class="c1"># Accuray Score on train dataset</span>
<span class="n">accuracy_train</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">train_y</span><span class="p">,</span><span class="n">predict_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">accuracy_score on train dataset : {:.2f}%&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_train</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>

<span class="c1"># predict the target on the test dataset</span>
<span class="n">predict_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span>

<span class="c1"># Accuracy Score on test dataset</span>
<span class="n">accuracy_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span><span class="n">predict_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">accuracy_score on test dataset : {:.2f}%&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_test</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div>
                <p>Kết quả:</p>
                <div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">accuracy_score</span> <span class="n">on</span> <span class="n">train</span> <span class="n">dataset</span> <span class="p">:</span> <span class="mf">91.41</span><span class="o">%</span>
<span class="n">accuracy_score</span> <span class="n">on</span> <span class="n">test</span> <span class="n">dataset</span> <span class="p">:</span> <span class="mf">86.05</span><span class="o">%</span>
</code></pre></div>
                <p>Thay <code>CatBoostRegressor</code> cho <code>CatBoostClassifier</code> trong bài toán <code>regression</code>.</p>
                <p>Một số tham số quan trọng:</p>
                <ul>
                    <li>loss_function: Định nghĩa <code>loss_function</code> sử dụng để training model.</li>
                    <li>iterations: Số lượng <code>weak learner</code>.</li>
                    <li>learning_rate: Learning rate của mỗi tree.</li>
                    <li>depth: Độ sâu của mỗi tree.</li>
                </ul>
                <p><strong>3. Kết luận</strong></p>
                <p>Chúng ta đã cùng nhau đi qua 2 phần khá dài để tìm hiểu về <code>Ensemble Learning</code>. Rất nhiều khía cạnh đã được bàn bạc và kèm theo code ví dụ. Hi vọng các bạn đã có cái nhìn rõ hơn về <code>Ensemble Learning</code>. Trong các bài
                    tiếp theo, mình sẽ đi sâu hơn về XGBoost, một thuật toán mạnh mẽ, chiến thắng trong hầu như mọi cuộc thi Kaggle. Hãy đón đọc!</p>
                <p><em>Toàn bộ source code của bài này các bạn có thể tham khảo trên github cá nhân của mình tại <a href="https://github.com/tiensu/xgboost-algorithm/tree/master/ensemble_learning">github.</a></em></p>
                <p>Bài viết có tham khảo tại <a href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models">đây</a>.</p>

            </div>
            <div class="info post_meta">
                <time datetime=2020-08-25T00:00:00Z class="date">Tuesday, August 25, 2020</time>

                <ul class="tags">

                    <li> <a href="https://tiensu.github.io/tags/ai">AI</a> </li>

                    <li> <a href="https://tiensu.github.io/tags/ml">ML</a> </li>

                    <li> <a href="https://tiensu.github.io/tags/ensemble-learning">Ensemble Learning</a> </li>

                    <li> <a href="https://tiensu.github.io/tags/xgboost">XGBoost</a> </li>

                </ul>


            </div>
            <div class="clearfix"></div>
        </article>

        <div class="other_posts">

            <a href="https://tiensu.github.io/posts/xgboost/03_comprehensive_guide_to_ensemble_model_1/" class="prev">XGBoost - Bài 1: Toàn cảnh về Ensemble Learning - Phần 1</a>


            <a href="https://tiensu.github.io/posts/xgboost/05_build-xgboost-model/" class="next">XGBoost - Bài 3: Xây dựng XGBoost model</a>

        </div>
        <aside id="comments">
        </aside>


    </section>

    <a id="back_to_top" title="Go To Top" href="#">
        <span>
    <svg viewBox="0 0 24 24">
      <path fill="none" d="M0 0h24v24H0z"></path>
      <path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path>
    </svg>
  </span>
    </a>

    <footer id="footer">
        <p>
            <span>&copy; 2020 <a href="https://tiensu.github.io/" title="ML in Practical">ML in Practical</a> </span>
            <span>Built with <a rel="nofollow" target="_blank" href="https://gohugo.io">Hugo</a></span>
            <span>Theme by <a rel="nofollow" target="_blank" href="https://github.com/wayjam/hugo-theme-mixedpaper">WayJam</a></span>
        </p>
        <script src="https://tiensu.github.io/js/main.min.8b182175f5874aeed0acc0979345c98d4bde22208ec4f36cc1d6e3102acb4b10.js" integrity="sha256-ixghdfWHSu7QrMCXk0XJjUveIiCOxPNswdbjECrLSxA=" crossorigin="anonymous" async></script>
    </footer>

</body>

</html>