<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="format-detection" content="telephone=no" />

    <title>
        Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 2 | ML in Practical
    </title>


    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/manifest.json" />
    <meta name="theme-color" content="#ffffff" />


    <link rel="stylesheet" href="https://unpkg.com/modern-normalize@0.6.0/modern-normalize.css" />





    <link rel="stylesheet" href="https://tiensu.github.io/style.min.388cbd0ce358245ec0dfcee3b8889b3cc50e2bb8a5b2bcd40f8bd092ebefb81a.css" integrity="sha256-OIy9DONYJF7A387juIibPMUOK7ilsrzUD4vQkuvvuBo=" />




    <script type="application/javascript">
        var doNotTrack = false;
        if (!doNotTrack) {
            (function(i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function() {
                    (i[r].q = i[r].q || []).push(arguments)
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                    m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m)
            })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
            ga('create', 'UA-180180568-1', 'auto');
            ga('set', 'anonymizeIp', true);
            ga('send', 'pageview');
        }
    </script>


</head>

<body>
    <header id="header">

        <script type="application/javascript">
            var doNotTrack = false;
            if (!doNotTrack) {
                (function(i, s, o, g, r, a, m) {
                    i['GoogleAnalyticsObject'] = r;
                    i[r] = i[r] || function() {
                        (i[r].q = i[r].q || []).push(arguments)
                    }, i[r].l = 1 * new Date();
                    a = s.createElement(o),
                        m = s.getElementsByTagName(o)[0];
                    a.async = 1;
                    a.src = g;
                    m.parentNode.insertBefore(a, m)
                })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
                ga('create', 'UA-180180568-1', 'auto');
                ga('set', 'anonymizeIp', true);
                ga('send', 'pageview');
            }
        </script>

        <div class="header_container">
            <h1 class="sitetitle">
                <a href="https://tiensu.github.io/" title="ML in Practical">ML in Practical</a>
            </h1>
            <nav class="navbar">
                <ul>
                    <li><a href="https://tiensu.github.io/">Home</a></li>

                    <li>
                        <a href="/about/">

                            <span>About</span>
                        </a>
                    </li>

                    <li>
                        <a href="/tags/">

                            <span>Tags</span>
                        </a>
                    </li>

                    <li>
                        <a href="/archives/">

                            <span>Archives</span>
                        </a>
                    </li>

                    <li class="hide-sm"><a href="https://tiensu.github.io/index.xml" type="application/rss+xml">RSS</a></li>
                </ul>
            </nav>
        </div>
        <script>
            MathJax = {
                tex: {
                    inlineMath: [
                        ['$', '$'],
                        ['\\(', '\\)']
                    ],
                    displayMath: [
                        ['$$', '$$'],
                        ['\\[', '\\]']
                    ],
                    processEscapes: true,
                    processEnvironments: true
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                }
            };

            window.addEventListener('load', (event) => {
                document.querySelectorAll("mjx-container").forEach(function(x) {
                    x.parentElement.classList += 'has-jax'
                })
            });
        </script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    </header>


    <section id="main">
        <article class="post content">
            <h2 class="title">Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 2</h2>
            <div class="post_content">
                <p>Vì sử dụng trực tiếp <code>raw pixel</code> của image nên so với CNN, FCN (<em>Fully Connected Network</em>) có 2 nhược điểm kích thước của image tăng lên:</p>
                <ul>
                    <li>Hiệu năng giảm mạnh.</li>
                    <li>Kích thước của mạng tăng nhanh. Kết quả thực nghiệm cho thấy, khi áp dụng <code>Fully Connected Network</code> vào bộ dataset CIFAR-10, độ chính xác chỉ đạt được 52%.</li>
                </ul>
                <p>CNN, theo một cách khác, sắp xếp các layers theo dạng <em>3D volume</em> với 3 chiều: Width, Height, Depth. Các neurons trong mỗi layer chỉ kết nối tới 1 <em>small region</em> của layer trước đó - gọi là <em><strong>local connectivity</strong></em>.
                    Điều này giúp giảm bớt rất nhiều kích thước của mạng.</p>
                <p><strong>2. Các loại layers</strong></p>
                <p>Có khá nhiều các dạng layers khác nhau để xây dựng nên CNNs. Các loại dưới đây được sử dụng phổ biến:</p>
                <ul>
                    <li>Convolutional (CONV)</li>
                    <li>Activation (ACT, RELU, SOFTMAX)</li>
                    <li>Pooling (POOL)</li>
                    <li>Fully-connected (FC)</li>
                    <li>Batch normalization (BN)</li>
                    <li>Dropout (DO)</li>
                </ul>
                <p>Sắp xếp các dạng layers trên liên tiếp nhau theo thứ tự nào đó sẽ cho ta một CNN.</p>
                <p>Ví dụ: <strong>INPUT =&gt; CONV =&gt; RELU =&gt; FC =&gt; SOFTMAX</strong></p>
                <p>Ở đây, ta định nghĩa một CNN đơn giản, nhận input, áp dụng &ldquo;convolution layer&rdquo;, sau đó là 1 &ldquo;activation layer&rdquo; (RELU), tiếp theo là 1 &ldquo;fully-connected layer&rdquo;. Cuối cùng là một &ldquo;activation layer&rdquo;
                    nữa (SOFTMAX) để đạt được xác suất của output theo các nhãn cần phân loại.</p>
                <p>Trong số các lớp trên, chỉ có CONV và FC là chứa các tham số được cập nhật trong quá trình training. POOL có tác dụng thay đổi kích thước không gian của image khi nó di chuyển qua các lớp của CNN.</p>
                <p>CONV, POOL, RELU và FC là 4 layers quan trọng nhất, gần như không thể thiếu khi xây dựng CNNs.</p>
                <p><em><strong>2.1 CONV</strong></em></p>
                <p>CONV chứa một tập K <code>learnable filters</code> (<em>ví dụ: Kernel</em>), mỗi <code>filter</code> có kích thước <code>width</code> x <code>height</code>. Mặc định,<code>width</code> luôn luôn bằng <code>height</code>, trừ khi có lý
                    do đặc biệt. Hai giá trị này có thường nhỏ (1, 3, 5, 7) nhưng K thì có thể rất lớn (4, 8, 16, 32, 64, 128, &hellip;). K cũng được gọi là độ sâu (<em>depth</em>) của CONV layer.</p>
                <p>Cùng xem xét <code>forward-pass</code> của một CNN. CONV có K filters, áp dụng vào một input volumn có kích thước WxH. Tưởng tượng rẵng, mỗi filters sẽ trượt ngang qua toàn bộ input volumn, tính toán <code>convolution</code>, sau đó lưu
                    kết quả ra một mảng 2 chiều, gọi là <code>activation map</code>. Xem hình bên dưới:</p>


                <div style="text-align:center">
                    <img src="/conv.png">
                </div>


                <p>Sau khi áp dùng K filters lên input volumn, chúng ta thu được K, <code>2-dimensional activation maps</code>. Xếp chồng (<em>Stack</em>) những <code>activation maps</code> này theo chiều sâu sẽ thu được kết qủa cuối cùng (<em>output volumn</em>).</p>


                <div style="text-align:center">
                    <img src="/conv_2.png">
                </div>


                <p>Xét về kích thước của <code>output volumn</code>, có 3 tham số tác động. Chúng là <strong>depth</strong>, <strong>stride</strong> và <strong>zero-padding</strong> size.</p>
                <ul>
                    <li><strong>Depth</strong></li>
                </ul>
                <p>Như đã nói ở trên, <strong>depth</strong> chính là số lượng filters trong CONV layer, có giá trị là K.</p>
                <ul>
                    <li><strong>Stride</strong></li>
                </ul>
                <p><strong>Stride</strong> là kích thước của step khi các filters trượt qua input volumn. Giá trị của <strong>stride</strong> thường là 1 hoặc 2 (<em>S=1 hoặc S=2</em>), tương ứng với step là 1 hoặc 2 pixel. S nhỏ sẽ sinh ra ouput volumn
                    lớn, và có nhiều vùng được bị trùng lặp trong quá trình trượt và tính covolution của các filters. Đối với S lớn, kết quả sẽ ngược lại.</p>
                <p>Xem ví dụ sau:</p>
                <p>Trong hình bên dưới, ma trận bên trái là input volumn, ma trận bên phải là filter.</p>


                <div style="text-align:center">
                    <img src="/stride.png">
                </div>


                <p>Sử dụng S = 1 và S = 2 cho convolution, thu được kết quả tương ứng bên trái, phải trong hình sau:</p>


                <div style="text-align:center">
                    <img src="/stride_2.png">
                </div>


                <p>Cùng với <strong>pooling</strong> (<em>xem phần bên dưới</em>), <strong>stride</strong> có thể được sử dụng để giảm kích thước của input volumn.</p>
                <ul>
                    <li><strong>Zero-padding</strong></li>
                </ul>
                <p>Sử dụng <strong>stride</strong> làm giảm kích thước của input volumn. Vậy nếu muốn giữ nguyên kích thước của input volumn thì sao? <strong>Zero-padding</strong> chính là câu trả lời.</p>
                <p><strong>Zero-padding</strong> tức là gắn thêm (<em>pad</em>) viền (<em>border</em>) cho input volumn. Các giá trị được gắn thêm đều là 0, vì thế mà có tên <strong>zero-padding</strong>.</p>
                <p>Xem ví dụ sau:</p>


                <div style="text-align:center">
                    <img src="/padding.png">
                </div>


                <p>Bên trái là 3x3 ouput khi áp dụng 3x3 convolution tới 5x5 input.</p>
                <p>Bên phải là khi áp dụng <strong>zero-padding</strong> vào input, thu được input mới có kích thước 7x7. Giá trị của <strong>zero-padding</strong> trong trường hợp này là P = 1.</p>
                <p>Bên dưới là 5x5 ouput khi áp dụng 3x3 convolution tới 7x7 input. Ta thấy kích thước 5x5 của input ban đầu được duy trì trong output.</p>
                <p>Nếu không sử dụng <strong>zero-padding</strong>, kích thước của input volumn sẽ giảm rất nhanh, do đó không thể xây dựng CNN nhiều layers.</p>
                <p>Công thức tính kích thước của ouput volumn như sau:
                    <strong>O = ((W - F + 2P)/S) + 1</strong></p>
                <p><em>Trong đó:</em></p>
                <ul>
                    <li><em>0: kích thước của output volumn.</em></li>
                    <li><em>W: kích thước input volumn</em></li>
                    <li><em>F: kích thước của filter</em></li>
                    <li><em>P: kích thước zero-padding</em></li>
                    <li><em>S: kích thước stride</em></li>
                </ul>
                <p>Nếu O không phải là số nguyên, cần thay đổi lai giá trị của S.</p>
                <p>Thử áp dụng công thức này vào ví dụ bên trên:</p>
                <p><strong>O = ((5 - 3 + 2*1)/1) = 5</strong></p>
                <p><em><strong>2.2 Activation Layers</strong></em></p>
                <p><code>Activation layer</code> thường được áp dụng sau mỗi CONV layer trong một mạng CNN. Các <code>activation layer</code> hay dùng là các hàm phi tuyến, giống như: ReLU, ELU, Leaky ReLU. Trong các <code>public paper</code>, ReLU được
                    sử dụng rất phổ biến, gần như là mặc định. Khi viết ACT, ta ngầm hiểu đó là ReLU.</p>
                <p>Trên thực tế, <code>activation layer</code> không được coi là một <code>layer</code> theo đúng nghĩa. Bởi vì nó không có parameters nào được học trong quá trình huấn luyện mô hình. Trong một số diagram kiến trúc của mạng, nó có thể không
                    xuất hiện và được ngầm hiểu rằng nó nằm ngay sau CONV layer.</p>
                <p>Ví dụ với kiến trúc sau:</p>
                <p><strong>INPUT =&gt; CONV =&gt; RELU =&gt; FC</strong></p>
                <p>Có thể được viết gọn thành:</p>
                <p><strong>INPUT =&gt; CONV =&gt; FC</strong></p>
                <p>Một <code>activation layer</code> nhận input volumn có kích thước là $W_{input}$x$H_{input}$x$D_{input}$, áp dụng <code>activation function</code> theo kiểu <code>element-wise</code> nên kích thước của output volumn đúng bằng kích thước
                    của input volumn: $W_{input}$=$W_{output}$, $H_{input}$=$H_{output}$, $D_{input}$=$D_{output}$.</p>
                <p><em><strong>2.3 Pooling Layers</strong></em></p>
                <p>Có 2 phương pháp để giảm kích thước của input volumn: CONV layer (với <em>stride &gt; 1</em>) và POOL layer.</p>
                <p>Thông thường, POOL layer được đặt ngay sau ACT layer và trước CONV layer. Trong trường hợp không có ACT layer thì nó nằm giữa 2 CONV layer.</p>
                <p>Ví dụ kiến trúc mạng sau:</p>
                <p><strong>INPUT =&gt; CONV =&gt; RELU =&gt; POOL =&gt; CONV =&gt; POOL =&gt; FC</strong></p>
                <p>Chức năng đầu tiên của POOL layer là giảm kích thước không gian của input volumn một các từ từ. Điều này dẫn đến việc giảm số lượng tham số và sự phức tạp tính toán của mạng. Vì thế mà nó là một trong những phương pháp hiệu của để tránh
                    hiện tượng Overfitting cho mạng DL.</p>
                <p>POOL layer hoạt động độc lập trên mỗi <code>depth slice</code> của input volumn, sử dụng hàm <strong>max</strong> hoặc <strong>average</strong>. Hai cái tên có lẽ cũng đã nói lên cách thức hoạt động của chúng. <code>Max pooling</code>                    chỉ giữ lại giá trị lớn nhất trong phạm vi kích thước của nó còn <code>average pooling</code> thì lấy giá trị trung bình của các giá trị trong phạm vi kích thước của POOL layer. Về vị trí trong kiến trúc, trong khi <code>max pooling</code>                    thường được đặt ở giữa của kiến trúc mạng DL để giảm kích thước, còn <code>average pooling</code> lại hay được đặt ở các layers cuối (<em>hoặc gần cuối</em>) (<em>VD: GoogLeNet, SqueezeNet, ResNet</em>) để thay thế cho các FC layers,
                    giúp giảm độ phức tạp cả model.</p>
                <p>Kích thước của POOL layer hay được sử dụng là 2x2. Mặc dù vậy, với input volumn có kích thước &gt; 200x200, ta có thể sử dụng kích thước 3x3 của POOL ở những layer đầu.</p>
                <p>Bước nhảy (<em>stride, S</em>) của mỗi POOL layer thường là 1 hoặc 2. Hình bên dưới minh họa kết quả hoạt dộng của <code>max pooling</code> với S =1,2, tương ứng.</p>


                <div style="text-align:center">
                    <img src="/pool.png">
                </div>


                <p>Công thức tính kích thước của output volumn sau khi qua POOL layer như sau:</p>
                <ul>
                    <li>$W_{output}$ = (($W_{input}$ - $F$)/$S$) + 1</li>
                    <li>$H_{output}$ = (($H_{input}$ - $F$)/$S$) + 1</li>
                    <li>$D_{output}$ = $D_{input}$</li>
                </ul>
                <p>Trong đó:</p>
                <ul>
                    <li>$W_{input}$x$H_{input}$x$D_{input}$: kích thước của input volumn.</li>
                    <li>$W_{output}$x$H_{output}$x$D_{output}$: kích thước của output volumn.</li>
                    <li>F: kích thước của POOL layer (<em>cũng gọi là <code>pool size</code></em>).</li>
                    <li>S: stride</li>
                </ul>
                <p>Trong các bài toán thực tế, có 3 dạng <code>max pooling</code> thường hay được sử dụng.</p>
                <ul>
                    <li><strong>Dạng 1</strong>: (F = 3, S = 2), gọi là <code>overlapping pooling</code>, thường áp dụng đối với các images/input volumn có kích thước lớn (&gt; 200x200 pixels)</li>
                    <li><strong>Dạng 2</strong>: (F = 2, S = 2), gọi là <code>non-overlapping pooling</code>, thường được áp dụng với các images/input volumn có kích thước trung bình (&gt; 64x64 pixels và &lt; 200x200 pixels)</li>
                    <li><strong>Dạng 3</strong>: (F = 2, S = 1), gọi là <code>small pooling</code>, áp dụng với các images/input volumn nhỏ (&lt; 64x64 pixels)</li>
                </ul>
                <p><strong>Đến đây, có thể các bạn sẽ thắc mắc, khi nào thì dùng CONV layer, khi nào thì dùng POOL layer để giảm kích thước của input volumn?</strong></p>
                <p><em>Springenberg et al</em>, trong paper <a href="https://arxiv.org/abs/1412.6806">Striving for Simplicity: The All Convolutional Net</a> xuất bản năm 2014 của họ đã đề xuất loại bỏ hoàn toàn POOL layer, chỉ sử dụng CONV layer (với S&gt;1).
                    Họ đã chứng minh tính hiệu của cách tiếp cận này trên một số tập dữ liệu, bao gồm cả CIFAR-10 (<em>small images, low number of class</em>) và ImageNet (<em>large input images, 1000 classes</em>). Xu hướng này cũng xuất hiện trong kiến
                    trúc của mạng <a href="http://arxiv.org/abs/1512.03385">Resnet</a> năm 2015 và đang dần dần trở nên phổ biến hơn. Có lẽ trong tương lai không xa, chúng ta sẽ không sử dụng POOL layer (cụ thể là <code>max pooling</code>) trong phần
                    giữa các kiến trúc mạng DL hiện đại nữa mà chỉ sử dụng <code>average pooling</code> tại các layer cuối để thay thế cho FC layer vốn cồng kềnh và phức tạp. Tuy nhiên, trước mắt thì <code>max pooling</code> vẫn chưa thể biến mất hoàn
                    toàn được, nên chúng ta vẫn cần phải học, hiểu và áp dụng chúng trong việc xây dựng kiến trúc mạng DL của riêng mình, cũng như đọc hiểu các kiến trúc mạng DL kinh điển khác.</p>
                <p><em><strong>2.4 Full-Connected (FC) Layers</strong></em></p>
                <p>FC Layer chính là <code>Feedforward Neural Network</code> mà chúng ta đã tìm hiểu trong bài <a href="">&hellip;</a>. Nó luôn được đặt ở cuối trong các kiến trúc mạng DL.</p>
                <p>Ví dụ kiến trúc mạng sau,</p>
                <p><strong>INPUT =&gt; (CONV =&gt; RELU =&gt; POOL)x2 =&gt; FC =&gt; FC =&gt; SOFTMAX.</strong></p>
                <p>ta đã sử dụng 2 FC layers ở gần cuối mạng, theo sau là ACT layer (<em>SOFTMAX</em>) để phân loại (<em>tính toán xác suất của mỗi classes</em>).</p>
                <p><em><strong>2.5 Batch Normalization (BN)</strong></em></p>
                <p>Được giới thiệu lần đầu vào năm 2015 trong paper <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> của Ioffe và Szegedy, BN đã nhanh chóng trở nên
                    phổ biến trong các kiến trúc mạng DL. Đúng như tên của nó, BN layer có tác dụng normalize input volumn trước khi đi vào layer tiếp theo.</p>
                <p>Giả sử, ta có input tại thời đểm i (<em>mini-batch i</em>) là $x_i$, sau khi đi qua BN layer, ta thu được giá trị $\widehat{x_i}$ theo công thức:</p>


                <div style="text-align:center; font-size:25px">
                    $\widehat{x_i} = \frac{x_i - \mu_ \beta}{\sqrt{\sigma^2_ \beta + \varepsilon}}$
                </div>


                <ul>
                    <li>Trong đó:
                        <ul>
                            <li>


                                <div style="font-size:25px">
                                    $\mu_ \beta = \frac{1}{M}\sum_{i=1}^m x_i$
                                </div>


                            </li>
                            <li>


                                <div style="font-size:25px">
                                    $\sigma^2_ = \frac{1}{M}\sum_{i=1}^m (x_i - \mu_\beta)^2$
                                </div>


                            </li>
                        </ul>
                    </li>
                </ul>
                <p>Giá trị của $\varepsilon$ được chọn là giá trị dương đủ nhỏ (<em>VD: 1e-7</em>) để tránh việc chia cho 0. Sau khi áp dụng BN, input volumn sẽ có trung bình (<em>mean</em>) xấp xỉ 0 và độ lệch chuẩn (<em>variance</em>) xấp xỉ 1 (<em>còn gọi là <strong>zero-centered</strong></em>).</p>
                <p>Khi sử dụng model để test, ta thay thế $\mu_\beta$ và $\sigma_\beta$ bằng giá trị trung bình của chúng trong suốt quá trình training. Điều này đảm bảo cho ta có thể pass input volumn xuyên qua mạng DL mà không bị biased bởi giá trị $\mu_\beta$
                    và $\sigma_\beta$ tại thời điểm cuối cùng ($x_m$).</p>
                <p>BN layer tỏ ra hiệu quả cao trong việc làm cho quá trình training một NN <em>ổn định</em> hơn, giảm số lượng epochs cần thiêt để train model, và quan trọng nhất là hạn chế tình trạng Overfitting. Khi sử dụng BN, việc tuning các tham số
                    khác của model cũng trở nên đơn giản hơn bởi vì BN đã thu hẹp đáng kế phạm vi giá trị của các weights trong mạng.</p>
                <p>Hạn chế lớn nhất của BN có lẽ là nó làm tăng thời gian training của bạn do phải tính toán normalization và statistic tại mỗi nơi mà nó xuất hiện trong kiến trúc mạng. Thường là tăng gấp 2 đến 3 so với không sử dụng BN.</p>
                <p>Tuy nhiên, có lẽ hạn chế trên không đáng kể so với những ưu điểm mà BN mang lại. Vì vậy, lời khuyên ở đây là nên sử dụng BN thường xuyên trong bài toán của bạn.</p>
                <p>Cuối cùng là về vị trí đạt BN layer trong kiến trúc DL. Mặc dù trong paper gốc của tác giả BN layer được đặt trước ACT layer, nhưng điều này lại không hợp lý vê mặt thống kê. Bởi vì output của BN là <em>zero-centered</em>, khi đi qua ACT
                    layer (ReLU), phần giá trị âm sẽ bị triệt tiêu. Điều này vô tình làm mất đi bản chất của BN. Thực nghiệm rất nhiều cũng đã chỉ ra rằng, đặt BN layer ở sau ACT layer cho kết quả tốt hơn (<em>higher accuracy và lower loss</em>) trong
                    hầu hết mọi trường hợp. Vì thế, mặc định, hãy đặt BN layer sau ACT layer, trừ khi bạn có lý do đặc biệt nào khác.</p>
                <p>Ví dụ về việc đặt BN layer trong kiến trúc DL:</p>
                <p><strong>INPUT =&gt; CONV =&gt; RELU =&gt; BN &hellip;</strong></p>
                <p><em><strong>2.6 Dropout (DO) layer</strong></em></p>
                <p>DO thực chất là một dạng của <em>regularization</em>, mục đích là để hạn chế hiện tượng Overfitting. Tại mỗi <em>mini-batch</em> trong quá trình train, DO layer sẽ ngẫu nhiên ngắt kết nối các inputs giữa 2 layer liên tiếp, với xác suất
                    <em>p</em>.</p>
                <p>Ví dụ về DO với <em>p</em> = 5 giữa 2 FC layers như hình bên dưới:</p>


                <div style="text-align:center">
                    <img src="/dropout.png">
                </div>


                <p>DO chỉ hoạt động theo 1 chiều forwarding, chiều ngược lại (<em>backwarding</em>), các <code>dropped connections</code> sẽ được kết nối lại để tính toán.</p>
                <p>DO giúp giảm Overfitting theo cách như trên bởi vì khi đó, vai trò của các nodes trong mạng sẽ được phân phối đều hơn, không có nodes nào chịu trách nhiệm chính, nhiều hơn các nodes khác. Điều này sẽ giúp model <code>generalize</code>                    tốt hơn.</p>
                <p>Về vị trí trong kiến trúc mạng DL, DO layer thường được set với <em>p</em> = 0.5 và đặt xen kẽ 2 FC layers ở cuối.</p>
                <p>Ví dụ:</p>
                <p><strong>INPUT =&gt; CONV =&gt; RELU =&gt; POOL =&gt; FC =&gt; DO =&gt; FC =&gt; DO =&gt; SOFTMAX</strong></p>
                <p>Bài thứ 2 về CNN xin được kết thúc tại đây. Trong bài tiếp theo (cũng là bài cuối cùng về CNN), mình sẽ chia sẻ một số patterns và một số rules trong việc xây dựng kiến trúc CNN. Mời các bạn đón đọc!</p>
                <p><strong>Tham khảo</strong></p>
                <ul>
                    <li><a href="https://www.pyimagesearch.com/">Pyimagesearch</a></li>
                    <li><a href="https://d2l.ai/chapter_convolutional-neural-networks/index.html">Dive into Deep Learning</a></li>
                    <li><a href="https://cs231n.github.io/convolutional-networks/">CS231</a></li>
                </ul>

            </div>
            <div class="info post_meta">
                <time datetime=2020-10-30T00:00:00Z class="date">Friday, October 30, 2020</time>

                <ul class="tags">

                    <li> <a href="https://tiensu.github.io/tags/ai">AI</a> </li>

                    <li> <a href="https://tiensu.github.io/tags/deep-learning">Deep Learning</a> </li>

                    <li> <a href="https://tiensu.github.io/tags/neural-network">Neural Network</a> </li>

                    <li> <a href="https://tiensu.github.io/tags/cnn">CNN</a> </li>

                </ul>


            </div>
            <div class="clearfix"></div>
        </article>

        <div class="other_posts">

            <a href="https://tiensu.github.io/posts/20_convolutional_neural_network_1/" class="prev">Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 1</a>


            <a href="https://tiensu.github.io/posts/22_convolutional_neural_network_3/" class="next">Mạng thần kinh tích chập (Convolutional Neural Network (CNN) - Phần 3</a>

        </div>
        <aside id="comments">
        </aside>


    </section>

    <a id="back_to_top" title="Go To Top" href="#">
        <span>
    <svg viewBox="0 0 24 24">
      <path fill="none" d="M0 0h24v24H0z"></path>
      <path d="M12 2L4.5 20.29l.71.71L12 18l6.79 3 .71-.71z"></path>
    </svg>
  </span>
    </a>

    <footer id="footer">
        <p>
            <span>&copy; 2020 <a href="https://tiensu.github.io/" title="ML in Practical">ML in Practical</a> </span>
            <span>Built with <a rel="nofollow" target="_blank" href="https://gohugo.io">Hugo</a></span>
            <span>Theme by <a rel="nofollow" target="_blank" href="https://github.com/wayjam/hugo-theme-mixedpaper">WayJam</a></span>
        </p>
        <script src="https://tiensu.github.io/js/main.min.8b182175f5874aeed0acc0979345c98d4bde22208ec4f36cc1d6e3102acb4b10.js" integrity="sha256-ixghdfWHSu7QrMCXk0XJjUveIiCOxPNswdbjECrLSxA=" crossorigin="anonymous" async></script>
    </footer>

</body>

</html>